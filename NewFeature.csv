projectID,key,creationDate,resolutionDate,updateDate,dueDate,resolution,type,priority,fixVersions,versions,timeSpent,aggregatedTimeSpent,timeEstimate,timeOriginalEstimate,aggregateTimeEstimate,progressPercent,componentName,componentDescription,description,summary,watchCount,votes,labels,creatorName,creatorActive,assignee,reporter
commons-exec,EXEC-107,2018-07-04T12:09:47.000+0000,,2018-07-04T12:09:47.000+0000,,,New Feature,Major,,['1.3'],,,,,,,,,"Hi,

I would like to start an independent process (either another Java VM or something else) using Commons Exec.

This means that the child process is not terminated when the invoking Java process ends.

Currently Commons Exec does not support this.

I do not need to handle output, it can just be sent to a file.

Currently I invoke a script file containing ""nohup ... > ... 2>&1 &"" on Linux, and cmd.exe with a batch file on Windows. However, the latter method does not always seem to work.

I would love Commons Exec to allow starting independent processes without any further effort on the caller's part.

Many greetings,
Stefan Reich / BotCompany.de",Start independent process,1,,,stefanreich,True,,stefanreich
commons-exec,EXEC-88,2014-09-21T20:16:01.000+0000,,2014-09-21T20:16:01.000+0000,,,New Feature,Major,,,,,,,,,,,"Hello Commons Exec-ians,

see https://github.com/vorburger/MariaDB4j/blob/10747327a1822a719ff35aaba6b4a4a42103424c/src/main/java/ch/vorburger/exec/ManagedProcess.java

would my ManagedProcess helper (and a few related util classes from ch.vorburger.exec), which extends commons-exec with a higher-level abstraction over your  for controlling async ""daemon tools"" be of possibly interest as a contribution to commons-exec?

I wrote it for and am using it in MariaDB4j https://github.com/vorburger/MariaDB4j.","ManagedProcess, higher-level abstraction for controlling async ""daemon tools"" (contribution from MariaDB4j)",1,,,vorburger,True,,vorburger
commons-exec,EXEC-84,2014-02-21T09:38:39.000+0000,2014-02-21T16:31:46.000+0000,2014-02-22T21:58:34.000+0000,,Implemented,New Feature,Major,['1.2'],,,,,,,,,,"In my java application i have to execute various CommandLine applications. Now i want to send these applications as Actors to a remote machine. The most natural would be to have the CommandLine object of apache exec in the Actor message. However, the CommandLine does not implement the Serializable interface. Is there any problem tagging the CommandLine object as Serializable?",CommandLine implementing Serializeable,2,,,richtesn,True,,richtesn
commons-exec,EXEC-70,2012-11-04T05:01:39.000+0000,,2014-01-10T10:39:50.000+0000,,,New Feature,Minor,,,,,,,,,,,"Direct calls to Thread constructors are usually a bad idea. It's better to allow callers to configure a ThreadFactory, and to invoke that to create them for us. This would allow callers to adjust thread priorities, names, daemon/user thread levels, thread groups and UncaughtExceptionHandlers.",Delegate thread creation to java.util.concurrent.ThreadFactory,3,,,richarda,True,,richarda
accumulo,ACCUMULO-4852,2018-07-29T18:46:54.000+0000,2018-09-13T02:37:12.000+0000,2018-09-13T02:37:12.000+0000,,Won't Fix,New Feature,Major,,"['1.9.0', '2.0.0']",,,3600,3600,3600,,"['build', 'maven-plugin']","['tarball, packaging, and the Maven build', 'accumulo-maven-plugin issues']","OWASP DC makes an outbound REST call to MITRE Common Vulnerabilities & Exposures (CVE) to perform a lookup for each dependant .jar to list any/all known vulnerabilities for each jar.  This step is needed because a manual MITRE CVE lookup/check on the main component does not include checking for vulnerabilities in dependant libraries.

OWASP Dependency check : https://www.owasp.org/index.php/OWASP_Dependency_Check has plug-ins for most Java build/make types (ant, maven, ivy, gradle).  

Also, please add the appropriate command to the nightly build to generate a report of all known vulnerabilities in any/all third party libraries/dependencies that get pulled in. example : mvn -Powasp -Dtest=false -DfailIfNoTests=false clean aggregate

Generating this report nightly/weekly will help inform the project's development team if any dependant libraries have a reported known vulneraility.  Project teams that keep up with removing vulnerabilities on a weekly basis will help protect businesses that rely on these open source componets.",Please add OWASP Dependency Check to the build (pom.xml),2,,"['build', 'easyfix', 'security']",ABakerIII,True,ctubbsii,ABakerIII
accumulo,ACCUMULO-4826,2018-02-23T20:55:38.000+0000,2018-02-26T03:45:49.000+0000,2019-04-23T16:12:59.000+0000,,Fixed,New Feature,Major,['1.9.0'],,21000,21000,,,,100,,,,Support Hadoop 3,4,1,['pull-request-available'],elserj,True,elserj,elserj
accumulo,ACCUMULO-4808,2018-02-09T18:11:39.000+0000,2018-07-30T14:33:03.000+0000,2019-04-23T16:09:16.000+0000,,Duplicate,New Feature,Major,,,,,,,,,"['master', 'tserver']","['master', 'tablet server']","Add capability to add table splits at table creation. Recent changes now allow iterator and locality groups to be created at table creation. Do the same with splits. Comment below from [ACCUMULO-4806|https://issues.apache.org/jira/browse/ACCUMULO-4806] explains the motivation for the request:

{quote}[~etcoleman] added a comment - 2 hours ago
It would go al long way if the splits could be added at table creation or when table is offline.  When the other API changes were made by Mark, I wondered if this task could also could be done at that time - but I believe that it was more complicated.

The delay is that when a table is created and then the splits added and then taken offline there is a period proportional to the number of splits as they are off-loaded from the tserver where they originally got assigned.  (The re-online with splits distributed across the cluster is quite fast)

If the splits could be added at table creation, or while the table is offline so that the delay for shedding the tablets could be avoided, then the need to perform the actual import offline would not be as necessary.

 
{quote}
 ",Add splits to table at table creation.,4,,,jmark99,True,jmark99,jmark99
accumulo,ACCUMULO-4733,2017-10-31T18:00:31.000+0000,2018-10-09T16:15:19.000+0000,2018-10-09T16:15:19.000+0000,,Fixed,New Feature,Minor,['2.0.0'],,4200,4200,,,,100,,,Adding the ability to configure which Java security provider to use for crypto.,Implement configurable security provider for crypto,2,1,"['pull-request-available', 'release-notes']",PircDef,True,PircDef,PircDef
accumulo,ACCUMULO-4729,2017-10-24T07:59:29.000+0000,,2017-11-01T16:58:00.000+0000,,,New Feature,Minor,,['1.8.1'],,,,,,,['mini'],['the mini cluster'],"As developer I would like to have something like MiniAccumuloCluster.getInstance() 
That I can share with multiple tests",MiniAccumuloCluster should have a Singleton,3,,,jomach,True,,jomach
accumulo,ACCUMULO-4706,2017-09-13T20:26:08.000+0000,2017-12-12T19:36:43.000+0000,2017-12-12T19:36:43.000+0000,,Fixed,New Feature,Major,,['2.0.0'],12600,12600,,,,100,"['scripts', 'start']","['scripts for managing services, etc.', 'classloader code and startup scripts']","While there are some [Accumulo images|https://hub.docker.com/search/?isAutomated=0&isOfficial=0&page=1&pullCount=0&q=accumulo&starCount=0] on DockerHub, it looks the majority of the them are designed to run a single-node Accumulo instance in a Docker container for development and testing.

It would be great if Accumulo had an official image for running Accumulo processes in containers on a production cluster.  The image could be be published as an official image 'apache/accumulo' to DockerHub.  

In order to make this possible, I think work needs to be done to allow configuration to be passed to the Accumulo process in the docker container without using configuration files (as passing files to a running container is hard in Docker).  One way to do this is to add an option called {{--upload-accumulo-site}} to 'accumulo init' command which is called outside of Docker by the user.  This would set properties in accumulo-site.xml as system properties in Zookeeper during Accumulo initialization.  Accumulo processes in Docker containers could be started with minimal configuration by updating 'accumulo <service>' commands to have a {{-o key=value}} option to override configuration.  These changes to Accumulo would enable the following commands to start an Accumulo cluster in Docker:

{noformat}
accumulo init --upload-accumulo-site
docker pull apache/accumulo
docker run apache/accumulo master -o instance.zookeeper.host=zkhost:2181
docker run apache/accumulo tserver -o instance.zookeeper.host=zkhost:2181
docker run apache/accumulo monitor -o instance.zookeeper.host=zkhost:2181
docker run apache/accumulo tracer -o instance.zookeeper.host=zkhost:2181
{noformat}",Create official Accumulo Docker image,4,,['pull-request-available'],mikewalch,True,mikewalch,mikewalch
accumulo,ACCUMULO-4655,2017-06-16T16:09:59.000+0000,2017-07-23T16:59:04.000+0000,2019-04-23T16:12:52.000+0000,,Fixed,New Feature,Major,['1.9.0'],"['1.6.6', '1.8.1']",14400,14400,,,,100,['monitor'],['web page monitor'],"The monitor has a column for last contact.  Sometimes that is due to a tserver not responding.  If we show the response time for a tserver to return its status, then we would have the appropriate info.",monitor should show response time in addition to last contact,3,,,ivan.bella,True,ivan.bella,ivan.bella
accumulo,ACCUMULO-4654,2017-06-16T16:07:05.000+0000,2017-07-19T22:53:57.000+0000,2019-04-23T16:12:57.000+0000,,Fixed,New Feature,Major,['1.9.0'],"['1.6.6', '1.8.1']",26400,26400,,,,100,['master'],['master'],The HostRegexTableBalancer current halts all migrations when there are pending migrations.  I propose fixing this to allow adding additional migrations even when there are pending migrations up to a specified amount.,HostRegex balancer should allow migration even when we have pending migrations,2,,"['balancer', 'master', 'migration']",ivan.bella,True,ivan.bella,ivan.bella
accumulo,ACCUMULO-4628,2017-04-24T18:26:48.000+0000,,2017-04-24T18:34:36.000+0000,,,New Feature,Minor,,,,,,,,,['client'],['client code'],"Some fallout from HIVE-15795:

[~faganm] noticed that we don't have a lexicoder in Accumulo for {{BigDecimal}}. It would be nice to implement one that can do that.",Provide Lexicoder for decimal data,2,,,elserj,True,,elserj
accumulo,ACCUMULO-4595,2017-03-01T22:37:17.000+0000,,2019-06-11T05:12:35.000+0000,,,New Feature,Minor,,,,,,,,,,,"It would be useful to be able to write custom validation for specific parts of a Key or Value that could be applied to both mutations on live ingest and writing RFiles for bulk loads.  Initially I thought this could be done with a table constraint, but didn't consider the importdirectory use case where RFiles are bulk imported.  This validation does not need to track state though.  Thinking this is really only for simple validation.  Couple of contrived examples.

1) Validate the CF starts with the string ""abcd"".
2) Validate the CV only contains strings from a predefined list of strings.
3) Validate the row only contains letters or numbers.

",Simple validator for inserting data,3,,,mjwall,True,,mjwall
accumulo,ACCUMULO-4544,2016-12-28T18:42:58.000+0000,,2016-12-29T07:46:28.000+0000,,,New Feature,Major,,['1.7.1'],,,,,,,['shell'],['shell'],"Just as you can attach custom formatters for scan output in the shell, it would be great to be able to provide the inverse transformation for use when inserting. right now, if you are using a formatter on a table, you typically cannot copy lines to reinsert with modifications, without also manually reversing whatever your formatter is doing.",pluggable parser for insert shell command,2,,,mberman,True,,mberman
accumulo,ACCUMULO-4500,2016-10-17T04:02:10.000+0000,2017-03-20T15:06:18.000+0000,2017-03-20T15:35:26.000+0000,,Fixed,New Feature,Major,['2.0.0'],,45600,55200,,,,100,"['client', 'tserver']","['client code', 'tablet server']","Add support to quickly extract a histogram of all of the visibilities stored in an Accumulo table.

DISCUSS: https://lists.apache.org/thread.html/df5e764362a95277344fd2731a432e9fafc60595e7d30015d9a56b9c@%3Cdev.accumulo.apache.org%3E",Implement visibility histograms as a table feature,4,,,elserj,True,kturner,elserj
accumulo,ACCUMULO-4493,2016-10-06T17:16:54.000+0000,,2019-06-11T05:12:46.000+0000,,,New Feature,Minor,,,,,,,,,['shell'],['shell'],"Users should be able to launch the shell in a kerberos deployment using a keytab.

current workaround: use the system shell to kinit with the keytab, then launch the shell, then kdestroy

Workaround doesn't allow re-login from keytab for long running shell.",Shell should be able to use keytab login,3,,,busbey,True,,busbey
accumulo,ACCUMULO-4492,2016-10-06T17:10:50.000+0000,,2019-06-11T06:07:00.000+0000,,,New Feature,Major,,,,,,,,,,,"When converting an existing cluster to use Kerberos, existing user permissions aren't much use unless the user names happen to be formatted like Kerberos principals.

An offline tool that folks migrating can use to map existing user names to principals would be super useful.

Essentially something like:

{code}
$ accumulo kerberos-migration --include-users=* --exclude-users=root --no-instance --realm=EXAMPLE.COM
Migrating users matching '*' and not matching 'root'.
User principals will not have an instance component.
User principals will be in the realm 'EXAMPLE.COM'
Found user 'auser', converted to 'auser@EXAMPLE.COM'
Found user 'another_user', converted to 'another_user@EXAMPLE.COM'
Found user 'hpnewton', converted to 'hpnewton@EXAMPLE.COM'
Found user 'root', skipped due to exclusion rule
{code}",operations tool to migrate existing users when enabling Kerberos,2,,,busbey,True,,busbey
accumulo,ACCUMULO-4487,2016-10-05T22:04:44.000+0000,2019-04-16T19:01:54.000+0000,2019-04-16T19:01:54.000+0000,,Not A Problem,New Feature,Major,,,,,,,,,['test'],['system tests'],"In addition to TestCaseName, TestCaseStatus, TestSuiteName and elapsed time, we would like to gather the following: testCaseStartTime, testCaseEndTime, and testCaseAnnotations. 

",Enhance test reporting features to be extended beyond current granularity,3,,,cheng xu,True,cheng xu,cheng xu
accumulo,ACCUMULO-4307,2016-05-09T19:36:19.000+0000,,2019-04-23T20:27:01.000+0000,,,New Feature,Major,,,,,,,,,['build'],"['tarball, packaging, and the Maven build']","Found https://siom79.github.io/japicmp/MavenPlugin.html today and tested it out. Was wondering what thoughts are for incorporating this into the build. I tested it on 1.6.6-SNAPSHOT by dropping the following into the test pom file:

{noformat}
    <profile>
      <id>semver-compliance</id>
      <build>
        <plugins>
          <plugin>
            <groupId>com.github.siom79.japicmp</groupId>
            <artifactId>japicmp-maven-plugin</artifactId>
            <version>0.7.2</version>
            <executions>
              <execution>
                <id>client compliance</id>
                <goals>
                  <goal>cmp</goal>
                </goals>
                <phase>verify</phase>
                <configuration>
                  <oldVersion>
                    <dependency>
                      <groupId>org.apache.accumulo</groupId>
                      <artifactId>accumulo-core</artifactId>
                      <version>1.6.5</version>
                      <type>jar</type>
                    </dependency>
                  </oldVersion>
                  <newVersion>
                    <dependency>
                      <groupId>org.apache.accumulo</groupId>
                      <artifactId>accumulo-core</artifactId>
                      <version>${project.version}</version>
                      <type>jar</type>
                    </dependency>
                  </newVersion>
                  <parameter>
                    <includes>
                      <include>org.apache.accumulo.core.client</include>
                      <include>org.apache.accumulo.core.data</include>
                      <include>org.apache.accumulo.core.security</include>
                    </includes>
                    <excludes>
                      <exclude>*crypto*</exclude>
                      <exclude>*impl*</exclude>
                      <exclude>*thrift*</exclude>
                    </excludes>
                    <accessModifier>protected</accessModifier>
                    breakBuildBasedOnSemanticVersioning>true</breakBuildBasedOnSematicVersioning>
                    <onlyModified>true</onlyModified>
                  </parameter>
                </configuration>
              </execution>
            </executions>
          </plugin>
        </plugins>
      </build>
    </profile>
{noformat}

 I tried getting the previous release version number using the build-helper-maven-plugin, but it found the wrong version. If we use this we would also have to include an execution for minicluster and determine whether or not we want to use the reporting feature of the plugin.",semver compliance maven plugin,3,,,dlmarion,True,ctubbsii,dlmarion
accumulo,ACCUMULO-4061,2015-11-19T12:51:09.000+0000,2017-07-24T23:33:02.000+0000,2017-07-24T23:33:02.000+0000,,Fixed,New Feature,Minor,['2.0.0'],,5400,6600,,,,100,['tserver'],['tablet server'],"When upgrading by performing a rolling restart, realized that there was no way to determine the version of a running tserver other than using ps to get start time or maybe a versioned package on the classpath.

A typical deployment structure seems to be lay down the Accumulo directories with a version and then use a symbolic link to point to the version that is used at run-time. As an example:

/usr/local/accumulo
/usr/local/accumulo/accumulo-1.6.2
/usr/local/accumulo/accumulo-1.6.3
/usr/local/accumulo/accumulo-latest --> /usr/local/accumulo/accumulo-1.6.3

To upgrade without a complete shutdown, we lay down the new version, update the symbolic link and then perform a rolling restart of the tservers, with scripts, env,... using the symbolic link to specify which version are used.

Realized that if the rolling shutdown failed for any particular tserver, it would keep running the previous version.  The only way to determine if we were running the desired version was to use ps and check the running time of the tserver process.

The could also be a situation were a ""dead"" server would be offline during the upgrade and not receive the new version. If the server was resurrected and services started before updated versions are installed it could be difficult to determine exactly what version of the tserver was running. 

It would be nice if there was some way to ask a running tserver what version it is. That way, post-upgrade we could confirm that all running tservers reply with the expected version.

",Add way to determine Accumulo version from a running tserver,4,,,etcoleman,True,lstav,etcoleman
accumulo,ACCUMULO-3982,2015-08-31T14:34:29.000+0000,,2019-04-23T20:27:02.000+0000,,,New Feature,Major,,"['1.6.3', '1.7.0', '1.8.0']",,,,,,,"['client', 'tserver']","['client code', 'tablet server']","On larger systems, especially those of which may split frequently, I've seen churn in the TabletLocator caches. Could we possibly pre-fetch entries for a table so that we don't constantly access the metadata table for tablet locations. 

Additionally, instead of clearing the cache in the case of a miss or failure, could we simply update those entries in the cache? If we do this we may minimize cache misses overall and the need to scan the meta data table. If need be, we could have a threshold before clearing the cache entirely if people see the need for that.

It would also be helpful to add an LRU cache ( or maybe even priority based LRU cache ). In ACCUMULO-3549, we added a cache clear to TabletServer to avoid overwhelming memory. An LRU cache might address this concern and make the clear cache call unnecessary.

",Add LRU mechanism to address memory concerns and possibly pre-fetch Tablet Locations.,5,,,phrocker,True,ecn,phrocker
accumulo,ACCUMULO-3970,2015-08-22T06:10:18.000+0000,,2017-11-30T23:49:58.000+0000,,,New Feature,Minor,,,1800,1800,,,,100,,,"It would be useful to have the ability to generate different representations of a key-value pair at scan time, based on the scan authorizations.

For example, consider [HIPPA safe harbour de-identification|http://www.hhs.gov/ocr/privacy/hipaa/understanding/coveredentities/De-identification/guidance.html#dates]. One of the rules for de-identifying a patient's date of birth is that if a patient is 89 years old or younger, you can disclose his exact year of birth. If a patient is 90 years old or over, you pretend that he's 90 years old.

You can imagine implementing this as a key/value mapping in accumulo like,
{{(pt_id, demographic, pt_dob, PII_DOB) -> ""1925-08-22""}}
{{(pt_id, demographic, pt_dob, SHD_DOB) -> ""1925""}}
Where the value corresponding to visibility SHD_DOB is produced at scan-time, depending on the patient's current age.

Another example would be the ability to produce a salted hash of a unique identifier like a social security number or medical record number, where the salt (or the hash algorithm, or the work factor...) could be specified dynamically without having to re-code all the values in the system.

More broadly speaking, this feature would give organizations more flexibility to change how they deidentify, transform or anonymize data to suit different access levels.

Of course, to do this you'd need to have a pluggable component that can process key/value pairs before visibilities are evaluated. I can see why this might give a lot of people the heeby-jeebies but I'd like to gather as much feedback as possible. Looking forward to hearing your thoughts!",Generating multiple views of a value at scan time,4,,['pull-request-available'],rweeks,True,,rweeks
accumulo,ACCUMULO-3795,2015-05-11T18:12:08.000+0000,2015-07-01T15:21:50.000+0000,2015-07-01T15:25:51.000+0000,,Fixed,New Feature,Major,['1.8.0'],['1.6.2'],3000,3000,,,,100,['tserver'],['tablet server'],"For scans that may take a long time because of some underlying iterator that has to scan many keys and only returns one every so often, it would be great if we could force results to be returned to the user before the buffer has been completely filled (table.scan.max.memory).  I propose that this be time based.  Perhaps we would add a configuration property called something like table.scan.flush.ms.  Note that the buffer would only be flushed (i.e. returned to the client) iff it is non-empty, and the table.scan.flush.ms threshold has been reached since the beginning of the nextBatch call in Tablet.java.",flush the scan buffer if non-empty after configured timeout,4,,,ivan.bella,True,ecn,ivan.bella
accumulo,ACCUMULO-3733,2015-04-15T14:51:19.000+0000,,2015-04-15T15:28:19.000+0000,,,New Feature,Minor,,,,,,,,,,,"I had a thought about a potential performance/API improvement last night. Mutations are great for ensuring that multiple key/value changes to the same row happen atomically, but for some applications they require unnecessary transformation of key/value pairs in and out of the Mutation structure with no real change in information content. We might be able to improve both ingest performance and simplicity of the API by supporting ingesting streams or containers of key/value objects directly.",support an ingest path that avoids Mutation objects,3,,,afuchs,True,,afuchs
accumulo,ACCUMULO-3700,2015-03-27T15:35:03.000+0000,2017-02-15T21:23:37.000+0000,2017-02-15T21:23:37.000+0000,,Duplicate,New Feature,Minor,,['1.6.2'],,,,,,,,,"In a multi-tenant environment I would like to be able to segregate tables to subset of tablet servers for processing isolation and maintaining SLAs.

My thinking is this could be accomplished by defining a configuration that maps namespaces to tablet servers and a custom balancer will utilize this configuration to balance tablets only on the servers associated with its namespace. 
",Table Balancer support for multi-tennacy,4,,"['balancer', 'multi-tenant']",faganm,True,,faganm
accumulo,ACCUMULO-3678,2015-03-18T16:38:33.000+0000,2018-03-19T18:45:26.000+0000,2018-03-19T18:45:26.000+0000,,Incomplete,New Feature,Major,['upgrade-tests-1.0.0'],,,,,,,,['upgrade-tests'],['contrib for testing across Accumulo versions'],test whether a client of one version can read/write data when talking to a cluster of another version,Add client compatibility tests for data operations,1,,,busbey,True,,busbey
accumulo,ACCUMULO-3677,2015-03-18T16:36:41.000+0000,2018-03-19T18:45:28.000+0000,2018-03-19T18:45:28.000+0000,,Incomplete,New Feature,Major,['upgrade-tests-1.0.0'],,,,,,,,['upgrade-tests'],['contrib for testing across Accumulo versions'],"Add a means of fault injection during upgrade tests, so we can make sure

* the system is recoverable.
* Docs cover what to do when things go wrong
",Fault injection during upgrades,1,,,busbey,True,,busbey
accumulo,ACCUMULO-3676,2015-03-18T16:34:40.000+0000,2018-03-19T18:45:27.000+0000,2018-03-19T18:45:27.000+0000,,Incomplete,New Feature,Major,['upgrade-tests-1.0.0'],,,,,,,,['upgrade-tests'],['contrib for testing across Accumulo versions'],"Either vagrant or docker or the like.

Essentially we want a way for someone with a beefy node to spin up what looks like a distributed cluster for doing the tests without having to manage one on their own.",Add option to deploy containerized cluster in test setup,1,,,busbey,True,,busbey
accumulo,ACCUMULO-3675,2015-03-18T16:33:09.000+0000,2018-03-19T18:45:26.000+0000,2018-03-19T18:45:27.000+0000,,Incomplete,New Feature,Major,['upgrade-tests-1.0.0'],,,,,,,,['upgrade-tests'],['contrib for testing across Accumulo versions'],add a test that checks if data exported from one version can be imported in another,Add data compatibility test for import/export,1,,,busbey,True,,busbey
accumulo,ACCUMULO-3674,2015-03-18T16:32:27.000+0000,2018-03-19T18:45:28.000+0000,2018-03-19T18:45:28.000+0000,,Incomplete,New Feature,Major,['upgrade-tests-1.0.0'],,,,,,,,['upgrade-tests'],['contrib for testing across Accumulo versions'],"add tests that see if different client / server versions can perform admin operations.

{quote}
What about some more tricky things over the metadata table (clone, import, export, merge, split table)?
{quote}",Add client compatibility tests for admin operations,1,,,busbey,True,,busbey
accumulo,ACCUMULO-3671,2015-03-18T16:22:50.000+0000,2018-03-19T18:45:29.000+0000,2018-03-19T18:45:29.000+0000,,Incomplete,New Feature,Major,['upgrade-tests-1.0.0'],,,,,,,,['upgrade-tests'],['contrib for testing across Accumulo versions'],"Our data compatibility tests for upgrade should be expanded to include flexing different locality group options

* locality group in place from start of table
* locality group added, full majc has happened
* locality group added, full majc has not happened
* locality group removed, full majc has happened
* locality group removed, full majc has not happened

Each should be sure to have data in the specified locality group(s) as well as in hte default locality group.",Add locality groups to data compatibility tests,1,,,busbey,True,,busbey
accumulo,ACCUMULO-3648,2015-03-05T20:52:32.000+0000,,2015-03-05T23:00:50.000+0000,,,New Feature,Major,,,,,,,,,"['master', 'tserver']","['master', 'tablet server']","There are cases where you would like to scan a table in reverse key order.

For example: 
Consider a table using an inverted timestamp key.
This allows for very quick access to the latest records.

But  It would be helpful to quickly access earliest records without having to:
A) scan through all the records to build a collection to reverse sort
B) store the table again using the reverse key




",Ability to Scan Table Keys in Reverse Order,4,2,"['features', 'performance']",faganm,True,,faganm
accumulo,ACCUMULO-3477,2015-01-13T21:23:11.000+0000,,2019-06-11T06:06:42.000+0000,,,New Feature,Major,,,,,,,,,"['master', 'tserver']","['master', 'tablet server']","I re-read https://github.com/m1ch1/mapkeeper/wiki/Thrift-Java-Servers-Compared today, specifically the section on thshaserver vs tthreadedselectorserver.

{quote}
TThreadedSelectorServer performs better than THsHaServer when the network io is the bottleneck
{quote}

This made me think that in read-heavy environments, we may benefit from using the TThreadedSelectorServer instead of the THsHaServer. I know from previous experiments that we can spend a significant amount of time for a query just sending bytes over the wire from server(s) to a client. Improving this case may have benefit.

Like THsHaServer, TThreadedSelectorServer relies on the TFramedTransport, so this is only relevant for non-SSL and non-SASL cases.",Evaluate use of TThreadedSelectorServer,3,,,elserj,True,,elserj
accumulo,ACCUMULO-3369,2014-11-26T10:53:32.000+0000,2018-06-21T23:54:26.000+0000,2018-06-21T23:54:26.000+0000,,Won't Fix,New Feature,Minor,,,,,,,,,,,"importDirectory does not recurse through subdirectories to import rifles. It would be nice to add an option that would recurse through subdirectories or accept glob patterns to allow the user to specify multiple directories to import.

Here is a snippet of our email conversation from the mailing list to provide some context:
{quote}
Ah, ok. Thanks, Mike.

It wouldn't be too bad to ensure that the failures are unique (we could preserve the directory name in the failures dir). I thought that we assign a unique name on the backend (b-XXXXXXX.rf) so I think that's ok now.

What I'm boiling down to: sounds like a reasonable feature to request if you want to file an issue on JIRA, Ariel. :)

Mike Drob wrote:
Name collision of failures and I think name collision of successes might
cause problems sometimes too. Or maybe that's just with older versions.
Regardless, having to write your own code puts it out of the realm of
easy into at least middling territory - if import directory could
natively handle recursion then it would become easy.

On Tue, Nov 25, 2014 at 10:44 AM, Josh Elser  wrote:

    What's the difficulty, Mike? Handling name collision of failures?

    Mike Drob wrote:

        Ariel,

        There is not an easy way to do this recursively. Your best option is
        going to be writing your own wrapper around the import command. If
        you're using shell commands, this could be as easy as feeding the
        results of 'find . -type d' into a script, or in Java you might
        want to
        look at DirectoryWalker in Apache Commons as possible solutions.

        Mike

        On Tue, Nov 25, 2014 at 10:22 AM, Ariel Valentin
       

             Hello!

             We are running a couple of experiments using
        importDirectory and are
             curious if there is a simple way to import directories
        recursively.
             Based on looking at the source code it does not look like it
             currently supports that feature:


        (https://github.com/apache/__accumulo/blob/__1835c27ca41426ddd570cde14f9612__c45680b917/core/src/main/java/__org/apache/accumulo/core/__client/admin/__TableOperationsImpl.java
        https://github.com/apache/accumulo/blob/1835c27ca41426ddd570cde14f9612c45680b917/core/src/main/java/org/apache/accumulo/core/client/admin/TableOperationsImpl.java

             Are there plans to add it in the future? Or is there a
        simple way to
             do this right now?
{quote}",Recursive Directory Imports,1,,,arielvalentin,True,,arielvalentin
accumulo,ACCUMULO-3206,2014-10-05T02:44:48.000+0000,,2019-06-11T05:12:45.000+0000,,,New Feature,Minor,,['1.6.1'],,,,,,,"['client', 'monitor']","['client code', 'web page monitor']","The broader picture is public programmatic access to information in the Accumulo monitor.  Specifically I'm looking to obtain the number of entries per tablet and per tablet server for a given table.  The use case is to verify that manually set (or automatically set I suppose) table splits are effectively dividing Accumulo data among many tablets, that is, verifying load balancing.

I wrote Accumulo 1.5 code which uses non-public API to obtain this information in the same way the Monitor does via TabletStats. The tricky part was cross-referencing the Metadata table to find the assignment of tablets to tablet servers for a given table.  I rewrote that code for 1.6, switching the name of the Metadata table to ""accumulo.metadata"" and other associated changes, but it would be great to make this part of the public API so that people don't have to use non-public methods to obtain data that Accumulo has in the Monitor and Metadata table anyway.

We could approach this by adding to the TableOperations class or something similar.  A request could go to an Accumulo master which gathers the necessary information from the tablet servers just as the Monitor does, so that the client does not have to do it.",New Public API: Approximate data counts of Tablets and Tablet Servers,7,1,,shutchis,True,,shutchis
accumulo,ACCUMULO-3196,2014-10-03T20:10:39.000+0000,,2019-04-23T20:26:53.000+0000,,,New Feature,Major,,"['1.5.2', '1.6.1', '1.7.0']",,,,,,,"['docs', 'scripts']","['javadocs, manuals, etc.', 'scripts for managing services, etc.']","[From user@ thread|http://mail-archives.apache.org/mod_mbox/accumulo-user/201410.mbox/%3CCAGZGg1idg7U_YVJi_dH-KASK7VVY%3D4r9aReqxGWukVO1s0dmLw%40mail.gmail.com%3E]:

{quote}
ZooKeeper has security features built into it by way of access control
lists (ACLs) on nodes.  Once set, these ACLs can be very hard to get rid
of, especially if errant code has set up nodes that you no longer have any
password for.  This how-to guide shows you how to set up a root user inside
of ZooKeeper that can wipe out any ACLed node.
{quote}",Add support scripts and instructions for manual zookeeper cleanup,2,,,busbey,True,supermallen,busbey
accumulo,ACCUMULO-3091,2014-08-29T00:09:51.000+0000,2014-08-29T18:19:24.000+0000,2014-08-29T18:19:24.000+0000,,Duplicate,New Feature,Major,,,,,,,,,"['master', 'tserver']","['master', 'tablet server']","Accumulo supports user supplied application code running on the server side, but it does not currently share the configuration space with users.

Two specific examples come to mind, but there may be others:

1. Users can supply their own tablet balancer, but have no way to configure it. I can envision a balancer implementation that allows an administrator to modify the number of migrations per iteration by manipulating a property.

2. Users can supply their own iterator implementations. The same iterator class can be applied to multiple tables, each with their own configuration. The behavior of an iterator could be modified at runtime by an administrator by manipulating one property instead of modifying the iterator configuration on each table where it is being used.

Need to think about property visibility...",Support user defined properties,4,,,dlmarion,True,,dlmarion
accumulo,ACCUMULO-3083,2014-08-25T22:44:51.000+0000,,2014-08-27T23:08:25.000+0000,,,New Feature,Major,,,,,,,,,['master'],['master'],"I was thinking about balancing, in much the way that new container-based (yarn, mesos, etc) resource managers work.  The existing {{Balancers}} try to keep the number of tablets even over presumably homogeneous nodes.

But as clusters age and are improved and expanded in piecemeal, the hardware capabilities will drift apart.  Some nodes may be capable of handling more load than others.  We may even want some nodes to have a trivial number of tablets so the data they serve is always cached.

Tablet servers could advertise an attribute (or several).  Really, it would just be a name, like {{fatnode}} or {{metaonly}}.  These would then be used by the {{Balancer}} to favor or ignore tservers for balancing.
",capability balancing,3,,,ecn,True,,ecn
accumulo,ACCUMULO-2998,2014-07-16T16:06:58.000+0000,2014-07-16T18:55:47.000+0000,2015-05-18T19:40:37.000+0000,,Fixed,New Feature,Minor,['1.7.0'],,600,600,,,,100,"['client', 'master']","['client code', 'master']",It would be nice to wait for the balancer to run and find no work to do after creating a bunch of splits.,provide a mechanism to allow clients to wait for balance,4,,,ecn,True,ecn,ecn
accumulo,ACCUMULO-2883,2014-06-10T15:18:18.000+0000,2015-12-08T20:49:54.000+0000,2016-06-03T18:11:28.000+0000,,Fixed,New Feature,Major,['1.8.0'],,1200,1200,,,,100,['client'],['client code'],"TabletLocator already exists, but isn't officially a part of the ""public API"" and is clunky for users to invoke. In trying to co-locate external processes with the tabletservers that are hosting some data, it would be nice to have some means that users can invoke that will return them these assignments.

Memory concerns are an issue for tables with many splits (e.g. avoiding creating a Set of 100k tablet locations for a table), but we also want to provide the ability to ask pointed questions. Likely building something that accepts a Range (or Collection<Range>) would be best.",Add API method(s) that support fetching currently assigned locations for tablets,6,,,elserj,True,kturner,elserj
accumulo,ACCUMULO-2873,2014-06-09T13:28:17.000+0000,,2019-06-11T06:07:02.000+0000,,,New Feature,Major,,,,,,,,,,,"It would be very useful to have a utility that generates single line tablet info.  The output of this could be fed to sort, awk, grep, etc inorder to answer questions like which tablets have the most files.

The output could look something like the following

{noformat}
$accumulo admin listTablets --table bigTable3
#files #walogs #entries #size #status #location #tableid #endrow
6 2 40,001 50M ASSIGNED 10.1.9.9 4:9997[abc]  3 admin
3 1 50,002 40M ASSIGNED 10.1.9.9 5:9997[abc]  3 helpful
{noformat}

All of the information can be obtained by scanning the metadata table and looking into zookeeper.   Could possibly contact tablet servers to get info about entires in memory.

The order of the columns in the example above is arbitrary, except for end row.  Maybe end row column should come last because it can be of arbitrary length.  Also the end row could contain any character, could look into using a CSV library.   It would be nice to design the utility so that columns can be added in future versions w/o impacting current scripts that use the utility.",Create utility that generates single line tablet information,4,,['newbie'],kturner,True,,kturner
accumulo,ACCUMULO-2841,2014-05-23T18:20:26.000+0000,2014-09-12T20:10:38.000+0000,2016-06-03T17:57:07.000+0000,,Fixed,New Feature,Major,['1.7.0'],,1800,3600,,,,100,['client'],['client code'],"Application-level tags (tagName = tagValue) could be added to tables and namespaces, to allow applications to set application-level metadata about a namespace or table.

Use cases include management for billing, administrator notes, date created, last ingest time, stats, information about the table's schema... or anything else an application might wish to use tags for.

These tags could be stored in zookeeper, but are probably best stored in the metadata table (probably in a separate reserved area of the metadata table, ~tag) because they could be arbitrarily large and do not need to be persisted in memory.

This feature would include new APIs to manipulate table / namespace metadata. Considerations should be made to ensure users have appropriate permissions to add tags to an object.

This feature could be used to implement ACCUMULO-650.",Arbitrary namespace and table metadata tags,4,,"['metadata', 'newbie']",ctubbsii,True,hustjl22,ctubbsii
accumulo,ACCUMULO-2739,2014-04-25T21:59:54.000+0000,,2019-06-11T06:06:19.000+0000,,,New Feature,Major,,,,,,,,,['mini'],['the mini cluster'],"Two common griefs about MiniAccumuloCluster is that it's slow to start and it is difficult to debug things running inside of the Master or TabletServer (because they are in separate processes).

Running each Accumulo process inside of the parent process would likely increase startup speed and would make debugging much easier (as you wouldn't have to remote-attach the debugger anymore).

Such a MAC would likely also pave the way for easier migration away from MockAccumulo as both of the above complaints are some of the primary concerns for removing Mock with no replacemnt.",Create an In-Process MiniAccumuloCluster,4,,,elserj,True,,elserj
accumulo,ACCUMULO-2589,2014-03-28T19:38:15.000+0000,,2019-06-11T04:46:37.000+0000,,,New Feature,Major,,,1800,50400,,,,100,['client'],['client code'],"There are many issues with the current client API, and we've had a lot of lessons learned that could be incorporated into a new one.

Some of the issues this would address:

* questions like ""What is considered 'Public API'?""
* automated testing for incompatible API changes
* provide better support for alternative implementations of pluggable features
* distinguish between compile-time dependencies and runtime dependencies
* consistent exception handling
* consistent/symmetric getters/setters
* well-defined resource lifecycles
* better resource management
* simpler entry point to communicate with Accumulo
* better support for client-side configuration management
* logical layout of first class Accumulo objects in the API (Table, User, Scan, Connection)

Some of these goal may evolve during the development of this feature, and many previously identified issues can be moved to sub-tasks of this one. This ticket is intended to cover the overall feature, but the details will be handled in sub-tasks.",Create new client API,9,1,,ctubbsii,True,ctubbsii,ctubbsii
accumulo,ACCUMULO-2580,2014-03-28T02:31:41.000+0000,,2019-06-11T06:06:36.000+0000,,,New Feature,Major,,,,,,,,,['replication'],['replication'],"When a file import is requested, we also need to create a key to replicate so we know to replicate this file and not allow the GC to delete it.","Modify ""bulk ingest"" code path to create replication entries",1,,,elserj,True,,elserj
accumulo,ACCUMULO-2553,2014-03-26T01:42:57.000+0000,,2014-09-13T01:41:11.000+0000,,,New Feature,Minor,,,,,,,,,,,"This may not necessarily be something that would require changes in the AccumuloFileOutputFormat itself. Perhaps the ability to use it with Hadoop's MultipleOutputs is really the solution.

It would be useful if the user could specify multiple directories where RFiles should be placed and have a mechanism for populating the RFiles in the necessary directories based on a table name or group name. ",AccumuloFileOutputFormat should be able to support output for multiple tables.,4,,,sonixbp,True,sonixbp,sonixbp
accumulo,ACCUMULO-2518,2014-03-21T01:44:43.000+0000,2019-06-11T05:05:10.000+0000,2019-06-11T05:05:10.000+0000,,Won't Fix,New Feature,Minor,,,,,,,,,['client'],['client code'],"Many Apache projects (and other projects) are beginning to make packages that play nice with OSGi containers. The OSGi compendium http://www.osgi.org/Specifications/HomePage defines some constraints on how bundles should be packaged. For instance, service loading is not allowed in OSGi and packages shouldn't be split over different jars/bundles. It would be extremely useful if we could provide a ready-to-go bundle with solid documentation on how to deploy it.

Another notable feature would be to build a karaf-friendly features file where a user could more easily load in the required dependencies. Integrating the ""accumulo"" commands into the karaf client shell would be another good possiblity. ",Make accumulo-core OSGi friendly,2,,['proposal'],sonixbp,True,,sonixbp
accumulo,ACCUMULO-2439,2014-03-07T18:58:37.000+0000,2014-11-24T18:02:52.000+0000,2014-11-24T18:03:18.000+0000,,Won't Fix,New Feature,Minor,,['1.5.1'],,,,,,,['master'],['master'],"This patch adds a NOT ""!"" operator to ColumnVisibility.

The syntax is as follows:
!a
(!a)&(!b)
a&(!b)
a&(!(b|c))

Because of the nature of the current visibility parsing algorithm the additional parentheses are required.

In the shell, the ""\!"" requires escaping. This is due to how JLine parses the command and attempts to substitute ""!"" with history.",Add a NOT (!) operator to ColumnVisibility,3,,"['patch', 'security']",joeferner,True,,joeferner
accumulo,ACCUMULO-2413,2014-02-26T20:25:09.000+0000,2018-10-11T04:28:53.000+0000,2018-10-11T04:28:53.000+0000,,Won't Fix,New Feature,Major,,,,,,,,,,,"There are some failure cases that can occur, which put the metadata tables in a corrupted state. Under these circumstances, it'd be useful to have a robust tool for automatically correcting these mistakes.

Historically, there have been relatively hidden options added to the CheckForMetadataProblems class, which is an admin utility that is not part of the public API, for performing some of these tasks. However, these tasks were not safe to use and never really explained to the user what they did, and were dropped in ACCUMULO-2291.

A good tool would have a variety of options to cover the different situations which need to be fixed and would include good documentation, describing the various repair options, explaining the risks to the user, and prompting them to confirm, perhaps with the root password.",Add robust tools to automatically fix common metadata problems,1,,,ctubbsii,True,,ctubbsii
accumulo,ACCUMULO-2375,2014-02-17T05:00:28.000+0000,,2014-06-10T17:16:35.000+0000,,,New Feature,Minor,,,,,,,,,"['docs', 'examples']","['javadocs, manuals, etc.', 'examples included in the main distro']","For the developers interested in getting started with the code internals of accumulo seamless IDE integration should be documented and working e.g
http://wiki.apache.org/cassandra/RunningCassandraInEclipse


",Make Accumulo run via IDE(Eclipse),3,,,dallaybatta,True,dallaybatta,dallaybatta
accumulo,ACCUMULO-2365,2014-02-13T15:09:06.000+0000,2014-07-18T03:50:19.000+0000,2014-07-18T03:50:19.000+0000,,Won't Fix,New Feature,Major,,,,,,,,,,,"Create a Juju Charm for Accumulo in order for users to: 
do a one line install (juju quickstart bundle...) 
have a one line upgrade (juju upgrade-charm) 
instantly scale (juju add-unit) 
instantly integrate with other charms like Tomcat, Logstash, Nagios, Ganglia, etc. (juju add-relation)

More info on Juju Charms at http://juju.ubuntu.com/docs.",A Juju Charm for Accumulo,4,,,mectors,True,,mectors
accumulo,ACCUMULO-2362,2014-02-12T21:26:08.000+0000,2015-04-04T02:54:42.000+0000,2015-04-04T02:54:42.000+0000,,Won't Fix,New Feature,Major,,"['1.4.4', '1.5.0']",,,,,,,['client'],['client code'],"Presently, the {{Tables}} class contains a static map of instance to ZooCache that *many* of the classes in the client API use. The problem with this is that many of the methods on ZooCache ultimately are synchronized. When multiple threads using Connectors, TableOperations, Scanners/BatchScanners, and BatchWriters against the same Accumulo instance, you currently have a massive synchronization problem.

We should give some thought to heavy, concurrent access to Accumulo client API calls within the same JVM. Consideration should also be given to the consistency of wrapping zookeeper with a cache like is presently done.",Reduce blocking in client API (Tables),3,,,elserj,True,,elserj
accumulo,ACCUMULO-2338,2014-02-07T17:56:59.000+0000,,2016-12-28T18:27:18.000+0000,,,New Feature,Major,,,,,,,,,['shell'],['shell'],"The shell doesn't allow for very sophisticated scans.  You can't specify ranges on columns, which is often used in document-partitioned indexing schemes.  Similar to the ScanInterpreter feature, in which the scan args are translated, it would be nice if we supported a RangeInterpreter which would translate the range of the scan.
",add RangeInterpreter to the shell,3,,,ecn,True,,ecn
accumulo,ACCUMULO-2214,2014-01-17T15:44:33.000+0000,,2014-04-18T19:58:38.000+0000,,,New Feature,Major,,,,,,,,,['docs'],"['javadocs, manuals, etc.']","the dev list has had some [recent discussion on setup steps|http://mail-archives.apache.org/mod_mbox/accumulo-user/201401.mbox/%3CB9CB2B2BF27F0F46B8ECF781831E00E710CB60A5%400015-its-exmb10.us.saic.com%3E].

a quickstart guide for a particular OS would help users avoid some common pitfalls. we should also be able to use this to pull the ""getting started"" section out of the README per ACCUMULO-1515.

I'd recommend a markdown file in docs/. Then we can publish it ot the website.

I'd say start with 1.5.1-SNAPSHOT and we'll see if we need 1.5 v 1.6 specific versions.",Create a quickstart guide,2,,['newbie'],busbey,True,charlescva,busbey
accumulo,ACCUMULO-2180,2014-01-11T17:35:35.000+0000,2016-10-12T16:37:15.000+0000,2016-10-12T16:37:18.000+0000,,Done,New Feature,Trivial,,,,1800,,,,,,,"Presto (http://prestodb.io/) appears to be gaining support as a faster alternative to Hive.  

Presto can query different data stores using connectors that provide interfaces for fetching metadata, getting data locations, and accessing the data itself.   Currently Presto ships with a HD1, HD2 hive connectors. FB has a prototype connector for Hbase that they plan to open source at some point.  There is also a connector for Cassandra (https://github.com/MartinWeindel/presto/blob/master/presto-cassandra/README.md)

",provide an interface for FB Presto,10,,,arshak,True,,arshak
accumulo,ACCUMULO-2159,2014-01-08T22:10:48.000+0000,2015-04-06T02:09:30.000+0000,2015-04-06T02:09:30.000+0000,,Won't Fix,New Feature,Trivial,,,,,,,,,,,"Wikisearch example is great for users new to Accumulo for understanding the concepts of indexing. However since the k/v are protobuf encoded it makes it harder for a new user to understand the relationship of the base table to the indexing table, etc.

Perhaps there should be a flag that turns this on / off during ingest.  ",wikisearch example should allow user to choose protobuf on/off,4,,"['examples', 'newbie']",arshak,True,,arshak
accumulo,ACCUMULO-2158,2014-01-08T20:25:07.000+0000,,2019-06-11T04:52:35.000+0000,,,New Feature,Trivial,,,,,,,,,['shell'],['shell'],"Monitor displays (approximate?) entry counts currently for tables, tablets and tservers.   It would be useful to make this information available from the accumulo shell.  

Suggested workaround (http://mail-archives.apache.org/mod_mbox/accumulo-user/201310.mbox/%3CCANi8mMZxTi5wkyjqa_9fHG15Z0XkJcFZCfWw+y1kMzftkOAcYw@mail.gmail.com%3E) can be slow for large tables

accumulo shell -u username -p password -e ""scan -t foo"" | wc -l
 ","display count of entries (table, tablet, tserver) from the shell",4,,['usability'],arshak,True,,arshak
accumulo,ACCUMULO-2157,2014-01-08T19:30:31.000+0000,,2019-06-11T04:52:39.000+0000,,,New Feature,Trivial,,,,,,,,,['monitor'],['web page monitor'],"user can get this information from the accumulo shell, it would be nice if the output was also available in the monitor UI:

listcompactions   
listiter          
listscans         
listshelliter","display scans, compactions, iterators in the monitor UI",4,,['usability'],arshak,True,,arshak
accumulo,ACCUMULO-1957,2013-12-03T17:29:59.000+0000,2014-09-05T21:28:45.000+0000,2015-04-16T17:57:52.000+0000,,Fixed,New Feature,Minor,['1.7.0'],,4200,4200,,,,100,"['client', 'tserver']","['client code', 'tablet server']","There are several levels of durability:

* acknowledged
* walog'd
* walog'd and synced to datanodes
* walog'd and synced to datanode disks

Rather than specify this at the table configuration, allow the user to specify it when they create a batch writer.",specify durability during writes,4,,,ecn,True,ecn,ecn
accumulo,ACCUMULO-1817,2013-10-26T01:52:35.000+0000,2014-12-05T23:18:23.000+0000,2014-12-05T23:18:23.000+0000,,Fixed,New Feature,Major,['1.7.0'],,2400,3600,,,,100,,,"We currently expose JMX and it's possible (with external code) to bridge the JMX to solutions like Ganglia. It would be ideal if the integration were native and pluggable.

Turns out that Hadoop (hdfs, mapred) and HBase has ""direct"" metrics reporting to Ganglia through some nice code provided in Hadoop.

Look into the GangliaContext to see if we can implement Ganglia metrics reporting by Accumulo configuration alone.

References: http://wiki.apache.org/hadoop/GangliaMetrics, http://hbase.apache.org/metrics.html",Use Hadoop Metrics2,9,1,['proposed'],sonixbp,True,billie.rinaldi,sonixbp
accumulo,ACCUMULO-1802,2013-10-22T18:24:37.000+0000,2015-04-06T02:07:30.000+0000,2015-04-06T02:07:30.000+0000,,Later,New Feature,Trivial,,,,,,,,,['tserver'],['tablet server'],"The default compaction strategy has a tendency to put the oldest data in the largest files.  This leads to a lot of work when it is time to age off data.

One could imaging a compaction strategy that would split data into separate files based on the timestamp.  Additionally, if the min/max timestamps for a file were known, old data could be aged off by deleting whole files.

To accomplish this, will need to augment the configurable compaction strategy to support multiple output files, and saving/using extra metadata in each file.",Create a compaction strategy for aging off data,6,,,ecn,True,,ecn
accumulo,ACCUMULO-1753,2013-10-07T14:45:33.000+0000,,2013-10-07T14:45:33.000+0000,,,New Feature,Major,,,,,,,,,,,"Currently, there is a significant performance difference between running something like the IntersectingIterator in the TabletServer vs. in the ClientSideIteratorScanner, even on a single node with the client running alongside the TabletServer. There are many reasons for this difference, including:
1. Batching between the TabletServer and Scanner makes frequent seeks inefficient.
2. The wire protocol used by the Scanner makes seeks inefficient.
3. Interprocess communication in general adds latency.
4. Encoding and decoding adds latency.

Server-side iterators are still going to be better even if we completely optimize client-side iterators. However, server-side iterators come with risks to stability and security, especially if the set of iterators grows quickly. If we could optimize some of these problems, we could theoretically enable more programmability of complex operations with less risk to security and reliability of a multi-tenant instance of Accumulo.

This ticket is related to the concept of running iterators in a separate process, but includes the RPC aspect as well.",evaluate options for optimizing a SortedKeyValueIterator-like API on the client side,2,,,afuchs,True,,afuchs
accumulo,ACCUMULO-1680,2013-09-03T18:04:22.000+0000,2015-07-09T21:37:51.000+0000,2015-07-09T21:37:51.000+0000,,Fixed,New Feature,Minor,['1.8.0'],['1.6.2'],600,600,,,,100,"['client', 'master', 'shell']","['client code', 'master', 'shell']","Currently, adding and removing an auth presents a race condition because of the two separate calls required to get, modify, then set the authorizations for a user.  

Instead, it should be possible to add or remove one or more auths without there being a race condition.

I also propose adding two new commands to the shell to expose this functionality.  

It appears to be that this is not a difficult feature to add, by making use of  ZooReaderWriter.mutate() in the FATE packages.  I think the bulk of the work will be in changing the various interfaces (shell and programmatically) to expose this functionality.

I would like to do this work and I'm a accumulo newbie, so any guidance would be appreciated.",Atomic add and remove of authorizations,5,,"['newbie', 'summit2015']",cehardin,True,rwgdrummer,cehardin
accumulo,ACCUMULO-1639,2013-08-05T16:11:20.000+0000,2013-09-11T02:42:38.000+0000,2014-02-20T22:48:45.000+0000,,Fixed,New Feature,Minor,"['1.4.5', '1.5.1', '1.6.0']","['1.4.3', '1.4.4', '1.5.0']",,,,,,,,,"This filter is modeled off of the RegExFilter class in the org.apache.accumulo.core.iterators.user package. It allows you to specify a lexicographical start and end (including whether or not it is inclusive on each side) to a range of column qualifier keys that you would like returned from the retrieved data set on a Scanner or BatchScanner. It sets options on its IteratorSetting with a static method (setSlice), and is added to a Scanner/BatchScanner using addScanIterator like normal iterators/filters. This filter is intended to be an Accumulo equivalent to HBase's ColumnRangeFilter.",Server-side iterator/filter that allows you to specify a lexicographical range of column qualifier values you want to retrieve from a Scanner (ColumnSliceFilter),6,5,['features'],princjef,True,princjef,princjef
accumulo,ACCUMULO-1496,2013-06-05T17:41:40.000+0000,2015-01-22T22:58:38.000+0000,2015-03-12T19:58:36.000+0000,,Won't Fix,New Feature,Major,,,,,,,,,,,"If it can be done efficiently, I'd like to stop maintaining the explicitly enumerated list of things we can launch from the accumulo-start code. We should be able to discover these on the classpath, and automatically populate the help.

Annotations seems like it might be the right way to go, similar to how Servlet 3.0 annotations work (maybe implemented using something like scannotation).
",Use annotations to identify classes that are launchable by the start code,2,,,ctubbsii,True,ctubbsii,ctubbsii
accumulo,ACCUMULO-1495,2013-06-05T17:25:08.000+0000,2019-04-23T23:06:28.000+0000,2019-04-23T23:06:28.000+0000,,Won't Fix,New Feature,Major,,,,,,,,,,,"It might be worth providing read-only views of tablets on multiple servers, while writes go to a single authoritative server. This could improve query performance in some cases.

Some things to consider:

# Consistency: can I read what I've just written?
# Configuration: how many copies?
# Configuration: how long between syncs?
# Performance: locality, number of file replicas

Variation:

Permit multiple hosting only on tables that have been locked for writes (useful with a strictly enforced clone-and-lock feature).",Host multiple read-only versions of tablets for query performance,2,,,ctubbsii,True,,ctubbsii
accumulo,ACCUMULO-1488,2013-06-03T15:54:21.000+0000,2013-06-11T14:00:40.000+0000,2013-06-17T00:44:19.000+0000,,Fixed,New Feature,Major,"['1.5.1', '1.6.0']",,,,,,,,,,"Support sum, min, and max operations for decimal numbers using the java.math.BigDecimal String encoding and decoding functions (BigDecimal.toString() and BigDecimal(String)).",support BigDecimal encoding for basic built-in combiners,8,,,afuchs,True,,afuchs
accumulo,ACCUMULO-1427,2013-05-17T18:01:12.000+0000,2014-02-20T23:08:51.000+0000,2014-02-20T23:08:51.000+0000,,Won't Fix,New Feature,Minor,,,,,,,,,['tserver'],['tablet server'],I was investigating the possibility of allowing the user to specify an alternate location for the native map binaries. This would make the configuration more extensible and allow a system admin to reference the compiled binaries from anywhere on the system. One option could be to specify an ACCUMULO_NATIVE_MAP environment variable or something similar and having he InMemoryMap class do its System.load() from there if it exists.,Investigate a different way to inform the InMemoryMap where to load the NativeMap compiled binaries.,2,1,['proposed'],sonixbp,True,ctubbsii,sonixbp
accumulo,ACCUMULO-1419,2013-05-15T21:33:24.000+0000,2013-10-28T04:55:59.000+0000,2013-10-28T19:11:21.000+0000,,Fixed,New Feature,Major,['1.6.0'],,,,,,,,,,"This is a proposed feature for 1.6.

There's two ways we could do this:

 # Continue to support both hadoop1 and hadoop2, but switch the profiles, so hadoop2 is the default, and this is what we use to build binary artifacts for release voting.
 # Make hadoop2 required and get rid of the dependency profiles.
",Make hadoop2 the default,5,,"['feature', 'proposal']",ctubbsii,True,elserj,ctubbsii
accumulo,ACCUMULO-1399,2013-05-09T00:03:54.000+0000,2013-05-23T00:11:12.000+0000,2013-05-23T20:41:18.000+0000,,Fixed,New Feature,Minor,['1.6.0'],,,,,,,,['shell'],['shell'],"Proposing modification to the Shell to allow applications to create their own set of commands. This might be accomplished using java.util.ServiceLoader or something like it. Specifically, I'm thinking of a case where I have a create table command that is different than the one provided by the Shell. In my case, my create table command may create one or more tables and setup iterators on them.",Pluggable commands for the shell,6,,,dlmarion,True,dlmarion,dlmarion
accumulo,ACCUMULO-1398,2013-05-08T23:58:34.000+0000,2013-09-18T17:32:42.000+0000,2013-09-19T15:42:21.000+0000,,Fixed,New Feature,Minor,['1.6.0'],,,1800,,,,,['shell'],['shell'],"Many times the configuration of Accumulo changes over time while the system is running. Keeping a CM'd copy of the configuration makes sense, but a utility does not exist to dump the current configuration.

I'm proposing a new shell command to dump the running configuration to include the system and table configuration parameters. Something like:

dump -a -d <directoryName>
dump -t <tableName> -d <directoryName>",Create command to dump running configuration,6,,"['configuration', 'shell']",dlmarion,True,dlmarion,dlmarion
accumulo,ACCUMULO-1386,2013-05-07T16:53:18.000+0000,2019-04-22T23:12:35.000+0000,2019-04-22T23:12:35.000+0000,,Not A Problem,New Feature,Major,,,,,,,,,['mini'],['the mini cluster'],It would be very useful if users could be up and running with a single node Accumulo instance in minutes w/o setting up Hadoop or Zookeeper.,make it easy to run a single node accumulo instance,1,,,kturner,True,,kturner
accumulo,ACCUMULO-1337,2013-04-24T18:05:08.000+0000,2013-05-17T23:14:11.000+0000,2013-05-17T23:14:11.000+0000,,Fixed,New Feature,Trivial,['1.6.0'],,,,,,,,['tserver'],['tablet server'],"Add similar functionality as WholeRowIterator, but group by row/cf.  WholeColumnFamilyIterator.",Create Whole Column Family Iterator,6,,,pushpinder.heer,True,pushpinder.heer,pushpinder.heer
accumulo,ACCUMULO-1336,2013-04-24T17:56:04.000+0000,2013-05-05T02:45:44.000+0000,2013-05-23T20:41:19.000+0000,,Fixed,New Feature,Major,['1.6.0'],,,,,,,,,,"[Typo|https://github.com/keith-turner/typo] is something I created to experiment with another Java API for Accumulo.  It had two parts, an API with type paramaters and Lexicoders.  Lexicoders encode/serialize types in such a way that the lexicographic sort order matches the java types sort order.  I think having the Lexicoders available as part of Accumulo would be generally useful.  The parametric API is less useful, it assumes all values have the same type.",Add lexicoders from Typo to Accumulo,6,1,,kturner,True,sonixbp,kturner
accumulo,ACCUMULO-1266,2013-04-11T17:16:07.000+0000,,2016-09-21T18:51:41.000+0000,,,New Feature,Major,,,,,,,,,,,"For the following situation, there is a tipping point where it becomes beneficial to do a full major compaction.

 * a tablet is frequently scanned
 * scan time iterators supress a lot of data
 * a full major compaction would also supress that data 

Examples of this are tablets with lots of deletes, versions that are suppressed, data thats combined, and data thats filtered.   

If tablet servers kept track of statistics about scans, could this be used to determine when its beneficial to automatically compact?  In the following simple example, it seems obvious that a major compaction would be beneficial. In this example scans over the last hour have had to examine and throw away 20 million uneeded keys.  Alot of scan work could have been saved by doing a major compaction.

 * all scans over tabletA within the last hour have read 30 million keys and returned 10 million keys 
 * TabletA has 3 million keys
 * a major compaction would reduce tabletA to 1 million keys and result in future scans returning all keys read.

One complicating factor is that major compaction may have a different set of iterators configured.  Therefore its possible that scan may filter a lot of data, and major compactions may not.   Could possibly keep track of ratio of data dropped by compactions and the ratio of data dropped by scans.  This could be used when deciding if a major compaction should be done to improve scan performance.

What other situation can cause unnecessary major compactions and need to be defended against?

In the case where a compaction of just the data in memory would benefit scans, ACCUMULO-519 may solve the problem that this ticket is looking to solve.

So what should the formula be?  

{code:java}
  // k/v : key values
  // recentlyRead    : total number of k/v read before applying iterators by recent scans (recentlyRead - recentlyDropped equals # of k/v returned to users)
  // majcDropRatio   : ratio of k/v dropped by recent major compactions
  // totalKeyValues  : total # of k/v in tablet
  // R a user configurable ratio, like the current major compaction ratio that is based on files
  if((recentlyRead * majcDropRatio > R * totalKeyValues)){
     doFullMajorCompaction()
     resetScanStats()
  }
{code}

The example formula above has an issue, it may initiate a major compaction when scans are not reading a part of the tablet that drops data.  The formula below tries to remedy this.

{code:java}
  // k/v : key values
  // recentlyDropped : number of k/v dropped by recent scans
  // recentlyRead    : total number of k/v read before applying iterators by recent scans (recentlyRead - recentlyDropped equals # of k/v returned to users)
  // majcDropRatio   : ratio of k/v dropped by recent major compactions
  // totalKeyValues  : total # of k/v in tablet
  // R a user configurable ratio, like the current major compaction ratio that is based on files
  if((recentlyDropped > R * totalKeyValues) && (recentlyRead * majcDropRatio > R * totalKeyValues)){
     doFullMajorCompaction()
     resetScanStats()
  }
{code}

An issue with the above is that the total # of key values for a tablet may not be accurate because of bulk import and splits.",Automatically determine when a full major compaction would benefit scans,5,,,kturner,True,,kturner
accumulo,ACCUMULO-1256,2013-04-10T14:23:39.000+0000,,2013-07-18T19:46:59.000+0000,,,New Feature,Major,,,,,,,,,,,"It may be useful to provide an optional trash feature.  If this feature were enabled, then when a table is deleted it would go into the trash can.  Tables that had been in the trash for a while could would eventually be deleted.  Tables could be undeleted from the trash can.

What would the API and shell commands look like?  How would multiple tables in the trash can with the same name be handled in the API?  Would/should per table properties and pertable permissions be preserved?  Should these tables in the trash can show up in the monitor in some way?
",Add trash can for deleted tables,6,1,"['gsoc2013', 'mentor', 'newbie']",kturner,True,,kturner
accumulo,ACCUMULO-1228,2013-03-29T18:09:07.000+0000,2015-04-07T02:24:54.000+0000,2015-04-07T02:24:54.000+0000,,Won't Fix,New Feature,Minor,,['1.5.0'],,,,,,,"['client', 'tserver']","['client code', 'tablet server']","There's an inconsistency between what a server is capable of and what a client can tell it to do with respect to fetching column families.

Currently, a user can tell a {{Scanner}} to fetch some set of column families. The iterators support not only this, but also the converse where a user does not want to retrieve column families. An iterator implementation can do this by hand, but a client cannot specifically tell a Scanner to not return data from a set of column families. Clients should be able to specify this option.

There also seems to be an inconsistency with how locality groups are defined and then utilized. If I want to specify a set of column families as being part of a locality group, I have to provide a mapping of locality group name to a list of column families. If I want to fetch a locality group, I have to get the mapping first, rather than just set which locality group I want to use. It'd be more convenient to tell the scanner just to fetch which locality groups I want, and have the server know which column families that means.",Allow clients to disable column families and locality groups,4,,,bills,True,,bills
accumulo,ACCUMULO-1200,2013-03-23T16:37:44.000+0000,2013-04-12T14:38:12.000+0000,2013-04-24T07:37:26.000+0000,,Fixed,New Feature,Minor,['1.4.4'],,,,,,,,['proxy'],['client proxy'],,Back port the ProxySever from 1.5 to support 1.4.* as well.,4,,,sonixbp,True,sonixbp,sonixbp
accumulo,ACCUMULO-1045,2013-02-06T01:47:22.000+0000,,2014-04-22T14:46:31.000+0000,,,New Feature,Major,,,,,,,,,,,"Create a suite of command-line utilities (probably based on the shell's existing commands, or a subset of them), rather than be restricted to the limitations of the shell to issue these commands.

This adds benefit by being able to more simply pipe to external utilities, like awk, grep, sed, etc. It also has the potential to eliminate restrictions when scanning for binary data, standardize command line arguments better (especially by removing the shell ""current-table"" context), and leverage settings/profiles stored in some file (like $HOME/.accumulo/config).",Create a suite of shell commands or common features,4,,,ctubbsii,True,,ctubbsii
accumulo,ACCUMULO-1030,2013-02-03T19:37:25.000+0000,2013-10-31T21:37:23.000+0000,2015-04-06T21:04:50.000+0000,,Fixed,New Feature,Major,['1.6.0'],,,,,,,,,,"It would be great for users to be able to start up MiniAccumuloCluster with a maven plugin, so that they can test their code against Accumulo during the integration test phase of the Maven lifecycle.

MiniAccumuloCluster can be started during the pre-integration-test phase of the build lifecycle, then shut down during the post-integration-test phase.

This would satisfy use cases similar to how one might start up the jetty-maven-plugin to test a webapp.",Create a Maven plugin to run MiniAccumuloCluster for integration testing,5,1,,ctubbsii,True,ctubbsii,ctubbsii
accumulo,ACCUMULO-1021,2013-02-01T20:52:37.000+0000,2019-04-23T14:54:01.000+0000,2019-04-23T16:02:02.000+0000,,Won't Fix,New Feature,Major,,,,,,,,,"['master', 'tserver']","['master', 'tablet server']",There are a few tickets to support encrypting data at rest in Accumulo.   Encryption in a cluster is useless w/o good key management.   Users should have the ability to plug in their own key managment.  Out of the box Accumulo should provide a plugin for key management thats secure.,Provide default key management thats secure,5,,,kturner,True,,kturner
accumulo,ACCUMULO-1017,2013-01-31T03:16:39.000+0000,2015-04-04T02:26:20.000+0000,2015-04-04T02:26:26.000+0000,,Later,New Feature,Major,,,,,,,,,,,"Having resources to interact with Accumulo through the proxy in Ruby and Python is awesome, however the Ruby code isn't, persay, very Ruby-like, nor is the Python code very Python-like.

It would be neat to get to the point where we have wrapper libraries to further ease the barrier to entry to those who would rather not write Java.

There are pros and cons to this being versioned with Accumulo, so I'd also be curious what people think about maintaining such libraries our own, or letting the community grow them.",Build libraries around generated proxy code,3,,,elserj,True,,elserj
accumulo,ACCUMULO-1016,2013-01-31T03:10:16.000+0000,,2013-10-30T23:12:05.000+0000,,,New Feature,Major,,,,,,,,,['test'],['system tests'],"It's well known that the continuous-ingest suite is often a good place for verifying the validity of an Accumulo installation (much akin to Terasort for Hadoop).

The onus is on the user to actually extrapolate the necessary information from the generated reports to say ""I need to change property X because I saw feature Y in a graph"". I think we could do some automatic introspection on the results of the test to report some concrete recommended changes that could be made to increase performance.",Investigate additional knowledge that can be derived from continuous ingest tests,,,,elserj,True,,elserj
accumulo,ACCUMULO-1015,2013-01-31T03:01:29.000+0000,,2014-05-01T16:29:04.000+0000,,,New Feature,Trivial,,,,,,,,,['tserver'],['tablet server'],"Index block and data block caching get you part of the way, but it would be nice to have the ability to lock an entire table to memory. ",Lock a table to memory,,,,elserj,True,,elserj
accumulo,ACCUMULO-1009,2013-01-30T20:06:41.000+0000,2013-11-19T19:58:57.000+0000,2015-01-19T21:08:43.000+0000,,Fixed,New Feature,Major,['1.6.0'],,,,,,,,"['client', 'master', 'monitor', 'tserver']","['client code', 'master', 'web page monitor', 'tablet server']","Need to support encryption between ACCUMULO clients and servers.  Also need to encrypt communications between server and servers.   

Basically need to make it possible for users to enable SSL+thrift.

",Support encryption over the wire,9,,,kturner,True,mberman,kturner
accumulo,ACCUMULO-1000,2013-01-27T17:44:31.000+0000,2013-11-20T21:53:00.000+0000,2013-11-20T23:34:12.000+0000,,Fixed,New Feature,Major,['1.6.0'],,,,,,,,"['client', 'tserver']","['client code', 'tablet server']",Add support to mutation for compare and set operations.  This would allow user to specify that a row must contain certain data for a mutation to be applied.,support compare and set,10,,,kturner,True,kturner,kturner
accumulo,ACCUMULO-999,2013-01-27T17:40:41.000+0000,2018-10-05T18:13:59.000+0000,2018-10-05T18:13:59.000+0000,,Won't Fix,New Feature,Minor,,,,,,,,,['tserver'],['tablet server'],"Add a percolator like capability to Accumulo that support continuous data transformation and analysis.

http://research.google.com/pubs/pub36726.html",percolator,4,,,kturner,True,kturner,kturner
accumulo,ACCUMULO-990,2013-01-25T04:19:08.000+0000,2013-01-26T03:58:28.000+0000,2016-07-08T13:01:17.000+0000,,Fixed,New Feature,Major,['1.5.0'],['1.4.0'],,,,,,,,,"Once a user initiates a compaction on a table, there is no way to stop it. It will run to completion.",Need command to cancel compact table operation,3,,,kturner,True,kturner,kturner
accumulo,ACCUMULO-967,2013-01-15T18:35:36.000+0000,2013-01-23T02:16:09.000+0000,2013-01-23T02:16:09.000+0000,,Fixed,New Feature,Minor,['1.5.0'],,,,,,,,,,Create addAuths command for the shell as a convenience to users.,Add addAuths command to shell,3,,,dlmarion,True,dlmarion,dlmarion
accumulo,ACCUMULO-894,2012-12-07T18:08:06.000+0000,2014-07-18T02:49:13.000+0000,2014-07-18T03:27:06.000+0000,,Won't Fix,New Feature,Major,,,,,,,,,['tserver'],['tablet server'],"A few times we've wanted to have user-specific behavior, especially with respect to authorizations.  Unfortunately, this information is not available within the user-level iterators during scans.
",add user and authorization information to the iterator environment or initialization,3,1,,ecn,True,kturner,ecn
accumulo,ACCUMULO-875,2012-11-26T22:10:00.000+0000,2013-01-18T22:40:54.000+0000,2013-01-25T05:28:30.000+0000,,Fixed,New Feature,Major,['1.5.0'],,,,,,,,,,"The shell currently has a list scans command which is very useful for debugging.  Another command that would be useful is list compactions.   

The things that would be useful to know about a running compaction are the following :
 
 * Tablet server
 * Age
 * State (if showing queued compactions)
 * Type (User, Chop, System, Idle)
 * Tablet
 * Iterator info (iterators could have changed since it started and there are per compaction iterators)
 * Input files
 * Output file
 * Rate
 * Key/values read so far (could do bytes also)
 * Key/values written so far (could do bytes also)
 * Locality group compacting

Could consider using JMX to collect this info from tablet servers.   List scans currently uses a thrift call to get its info from tablet servers.   Making the info available via JMX would enable other tools to use the information. ",add list compactions command to shell,3,,,kturner,True,kturner,kturner
accumulo,ACCUMULO-820,2012-10-17T19:08:50.000+0000,2012-10-19T18:42:27.000+0000,2012-10-19T18:42:27.000+0000,,Invalid,New Feature,Trivial,,,,,,,,,['tserver'],['tablet server'],Will help identify servers that are slower at writing.,add hold time to tserver metrics bean,3,,,ecn,True,kturner,ecn
accumulo,ACCUMULO-809,2012-10-12T18:53:03.000+0000,,2014-04-22T20:41:20.000+0000,,,New Feature,Major,,,,,,,,,"['master', 'monitor']","['master', 'web page monitor']","The Fate Admin class can print a crude status for the running Fate operations. It would be nice to clean this information up and display it in the monitor. 

For example:

|Requester|Request|Table Locks|Executing|Details
|192.168.1.1|BulkImport|Index(R)|LoadFiles|Directory /accumulo/imports/import-2010-09-01|
|192.168.1.2|RangeOp|Documents (W, waiting)| |Merging Range (200809, 200810]|
|192.168.1.1|BulkImport| |Successful|Directory /accumulo/imports/import-2010-08-31|

| *Clear Successful Entries* |
",display Fate status in the monitor,2,,"['gsoc2013', 'mentor']",ecn,True,,ecn
accumulo,ACCUMULO-802,2012-10-10T23:49:30.000+0000,2013-12-16T18:38:46.000+0000,2015-10-19T21:50:33.000+0000,,Fixed,New Feature,Major,['1.6.0'],,,,,,,,"['client', 'master', 'tserver']","['client code', 'master', 'tablet server']","A large cluster is a valuable shared resource.  The current permission system and simple table naming structure does not allow for delegation of authority and safe partitioning within this shared resource.

Use cases:
 # create a namespace (like ""test"") and delegate the {{grant}} permission to tables created in that namespace to a user that would manage those tables. Presently, {{grant}} is never delegated.
 # create simple ""test"" and ""production"" namespaces that are trivial for users to switch between.  For example, instead of having tables ""test_index"" and ""test_documents"" the client would support ""index"" and ""documents"" with an API to support switching trivially between the the different environments.
 # create a set of tables in a namespace called ""latest""  This namespace is re-created periodically with a map-reduce job.  If code changes inadvertently create a corrupt ""latest,"" users can switch to the set of tables known as ""safest""  In this way, users can experiment, and provide feedback on incremental improvements, while have a safe fallback.
 # two applications hosted on the same cluster that can share a table, which has been ""aliased"" into their namespace.  Namespace-local permissions are ignored, but a (most likely read-only) view of the table is available.  This would be helpful for reference tables.
 # quotas/priorities.  Implement namespace-specific priorities and resource allocations.  It is reasonable to run namespace-specific queries and ingest on production equipment. Large cluster resources are always limited, and often the *only* place where near-production quality software can be run at full scale.

",table namespaces,6,1,,ecn,True,shickey,ecn
accumulo,ACCUMULO-792,2012-10-08T17:05:56.000+0000,2013-04-23T17:57:30.000+0000,2013-04-30T03:43:04.000+0000,,Fixed,New Feature,Major,['1.5.0'],,,,,,,,['scripts'],"['scripts for managing services, etc.']","The current start-up scripts use lists of hosts, and ssh to start and stop processes. A better set of scripts would be installed with rpms/debs and would start processes using /etc/init.

    turn on the box, the processes start and wait for hadoop/zooekeeper (done)
    turn on a master, and the system should go into ONLINE mode

Once this work is done, the ability to have rpms control the init.d and rc.d levels comes naturally with future packaging work, either in Accumulo or BigTop",Better scripts for start-up (RHEL),3,,,vines,True,vines,vines
accumulo,ACCUMULO-791,2012-10-08T17:04:49.000+0000,2012-12-18T17:00:16.000+0000,2012-12-18T17:00:16.000+0000,,Fixed,New Feature,Major,['1.5.0'],,,,,,,,['scripts'],"['scripts for managing services, etc.']","The current start-up scripts use lists of hosts, and ssh to start and stop processes. A better set of scripts would be installed with rpms/debs and would start processes using /etc/init.

    -turn on the box, the processes start and wait for hadoop/zooekeeper-
    -turn on a master, and the system should go into ONLINE mode-

These scripts can then be included in future packaging work (either in Accumulo or BigTop) for more functionality",Better scripts for start-up (debian),3,,,vines,True,vines,vines
accumulo,ACCUMULO-780,2012-10-02T18:46:06.000+0000,2019-04-16T19:18:41.000+0000,2019-04-16T19:18:41.000+0000,,Won't Fix,New Feature,Major,,,,,,,,,['scripts'],"['scripts for managing services, etc.']",We currently have a few different footprints available for users. We should use those as a base and allow users to quickly and conveniently configure their system for whatever size footprint they want. We can use the current as a baseline and extrapolate/interpolate for whichever setup they're using. This way we don't have to worry about maintaining a bunch of different sizes and instead just have an algorithm that needs occasional loving.,Accumulo should have a configurator,7,,,vines,True,,vines
accumulo,ACCUMULO-739,2012-08-23T13:49:30.000+0000,2012-08-29T14:22:19.000+0000,2012-09-07T01:05:13.000+0000,,Fixed,New Feature,Critical,['1.5.0'],,,,,,,,"['client', 'master', 'rpc', 'test', 'tserver']","['client code', 'master', 'rpc-related code and scripts', 'system tests', 'tablet server']",Please add dot (.) as a valid character in column visibility tokens.  This would allow us to implement hierarchical access control mechanisms.,Please add dot (.) as a valid character in column visibility tokens,9,1,,ivan.bella,True,billie.rinaldi,ivan.bella
accumulo,ACCUMULO-735,2012-08-22T15:22:46.000+0000,2013-01-28T20:25:00.000+0000,2013-01-28T20:25:00.000+0000,,Fixed,New Feature,Major,['1.5.0'],,,,,,,,,,"Hooks in the shell that can manipulate the range and columns passed to a scan range would be nice.   

My use case for this wanting to undo what a formatter did.  For example, suppose I use the new HexFormatter to view a table that has binary data like the following.

Below, I scan my table w/o the hex formatter. And its hard to read.

{noformat}
root@test15 graph> scan
\x06\x07)d\x7F\x08\xA1\x00\x08\x01\x02\xEFd@\xC3\xD2S\xC8 edge:count []    1
\x06\x07)d\x7F\x08\xA1\x00\x08\x08\x1C\xCE>\x8Ed\xE7\xDA edge:count []    1
\x06\x07)d\x7F\x08\xA1\x00\x08\x0A\xC7\xCED\xA54\xC0, edge:count []    1
\x06\x07)d\x7F\x08\xA1\x00\x08\x0C\x04\\Nz\x12}\x92 edge:count []    1
\x06\x07)d\x7F\x08\xA1\x00\x08\x0E\x95\x80\x1A\xA6\xEE\xEF, edge:count []    1
\x06\x07)d\x7F\x08\xA1\x00\x08\x0E\xCDBHYeP\xE0 edge:count []    1

{noformat}


So I add the HexFormatter and scan again, now its easier to read.

{noformat}
root@test15 graph> formatter -t graph -f org.apache.accumulo.core.util.format.HexFormatter        
root@test15 graph> scan
0607-2964-7f08-a100-0801-02ef-6440-c3d2-53c8  6564-6765  636f-756e-74 [] 31
0607-2964-7f08-a100-0808-1cce-3e8e-64e7-da  6564-6765  636f-756e-74 [] 31
0607-2964-7f08-a100-080a-c7ce-44a5-34c0-2c  6564-6765  636f-756e-74 [] 31
0607-2964-7f08-a100-080c-045c-4e7a-127d-92  6564-6765  636f-756e-74 [] 31
0607-2964-7f08-a100-080e-9580-1aa6-eeef-2c  6564-6765  636f-756e-74 [] 31
0607-2964-7f08-a100-080e-cd42-4859-6550-e0  6564-6765  636f-756e-74 [] 31
0607-2964-7f08-a100-0816-ccf8-3526-daf4-27  6564-6765  636f-756e-74 [] 31
0607-2964-7f08-a100-0817-b645-dcf2-b73b-65  6564-6765  636f-756e-74 [] 31
{noformat}

However, if I want to scan a range of the table I have to use the \xXX convention which is cumbersome.  I would like to do the following : 

{noformat}
scan -b 0607-2964-7f08-a100 -e 0607-2964-7f08-a101
{noformat}  

To do this I propose adding the following hook to the shell. A user could configure a scan interpreter on the shell like they configure a formatter.   The scan interpreter would take the command line arguments and translate or interpret them.   For example, a HexScanInterpeter could take in 0607-2964-7f08-a100 and output the binary representation for that hex string.

{code:java}
public interface ScanInterpreter {
  
  public Text interpretRow(Text row);

  public Text interpretBeginRow(Text row);
  
  public Text interpretEndRow(Text row);
  
  public Text interpretColumnFamily(Text cf);
  
  public Text interpretColumnQualifier(Text cq);
}
{code}

Originally I was thinking of adding the methods above to the Formatter interface.  However Christopher Tubbs convinced me to create a separate interface.  His argument was that you may want to use something like the HexScanInterpreter with different Formatters.  Inorder to make configuration easier we discussed adding a '-i' option to the shell formatter command.  This option would configure a Formatter that also implements the ScanInterpreter interface, saving the user from entering two commands.",Add hooks to shell for transforming scan range,2,,,kturner,True,kturner,kturner
accumulo,ACCUMULO-732,2012-08-17T14:56:48.000+0000,2012-08-17T18:24:03.000+0000,2013-07-07T17:22:35.000+0000,,Fixed,New Feature,Major,['1.5.0'],,,,,,,,['client'],['client code'],"The shell will print unprintable chars as hex using 4 characters like \xDD.  It would be nice to have a formatter that prints everything in hex and in a more concise form.  Something like the following.

{noformat}

root@test15 graph> formatter -t graph -f org.apache.accumulo.core.util.format.HexFormatter
root@test15 graph> scan
0607-2964-7f08-a100-0801-02ef-6440-c3d2-53c8  6564-6765  636f-756e-74 [] 31
0607-2964-7f08-a100-0808-1cce-3e8e-64e7-da  6564-6765  636f-756e-74 [] 31
0607-2964-7f08-a100-080a-c7ce-44a5-34c0-2c  6564-6765  636f-756e-74 [] 31
0607-2964-7f08-a100-080c-045c-4e7a-127d-92  6564-6765  636f-756e-74 [] 31
0607-2964-7f08-a100-080e-9580-1aa6-eeef-2c  6564-6765  636f-756e-74 [] 31
0607-2964-7f08-a100-080e-cd42-4859-6550-e0  6564-6765  636f-756e-74 [] 31
0607-2964-7f08-a100-0816-ccf8-3526-daf4-27  6564-6765  636f-756e-74 [] 31
0607-2964-7f08-a100-0817-b645-dcf2-b73b-65  6564-6765  636f-756e-74 [] 31
0607-2964-7f08-a100-081a-8f72-d23a-6c4e-8e  6564-6765  636f-756e-74 [] 31
{noformat}",Shell needs hex formatter,3,,['newbie'],kturner,True,kturner,kturner
accumulo,ACCUMULO-712,2012-08-02T04:18:21.000+0000,,2014-06-02T18:09:46.000+0000,,,New Feature,Major,,,,,,,,,['client'],['client code'],"Similar to CombineFileInputFormat for hdfs files, there should be an InputFormat that supports more than one tablet per input split if on the same tablet server. This should also work with ACCUMULO-391 to allow adding tablets from different compatible tables into a single input split. This allows mappers that finish very quickly to have more work to do. This could be a new InputFormat that extends AccumuloInputFormat or an option added to AccumuloInputFormat.",InputFormat that supports more than one tablet per input split if on the same tablet server,2,,,jdonofrio,True,,jdonofrio
accumulo,ACCUMULO-706,2012-07-28T01:16:48.000+0000,2012-09-21T23:01:30.000+0000,2013-06-27T13:02:34.000+0000,,Fixed,New Feature,Major,['1.5.0'],,,,,,,,['client'],['client code'],The BatchWriter needs a user configurable timeout.  The current behavior when a tablet or tablet server can not be successfully written to is that it will hang indefinitely retrying.  The timeout could default to max long to preserve current behavior.  ,Batch Writer needs timeout,4,,,kturner,True,kturner,kturner
accumulo,ACCUMULO-705,2012-07-28T01:13:13.000+0000,2012-08-08T20:13:54.000+0000,2018-10-18T18:39:31.000+0000,,Fixed,New Feature,Major,['1.5.0'],,10200,10200,,,,100,['client'],['client code'],"The batch scanner needs a user configurable time out.  When the batch scanner is used to query lots of tablet in parallel, if one tablet or tablet server is unavailable for some reason it will cause the scan to hang indefinitely.  Users need more control over this behavior.

It seems like the batch scanner could behave in one of the following ways : 
  * Read as much data as possible, then throw an exception when a tablet or tablet server has timed out
  * Throw an exception as soon as a tablet or tablet server times out, even if data could still be read from other tablets successfully.

The timeout can default to max long to preserve the current behavior.


  ",Batch Scanner needs timeout,3,,['pull-request-available'],kturner,True,kturner,kturner
accumulo,ACCUMULO-671,2012-07-05T17:59:49.000+0000,,2014-04-23T16:05:27.000+0000,,,New Feature,Minor,,,,,,,,,['master'],['master'],"In a case where ZooKeeper data is lost but the Accumulo footprint in HDFS still exists, it would be nice to be able to properly re-initialize just ZooKeeper (to the extent possible).

See [mailing list|http://mail-archives.apache.org/mod_mbox/accumulo-user/201207.mbox/%3CCAGUtCHrfNCApFb5iAVJX9R%3DuzwP5zT7FHw7zBrytMQwH1voJcw%40mail.gmail.com%3E] for discussion",Create utility to re-initialize just ZooKeeper from existing Accumulo install,4,,,krrai,True,,krrai
accumulo,ACCUMULO-638,2012-06-15T14:44:57.000+0000,,2015-08-13T16:43:00.000+0000,,,New Feature,Minor,,,,,,,,,['contrib'],['add-on projects'],"Titan graph database (https://github.com/thinkaurelius/titan) was recently open sourced (Apache 2 License).  It is a ""distributed graph database optimized for processing massive-scale graphs represented over a machine cluster"".  It currently supports storing data in both Cassandra and HBase.  I believe Accumulo support should be added as well.

Other references:
 - http://thinkaurelius.github.com/titan/
 - https://speakerdeck.com/u/okram/p/titan-the-rise-of-big-graph-data
",Accumulo integration with Titan Graph database,12,2,,jasontrost,True,rweeks,jasontrost
accumulo,ACCUMULO-626,2012-06-07T12:05:49.000+0000,2015-12-14T05:33:46.000+0000,2016-06-03T18:10:57.000+0000,,Fixed,New Feature,Major,['1.8.0'],,1800,1800,,,,100,['test'],['system tests'],"Users often write iterators without fully understanding its limits and lifetime. Accumulo should have an iterator fuzz-tester which will take user data and run the iterator under extreme conditions.  For example, it should re-create and re-seek the iterator with every key returned.  It could automatically compare results of such a run with the naive run, which seeks to the beginning and scans all the data.",create an iterator fuzz tester,4,,,ecn,True,elserj,ecn
accumulo,ACCUMULO-617,2012-06-05T16:10:42.000+0000,2013-06-13T17:50:56.000+0000,2013-06-13T17:50:56.000+0000,,Duplicate,New Feature,Major,,,,,,,,,['tserver'],['tablet server'],It would be nice to set a default access to a table so adding a new user does not require adding all the permissions for each table they will read. ,default access for a table,5,,,ecn,True,vines,ecn
accumulo,ACCUMULO-594,2012-05-16T15:12:47.000+0000,,2014-04-19T21:21:20.000+0000,,,New Feature,Trivial,,,,,,,,,['monitor'],['web page monitor'],The monitor overview page plots accumulo performance data over time for all tables.  I would like to be able to see this data over time per table (and maybe per tablet server).  Could add this to the monitor page.  Eric suggested maybe using OpenTSDB to collect and view this data.,Show per table plots,5,,"['gsoc2013', 'mentor']",kturner,True,,kturner
accumulo,ACCUMULO-582,2012-05-10T12:54:03.000+0000,2012-05-10T14:58:04.000+0000,2014-02-20T23:25:30.000+0000,,Duplicate,New Feature,Major,,,,,,,,,"['client', 'master', 'tserver']","['client code', 'master', 'tablet server']","The premise here is that a user wants to do a one time manipulation of a lot of data. Maybe a transformation, maybe a delete, who knows. Currently, a user can set an iterator, force the action, wait for it to complete, and then disable it. This is kinda crappy, because it means the user needs to track the action. And, with FATE, this can be even better. This way a user can just let the operation go and it will handle the entirety of the process.",Ability to force compactions with one time iterator configuration,,,,jvines,True,,vines
accumulo,ACCUMULO-571,2012-05-03T15:01:57.000+0000,,2014-10-16T01:04:55.000+0000,,,New Feature,Major,,,,,,,,,"['client', 'tserver']","['client code', 'tablet server']","This is idea that was recently brought to my attention. The use case is a user wants to essentially clone a subset of a table into an existing table. Currently cloning does not allow this. Current option is to copy the files in hdfs and then bulk import, since bulk import moves the files. This is pretty wasteful. Under the hood, the system can handle the cross-linking between files like that. We just need a mechanism to provide the ability to assign a subset of data to another region.

Potential uses include the above mentioned, as well as the potential for users to bring fresh data into a table which was cloned and modified. There may be other cases, but I haven't fully thought out this problem space.

The biggest problem with this is it does put the onus on the user for ensuring that data in the in memory maps is flushed before moving, as well as for handling the possibility of duplicate data.",Merge table into existing table,3,,"['gsoc2013', 'mentor']",jvines,True,,vines
accumulo,ACCUMULO-569,2012-05-01T20:00:20.000+0000,2019-06-11T04:54:42.000+0000,2019-06-11T04:54:42.000+0000,,Won't Fix,New Feature,Minor,,,,,,,,,,,"As mentioned in ACCUMULO-123, display the current configuration (minus passwords, of course) in the monitor.  Basically, display the same thing you can get in the shell.  This is helpful for remote debugging with less sophisticated users.
",enable the monitor to show the current configuration,2,,,ecn,True,,ecn
accumulo,ACCUMULO-549,2012-04-20T21:12:24.000+0000,,2012-12-18T18:23:26.000+0000,,,New Feature,Major,,,,,,,,,,,"Before a release an extensive amount of testing is done that usually involves running test like continuous ingest and random walk on a cluster for extended periods of time.  When bugs are found by these test it can take a lot of time to track the issue down sometimes.  Inorder to make tracking these issues down easier the write ahead logs are archived.  These walogs archives make it possible to answer questions about a tablets history because everything ever written to the metadata table is there.  It would be nice to always have this capability on an accumulo system, and have it be easy to use.  Spelunking around in the write ahead logs is not an easy task.  

It would be nice if accumulo could answer questions like the following. 
 
 * Where has a tablet been assigned
 * What compactions has a tablet done
 * What split or merge created a tablet

These questions can currently be answered with walogs and log4j logs, but its painful.",Create infrastructure that supports debugging.,4,,,kturner,True,,kturner
accumulo,ACCUMULO-532,2012-04-13T07:25:16.000+0000,2013-04-18T01:26:58.000+0000,2013-04-18T01:26:58.000+0000,,Fixed,New Feature,Major,,,,,,,,,['contrib'],['add-on projects'],I've just wrote basic BSP input formats and its unit tests.,Add BSP input/output formats to client package,6,,,udanax,True,udanax,udanax
accumulo,ACCUMULO-508,2012-04-02T14:49:10.000+0000,2019-01-06T01:15:47.000+0000,2019-01-06T01:15:47.000+0000,,Not A Problem,New Feature,Major,,,,,,,,,['client'],['client code'],"Maybe for 1.4.1.

Our current input format will always apply one range (potentially split at tablet boundaries) per mapper. This is great for situations where you have a few larger ranges. However, there is a potential use case for many small ranges. Aside from the problem with a large job configuration (ACCUMULO-507), this will result in a LOT of mappers doing very little work. We should have an expanded input format which will bundle ranges together to a single mapper, ideally while trying to maintain locality. This will optimize jobs with a lot of ranges by reducing the amount of mapper overhead involved. I think very little will change with the RecordReader. The onus should still go to the end user to detect when a range change has been made (via Key change), so it will still emit Key/Value pairs, just like the regular input format.

This could possibly be extended to the whole row input format as well.",Multi-range input format,3,,"['mapreduce', 'newbie']",jvines,True,,vines
accumulo,ACCUMULO-492,2012-03-23T16:06:08.000+0000,2012-05-02T15:27:51.000+0000,2014-04-25T18:04:12.000+0000,,Invalid,New Feature,Major,,,,,,,,,['client'],['client code'],"Currently the only way to gather stats is scraping the monitor page or parsing it's XML. There may be times you would want to fine tune things in your task based on the number of tservers, etc. We should provide an API mechanism to the client to gather various system stats (with appropriate permissions of course).",Provide method for gathering system stats to API,,,,jvines,True,,vines
accumulo,ACCUMULO-490,2012-03-22T23:46:41.000+0000,2014-04-23T16:34:37.000+0000,2014-04-23T16:34:37.000+0000,,Won't Fix,New Feature,Minor,,,,,,,,,,,"This web service will:

 - provide cross language compatibility.
 - provide data access without direct access to nodes.
 - provide work-around for port restrictions.",Provide REST-ful Web Service,3,,,medined,True,,medined
accumulo,ACCUMULO-483,2012-03-22T14:48:02.000+0000,,2012-12-18T18:38:27.000+0000,,,New Feature,Minor,,,,,,,,,['client'],['client code'],"For some high capacity ingest, the desired path is to do some pre-splits, bulk import, and then let it naturally split down the rest of the way. If all of the pre-split tablets made split evenly, then the system will have continuous ranges bundled together on tservers. This poor distribution can impact performance depending on the operations performed. This could be handled in the load balancer, but it could be tricky. You don't want to randomly reassign tablets with any sort of frequency. Rather, you want to do a one-time operation in doing so. Given the initial assignment code is a bit random (needs to be validated), this could easily be done by offlining a table, purging all location records for it from the !METADATA table, and bringing it back online. The balancer will assign the table randomly, at which point the user could force a major compaction to restablish locality (as well some permanence in tablet assignment).",Create purge locality utility,2,,['newbie'],jvines,True,,vines
accumulo,ACCUMULO-482,2012-03-22T05:40:53.000+0000,2013-02-26T20:30:43.000+0000,2014-04-21T20:48:55.000+0000,,Fixed,New Feature,Major,['1.5.0'],,,,,,,,,,Add a thrift proxy server to make integration with other languages besides Java a bit easier.  This should work like http://wiki.apache.org/hadoop/Hbase/ThriftApi.,Add a thrift proxy server,12,,,sapanbshah42,True,cmccubbin,sapanbshah42
accumulo,ACCUMULO-456,2012-03-10T01:20:15.000+0000,2012-10-09T20:40:30.000+0000,2014-06-02T19:19:29.000+0000,,Fixed,New Feature,Major,['1.5.0'],,,,,,,,,,"Need a utility to to export and import tables.  A use case would be export table on cluster A, distcp to cluter B, import.  ",Need utility for exporting and importing tables,7,,,kturner,True,kturner,kturner
accumulo,ACCUMULO-454,2012-03-09T17:09:27.000+0000,2012-05-30T20:28:55.000+0000,2014-02-20T23:26:10.000+0000,,Duplicate,New Feature,Major,,,,,,,,,['client'],['client code'],"We currently provide InputFormats for reading from Accumulo and output formats for both direct input as well as outputting RFiles. But we provide no mechanism for doing a mapreduce over existing RFiles, which may be useful for optimizing data flow. We already have input formats which use RFiles directly for input (The offline input format Keith just finished), but that still relies on the Accumulo structure. We should go ahead and also create an input format that just hits RFiles like the other standard file input formats.",RFile Input Format,2,,,jvines,True,,vines
accumulo,ACCUMULO-452,2012-03-08T03:32:47.000+0000,2019-04-22T20:04:56.000+0000,2019-04-22T20:04:56.000+0000,,Not A Problem,New Feature,Major,,,,,,,,,,,"Locality groups are a neat feature, but there is no reason to limit partitioning to column families.  Data could be partitioned based on any criteria.  For example if a user is interested in querying recent data and ageing off old data partitioning locality groups based in timestamp would be useful.  This could be accomplished by letting users specify a partitioner plugin that is used at compaction and scan time.  Scans would need an ability to pass options to the partitioner.

",Generalize locality groups,1,,,kturner,True,,kturner
accumulo,ACCUMULO-448,2012-03-06T21:06:09.000+0000,2014-06-10T16:06:19.000+0000,2014-06-10T16:06:19.000+0000,,Won't Fix,New Feature,Minor,,,,,,,,,['monitor'],['web page monitor'],"Currently, as you make the nodes larger they appear to be moving slower because their diameters are increasing at the same rate.  Instead, the period could be fixed so that the nodes take the same amount of time to move from their smallest diameter to their largest diameter.",Experiment with standardizing the period of oscillation for server activity,,,['visualization'],billie.rinaldi,True,billie.rinaldi,billie.rinaldi
accumulo,ACCUMULO-431,2012-02-27T23:30:10.000+0000,2012-03-05T18:59:08.000+0000,2013-01-29T20:59:32.000+0000,,Fixed,New Feature,Major,['1.4.0'],,,,,,,,['monitor'],['web page monitor'],"A qualitative visual display can be useful for giving an overall impression of cluster status, and this won't be too hard to add to the existing Accumulo monitor.",Add server status visualization to the monitor,,,['visualization'],billie.rinaldi,True,billie.rinaldi,billie.rinaldi
accumulo,ACCUMULO-420,2012-02-21T15:56:45.000+0000,2013-01-26T03:55:29.000+0000,2015-03-13T15:17:08.000+0000,,Fixed,New Feature,Major,['1.5.0'],,,,,,,,,,"It may be useful to allow the compact command to specify an iterator to be used for that compaction.  For example if someone wanted to apply a filter once to a table, they could force a compaction with that filter.",Allow per compaction iterator settings,3,,,kturner,True,kturner,kturner
accumulo,ACCUMULO-418,2012-02-18T17:45:30.000+0000,2019-04-22T19:55:22.000+0000,2019-04-22T19:55:22.000+0000,,Won't Fix,New Feature,Major,,,,,259200,259200,259200,,"['master', 'tserver']","['master', 'tablet server']","There are times when iterating over RFiles is useful in map-reduce jobs.  I know that RFiles logically can be split on the block boundary, however there is no easy way to do this currently as there is no RFile RecordReader or InputFormat provided.",Make RFiles splittable,2,1,"['RFile', 'hadoop', 'mapreduce']",ivan.bella,True,,ivan.bella
accumulo,ACCUMULO-404,2012-02-15T16:53:20.000+0000,2012-05-01T20:50:57.000+0000,2012-12-18T18:39:49.000+0000,,Fixed,New Feature,Major,"['1.4.1', '1.5.0']",,,,,,,,,,"Hadoop 0.20.20x, 1.0.x and 0.23.x all support requiring kerberos for strong authentication in order to talk to HDFS. It would be useful if Accumulo could be configured with keytab files for the TabletServers, Master, etc. so that it can be run on a Kerberos-enabled cluster.",Support running on-top of Kerberos-enabled HDFS,3,,,fwiffo,True,,fwiffo
accumulo,ACCUMULO-403,2012-02-15T16:37:52.000+0000,2012-02-28T19:06:09.000+0000,2012-03-12T15:46:44.000+0000,,Fixed,New Feature,Major,['1.4.0'],,,,,,,,"['client', 'tserver']","['client code', 'tablet server']","The WholeRowIterator support filtering rows that meet a certain criteria.  However it reads the entire row into memory.  It is possible to efficiently select rows w/o reading them into memory by using two iterators.  One iterator for selection, one for reading.  When its determined that a row is not needed using the selection iterator, then seek the read iterator over the row.  

This pattern could be made into an easy to use iterator that users extend.  The iterator could have an abstract method that user implement to decide if they want to select or filter a row.  Could look something like the following.


{noformat}

class RowSelectionIterator extends WrappingIterator {

   public abstract boolean selectRow(SortedKeyValueIterator row);

}

{noformat}


Below is a simple example of a row selection iterator that returns rows that have the columns foo and bar.


{noformat}

class FooBarRowSelector extends  RowSelectionIterator {
   public boolean selectRow(SortedKeyValueIterator row){
      
      Text row = row.getTopKey().getRow();
      //seek instead of scanning, this more efficient for large rows w/ lots of columns... 
      //if the row only has a few columns scanning is probably faster... also seeking the 
      //columns in sorted order is more efficient.
      row.seek(Range.exact(row, 'bar');
      boolean sawBar = row.hasTop();

      if(!sawBar)
        return false;

      row.seek(Range.exact(row, 'foo'));
      boolean sawFoo = row.hasTop();

      return sawFoo;
   }
}

{noformat}",Create general row selection iterator,,,,kturner,True,kturner,kturner
accumulo,ACCUMULO-394,2012-02-13T17:27:33.000+0000,2019-04-22T20:01:31.000+0000,2019-04-22T20:01:31.000+0000,,Won't Fix,New Feature,Trivial,,,,,,,,,,,"Currently the combiner frameworks provides mechanisms for users to reduce the values of all Keys down to the column visibility label. I imagine there are cases where users want to combine all values for all visibilities of a column family, or even a row. Perhaps even just combine visibilities. Also, to extend combination to those Key elements instead of just values. There's a lot of ways to generalize this problem space, and I think it's something worth looking into. I think we should be able to use something similar to the current Combiner framework, except the reduce function's input Key is a partial key for the amount of uniqueness being saught. The majority of it should be straight forward, with the exception of visibilities which should be logically ANDed together (but this may be something we want to defer to the combiner's writer, I don't know).",Provide framework for combining multiple columns,1,,,jvines,True,,vines
accumulo,ACCUMULO-391,2012-02-13T01:58:21.000+0000,2013-10-17T01:55:51.000+0000,2013-10-24T23:54:09.000+0000,,Fixed,New Feature,Minor,['1.6.0'],,,,,,,,,,"Just realized we had no MR input method which supports multiple Tables for an input format. I would see it making the table the mapper's key and making the Key/Value a tuple, or alternatively have the Table/Key be the key tuple and stick with Values being the value.",Multi-table Accumulo input format,14,3,"['mapreduce,']",jvines,True,sonixbp,vines
accumulo,ACCUMULO-387,2012-02-10T16:40:43.000+0000,2012-03-07T22:19:27.000+0000,2012-03-07T22:19:27.000+0000,,Fixed,New Feature,Major,['1.4.0'],,,,,,,,,,Support map reduce jobs that directly read Accumulo files.,Support map reduce directly over files,,,,kturner,True,kturner,kturner
accumulo,ACCUMULO-378,2012-02-07T17:16:53.000+0000,2014-06-14T04:55:43.000+0000,2016-09-09T16:09:37.000+0000,,Fixed,New Feature,Minor,['1.7.0'],,600,4200,,,,100,['replication'],['replication'],"The use case here is where people have multiple data centers and need to replicate the data in between them.  Accumulo can model this replication after the way that HBase currently handles the replication as detailed here (http://hbase.apache.org/replication.html).  

There will be one master Cluster and multiple slave clusters.  Accumulo will use the Master-Push model to replicate the statements from the master clusters WAL to the various slaves WALs.",Multi data center replication,5,,,sapanbshah42,True,elserj,sapanbshah42
accumulo,ACCUMULO-359,2012-02-01T14:36:50.000+0000,,2014-04-22T18:11:42.000+0000,,,New Feature,Major,,,,,,,,,['test'],['system tests'],"We have many tests for validating functionality of the system, but we lack any standard tests for any form of benchmarking. We should create appropriate performance tests in order to test performance differences between versions, impacts from code changes, and for just generally charting our growth as a project.",Need to create automatic performance tests,2,,,jvines,True,,vines
accumulo,ACCUMULO-347,2012-01-25T12:23:08.000+0000,2012-01-25T14:29:18.000+0000,2014-04-25T18:04:16.000+0000,,Invalid,New Feature,Minor,,['1.4.0'],,,345600,345600,345600,,['contrib'],['add-on projects'],some more features should be added to screen.,some more features should be added to screen,,,['test'],srinu k,True,,srinu k
accumulo,ACCUMULO-312,2012-01-13T17:50:55.000+0000,2012-10-08T17:06:48.000+0000,2013-05-02T02:29:53.000+0000,,Duplicate,New Feature,Major,,,,,,,,,['scripts'],"['scripts for managing services, etc.']","The current start-up scripts use lists of hosts, and ssh to start and stop processes.  A better set of scripts would be installed with rpms/debs and would start processes using /etc/init.

 - turn on the box, the processes start and wait for hadoop/zooekeeper (done)
 - turn on a master, and the system should go into ONLINE mode
 - rpms/debs turn on/off services if they are installed
 - relocatable, and perhaps installed multiple locations/user ids?


 ",better scripts for start up,4,,,ecn,True,vines,ecn
accumulo,ACCUMULO-261,2012-01-05T19:58:48.000+0000,,2019-06-11T06:06:18.000+0000,,,New Feature,Major,,,,,,,,,['client'],['client code'],Currently the scanner allows a user to set batch size in numbers of entries. Unfortunately this isn't too useful if you have widely varied entry size and you want to keep your internal footprint within a threshold. So we should also allow users to set batch size in maximum number of bytes to bring back.,Scanner should support batch size specified in bytes,5,,,jvines,True,,vines
accumulo,ACCUMULO-254,2012-01-05T19:05:54.000+0000,2012-06-05T13:32:14.000+0000,2014-04-03T22:05:35.000+0000,,Won't Fix,New Feature,Major,,,,,,,,,"['logger', 'monitor']","['write-ahead log', 'web page monitor']",We should watch write-ahead log disk usage and free space.,loggers could be monitored better,2,,,ecn,True,ecn,ecn
accumulo,ACCUMULO-238,2011-12-30T15:58:59.000+0000,2017-11-14T21:20:54.000+0000,2017-11-14T21:20:54.000+0000,,Won't Fix,New Feature,Major,,,,,,,,,['monitor'],['web page monitor'],"ACCUMULO-196 discusses adding more command and control functionality to the monitor page. To properly secure these features in our customers' environments, we will need to be able to interface with different authorization providers. Enter pluggable authorization provider interface, stage left.",support pluggable authorization providers for the monitor,2,,,afuchs,True,afuchs,afuchs
accumulo,ACCUMULO-220,2011-12-14T17:19:46.000+0000,2012-05-02T13:25:17.000+0000,2012-12-18T18:38:24.000+0000,,Invalid,New Feature,Major,,,,,,,,,"['gc', 'logger', 'master', 'monitor', 'tserver']","['garbage collector', 'write-ahead log', 'master', 'web page monitor', 'tablet server']","I have strong reason to believe that the mechanism we have in place for grabbing the IP address for a hostname does not work for hostnames defined in the hosts file. That is, I believe it will only hit against DNS for doing the name->IP lookup for the provided server name. This needs to be tested and, if validated, updated to check both DNS and the hosts file.",Update mechanism for services to define which IP address to use,,,"['connection', 'network']",jvines,True,ecn,vines
accumulo,ACCUMULO-196,2011-11-30T19:06:32.000+0000,2013-04-17T16:09:07.000+0000,2013-04-17T20:50:18.000+0000,,Fixed,New Feature,Minor,['1.5.0'],,,,,,,,['monitor'],['web page monitor'],"Spinning off of ACCUMULO-123, the premise here is the ability to have a monitor page that is interactive. This originally came to me with the ability for a user to authenticate themselves with it and then make system/table level configuration changes via the monitor instead of having them go through the shell. Essentially, make use of the monitor to provide a GUI for real-time configuration changes. Initial steps should be for displaying current configuration information and then steps could be made to provide user level access. This may go so far as providing a web interfaced Accumulo shell. But for now this is a bit of a toy idea.",Add command and control features to the Monitor,6,,"['interface', 'monitor', 'shell', 'user']",jvines,True,ctubbsii,vines
accumulo,ACCUMULO-132,2011-11-10T15:27:43.000+0000,2014-12-17T04:06:18.000+0000,2014-12-17T04:06:18.000+0000,,Later,New Feature,Minor,,,,,,,,,['client'],['client code'],"An accumulo user has a need to perform a map/reduce over an entire table.   Right now, if a tablet server is running on a machine that has become slow due to failing hardware (it happens... a lot), the mapper using that server also runs slowly.  If we allow speculative execution, the second map just goes back to the same tablet server, doubling the load on our already slow server.  However, the data behind the tablet server is sitting there in multiple locations in HDFS.  The user can live without pulling the most recent updates in the tablet server's memory.

Possible solutions:
 # provide read-only hosting of a tablet from another node, possibly on demand
 # convenient way of re-creating the tablet server iterator stack and reading the files directly
 # clone the table/tablet (has the benefit that a reference is maintained in the metadata table)
",read-only access to existing tables for speculative execution,2,,,ecn,True,,ecn
accumulo,ACCUMULO-49,2011-10-19T16:31:36.000+0000,2012-05-02T15:16:00.000+0000,2012-05-02T15:16:00.000+0000,,Fixed,New Feature,Minor,"['1.4.1', '1.5.0']",,,,14400,14400,14400,,"['logger', 'master', 'trace', 'tserver']","['write-ahead log', 'master', 'distributed tracing', 'tablet server']",Linux kernel will eagerly swap idle memory (such as the tablet server) for disk cache unless the /proc/sys/vm/swappiness setting is set to 0. A swapped-out server is sluggish enough that it loses its zookeeper lock.,optionally monitor swappiness on every server,2,,,ecn,True,ecn,ecn
accumulo,ACCUMULO-47,2011-10-18T20:29:52.000+0000,2011-10-20T21:29:16.000+0000,2011-11-18T15:10:43.000+0000,,Fixed,New Feature,Blocker,['1.3.5-incubating'],,,,172800,172800,172800,,,,"There are class names stored in bulk-imported old map files, and walog files contain the old package name.  There are class names for iterators and extensions in zookeeper.  It would be ideal to move things around in hdfs and zookeeper, too.
",create a script to allow current users to upgrade to the new name,,,,ecn,True,ecn,ecn
accumulo,ACCUMULO-19,2011-10-11T05:44:00.000+0000,2012-02-28T15:10:16.000+0000,2019-04-23T16:21:13.000+0000,,Fixed,New Feature,Minor,['1.4.0'],['1.4.0'],,,,,,,['build'],"['tarball, packaging, and the Maven build']","This ticket is for improving Accumulo ease-of-installation by adding built-in support for debian packing. 

Debian pkg support requires:
1. Creating the deb package lifecycle hook scripts, such as 'preinst' and 'postinst'
2. Creating an init.d script (one of these already exists in bin/)
3. Integrating deb construction into the build lifecycle.  There are some mvn dpkg plugins, but Im not sure how well these work.  It might just be easier to run the deb construction as a delegated ant task (via the maven's antrun plugin)

Outline of required files: Putting all related packing files under src/packages, which is consistent with other Apache projects, such as zookeeper.
The following dpkg hooks go in src/packages/deb/accumulo.control/:
conffile, control, postinst, postrm, preinst, prerm
An init.d script (which I think can just be a copy of bin/etc_initd_accumulo) goes in src/packages/init.d:
",Debian packaging support,5,3,,skuehn,True,,skuehn
ambari,AMBARI-25208,2019-03-23T14:51:45.000+0000,,2019-07-10T06:56:32.000+0000,,,New Feature,Critical,"['trunk', '2.7.4']","['trunk', '2.7.4']",9000,9000,,,,100,['ambari-server'],['Ambari Server'],"Ambari should provide an API level implementation to enable replication between 2 HBase clusters.

In HBase, cluster replication refers to keeping one cluster state synchronized with that of another cluster, using the write-ahead log (WAL) of the source cluster to propagate the changes. Replication is enabled at column family granularity.

As part of this JIRA, we can provide functionality to add/remove peer cluster keys to existing HBase cluster.

 ",Provide a way to setup HBase Replication,1,,['pull-request-available'],vjasani,True,vjasani,vjasani
ambari,AMBARI-25193,2019-03-14T20:12:02.000+0000,,2019-03-15T03:45:37.000+0000,,,New Feature,Minor,,['2.7.3'],,,,,,,['ambari-views'],['Ambari Views framework and Views themselves.  Specific Views that are built on the framework will be handled with labels.'],"From hadoop 3.1.2  Administrators could specify absolute resources to a queue instead of providing percentage based values. This provides better control for admins to configure required amount of resources for a given queue.

But *capacity scheduler view* does not have the option to specify absolute resources to a queue instead of providing percentage based values.

We need to directly login to Ambari>>services > Yarn > configs , and need to change below values 

yarn.scheduler.capacity.<queue-path>.capacity

yarn.scheduler.capacity.<queue-path>.maximum-capacity

 

Below link will be helpful to understand the function of this properties

https://hadoop.apache.org/docs/r3.1.2/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html

This feature should be implemented in Capacity Scheduler View
",Configure queue capacities on absolute value basis in capacity scheduler view,1,,,ashelke,True,,ashelke
ambari,AMBARI-25053,2018-12-19T09:50:32.000+0000,2019-01-24T14:06:40.000+0000,2019-02-26T17:53:47.000+0000,,Done,New Feature,Major,['2.8.0'],,1200,1200,,,,100,['ambari-server'],['Ambari Server'],Verify that SQL schema creation scripts are valid using different DBs in Docker container.,Validate SQL schema creation scripts,2,,['pull-request-available'],adoroszlai,True,adoroszlai,adoroszlai
ambari,AMBARI-24769,2018-10-12T16:51:21.000+0000,,2018-10-12T16:51:21.000+0000,,,New Feature,Major,,,,,,,,,['ambari-server'],['Ambari Server'],"As an advanced user I want to be able to easily add a service to an existing cluster using the API by simply specifying the service name, master/slave placement, and a subset of configuration that I want to override the defaults.",Add Service easily using the Ambari API's,2,1,,paulcodding,True,,paulcodding
ambari,AMBARI-24748,2018-10-08T19:51:14.000+0000,,2019-01-10T10:43:35.000+0000,,,New Feature,Major,,,,,,,,,['ambari-server'],['Ambari Server'],"Feature use :

 

Zookeeper Observers are non-voting members of an ensemble which only hear the results of votes, not the agreement protocol that leads up to them.

When zookeeper usage increases (ex: heavy Kafka or HBase usage ), we can leverage Observers to “sanctify” / isolate the Quorum and front client connection using Observers.

 

Issue with current implementation :

The current handling of zookeeper configuration in Ambari essentially prevents the use of the features.

As is, Observers cannot properly configured using Ambari. While we setup using manual configurations to enable Observers, they are however overwritten by Ambari at every service restart.

 

Explanation :

As per, https://zookeeper.apache.org/doc/r3.4.6/zookeeperObservers.html, Observers require 2 changes in the zookeeper config file(zoo.cfg).

 

Every node acting as an Observer, must include the following property:

 

peerType=observer

 

Secondly, all nodes of the ensemble acting as an observer must have “:observer” appended to the server definition line of members.

For example:

 

server.1=host1.example.com:2888:3888

server.2=host2.example.com:2888:3888

server.3=host3.example.com:2888:3888

server.4=host4.example.com:2888:3888:observer

 

This tells every member of the ensemble that server.4 is an Observer.

 

From Ambari’s perspective :

The first configuration « peerType = observer » can be easily handled using a specific Ambari configuration group that includes an extra entry in the “custom zoo.cfg” tab of configurations

 

The second configuration is more problematic :

The server list is automatically generated by ambari using the list of node for which a zookeeper service has been installed.

 

=> It cannot be modified manually.

To get around that we have tried to override the property “server.4” by including it in the “custom zoo.cfg” tab.

 

=> this essentially created a second line of the server4 property with the correct property ( dirty but… )

 

For example :

 

server.4=host4.example.com:2888:3888:observer

…

server.1=host1.example.com:2888:3888

server.2=host2.example.com:2888:3888

server.3=host3.example.com:2888:3888

server.4=host4.example.com:2888:3888

 

Note that custom configuration seem to be systematically placed at the beginning of the file.

 

Unfortunately only the last version of the property is actually retained. Ie. In the above example “server.4=host4.example.com:2888:3888:observer” is ignored, making it impossible to properly set configurations for the ensemble

 

Request :

The target would be to make Observers a specific feature of Ambari Zookeeper configurations.

 

As a workaround, would it be possible to modify the way Ambari generates the list of servers in zoo.cfg ?

Here are a few ideas :

Option 1 : Replace automatically generated property and only retain value defined in the custom property tab

Option 2 : Systematically place custom properties at the bottom of the file

Option 3 : Comment the automatically generated property when a custom property of a different value exists ?",Option to configure Zookeeper Observers via Ambari,2,1,,graj,True,,graj
ambari,AMBARI-24710,2018-09-28T13:50:37.000+0000,,2018-09-29T11:30:14.000+0000,,,New Feature,Critical,,,,,,,,,['ambari-views'],['Ambari Views framework and Views themselves.  Specific Views that are built on the framework will be handled with labels.'],"*Challenge faced :*

_{color:#d04437}In the WFM view of ambari, there is a Filter option available to search for the Workflows, bundles or coordinators but  could not perform a regex based search. In order to search for the desired workflow, one has to type the full name of workflow, coordinators, and bundles else it does not return anything which becomes a time-consuming job.{color}_

*Feature description:*

_{color:#14892c}There is a need for Regex based filter option in order to search the workflows without the need of entering the complete name of the workflow{color:#14892c} or coordinator or bundles{color} rather by just typing the first three letters of the workflow or coordinator or bundles post which it should populate suggestions based on the first three letters of the workflow, coordinator, and bundles through which we can refine and optimize the searching mechanism.{color}_",Regex based search option for searching workflows in the WFM-View of Ambari,1,,['usability'],Krishna93,True,,Krishna93
ambari,AMBARI-24469,2018-08-13T20:48:55.000+0000,,2018-08-13T20:48:55.000+0000,,,New Feature,Major,['3.0.0'],['3.0.0'],,,,,,,['ambari-server'],['Ambari Server'],"AMBARI-24446 removed all of the stacks which were checked into Apache's codebase. This included the HDP stack. As of Ambari 3.0, no stacks will ship with Ambari, which means that all of the stacks (and the stack test code) has to be removed.

For users of Ambari which will be providing their own stacks, such as HDP 3.0, they will need a way to consume the Ambari Python libraries and run their own tests against their stack.
 - {{RMFTestCase}} must be refactored to allow for invokers to specify paths to their custom stacks
 - The test framework and common python library must be exposed in a consumable package for testing
 ",Allow For Pluggable Stack Tests,1,,,sduan,True,sduan,sduan
ambari,AMBARI-24468,2018-08-13T20:48:01.000+0000,2018-08-23T18:25:07.000+0000,2018-08-23T18:25:07.000+0000,,Fixed,New Feature,Major,['3.0.0'],['3.0.0'],5400,5400,,,,100,['ambari-server'],['Ambari Server'],These two modules should help retrieve clusterSettings and stackSettings configuration from command.json,Add ClusterSettings and StackSettings module in execution_command library,1,,['pull-request-available'],sduan,True,sduan,sduan
ambari,AMBARI-24387,2018-07-31T10:56:32.000+0000,2018-08-08T09:35:03.000+0000,2018-08-08T09:39:38.000+0000,,Fixed,New Feature,Major,"['trunk', '2.7.1']",['2.7.0'],9000,9000,,,,100,['ambari-views'],['Ambari Views framework and Views themselves.  Specific Views that are built on the framework will be handled with labels.'],"The CapacityScheduler supports the following parameters to lifetime of an application:

At high level, we need a mechanism to set timeout values for each leaf queue from Capacity Scheduler view. The sample configuration that should reflected in backend is


{code:java}
yarn.scheduler.capacity.root.default.maximum-application-lifetime=<values in seconds>
yarn.scheduler.capacity.root.default.default-application-lifetime=<values in seconds>
{code}


Capacity Scheduler view should support following tags.

description of this tags can be found in : https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html
",Support YARN Application timeout feature in Ambari Capacity Scheduler View,2,,['pull-request-available'],akhilsnaik,True,akhilsnaik,akhilsnaik
ambari,AMBARI-24376,2018-07-30T00:04:38.000+0000,2018-07-30T00:09:41.000+0000,2018-07-30T00:09:41.000+0000,,Invalid,New Feature,Major,,"['3.0.0', 'trunk, 2.6.2', '2.7.1']",,,,,,,"['ambari-admin', 'ambari-client', 'ambari-server', 'ambari-shell', 'ambari-views', 'ambari-web', 'security']","['Ambari Admin', 'Ambari Client (Python, Groovy, and others)', 'Ambari Server', 'Ambari Shell', 'Ambari Views framework and Views themselves.  Specific Views that are built on the framework will be handled with labels.', 'Ambari Web UI', 'Ambari security features, including Kerberos']","Please add OWASP Dependency Check to the build (pom.xml) of the core components and all sub-components. OWASP DC makes an outbound REST call to MITRE Common Vulnerabilities & Exposures (CVE) to perform a lookup for each dependant .jar to list any/all known vulnerabilities for each jar. This step is needed because a manual MITRE CVE lookup/check on the main component does not include checking for vulnerabilities in components or in dependant libraries.

OWASP Dependency check : https://www.owasp.org/index.php/OWASP_Dependency_Check has plug-ins for most Java build/make types (ant, maven, ivy, gradle).

Also, add the appropriate command to the nightly build to generate a report of all known vulnerabilities in any/all third party libraries/dependencies that get pulled in. example : mvn -Powasp -Dtest=false -DfailIfNoTests=false clean aggregate

Generating this report nightly/weekly will help inform the project's development team if any dependant libraries have a reported known vulnerailities. Project teams that keep up with removing vulnerabilities on a weekly basis will help protect businesses that rely on these open source componets.",Please add OWASP Dependency Check to the build (pom.xml),1,,"['build', 'easyfix', 'security']",ABakerIII,True,,ABakerIII
ambari,AMBARI-24373,2018-07-27T16:42:03.000+0000,,2018-07-27T16:45:05.000+0000,,,New Feature,Major,,"['2.5.2', '2.6.0']",,,,,,,['ambari-server'],['Ambari Server'],"Request for a method to compare cluster configurations across different clusters managed by different Ambari servers.

This is a request that has come up repeatedly from different Hortonworks customers.

I don't think Blueprints dumping and diffing is well suited to this.

When Ambari supports managing multiple clusters from a single Ambari server then maybe it'll be possible to compare two clusters within the single Ambari instance, but I think there should be a more generic method of doing this across different Ambari instances as well because some customers will almost certainly still want to have separate Ambari instances managing different clusters for reasons of segregation, network isolation between different environments (test/staging/production), or segregation of duties / teams - but they'll still want to be able to audit standard settings and changes have been applied across environments.",Ambari Config comparison across clusters,1,,,harisekhon,True,,harisekhon
ambari,AMBARI-24370,2018-07-27T10:23:26.000+0000,,2018-08-07T08:45:57.000+0000,,,New Feature,Major,,"['3.0.0', '2.5.2', '2.7.0']",,,,,,,['ambari-server'],['Ambari Server'],"Config Group overrides are unable to remove a setting from being set at all in existing default config.

For example, setting bucket cache on HBase default config group will break HMasters so if you put the HMasters in a separate config group you are unable to remove the settings (setting to zero or none still breaks HMasters, you need to completely unset the config from existing).

Instead you have to move all the RegionServers to another config group, leave the default config group without the setting and then set the config only as an override such that it doesn't exist in default group that HMasters use.",Config Group - unable to remove config setting,2,,,harisekhon,True,,harisekhon
ambari,AMBARI-24306,2018-07-19T10:05:30.000+0000,,2018-07-19T10:36:31.000+0000,,,New Feature,Major,,,,,,,,,"['ambari-metrics', 'metrics']","['Ambari Metrics System', '']","Feature Request to add Grafana graph of last value (not average please) LastGcInfo duration for all 3 major garbage collectors :
 * G1GC Young Gen
 * G1GC Old Generations
 * CMS
 * ParallelNew

CMS and ParNew example taken from NameNode JMX metrics:
{code:java}
  }, {
    ""name"" : ""java.lang:type=GarbageCollector,name=ConcurrentMarkSweep"",
    ""modelerType"" : ""sun.management.GarbageCollectorImpl"",
    ""LastGcInfo"" : {
      ""GcThreadCount"" : 11,
      ""duration"" : 5206,
...
  }, {
    ""name"" : ""java.lang:type=GarbageCollector,name=ParNew"",
    ""modelerType"" : ""sun.management.GarbageCollectorImpl"",
    ""LastGcInfo"" : {
      ""GcThreadCount"" : 11,
      ""duration"" : 6,
 {code}
G1GC Young and Old Gen example taken from RegionServer JMX metrics:
{code:java}
  }, {
    ""name"" : ""java.lang:type=GarbageCollector,name=G1 Young Generation"",
    ""modelerType"" : ""sun.management.GarbageCollectorImpl"",
    ""LastGcInfo"" : {
      ""GcThreadCount"" : 24,
      ""duration"" : 120,
{code}
{code:java}
  }, {
    ""name"" : ""java.lang:type=GarbageCollector,name=G1 Old Generation"",
    ""modelerType"" : ""sun.management.GarbageCollectorImpl"",
    ""LastGcInfo"" : {
      ""GcThreadCount"" : 24,
      ""duration"" : 19641,
{code}
Yes this old gen GC is atrocious which is why I'm here to tune this, but it helps if this stuff is monitored properly in the first place to know there is a problem without waiting until there are random RegionServer deaths due to long GC pauses.

Right now Ambari's Grafana has GCTimeMillis which would make one think this is not a problem as it only shows an averaged out 40ms per sec of GC time which isn't very helpful to spotting this long GC pause problem.","Ambari Metrics + Grafana - add LastGcInfo duration graphs for all server components for all GCs - G1GC Young + Old Gens, CMS and ParallelNew",1,,,harisekhon,True,,harisekhon
ambari,AMBARI-24305,2018-07-19T09:43:45.000+0000,,2018-07-19T09:43:45.000+0000,,,New Feature,Major,,['2.5.2'],,,,,,,"['ambari-metrics', 'metrics']","['Ambari Metrics System', '']","Feature Request to add Grafana graphs of staticIndexSize for all HBase RegionServers as this is helpful for performance tuning calculations as we want to see the usage over the last month for heap sizing calculations.

Right now I'm assuming this doesn't fluctuate much and just using the current figures scraped from JMX across all RegionServers.",Ambari Metrics + Grafana - add HBase staticIndexSize graph for all RegionServers,1,,,harisekhon,True,,harisekhon
ambari,AMBARI-24301,2018-07-18T09:35:46.000+0000,,2018-07-19T10:26:38.000+0000,,,New Feature,Major,,['2.6.0'],,,,,,,"['ambari-admin', 'ambari-server', 'rolling-upgrade']","['Ambari Admin', 'Ambari Server', '']","HDFS Upgrade Domains is configured in a static JSON file.

Feature Request to add an Ambari API to be able to manage placement policy dynamically as the JSON file does not lend itself to scripted automation adding of datanodes. If needed, generate the JSON file (or otherwise press the HDFS developers to improve the design and then use their API).

[http://hadoop.apache.org/docs/r3.1.0/hadoop-project-dist/hadoop-hdfs/HdfsUpgradeDomain.html]

 ",API for HDFS Upgrade Domains management for scripted node additions,1,,,harisekhon,True,,harisekhon
ambari,AMBARI-24294,2018-07-17T10:11:51.000+0000,,2018-07-20T08:58:49.000+0000,,,New Feature,Major,,['2.5.2'],,,,,,,"['ambari-metrics', 'metrics']","['Ambari Metrics System', '']","Add Prometheus metrics integration by exposing all metrics via a new /metrics endpoint in Prometheus expected format.

[https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config]

Prometheus metrics format for that endpoint:

[https://github.com/prometheus/docs/blob/master/content/docs/instrumenting/exposition_formats.md]

 

 ",Prometheus /metric endpoint to expose Ambari Metrics Service for Prometheus to use as a scrape target,1,,,harisekhon,True,,harisekhon
ambari,AMBARI-24196,2018-06-26T20:39:31.000+0000,2018-06-29T19:15:52.000+0000,2018-06-29T19:15:52.000+0000,,Fixed,New Feature,Major,['3.0.0'],['3.0.0'],14400,14400,,,,100,['ambari-server'],['Ambari Server'],"The Mpack Instance manage is producing JSON output which cannot be parsed by non-Python parsers:
- Includes unicode markers for strings (like \{{u'ZOOKEEPER'}})
- Uses non-standard single quotes
{code}
[root@c7403 mpack-instance-manager]$ python mpack-instance-manager.py list-instances
{'mpacks': \{'hdpcore': {'mpack-instances': {'HDPCORE': {'name': 'HDPCORE', 'subgroups': {'default': {'modules': {'zookeeper': {'category': u'SERVER', 'name': 'zookeeper', 'components': {'zookeeper_server': {'component-instances': {'default': {'path': '/usr/hwx/mpacks/hdpcore/1.0.0-b450/zookeeper_server', 'name': 'default'}}}}}, 'zookeeper_clients': \{'category': u'CLIENT', 'name': 'zookeeper_clients', 'components': {'zookeeper_client': {'component-instances': {'default': {'path': '/usr/hwx/mpacks/hdpcore/1.0.0-b450/zookeeper_client', 'name': 'default'}}}}}}}}}}}}}
{code}

It also fails to provide information about the mpack version and component version. The output from the command \{{list-instances}} needs to provide both the mpack version and the module component version. Otherwise, this data cannot be reported back.

If the \{{list-instances}} is not the correct command for this, then that's fine, but we still need a command we can invoke to get this information back in a structure manner.",Mpack Instance Manager Produces Bad JSON and Doesn't List Versions,1,,['pull-request-available'],sduan,True,sduan,sduan
ambari,AMBARI-24014,2018-06-01T22:42:19.000+0000,,2018-06-01T23:54:52.000+0000,,,New Feature,Major,['3.0.0'],['3.0.0'],1800,1800,,,,100,['ambari-server'],['Ambari Server'],,Python Mpack Advisor should return MpackInstance block during Host Component Layout Recommendation,1,,['pull-request-available'],sduan,True,sduan,sduan
ambari,AMBARI-23993,2018-05-31T06:41:12.000+0000,,2018-06-01T02:25:26.000+0000,,,New Feature,Major,['3.0.0'],['3.0.0'],7800,7800,,,,100,['ambari-server'],['Ambari Server'],,Mpack Instance Manager should create pid dir and log dir for each instance,1,,['pull-request-available'],sduan,True,sduan,sduan
ambari,AMBARI-23962,2018-05-28T19:49:22.000+0000,2018-05-31T17:05:17.000+0000,2018-05-31T17:05:17.000+0000,,Fixed,New Feature,Major,,['2.7.0'],7800,7800,,,,100,['ambari-server'],['Ambari Server'],"CredentialApi.jar includes features:
 # Create Alias and password
 # Get Password
 # Delete Alias
 # List Alias

However, the python wrapper only has the get password feature. 

 

This adds all of the above features and be used by other python scripts",Improvements for credential_store_helper to include all features,2,,['pull-request-available'],Amer Issa,True,,Amer Issa
ambari,AMBARI-23858,2018-05-15T23:54:10.000+0000,,2018-05-15T23:58:05.000+0000,,,New Feature,Minor,,,,,,,,,['ambari-metrics'],['Ambari Metrics System'],"Ambari metrics grafana datasource currently doesn't allow users to customize the time window for which the aggregator is applied.

This feature would allow have a separate text field in Ambari metrics grafana datasource that will allow users to override the default aggregation window i.e. group by time which is used for functions like avg, sum, min, max etc. E.g. if the user wants to granularity by the minute we can configure the time window for 1m and not loose resolution in the graph. 

This increases the number of datapoints rendered in grafana therefore it is an advanced feature since theoretically a user may trigger a query that slows the UI. ",Allow ability to select windowing for time window operators in AMS,1,,,ashar103,True,,ashar103
ambari,AMBARI-23619,2018-04-19T05:11:34.000+0000,2018-05-31T06:41:34.000+0000,2018-05-31T06:41:34.000+0000,,Fixed,New Feature,Major,['3.0.0'],['3.0.0'],9600,9600,,,,100,['ambari-server'],['Ambari Server'],,Mpack Advisor Python Module,1,,['pull-request-available'],sduan,True,sduan,sduan
ambari,AMBARI-23589,2018-04-16T17:18:45.000+0000,2018-04-20T13:20:21.000+0000,2018-04-20T13:20:21.000+0000,,Fixed,New Feature,Critical,['2.7.0'],['2.7.0'],,,,,,,['ambari-server'],['Ambari Server'],"*Usecase*
The Spark History Server should have authentication enabled out of the box. By default only authenticated Ambari Admin user should have access to SHS UI. Ambari Admin should be able to see history of all jobs. For others, the job submitter should see history of their own jobs and not anyone else's.

*TestCase*
* Verify that SHS enables Authentication OOB
* Enable Kerberos in the Cluster with Ambari, verify that SHS is still enabled for authentication
* Verify that Admin can see history of all jobs
* Verify a job submitter only sees history or their own jobs

Current Spark's Web UI only has SSL encryption, doesn't have mutual authentication. From my understanding it is necessary and valuable to add this support, both for live UI and history UI.",Enable Security and ACLs in History Server,2,,,vbrodetskyi,True,vbrodetskyi,vbrodetskyi
ambari,AMBARI-15000,2016-02-11T00:45:01.000+0000,,2016-02-11T00:45:01.000+0000,,,New Feature,Major,,,,,,,,,['ambari-server'],['Ambari Server'],"In the Spark component, add a ""Custom Spark-thrift-server"" section in the configs so that a user can set properties specifically for the Spark Thrift Server.",Spark Thrift Server custom properties,1,,,evanchen92,True,,evanchen92
ambari,AMBARI-14999,2016-02-11T00:03:22.000+0000,2016-02-16T20:16:52.000+0000,2016-02-17T01:21:43.000+0000,2016-02-12,Fixed,New Feature,Major,['2.2.2'],['2.2.2'],,,,,,,['ambari-metrics'],['Ambari Metrics System'],Allows for Templated Dashboards in Grafana to select multi-hosts in addition to the already existing single host option.,Multi-host option for Templated Dashboards,3,,,prajwal,True,prajwal,prajwal
ambari,AMBARI-14985,2016-02-10T00:17:11.000+0000,2016-02-10T18:06:58.000+0000,2016-02-10T21:30:42.000+0000,2016-02-12,Fixed,New Feature,Major,['2.2.2'],['2.2.2'],,,,,,,['ambari-metrics'],['Ambari Metrics System'],This patch allows creation of templated dashboard that allows viewing of dashboard graphs on a per host level.,Templated Dashboard for Ambari-Metrics-Grafana,3,,,prajwal,True,prajwal,prajwal
ambari,AMBARI-14947,2016-02-05T20:17:20.000+0000,,2016-02-05T20:37:29.000+0000,,,New Feature,Minor,,,,,,,,,['ambari-server'],['Ambari Server'],"It would be nice to have a feature where ambari can send alert after deploying the Hadoop cluster.
For example: After giving the cluster name at first step, we can add a page where users can provide an email address to get notified. ",send mail after cluster deployment is completed through Ambari,1,,"['ambari', 'email']",nsabharwal,True,,nsabharwal
ambari,AMBARI-14910,2016-02-03T22:44:47.000+0000,2016-02-05T21:25:37.000+0000,2016-07-14T18:16:56.000+0000,,Fixed,New Feature,Minor,['2.2.2'],,,,,,,,['ambari-web'],['Ambari Web UI'],"Scenario:
- Install Ambari
- Install a cluster with HDFS and HAWQ
- Enable NN HA

After enabling NN HA, HAWQ's hdfs-client.xml is not populated with the HA related parameters.

Ensure that HAWQ works after NN HA enabled on a cluster with HAWQ.",HAWQ hdfs-client.xml should be updated when NN HA in enabled,3,,,mithmatt,True,mithmatt,mithmatt
ambari,AMBARI-14909,2016-02-03T22:00:52.000+0000,2016-02-06T00:12:41.000+0000,2016-02-06T04:34:34.000+0000,,Fixed,New Feature,Major,"['2.3.0', '2.2.2']",['2.2.1'],,,,,,,,,"In the HAWQ add slave wizard, in step3, add a confirmation pop-up asking the user if the data directory has been cleaned up.",Add a clean data dir confirmation pop-up to HAWQ add slave wizard,3,,,nalex,True,nalex,nalex
ambari,AMBARI-14893,2016-02-03T01:35:15.000+0000,2016-02-04T19:07:06.000+0000,2016-02-05T02:34:38.000+0000,,Fixed,New Feature,Major,['2.2.2'],['2.2.2'],,,,,,,['ambari-metrics'],['Ambari Metrics System'],This is a Grafana datasource that allows visualization of metrics stored in Ambari Metrics Collector.,Add Grafana-based Ambari Metrics Dashboard Builder,3,,,u39kun,True,prajwal,u39kun
ambari,AMBARI-14855,2016-02-01T08:19:49.000+0000,2016-02-03T18:59:26.000+0000,2016-07-14T18:17:12.000+0000,,Fixed,New Feature,Minor,['2.2.2'],,,,,,,,['stacks'],['Ambari Stacks'],HAWQSTANDBY can go out of sync with HAWQMASTER. Show an alert when HAWQSTANDBY is not in sync with HAWQMASTER.,Add Alert for HAWQSTANDBY sync status with HAWQMASTER,3,,,mithmatt,True,mithmatt,mithmatt
ambari,AMBARI-14820,2016-01-27T23:26:40.000+0000,2016-02-11T00:46:01.000+0000,2016-07-14T18:16:00.000+0000,,Fixed,New Feature,Major,['2.2.2'],['trunk'],,,,,,,['stacks'],['Ambari Stacks'],"h1. Activate HAWQ Standby Wizard
h4. Abstract:
All configuration changes done on Ambari managed clusters must be done via Ambari, otherwise during restart of services, Ambari pushes the configuration stored in its database and overwrites the changes done manually.

Activation of HAWQ Standby to active master involves manual command execution which updates the configuration files. In order to keep the configuration in sync and reflect the correct state of the HAWQ components on Ambari, we need to introduce a wizard, i.e *“Activate HAWQ Standby Wizard”*. 

This wizard will also automate the process of activating standby and the user now will need to walk-through the wizard to activate standby. 

h4. Overview of activate standby operation:
* In HA configuration, HAWQ is configured with a master and standby component. 
* To activate / promote standby to a master, “hawq activate standby” command needs to be executed on the standby
* This operation updates the value of hawq_master_address_host and hawq_standby_address_host properties available in hawq-site.xml to below
** hawq_master_address_host : Initially Configured HAWQ Standby Host
** hawq_standby_address_host : This property is removed.
* After the operation is completed, HAWQ cluster will be running in non-ha mode and will  not have a configured standby.

h4. Overview of the wizard pages and operation performed:
The wizard consists of 3 pages, a brief of them is as below:

# “Get Started”: This page is the first page seen by the user, and it gives introduction of the activity going to be performed. 
# “Review”: This page is the second page which informs the user about the changes which will happen. A confirmation is requested before proceeding to the next step.
# “Configure Components”: This page is the third and last page which shows the progress for the steps executed. It does the below:
## Execute command to activate HAWQ standby
## Stop HAWQ service
## Reconfigure HAWQ to push updated configs
### hawq_standby_address_host is removed
### hawq_master_address_host is updated to previous standby
## Install HAWQ Master Ambari Component on the Standby Host
## Delete HAWQ Standby Ambari Component on the Standby Host
## Delete HAWQ Master Ambari Component on the Master host
## Start HAWQ Service
",Implement Activate HAWQ Standby wizard to allow doing failover in HA mode,5,,,bhuvnesh2703,True,bhuvnesh2703,bhuvnesh2703
ambari,AMBARI-14749,2016-01-20T22:31:18.000+0000,2016-01-27T19:59:51.000+0000,2016-07-14T18:30:19.000+0000,,Fixed,New Feature,Major,['2.2.0'],,,,,,,,['stacks'],['Ambari Stacks'],"This patch introduces another high availability wizard which adds HAWQ Standby component to the HAWQ cluster.

Why it is required?
Apache HAWQ database has the below components.
- Master
- Standby
- Segments

At any point of time, database connections are only accepted by the master and it is the single point of entry for read and write operations. In the background, there is a sync-up process which ensures that standby component is in sync with master, so that in case master goes down, HAWQ standby can be promoted to perform the role of master and there is minimum downtime.

However, in order to enable High Availability operation for HAWQ, HAWQ standby must be configured to the cluster and required configuration changes must be done to ensure that HAWQ Standby is operational and in sync.

After HAWQ Standby is added and if it has been promoted / activated to take the role of the HAWQ Master, the HAWQ cluster now operates in non-HA mode. The old HAWQ master is out of the cluster and cannot be transitioned to the role of Standby. In order to get back to HA state, a standby has to be added to the cluster.

In this patch, the functionality to add HAWQ Standby Master has been introduced which performs the following functions:
- Disable / Enable ""Add HAWQ Standby"" option depending on the state of the cluster. If HAWQ Standby is configured, this option is not visible. If HAWQ Standby is not configured and the cluster is not single node, the option is visible.
- When the action is triggered, ""Add HAWQ Standby"" wizard comes into role - - The wizard contains of 4 pages which does a dedicated job
- Page 1: It gives information to the user for the activities which will be carried out by the wizard
- Page 2: It lets' the user chose the host on which standby should be installed
- Page 3: Informs the user about the configuration changes which will take place based on the inputs on Page 2
- Page 4: Below list of operations are carried.
1. Stop HAWQ Service
2. Install HAWQ Standby Component
3. Reconfigure HAWQ Service to add required properties
4. Start HAWQ service. During this step, hawq standby is initialized and started.
- After the wizard completes, HAWQ standby is added to the cluster and now HAWQ is operational in HA mode.

",Introduce ADD HAWQ STANDBY wizard to enable high availability in HAWQ,3,,,bhuvnesh2703,True,bhuvnesh2703,bhuvnesh2703
ambari,AMBARI-14691,2016-01-15T13:42:51.000+0000,2016-01-18T10:50:42.000+0000,2016-01-18T12:53:31.000+0000,,Fixed,New Feature,Major,['2.4.0'],['2.4.0'],,,,,,,['ambari-web'],['Ambari Web UI'],Implement calls to server API. Clear removed service from model in ambari-web.,Service uninstall: Integration with API,3,,,atkach,True,atkach,atkach
ambari,AMBARI-14682,2016-01-15T10:06:55.000+0000,2016-01-15T12:46:57.000+0000,2016-03-26T00:44:59.000+0000,,Fixed,New Feature,Major,['2.4.0'],['2.4.0'],,,,,,,['ambari-web'],['Ambari Web UI'],"Add Remove button under Services > SERVICE_NAME > Service Actions menu > Delete
Add Confirmation popup:
Popup should show list of dependent services, that they will be removed as well, 
and all services: chosen and dependent should be stopped first before remove.",Service uninstall: add UI elements,3,,,atkach,True,atkach,atkach
ambari,AMBARI-14622,2016-01-12T02:31:55.000+0000,2016-01-13T02:07:17.000+0000,2016-02-24T19:53:31.000+0000,2016-01-12,Fixed,New Feature,Minor,,,,,,,,,,,,Stack Advisor should display warning when PXF component not collocated with NAMENODE and DATANODE,3,,,mithmatt,True,mithmatt,mithmatt
ambari,AMBARI-14613,2016-01-11T19:56:06.000+0000,2016-01-12T01:36:40.000+0000,2016-07-14T18:13:40.000+0000,2016-01-12,Fixed,New Feature,Major,['2.2.1'],,,,,,,,['ambari-server'],['Ambari Server'],"The following metrics have to be exposed for hosts which have HAWQSEGMENTs:
- CPU
- Network
- Disk",Metrics for HAWQ Service Dashboard,3,,,mithmatt,True,mithmatt,mithmatt
ambari,AMBARI-14533,2016-01-01T00:20:03.000+0000,2016-01-04T18:29:31.000+0000,2016-01-04T21:35:48.000+0000,,Fixed,New Feature,Minor,"['2.3.0', '2.2.1']",['2.2.1'],,,,,,,['ambari-server'],['Ambari Server'],User should be able to secure HAWQ after it has been deployed so that HAWQ continues to work as-is in security mode.,User should be able to secure hawq after it has been deployed,3,,,odiachenko,True,odiachenko,odiachenko
ambari,AMBARI-14528,2015-12-31T01:41:55.000+0000,2016-01-15T01:31:03.000+0000,2016-07-14T18:30:21.000+0000,,Fixed,New Feature,Major,['2.2.0'],['trunk'],,,,,,,['stacks'],['Ambari Stacks'],"Hawq yarn-client.xml HA parameters should be shown on UI if yarn rm ha is enabled.

Parameters to be added:
yarn.resourcemanager.ha
yarn.resourcemanager.scheduler.ha",Hawq yarn-client.xml HA parameters should be shown on UI if yarn rm ha is enabled.,4,,,bhuvnesh2703,True,bhuvnesh2703,bhuvnesh2703
ambari,AMBARI-14413,2015-12-17T03:11:09.000+0000,,2015-12-17T03:22:35.000+0000,,,New Feature,Minor,,['2.1.1'],,,,,,,['stacks'],['Ambari Stacks'],"ambari should config default users password in order to enable user can login in system.If every time use root,it is not safe.",config default users  password  in Ambari ,1,,,yandongrong,True,,yandongrong
ambari,AMBARI-14412,2015-12-17T01:37:32.000+0000,2015-12-30T19:41:51.000+0000,2016-07-14T18:29:51.000+0000,,Fixed,New Feature,Minor,['2.2.1'],['trunk'],,,,,,,['stacks'],['Ambari Stacks'],It is recommended that PXF component should be installed on the same hosts which has NAMENODE or/and DATANODE.,Recommend PXF component to be collocated with NAMENODE and DATANODE,4,1,,mithmatt,True,mithmatt,mithmatt
ambari,AMBARI-14353,2015-12-12T02:13:30.000+0000,2015-12-29T23:55:32.000+0000,2015-12-30T02:58:31.000+0000,,Fixed,New Feature,Major,"['2.3.0', '2.2.1']",['2.3.0'],,,,,,,['stacks'],['Ambari Stacks'],"Update command context for background message when custom command is executed for HAWQ.

For custom command: IMMEDIATE_STOP_CLUSTER, background box shows Execute IMMEDIATE_STOP_CLUSTER.

It should be user friendly message.",Update command context for background message when custom command is executed for HAWQ,4,,,bhuvnesh2703,True,bhuvnesh2703,bhuvnesh2703
ambari,AMBARI-14351,2015-12-12T00:44:18.000+0000,2016-01-07T01:49:08.000+0000,2016-01-07T01:49:08.000+0000,,Fixed,New Feature,Major,['2.2.1'],['trunk'],,,,,,,['stacks'],['Ambari Stacks'],"1. Add custom command for hawq master to allow stopping the cluster with immediate mode.
2. Add custom command for hawq segment to allow stopping the segments with immediate mode.
",Add custom action to stop hawq cluster and segments with immediate mode,4,,,bhuvnesh2703,True,bhuvnesh2703,bhuvnesh2703
ambari,AMBARI-14339,2015-12-11T00:46:12.000+0000,2015-12-24T22:19:43.000+0000,2015-12-25T01:21:04.000+0000,,Fixed,New Feature,Major,['2.2.1'],['2.1.1'],,,,,,,['ambari-server'],['Ambari Server'],"Ensure that stack advisor 
- recommends a non 5432 port when HAWQ master components are collocated with Ambari Server
- recommends a non root directory for all the data and temp directories or HAWQ",Stack advisor validations for HAWQ,3,,,nalex,True,nalex,nalex
ambari,AMBARI-14329,2015-12-10T19:07:41.000+0000,,2017-02-05T07:12:44.000+0000,,,New Feature,Major,,['2.2.1'],,,,,,,['2.1.3'],[''],Add Ambari For Power Linux。,Add Ambari For Power Linux ,4,,['PPC'],tongxiaojun,True,,tongxiaojun
ambari,AMBARI-14313,2015-12-10T01:56:09.000+0000,2015-12-29T20:42:10.000+0000,2016-07-14T18:29:49.000+0000,,Fixed,New Feature,Minor,['2.2.1'],['trunk'],,,,,,,['stacks'],['Ambari Stacks'],A user should be able to add hawq as a service to an existing HDFS secured cluster.,Allow hawq to be installed on a secured cluster,3,,,bhuvnesh2703,True,bhuvnesh2703,bhuvnesh2703
ambari,AMBARI-14284,2015-12-09T06:01:03.000+0000,2015-12-31T00:28:47.000+0000,2015-12-31T01:46:06.000+0000,,Fixed,New Feature,Minor,['2.2.1'],['trunk'],,,,,,,['stacks'],['Ambari Stacks'],"Push yarn.resourcemanager.ha and yarn.resourcemanager.scheduler.ha properties when RM HA is enabled on cluster with hawq already installed.

yarn.resourcemanager.ha - > rmhost1:8032,rmhost2:8032
yarn.resourcemanager.scheduler.ha -> rmhost1:8030,rmhost2:8030",Push yarn.resourcemanager.ha and yarn.resourcemanager.scheduler.ha properties when RM HA is enabled on cluster with hawq,4,,,bhuvnesh2703,True,bhuvnesh2703,bhuvnesh2703
ambari,AMBARI-14225,2015-12-05T00:00:57.000+0000,2015-12-31T01:15:17.000+0000,2015-12-31T03:27:10.000+0000,,Fixed,New Feature,Minor,['2.2.1'],['trunk'],,,,,,,['stacks'],['Ambari Stacks'],"Update hawq_dfs_url with nameservice when nn ha is configured via ambari. hawq_dfs_url points to hdfs data directory for hawq, and since hawq is already initialized, namenode ha wizard should update the dependent property",NN HA wizard should update hawq_dfs_url with nameservice when ha is enabled by ambari,5,,,bhuvnesh2703,True,bhuvnesh2703,bhuvnesh2703
ambari,AMBARI-14208,2015-12-04T06:06:53.000+0000,2015-12-24T20:47:27.000+0000,2016-07-14T18:29:48.000+0000,,Fixed,New Feature,Minor,['2.2.1'],['stacks'],,,,,,,['stacks'],['Ambari Stacks'],Update hawq_dfs_url with nameservice when hawq is installed on a ha cluster.,hawq_dfs_url should use nameservice when hawq is installed on a HDFS HA cluster,3,,,bhuvnesh2703,True,bhuvnesh2703,bhuvnesh2703
ambari,AMBARI-14165,2015-12-02T22:04:34.000+0000,2015-12-24T20:13:44.000+0000,2016-07-14T18:29:51.000+0000,,Fixed,New Feature,Major,['2.2.1'],['trunk'],,,,,,,,,"Populate hawq's yarn-client xml under /usr/local/hawq/etc.

If Yarn ha is enabled, ensure that /usr/local/hawq/etc/yarn-client.xml is updated with required ha parameters.",Populate hawq's yarn-client xml under /usr/local/hawq/etc,3,1,,bhuvnesh2703,True,bhuvnesh2703,bhuvnesh2703
ambari,AMBARI-14143,2015-12-01T19:38:44.000+0000,2016-01-15T01:34:13.000+0000,2016-01-15T01:34:13.000+0000,,Duplicate,New Feature,Major,,['trunk'],,,,,,,,,"Hawq configuration property should be updated when yarn ha is enabled.

The below properties should be updated in yarn-client.xml
{code}
<property>
   <name>yarn.resourcemanager.ha</name>        <value>%RESOURCEMANAGER%:9980,%RESOURCEMANAGER2%:9980</value>
</property>
<property>
     <name>yarn.resourcemanager.scheduler.ha</name>
  <value>%RESOURCEMANAGER%:9981,%RESOURCEMANAGER2%:9981</value>
</property>
{code}

In Hadoop Yarn configuration, these information are configured in yarn-site.xml, below is an example. They are corresponding to each other(but use different property names).
{code}
    <property>
        <name>yarn.resourcemanager.address.rm1</name>
        <value>%RESOURCEMANAGER%:9980</value>
    </property>
    <property>
        <name>yarn.resourcemanager.address.rm2</name>
        <value>%RESOURCEMANAGER2%:9980</value>
    </property>		
    <property>
        <name>yarn.resourcemanager.scheduler.address.rm1</name>
        <value>%RESOURCEMANAGER%:9981</value>
    </property>
    <property>
        <name>yarn.resourcemanager.scheduler.address.rm2</name>
        <value>%RESOURCEMANAGER2%:9981</value>
    </property>	
{code}",Hawq configuration property should be updated when yarn ha is enabled,1,,,bhuvnesh2703,True,bhuvnesh2703,bhuvnesh2703
ambari,AMBARI-14053,2015-11-25T00:50:41.000+0000,2015-12-23T23:49:03.000+0000,2016-07-14T18:29:51.000+0000,,Fixed,New Feature,Major,['2.2.1'],['trunk'],,,,,,,['stacks'],['Ambari Stacks'],PXF should get secured when security is enabled on cluster via kerberos wizard on ambari. Need to add kerberos.json and the relevant code.,PXF should get secured when security is enabled on cluster via kerberos wizard on ambari,4,,,bhuvnesh2703,True,bhuvnesh2703,bhuvnesh2703
ambari,AMBARI-14029,2015-11-24T01:35:40.000+0000,2015-12-28T18:30:35.000+0000,2016-07-14T18:29:50.000+0000,,Fixed,New Feature,Minor,['2.2.1'],['trunk'],,,,,,,['ambari-server'],['Ambari Server'],HAWQ is currently not supported on Namenode HA clusters. This feature ensures that HAWQ is supported on Namenode HA clusters.,HAWQ support on Namenode HA,4,,,mithmatt,True,mithmatt,mithmatt
ambari,AMBARI-13911,2015-11-16T18:24:42.000+0000,,2015-11-16T18:24:42.000+0000,,,New Feature,Minor,,['2.1.2'],,,,,,,,,"The ambari software requires a URL.  It would be beneficial if an option for Spacewalk/Satellite users was available, as the URL's are no longer required at this point.  Also, if this option is selected the /etc/yum.repos.d/*HADOOP*.repos are no longer required.

Work around is implemented but would be good to have an option in configuring URL where we can just not have to pass any URL. ",REPO Url  Option to select or not select,1,,,logic4fun,True,,logic4fun
ambari,AMBARI-13910,2015-11-16T18:20:35.000+0000,,2015-11-16T18:20:35.000+0000,,,New Feature,Minor,,['2.1.2'],,,,,,,"['ambari-agent', 'ambari-server']","['Ambari Agent', 'Ambari Server']","Unable to install the software through the ambari software.  It would be beneficial to be able to pass standard yum options or a .conf file to allow additional yum options (ie: --enablerepo=""*CHANNEL*"" to be passed to /usr/lib/ambari-server/lib/resource_management/core/providers/package/yumrpm.py


Following is an environment preview

We are Using Spacewalk/Satellite to manage ALL yum based repositories via channels in our server environment, as the servers are behind a firewall with no internet.
	The Hadoop channel names are unique to the server's dev/test/prod environment.  
	The channels are disabled via /etc/yum/pluginconf.d/rhnplugin.conf, with the line include=path2.conf file.  The path2.conf file contains [channel] enabled=0 gpgcheck=1. 
	In addition to this, we also override the default /etc/yum.repos.d directory in /etc/yum.conf by adding reposdir=path2yumdir.",Additional Yum Options,1,,,logic4fun,True,,logic4fun
ambari,AMBARI-13748,2015-11-05T17:58:55.000+0000,2015-11-16T22:08:34.000+0000,2015-12-03T22:14:48.000+0000,,Fixed,New Feature,Major,['2.3.0'],['2.1.0'],,,,,,,"['ambari-agent', 'ambari-server']","['Ambari Agent', 'Ambari Server']","Need a way for Ambari to trigger a repository clean (yum clean expire-cache, for example) when updating repository files during an Ambari update as well as fresh installation of a cluster.

Currently, this must be done occasionally through a manual step, and if not done will lead to problems during install.",Being able to clean repository cache before installing open source components,3,,,dili,True,dili,dili
ambari,AMBARI-13738,2015-11-05T12:25:31.000+0000,,2015-11-18T11:51:05.000+0000,,,New Feature,Trivial,,,,,,,,,"['ambari-agent', 'ambari-server', 'ambari-web']","['Ambari Agent', 'Ambari Server', 'Ambari Web UI']","Swappiness parameter is set to 60 by default. This is not suitable for Hadoop clusters.  When installing new cluster,  Ambari should be check  if the swappiness parameter is less than 10 .",Checking the vm.swappiness Kernel Parameter,1,,,dandelion,True,,dandelion
ambari,AMBARI-13733,2015-11-05T11:11:48.000+0000,,2015-11-12T18:35:29.000+0000,,,New Feature,Major,,,,,,,,,"['ambari-server', 'ambari-web']","['Ambari Server', 'Ambari Web UI']","Adding checking selinux status mechanism on each host when creating new cluster. 
Patch added.",Checking selinux status on each host when creating new cluster,1,,['patch'],zyuzuguldu,True,,zyuzuguldu
ambari,AMBARI-13680,2015-11-02T18:02:33.000+0000,,2015-11-02T21:25:03.000+0000,,,New Feature,Major,,,,,,,,,['ambari-server'],['Ambari Server'],"A custom (or value-add) service installed after the Ambari stack has been installed may need to change the configuration of an existing service.  For example, a change to the hive-env configuration file to add jar files to the HIVE_AUX_JARS_PATH.

I understand the current recommendation for changing another service is through the stackadvisor python script.  However, this is only done during the installation of the stack, not afterward.   

A custom service added afterward uses the REST APIs to perform the configuration update of an existing service.  This requires the user input their Ambari admin username and password when the custom service is being installed.  There is concern that users will complain about this requirement.

This feature request is for an internal API which can be used by custom services to request configuration updates through the ambari-server, and which does not require the Ambari admin username or password.",Internal API for custom services to make configuration updates,1,,['features'],ealton,True,,ealton
ambari,AMBARI-13606,2015-10-28T15:02:51.000+0000,2015-11-04T19:42:37.000+0000,2015-11-04T19:42:37.000+0000,2015-10-30,Fixed,New Feature,Blocker,['2.2.0'],['2.1.1'],,,,,,,['ambari-views'],['Ambari Views framework and Views themselves.  Specific Views that are built on the framework will be handled with labels.'],"Enable the preview files in WEBHDFS. Taking it a step further, User should be able to preview of compressed file types like .bz2 and .gz etc.",Files View: Enable to preview files in WEBHDFS,2,,,pallavkul,True,pallavkul,pallavkul
ambari,AMBARI-13604,2015-10-28T14:51:54.000+0000,,2015-10-28T14:51:54.000+0000,,,New Feature,Major,,['2.1.0'],,,,,,,"['ambari-server', 'ambari-web']","['Ambari Server', 'Ambari Web UI']",Feature Request for Ambari to support an option to download the client jars for Hive etc.,Download Hive client jars,1,,,harisekhon,True,,harisekhon
ambari,AMBARI-13510,2015-10-21T18:54:44.000+0000,2015-10-22T18:17:14.000+0000,2015-10-22T22:51:45.000+0000,,Fixed,New Feature,Major,['2.3.0'],['2.3.0'],,,,,,,['stacks'],['Ambari Stacks'],"User should be able to see the values of sysctl and limits parameters which are configured for HAWQ.
Sysctl parameters except vm.overcommit.memory are editable only during install, after install they are read only. overcommit parameter can be changed as needed.
limits for hawq user(gpadmin) are editable.",Display sysctl and limits configuration on the HAWQ config UI screen,3,,,bhuvnesh2703,True,bhuvnesh2703,bhuvnesh2703
ambari,AMBARI-13421,2015-10-14T12:17:17.000+0000,2015-10-28T12:12:09.000+0000,2015-12-09T00:55:22.000+0000,,Fixed,New Feature,Major,['2.2.0'],['2.0.0'],,,,,,,['blueprints'],['Ambari Blueprints'],Add support for installing Ranger and enabling ranger plugins through Blueprints.,"Blueprints: install for Ranger Components (ranger-admin, ranger-usersync, ranger-kms, ...)",3,,,stoader,True,stoader,stoader
ambari,AMBARI-13367,2015-10-09T02:19:40.000+0000,2016-03-15T05:46:20.000+0000,2016-03-15T05:47:04.000+0000,,Duplicate,New Feature,Minor,,,,,,,,,['ambari-server'],['Ambari Server'],"Currently, many of the methods in Ambari has HDP prefix, infix, or postfix.  This can cause confusion when the methods are used in context of other stack such as OPD, PHD or IOP.   With the parameterized stack info, remove these *fixes should have with reduce much confusion for developers.",rename stack related methods to be more generic for better reuse,2,1,,tctruong213,True,jluniya,tctruong213
ambari,AMBARI-13366,2015-10-09T02:14:46.000+0000,2016-03-15T05:56:34.000+0000,2016-03-15T05:56:34.000+0000,,Duplicate,New Feature,Major,,"['2.0.0', '2.1.0']",,,,,,,['ambari-server'],['Ambari Server'],"Currently, there is an conf-select script is that is used in HDP distribution. However, this is not in Ambari. We propose to add the conf-select script into Ambari but with parameterized stack info.",Add conf-select python script into Ambari for conf version selection,2,1,,tctruong213,True,jluniya,tctruong213
ambari,AMBARI-13365,2015-10-09T02:11:49.000+0000,2016-03-15T05:55:57.000+0000,2016-03-15T05:56:49.000+0000,,Duplicate,New Feature,Major,,"['2.0.0', '2.1.0']",,,,,,,['ambari-server'],['Ambari Server'],"Currently, there is an hdp-select script is that is used in HDP distribution. However, this is not in Ambari.  We propose to add a stack-select script into Ambari that will perform similar function to hdp-select, but with parameterized stack info.",Add stack-select python script into Ambari for stack-select package,2,1,,tctruong213,True,jluniya,tctruong213
ambari,AMBARI-13349,2015-10-07T23:37:07.000+0000,2016-04-25T13:59:25.000+0000,2016-04-25T17:19:12.000+0000,,Fixed,New Feature,Major,['2.4.0'],,,,,,,,,,"This task is to create a new keytab for ranger admin process to do name lookup on components (HDFS/Hive/Hbase/Knox/Storm/Yarn etc).

1] When the cluster is kerberized, this new keytab should be created for ranger. 
2] When new repos are created, keytab/principal should be automatically updated with this new keytab info
",Create all necessary  keytabs and principals for Ranger Service,4,,,vperiasamy,True,mugdha.varadkar,vperiasamy
ambari,AMBARI-13348,2015-10-07T23:32:30.000+0000,2015-11-23T07:45:25.000+0000,2015-11-23T10:45:19.000+0000,,Fixed,New Feature,Major,"['2.3.0', '2.2.0']",['2.1.0'],,,,,,,['ambari-server'],['Ambari Server'],Modify Ambari screen to add SSO specific configurations for ranger-admin component.,Additional config elements for Ranger Admin to support SSO,3,,,vperiasamy,True,gautamborad,vperiasamy
ambari,AMBARI-13288,2015-10-01T13:03:59.000+0000,,2015-10-01T13:03:59.000+0000,,,New Feature,Major,,['2.1.0'],,,,,,,"['ambari-server', 'ambari-web']","['Ambari Server', 'Ambari Web UI']",Feature Request for an Ambari monitoring view for read only non-admin users to be able to see the state and health of the cluster and it's components.,Ambari Monitoring View,1,,,harisekhon,True,,harisekhon
ambari,AMBARI-13220,2015-09-24T09:53:48.000+0000,,2018-07-27T16:45:04.000+0000,,,New Feature,Major,,['2.1.0'],,,,,,,,,"Feature Request to ask that when comparing two config revisions in Ambari, there should be some provision to be able to export it to a text diff or something similar for putting in tickets to debug issues like AMBARI-13189",Ambari revision compare tool -> text diff export,2,,,harisekhon,True,,harisekhon
ambari,AMBARI-13139,2015-09-18T08:59:54.000+0000,2015-09-18T23:35:06.000+0000,2015-09-19T02:56:09.000+0000,,Fixed,New Feature,Major,['2.2.0'],['2.2.0'],,,,,,,['ambari-web'],['Ambari Web UI'],"Currently, externalized strings can be defined for English only (in messages.js). One should be able to define those strings also for other languages to support internationalization.

As a solution, a small change in initialize.js (in ambari-web) is proposed. In this solution, the language of the web user interface is selected according to the language setting of the browser. This is an approach also used in some other web-based solutions. The default locale is defined as English. This applies when there is no support for the current language of the browser.

For this solution to work, a ""locales"" directory should be created in ambari-web (e.g. app/locales/) and current messages.js file should be moved to app/locales/en/ where ""en"" stands for English. When other languages will be added, only a new messages.js file needs to be added to the directory for that language in ""locales"" directory.",Implement multi-language support for Ambari Web user interface,6,,,canan.pembe,True,canan.pembe,canan.pembe
ambari,AMBARI-13094,2015-09-14T23:25:55.000+0000,2015-09-25T20:48:41.000+0000,2016-07-14T18:30:37.000+0000,,Fixed,New Feature,Major,['2.1.2'],['trunk'],,,,,,,,,"New feature to add spark thrift server support on Ambari. 

Design specification attached. 

Instruction to add thrift server to an existing cluster:
1) If running on HDP distro, Update metainfo.xml @ /var/lib/ambari-server/resources/stacks/HDP/2.3/services/SPARK/metainfo.xml. Change <deleted>true</deleted> to <deleted>false</deleted>. 
2) Use Ambari UI if spark has not been installed before. It will show up as a installable service alongside with spark history server. 
3) If spark component has been installed already, use API to add thrift-server as a new service. Service name: SPARK_THRIFTSERVER


",Add Spark Thrift Ambari Service,3,,['patch'],judynash,True,judynash,judynash
ambari,AMBARI-12886,2015-08-26T19:43:57.000+0000,,2015-08-26T19:43:57.000+0000,,,New Feature,Major,,,,,,,,,"['ambari-agent', 'ambari-server', 'ambari-web']","['Ambari Agent', 'Ambari Server', 'Ambari Web UI']","This has a prerequisite of https://issues.apache.org/jira/browse/AMBARI-12885

The idea here is that if multiple stacks are supported it would be better to allow the stacks to be upgraded separately.  In case the release cycles are different.  This would mean that a third party stack would need to be validated to work with certain versions of the base stack and upgrading would only be recommended on validated combinations.",Separate upgrade for multiple stacks,2,1,,Tim Thorpe,True,,Tim Thorpe
ambari,AMBARI-12885,2015-08-26T19:33:04.000+0000,2016-06-22T21:36:16.000+0000,2016-07-07T18:03:02.000+0000,,Fixed,New Feature,Major,['2.4.0'],,,,,,,,"['ambari-agent', 'ambari-server', 'ambari-web']","['Ambari Agent', 'Ambari Server', 'Ambari Web UI']","The purpose of this proposal is to facilitate adding custom services to an existing stack.  Ideally this would support adding and upgrading custom services separately from the core services defined in the stack.  In particular we are looking at custom services that need to support several different stacks (different distributions of Ambari).  The release cycle of the custom services may be different from that of the core stack; that is, a custom service may be upgraded at a different rate than the core distribution itself and may be upgraded multiple times within the lifespan of a single release of the core distribution.    

One possible approach to handling this would be dynamically extending a stack (after install time).  It would be best to extend the stack in packages where a stack extension package can have one or more custom services.",Dynamic stack extensions - install and upgrade support for custom services,18,2,,Tim Thorpe,True,Tim Thorpe,Tim Thorpe
ambari,AMBARI-12767,2015-08-13T18:32:32.000+0000,,2016-04-20T23:49:15.000+0000,,,New Feature,Minor,,['2.1.0'],,,,,,,['ambari-server'],['Ambari Server'],"While Ambari can monitor the status of a process using pid or using other methods to check status, it would be nice to have a restart attempt feature to make an attempt to automatically restart the process in case of a failure. ",Ability to restart failed process automatically,3,2,,pgunasekaran,True,,pgunasekaran
ambari,AMBARI-12761,2015-08-13T14:35:15.000+0000,,2017-07-17T08:47:39.000+0000,,,New Feature,Major,,['2.1.0'],,,,,,,"['ambari-server', 'ambari-web']","['Ambari Server', 'Ambari Web UI']","Feature request for global config view + diff capability. Use case is if something strange has happened to the cluster and you want to see all changes in one view, otherwise you might have to go to every single service and do a service by service config compare.

Ambari currently does not allow one to do a basic global ""git log"" equivalent across all services.",Global Config View + Diff History,1,,,harisekhon,True,,harisekhon
ambari,AMBARI-12724,2015-08-11T16:41:43.000+0000,,2015-08-11T16:41:43.000+0000,,,New Feature,Major,,['2.1.0'],,,,,,,"['ambari-server', 'ambari-web']","['Ambari Server', 'Ambari Web UI']","Feature request to add a cluster wide ""Restart All Affected"" button.

When changing configurations and there are several services that need component restarts it's tedious to have to click through several services and hit the ""Restart All Affected"" button in each one.

ps.  please also finally fix AMBARI-8509 and add a cluster wide restart button, it's been several major releases and has gone unanswered.","Cluster Wide ""Restart All Affected""",1,,,harisekhon,True,,harisekhon
ambari,AMBARI-12650,2015-08-05T17:26:31.000+0000,2016-08-15T23:22:00.000+0000,2016-08-15T23:22:01.000+0000,,Invalid,New Feature,Minor,['trunk'],['trunk'],,,,,,,['ambari-web'],['Ambari Web UI'],"When there is a host error, a javascript alert box is opened.  It would be nice to use a popup modal as is done in other alerts throughout Ambari.  Also javascript alert box has different behavior in Chrome versus FF/IE.  FF/IE opens a new tab with the alert.  A popup modal would allow behavior to be consistent between the browsers.",Quicklinks open URLs but in case of error use Javascript Alert,2,,,arpsian,True,anitajebaraj,arpsian
ambari,AMBARI-12615,2015-07-31T22:04:46.000+0000,2015-08-06T06:30:06.000+0000,2015-09-03T07:11:43.000+0000,,Fixed,New Feature,Major,['2.1.2'],['2.3.0'],,,,,,,['ambari-views'],['Ambari Views framework and Views themselves.  Specific Views that are built on the framework will be handled with labels.'],"There is no existing visualization tool to visualize the result set data in Hive View.
Hive view should support a visualization tool with the following features.
1) Ability to visualize result set data in Hive View.
2) Ability to visualize multiple dimensions in the result set with different types of charts
3) Support all modern browsers.",Add Data Visualization to Hive View,3,,,vivekratnavel,True,vivekratnavel,vivekratnavel
ambari,AMBARI-12558,2015-07-27T21:55:55.000+0000,,2015-07-27T21:59:52.000+0000,,,New Feature,Major,,,,,,,,,['stacks'],['Ambari Stacks'],"Apache Geode (http://geode.incubator.apache.org) is distributed, in-memory database. It is platform agnostic.
It is platform agnostic and reused across multiple Stacks. Therefore the common services would be a good location for the plugin. 
",Create Apache Geode plugin under the Common Services ,3,,,tzolov,True,,tzolov
ambari,AMBARI-12439,2015-07-16T20:14:59.000+0000,,2015-09-16T23:12:50.000+0000,,,New Feature,Major,,['1.7.0'],,,,,,,"['ambari-server', 'ambari-web', 'stacks']","['Ambari Server', 'Ambari Web UI', 'Ambari Stacks']","h3. Problem
Some services' components could have dependency to be collocated with at least one component from list.
For example, component X requires DATANODE or NAMENODE to be installed on the same host.
As for now we can define only strict dependencies to all of three components, like this: 
{code:xml}
                <dependency>
                    <name>HDFS/DATANODE</name>
                    <scope>host</scope>
                    <auto-deploy>
                        <enabled>true</enabled>
                    </auto-deploy>
                </dependency>
                <dependency>
                    <name>HDFS/NAMENODE</name>
                    <scope>host</scope>
                    <auto-deploy>
                        <enabled>true</enabled>
                    </auto-deploy>
                </dependency>
{code}
Literally Ambari supports only AND operator.
But this definition will ask user to install both components, not only one of them.
h3. Proposal
For that sake it would be nice to support more complex definitions of dependencies, OR operator as well.
For example:
{code:xml}
                <dependency>
                    <name>(HDFS/DATANODE|HDFS/NAMENODE)</name>
                    <scope>host</scope>
                    <auto-deploy>
                        <enabled>true</enabled>
                    </auto-deploy>
                </dependency>
{code} - means component requires at least one of DATANODE or NAMENODE components.",Implement support of logical expressions in dependencies for stack components,2,,,odiachenko,True,jaoki,odiachenko
ambari,AMBARI-12115,2015-06-24T09:20:14.000+0000,,2015-06-24T09:20:14.000+0000,,,New Feature,Blocker,,['2.0.0'],,,,,,,"['ambari-server', 'ambari-web']","['Ambari Server', 'Ambari Web UI']","I'm decommissioning several servers including those housing the master components for HDFS HA, but there is no ability to move the Journal nodes or ZKFC roles to other servers, leaving the whole operation blocked.

I can't even disable HDFS HA, move and re-enable later since Ambari still doesn't support disabling HDFS HA (AMBARI-11681).",Ability to Move HDFS HA Journal / ZKFC nodes to other servers for decommissioning servers,3,,,harisekhon,True,,harisekhon
ambari,AMBARI-12001,2015-06-18T14:38:17.000+0000,,2015-06-18T14:38:17.000+0000,,,New Feature,Minor,,,,,,,,,['ambari-admin'],['Ambari Admin'],"It would be nice to have a way to manage client interactions (install clients, refresh client configurations, etc.) at a host level grouping.  Currently you are limited to installing/refreshing clients at the individual host view.  Providing the ability to do this at the Host menu by selecting some or all hosts would be very beneficial. ",Ambari - New Client Service Grouping,1,,,mbsharp85,True,,mbsharp85
ambari,AMBARI-11860,2015-06-11T15:45:29.000+0000,,2015-06-11T15:45:29.000+0000,,,New Feature,Major,,,,,,,,,,,"When creating custom services it would be nice to be able to specify that the custom service component that you're creating is a dependency of some already existing services component. 

For example take the following use case:

I am writing a custom service with a client component that contains java libraries that contain custom HBase coprocessors. My custom component and its custom libraries must exist on all Nodes running an HBase component. I should be able to specify this in the metainfo.xml file of my custom service without needing to alter the HBase services metainfo.xml file.",Add the capability for a component to specify in its own metainfo that it is a mandatory dependency to another component.,1,,,jackson,True,,jackson
ambari,AMBARI-11098,2015-05-13T10:17:59.000+0000,,2016-06-06T05:21:52.000+0000,,,New Feature,Critical,,['2.0.0'],,,,,,,"['ambari-server', 'stacks']","['Ambari Server', 'Ambari Stacks']","Feature Request to allow fast offline stack upgrades / downgrades instead of the only option being slow rolling upgrades.

Currently rolling upgrades take a very, very long time and there many hiccups in the process (pages of jiras on rolling upgrade problems).

For many people's clusters which are mainly doing batch analytics / data warehouse reports etc it's simply not necessary to try for the 100% uptime and waste all day(s) babysitting and rolling back every time there is any problem in the rolling upgrade, this in fact wastes a huge amount of operator time and blocks users getting their bug fixes or new features they need.

Operators should have the option of doing a fast and simple offline upgrade/downgrade:

1. Shut down entire cluster
2. Upgrade (or Downgrade) stack in parallel quickly across all nodes
3. Start up whole cluster again on different version

This is the difference between taking 30 mins or taking 8 hours to do an upgrade/downgrade and is preferable to only being allowed to do rolling upgrades. I've even done fast offline upgrade on another vendor in prod because sometimes it's just the better thing to do to get things finished quickly and allow users to utilize the fixes the upgrades provides immediately, especially where they're blocked on needing that upgrade or if wanting to very quickly roll back to a previous version after discovering some issue later (please also add arbitrary downgrades AMBARI-11097).

This would make testing much much faster and easier, and gives users access to new features or bug fixes they may be blocked on more quickly, rather than having to be blocked behind the long rolling upgrade process.",Fast Offline Stack Upgrades (better than rolling upgrades in many cases),2,,,harisekhon,True,,harisekhon
ambari,AMBARI-11097,2015-05-13T10:00:00.000+0000,,2015-05-13T10:00:00.000+0000,,,New Feature,Critical,,['2.0.0'],,,,,,,"['ambari-server', 'ambari-web', 'stacks']","['Ambari Server', 'Ambari Web UI', 'Ambari Stacks']","Feature Request to add arbitrary Downgrade capability to any stack. This is useful if you've upgraded and finalized only to find an issue a few weeks later that you were sure didn't affect the last version and would like to do a wholesale Downgrade to test.

Currently after finalizing an upgrade of an Ambari Stack, the ability to deploy any previous Stack version disappears on the page /#/main/admin/stack/versions and going to /views/ADMIN_VIEW/2.0.0/INSTANCE/#/stackVersions and doing ""Install On"" on a previous stack version does nothing, it returns to the previous page but the previous stack is not displayed at all, only the current latest stack is displayed.",Stack Downgrade capability,1,,,harisekhon,True,,harisekhon
ambari,AMBARI-11067,2015-05-12T15:20:35.000+0000,,2015-05-12T15:20:35.000+0000,,,New Feature,Major,,['2.0.0'],,,,,,,"['ambari-server', 'ambari-web']","['Ambari Server', 'Ambari Web UI']","Feature Request to add a button to ""Check Recommended Settings"" in Ambari web UI.

When saving a service configuration change, Ambari prompts with settings that deviate from what it believes should be the recommended value. If you click ""Proceed anyway"" then you can't go back and check what those recommended settings were again afterwards. The workaround currently is do some kind of trivial change to get that dialogue back on savnig the config.",Ambari check recommended settings,1,,,harisekhon,True,,harisekhon
ambari,AMBARI-10848,2015-04-29T18:24:56.000+0000,,2015-04-30T16:32:47.000+0000,,,New Feature,Major,,,,,2419200,2419200,2419200,,['security'],"['Ambari security features, including Kerberos']","Currently Ambari does not support active directory integration with kerberos authentication . this issue will be addressed in JIRA ""AMBARI-10709"" .

What we suggest is contributing an adapter to invoke the output of the  IBM kerberos automation toolkit for hadoop (https://developer.ibm.com/hadoop/blog/2014/12/11/ibms-kerberos-automation-toolkit-hadoop/) which will be contributed in JIRA ""AMBARI-10709"" and therefore some of the clients will have the option to run the framework automatically .


",kerberos automated script adapter for Active Directory,2,,,lazurinke,True,,lazurinke
ambari,AMBARI-10760,2015-04-27T10:56:41.000+0000,,2015-04-27T10:56:41.000+0000,,,New Feature,Minor,,['2.0.0'],,,,,,,['ambari-web'],['Ambari Web UI'],"Feature request for Rolling Restarts of ZooKeeper servers in the ZooKeeper service.

Hari Sekhon
http://www.linkedin.com/in/harisekhon",ZooKeeper Rolling Restarts,2,,,harisekhon,True,,harisekhon
ambari,AMBARI-10737,2015-04-24T18:54:33.000+0000,,2015-09-25T22:45:35.000+0000,,,New Feature,Major,,,,,,,,,['ambari-server'],['Ambari Server'],"Currently to setup non-root user you have to step through the interactive setup routine. To facilitate automated installs using the existing -s command line option add new ""-u username""
{code}
ambari-server setup -s -j /opt/teradata/jvm64/jdk7 -u ambari
{code}",Add new command line option for non-root user,2,,,miharp,True,,miharp
ambari,AMBARI-10719,2015-04-24T03:37:24.000+0000,,2015-04-24T03:37:24.000+0000,,,New Feature,Major,,['2.0.0'],,,,,,,['ambari-server'],['Ambari Server'],,Enable httpfs in ambari,2,,,fang fang chen,True,,fang fang chen
ambari,AMBARI-10709,2015-04-23T19:18:34.000+0000,,2015-04-30T00:07:56.000+0000,,,New Feature,Major,,,,,,,,,['security'],"['Ambari security features, including Kerberos']","Currently Amabari project does not provide a way for very security conscious clients to get editable scripts to be run on their system .

In a real world scenario with enterprise level customers, there will be no way that these customers will let the Ambari wizard run anything on the organization LDAP server .

What we suggest to contribute  is a framework which is based on ""IBM''s kerberos automation toolkit for hadoop "",  more details can be found here :
https://developer.ibm.com/hadoop/blog/2014/12/11/ibms-kerberos-automation-toolkit-hadoop/

This toolkit supports building scripts and other resources for both ""openLDAP"" and ""Active Directory"" in order to ease up the configuration of kerberos on hadoop enviornments for security concious clients . these scripts and artifacts are created based on the clients topology . and have been tested at various client sites. ",Kerberos automation via generated scripts,3,,,lazurinke,True,,lazurinke
ambari,AMBARI-10670,2015-04-22T20:20:52.000+0000,,2015-04-22T20:20:52.000+0000,,,New Feature,Major,,['2.1.0'],,,,,,,['blueprints'],['Ambari Blueprints'],"Blueprint supports application of configurations to cluster to be installed. In the case of installing a cluster with heterogeneous nodes (more/less total memory, different tuning to adjust for different sets of services) it would be very convenient to be able to define these services in the blueprint.

Example:
{code}
{
  ""configurations"": [
    {
      ""some-config"": {
        ""some.property"": ""value""
      }
    }
  ],
  ""ConfigGroups"": [
    {
      ""group_name"": ""big-nodes-group"",
      ""tag"": ""YARN"",
      ""desired_configs"": {
        ""some.property"": ""big-value""
      }
    },
    {
      ""group_name"": ""small-nodes-group"",
      ""tag"": ""YARN"",
      ""desired_configs"": {
        ""some.property"": ""small-value""
      }
    }
  ],
  ""host_groups"": [
    {
      ""name"": ""big_group"",
      ""ConfigGroups"": [
        {
          ""tag"": ""YARN"",
          ""group_name"": ""big-nodes-group""
        }
      ],
      ""components"": [...etc...]
    },
    {
      ""name"": ""small_group"",
      ""ConfigGroups"": [
        {
          ""tag"": ""YARN"",
          ""group_name"": ""small-nodes-group""
        }
      ],
      ""components"": [...etc...]
    }
  ]
}

{code}",Support for config groups in Blueprint,1,,,ddtd,True,,ddtd
ambari,AMBARI-10497,2015-04-15T10:47:13.000+0000,,2015-04-15T10:47:13.000+0000,,,New Feature,Major,,['2.0.0'],,,,,,,['ambari-server'],['Ambari Server'],"It seems Ambari still doesn't have support for deploying and managing HttpFS. This is needed for HDFS HA failover tracking for client apps using the WebHDFS API, such as Hue.

Hari Sekhon
http://www.linkedin.com/in/harisekhon",Add support for deploying HttpFS service,4,3,,harisekhon,True,,harisekhon
ambari,AMBARI-10401,2015-04-08T14:43:27.000+0000,,2015-04-08T14:43:27.000+0000,,,New Feature,Minor,,['1.7.0'],,,,,,,['ambari-web'],['Ambari Web UI'],"During component installation a bubble diagram of hosts and services appears on the right hand side of the screen.

This is a very good visual depiction of where services have landed and it would be nice to have this visual as part of the standard Hosts view.",Better display of component location,1,,,skahler,True,,skahler
ambari,AMBARI-10318,2015-04-01T21:37:43.000+0000,2015-04-10T18:49:33.000+0000,2015-04-10T20:07:35.000+0000,,Implemented,New Feature,Major,['2.1.0'],['2.1.0'],,,,,,,['ambari-server'],['Ambari Server'],Add and enable 2.3.GlusterFS stack for HDP,Add and enable 2.3.GlusterFS stack for HDP,3,,,screeley,True,screeley,screeley
ambari,AMBARI-10141,2015-03-19T22:20:54.000+0000,,2015-03-19T23:59:44.000+0000,,,New Feature,Major,,,,,,,,,['ambari-metrics'],['Ambari Metrics System'],The new Ambari Metrics system leverages HBase to aggregate and store the metrics.   There has been some customer requirements that customers would like to view the system metrics in a continuous time sliding window.  We would like to leverage the spark streaming framework which keeps pulling metrics in real time from the Kafka metrics data hub and presents the metrics in a streaming fashion that a customer can view the metrics trending in a time sliding window. ,Leveraging Spark to Build a Streaming Metrics Framework that Enables Presenting Metrics in a Continuous Time Sliding Window,3,,,tanping,True,,tanping
ambari,AMBARI-10140,2015-03-19T22:08:25.000+0000,,2015-03-19T23:59:32.000+0000,,,New Feature,Major,,['2.1.0'],,,,,,,['ambari-metrics'],['Ambari Metrics System'],"In the new Ambari Metrics system, introducing Kafka as a centralized data hub that stores either raw or processed metrics.  Consumers of Kafka can directly pull data on a streaming fashion as long as metrics is available or in a batch fashion in order to aggregate the metrics for analytic purpose.  ",Using Kafka as the Centralized Metrics Data Hub in Ambari Metrics System ,3,,,tanping,True,,tanping
ambari,AMBARI-10029,2015-03-11T16:26:22.000+0000,2015-06-10T01:53:14.000+0000,2016-04-24T20:27:45.000+0000,,Fixed,New Feature,Major,['2.1.0'],['2.0.0'],,,,,,,"['ambari-agent', 'ambari-server']","['Ambari Agent', 'Ambari Server']","Using blue-print, it is possible to perform a zero-touch install of hadoop clusters using Ambari. This is especially useful in the cloud environment. However, cloud environment also can be dynamic in the sense that nodes will get rebooted or reset to the original image.

Reset, being that the node (usually VM) gets reverted to original state where it joined the cluster. It is assumed that a reset node has ambari-agent installed and configured to communicate with the server. The node may also have all packages pre-instaled.

Node recovery is the feature to bring back a rebooted/reset online by starting or installing and then starting the host components that are already on the host.

In general, temporarily losing a node and then performing node recovery on a slave host should not affect the whole cluster. If its is a master node then there can be some disruption based on what is deployed on the master host and if HA is enabled for the master services or not.

Node recovery, discussed in this JIRA, only addresses the ability to automatically INSTALL/CONFIGURE/START host components on the node so that the desired state of the host component matches the actual state.",Node auto-recovery,7,1,,sumitmohanty,True,sumitmohanty,sumitmohanty
ambari,AMBARI-9946,2015-03-05T15:48:21.000+0000,,2015-03-05T15:48:21.000+0000,,,New Feature,Minor,,['1.7.0'],,,,,,,['ambari-web'],['Ambari Web UI'],"Feature request for Node Actions for all services.

Currently there is no way to restart all of one type of component across all nodes in the web UI via Node Actions. It currently only gives the option to restart DataNodes/NodeManagers and no other components.

Regards,

Hari Sekhon
http://www.linkedin.com/in/harisekhon",Host Actions should allow service management of other components than just DataNode/NodeManager,1,,,harisekhon,True,,harisekhon
ambari,AMBARI-9945,2015-03-05T15:35:00.000+0000,2015-03-05T15:37:23.000+0000,2015-03-05T15:37:23.000+0000,,Duplicate,New Feature,Major,,['1.7.0'],,,,,,,,,"Rolling restarts are still only applied to slave roles, needs to be at the service-level and cluster-level and included HA masters such as Resource Managers.

Regards,

Hari Sekhon
http://www.linkedin.com/in/harisekhon",Rolling restart including masters,1,,,harisekhon,True,,harisekhon
ambari,AMBARI-9633,2015-02-13T16:49:37.000+0000,,2015-02-13T16:49:37.000+0000,,,New Feature,Critical,,['1.7.0'],,,,,,,['ambari-server'],['Ambari Server'],"Feature request to add HDFS encryption support, automated deployment and configuration of Apache Key Management Server and related HDFS configs, as well as monitoring the health of the KMS and encrypted canary file health checks.

Best Regards,

Hari Sekhon
http://www.linkedin.com/in/harisekhon","HDFS encryption + Key Management Server, deployment/management/monitoring",2,,,harisekhon,True,,harisekhon
ambari,AMBARI-9455,2015-02-03T16:59:48.000+0000,,2015-03-10T17:13:57.000+0000,,,New Feature,Major,,['1.7.0'],,,,,,,,,"Feature request to add SolrCloud service deployment to Ambari.

Regards,

Hari Sekhon
http://www.linkedin.com/in/harisekhon",Add SolrCloud service deployment,3,2,,harisekhon,True,,harisekhon
ambari,AMBARI-9438,2015-02-02T23:14:31.000+0000,2015-03-24T17:56:02.000+0000,2015-03-24T20:24:16.000+0000,,Fixed,New Feature,Major,['2.1.0'],['2.1.0'],,,,,,,['ambari-web'],['Ambari Web UI'],"AMBARI-9224 adds the NFS gateway as an HDFS slave component.
This JIRA is to track the effort of adding configuration and monitor functions to that component.",Add configuration and monitor section for HDFS-NFS gateway ,6,,,brandonli,True,brandonli,brandonli
ambari,AMBARI-9393,2015-01-29T11:57:08.000+0000,,2018-08-07T17:12:13.000+0000,,,New Feature,Major,,"['1.7.0', '2.6.2']",,,,,,,['ambari-web'],['Ambari Web UI'],"Feature request to add a Retry button next to failed Operations in the Ambari web UI.

Screenshot shows each time a operation fails with an exclamaition point next to it, admin must navigate back through the menus instead of just clicking Retry as is available when deploying Ambari agents.

Regards

Hari Sekhon
http://www.linkedin.com/in/harisekhon",Add Retry button next to failed Operations,1,,,harisekhon,True,,harisekhon
ambari,AMBARI-9363,2015-01-27T23:39:49.000+0000,2015-11-13T23:02:26.000+0000,2016-03-23T18:52:59.000+0000,,Fixed,New Feature,Major,['2.3.0'],"['1.7.0', 'trunk']",,,,,,,['ambari-server'],['Ambari Server'],"In current stack definitions, role_command_order.json is at the stack level.
For example: HDP/2.2/role_command_order.json

Service definitions are all nicely separated into different directories, like HDP/2.2/services/{HDFS|YARN}, but not the role_command_order. It would be neater to separate role_command_order per service and would be very useful while adding a new service to Ambari.

Looking for something as below,
- HDP/2.2/services/HDFS/role_command_order.json
- HDP/2.2/services/YARN/role_command_order.json
Ambari server while starting should merge all role_command_order.json and create dependencies accordingly.

This is extremely useful for custom services which are potentially added after the cluster install.
",role_command_order.json should not be at stack level,8,1,['feature_custom_service'],vasanm,True,Tim Thorpe,vasanm
ambari,AMBARI-9208,2015-01-20T11:15:19.000+0000,,2015-01-20T11:15:19.000+0000,,,New Feature,Minor,,['1.7.0'],,,,,,,"['ambari-server', 'ambari-web']","['Ambari Server', 'Ambari Web UI']","Feature request to add NameNode HA failover button to Ambari web UI.

Best Regards,

Hari Sekhon
http://www.linkedin.com/in/harisekhon",Add NameNode HA failover support to Web UI,2,,,harisekhon,True,,harisekhon
ambari,AMBARI-9156,2015-01-15T18:10:51.000+0000,,2015-02-25T21:07:30.000+0000,,,New Feature,Minor,,,,,,,,,,,"Apache NiFi is a dataflow system based on the concepts of flow-based programming. It is currently a part of the Apache Incubator.

Although Apache NIFI is still in incubation is a powerful tool that can be used for the transportation of data. Making NIFI available in Ambari will increase deployment and reduce time needed to configure not just a Hadoop cluster but, also the system that will transport data HDFS.

http://nifi.incubator.apache.org/

",Integrate Apache NIFI into Amabari,3,,,elcangri2124,True,,elcangri2124
ambari,AMBARI-9145,2015-01-15T13:29:49.000+0000,,2015-01-15T13:29:49.000+0000,,,New Feature,Critical,,,,,,,,,,,if a configuration change is made such as the location of config or logs and the directory is not created. Make ambari create the directory automatically. This will reduce deployment time.,when changing paths of the config make ambari create paths,1,,,elcangri2124,True,,elcangri2124
ambari,AMBARI-9083,2015-01-12T06:49:37.000+0000,,2019-06-12T09:01:38.000+0000,,,New Feature,Critical,,,,,,,,,['ambari-server'],['Ambari Server'],see title,support installation of apache spark on ambari,2,,,elcangri2124,True,,elcangri2124
ambari,AMBARI-8788,2014-12-18T13:08:44.000+0000,2015-05-18T06:06:28.000+0000,2015-05-18T10:27:05.000+0000,,Fixed,New Feature,Major,['2.1.0'],['1.7.0'],,,,,,,,,"Feature request to add management of the HDFS NFS gateway to Ambari.

This will need to notify user to create new Kerberos principal if adding to an already kerberized cluster (relates to AMBARI-8610).

Cloudera Manager already manages HDFS NFS gateway instance.

Regards,

Hari Sekhon
(ex-Cloudera)
http://www.linkedin.com/in/harisekhon",Add HDFS NFS gateway management,2,,,harisekhon,True,,harisekhon
ambari,AMBARI-8678,2014-12-12T03:02:01.000+0000,2014-12-17T05:20:23.000+0000,2014-12-17T06:17:10.000+0000,,Fixed,New Feature,Major,['2.0.0'],['2.0.0'],,,,,,,['ambari-server'],['Ambari Server'],"We should be able to create Kerberos Service Principals for hadoop services in Active Directory making programmatic calls from Ambari Server.

We would do this by an implementation of org.apache.ambari.server.serveraction.kerberos.KerberosOperationHandler.
The implementation would use JNDI to create principals in AD.

The implementation also would support checking for presence of principal, updating password and removing principal.",create kerberos principals in Active Directory programmatically,3,,,darumugam,True,darumugam,darumugam
ambari,AMBARI-8457,2014-11-26T16:50:46.000+0000,2014-11-26T17:24:54.000+0000,2014-11-26T18:55:34.000+0000,,Fixed,New Feature,Major,['2.0.0'],['2.0.0'],,,,,,,['ambari-web'],['Ambari Web UI'],Implement Popup that contain overall progress and tasks of upgrade process,Rolling Upgrade process: upgrade progress popup,3,,,atkach,True,atkach,atkach
ambari,AMBARI-8454,2014-11-26T10:43:16.000+0000,2014-12-08T22:02:39.000+0000,2014-12-08T23:54:32.000+0000,,Fixed,New Feature,Major,['2.0.0'],['2.0.0'],,,,,,,['ambari-server'],['Ambari Server'],"Create server-side actions to generate the Kerberos principals and keytabs.  These actions will be used when setting up Kerberos for services when Kerberizing a cluster. 
",Create server-side actions to create kerberos principals and keytabs,3,,"['kerberos', 'server-side']",rlevas,True,rlevas,rlevas
ambari,AMBARI-8441,2014-11-25T12:34:46.000+0000,2014-11-25T13:03:58.000+0000,2014-11-25T13:56:21.000+0000,,Fixed,New Feature,Major,['2.0.0'],['2.0.0'],,,,,,,['ambari-web'],['Ambari Web UI'],Stack and Upgrade page should show containers with current and upgrade versions.,Stack and Upgrade: pre-upgrade flow(containers with current and upgrade versions),3,,,atkach,True,atkach,atkach
ambari,AMBARI-8426,2014-11-24T14:18:45.000+0000,2014-12-09T01:00:09.000+0000,2014-12-09T01:00:09.000+0000,,Fixed,New Feature,Major,['2.0.0'],['2.0.0'],,,,,,,['ambari-server'],['Ambari Server'],"There should be a way to get access to the web server's session data from a (REST API) resource handler.  

This will allow a resource handler to access information such as a session encryption key that may be used to encrypt data during that session.  An example of this would be when performing Kerberos-related activities, the following flow can occur:

# Session encryption key is created
# User uploads KDC administrator credentials 
# administrator credential are encrypted using the session encryption key and persisted - maybe on disk, maybe in the Ambari database
# For every Kerberos administration action that needs to occur during that session, the administrative credentials may be loaded into memory, decrypted, used, and removed from memory 
# When the session terminates, the encryption key is lost and the persisted administrator credentials become lost
 

",Provide access to session from resource handler/provider,3,,"['encryption', 'kerberos', 'security', 'session']",rlevas,True,tbeerbower,rlevas
ambari,AMBARI-8425,2014-11-24T13:42:25.000+0000,2014-11-24T17:01:21.000+0000,2014-11-24T17:33:28.000+0000,,Fixed,New Feature,Major,['2.0.0'],['2.0.0'],,,,,,,['ambari-web'],['Ambari Web UI'],,Host Details: Integration stack versions table with server,2,,,atkach,True,atkach,atkach
ambari,AMBARI-8397,2014-11-20T15:19:38.000+0000,2014-11-20T15:40:02.000+0000,2014-11-20T16:36:22.000+0000,,Fixed,New Feature,Major,['2.0.0'],['2.0.0'],,,,,,,['ambari-web'],['Ambari Web UI'],"Implement the View/Edit Version page.
This is UI only without E2E integration.",Ambari View > Versions > View/Edit Version (with mock data),2,,,atkach,True,atkach,atkach
ambari,AMBARI-8394,2014-11-20T11:34:16.000+0000,2014-11-20T11:57:46.000+0000,2014-11-20T13:10:16.000+0000,,Fixed,New Feature,Major,['2.0.0'],['2.0.0'],,,,,,,['ambari-web'],['Ambari Web UI'],"* Create models and routes necessary for ""Register Version"" page.
* Create any necessary mock data
* Implement Register Version page
* This is UI only without E2E integration",Ambari View > Versions > Register Version (with mock data),2,,,atkach,True,atkach,atkach
ambari,AMBARI-8386,2014-11-19T18:53:16.000+0000,2014-11-19T19:28:19.000+0000,2014-11-19T20:53:27.000+0000,,Fixed,New Feature,Major,['2.0.0'],['2.0.0'],,,,,,,['ambari-web'],['Ambari Web UI'],"Implement Stack Versions table with mock data.
This is UI only without E2E integration.",Ambari View > Versions > Versions table (with mock data),2,,,atkach,True,atkach,atkach
ambari,AMBARI-8358,2014-11-17T23:33:02.000+0000,2014-12-18T19:13:35.000+0000,2014-12-18T20:32:29.000+0000,,Fixed,New Feature,Major,['2.0.0'],['2.0.0'],,,,,,,['ambari-server'],['Ambari Server'],,update service configs for kerberos related configs when kerberos is enabled ,4,,,darumugam,True,darumugam,darumugam
ambari,AMBARI-8357,2014-11-17T23:29:24.000+0000,2015-03-18T20:24:10.000+0000,2015-03-18T20:24:10.000+0000,,Fixed,New Feature,Major,['2.0.0'],['2.0.0'],,,,,,,['ambari-server'],['Ambari Server'],,Create web service API handler for regenerating Keytabs,3,,,darumugam,True,darumugam,darumugam
ambari,AMBARI-8356,2014-11-17T23:24:26.000+0000,2014-12-18T19:35:26.000+0000,2014-12-18T20:32:27.000+0000,,Fixed,New Feature,Major,['2.0.0'],['2.0.0'],,,,,,,['ambari-server'],['Ambari Server'],,Push kerberos keytabs from ambari server to appropriate service component host,4,,,darumugam,True,darumugam,darumugam
ambari,AMBARI-8343,2014-11-15T16:29:36.000+0000,2014-12-12T01:14:58.000+0000,2015-01-08T16:08:43.000+0000,,Fixed,New Feature,Critical,['2.0.0'],['2.0.0'],,,,,,,['ambari-agent'],['Ambari Agent'],"In order to properly handle the automated installation or removal of a security infrastructure (like Kerberos) in the cluster, Ambari needs to know whether each component on the hosts of the cluster is properly _secured_ or not.  This information may be compared with data on the Ambari server to help determine what steps should be taken to ensure the cluster is in the correct _secured_ state.

To do this, the current and desired component security state is maintained in the Ambari database.  The Ambari server will update the desired state details according to whether the cluster is to be secured or not and whether the relevant service has enough metadata to be secured.  If the desired and actual security state details do not match, the Ambari server will take the necessary steps to work towards synchronization. 

In order for a component to indicate its security status, a new property needs to be returned in the {{STATUS_COMMAND}} response message (from the Ambari agent).  This property should be named ‘securityState’ and should have one of the following values:

* {{UNKNOWN}} - Indicates that it is not known whether the service or component is secured or not
* {{UNSECURED}} - Indicates service or component is not or should not be secured
* {{SECURED_KERBEROS}} - Indicates component is or should be secured using Kerberos
* {{ERROR}} - Indicates the component is not secured due to an error condition

To properly set this state value, a call needs to be executed per component querying for its specific state.  Due to the differences on how each component is secured and how it may be determined if security is setup what type is configured, and working is it properly, it is necessary for each component to have its own logic for determining this state. Therefore the ambari-agent process will need to call into the component’s configured (lifecycle) script and wait for its response - not unlike how it determines whether the component is up and running.

After the infrastructure is in place, each service definition needs to be updated to implement the new security status check function.  The function should perform the following steps:

* Determine if security is enabled or disabled
** If disabled, return ""UNSECURED""
** If enabled, determine what type of security is enabled
*** If Kerberos is configured
**** Perform tests (kinit?, ping KDC?) to determine if the configuration appears to be working
***** If working, return “SECURED_KERBEROS”
***** If not working, return “ERROR”
*** Else, return ""UNKNOWN""

If no function is available, the Ambari agent should return “UNKNOWN”.",Components should indicate Security State (via ambari-agent),3,,"['kerberos', 'states']",rlevas,True,rlevas,rlevas
ambari,AMBARI-8336,2014-11-14T20:16:50.000+0000,2014-11-24T17:51:28.000+0000,2015-01-08T16:08:08.000+0000,,Fixed,New Feature,Blocker,['2.0.0'],['2.0.0'],,,,,,,['ambari-server'],['Ambari Server'],"In order to track which services and components have been or need to be secured, several tables need to be updated with a {{security_state}} column to contain one the following values:

* {{UNKNOWN}} - Indicates that it is not known whether the service or component is secured or not
* {{UNSECURED}} - Indicates service or component is not or should not be secured
* {{SECURED_KERBEROS}} - Indicates component is or should be secured using Kerberos
* {{SECURING}} - Indicates the component is in the process of being secured
* {{UNSECURING}} - Indicates the component is in the process of being unsecured
* {{ERROR}} - Indicates the component is not secured due to an error condition

The following tables need to be updated:

* hostcomponentdesiredstate - To indicate whether the component needs security added or removed
* hostcomponentstate - To indicate whether the component is currently configured for security or not 
* servicedesiredstate - To indicate whether the service (and it components) should or should not be secured",Add Security State to Ambari database,3,,['kerberos'],rlevas,True,rlevas,rlevas
ambari,AMBARI-8304,2014-11-13T10:43:48.000+0000,2014-11-13T11:21:04.000+0000,2014-11-13T12:12:25.000+0000,,Fixed,New Feature,Major,['2.0.0'],['2.0.0'],,,,,,,['ambari-web'],['Ambari Web UI'],"Add column ""Versions"" and filtering hosts by stack version.",Hosts: Add Stack versions column to Hosts table,2,,,atkach,True,atkach,atkach
ambari,AMBARI-8277,2014-11-11T15:34:56.000+0000,2015-01-23T11:26:18.000+0000,2015-01-23T11:26:18.000+0000,,Duplicate,New Feature,Blocker,['2.0.0'],['2.0.0'],,,,,,,['ambari-server'],['Ambari Server'],"To help with _kerberizing_ a cluster, a {{Kerberos Plan}} may be used set relevant security properties for a service.

A {{Kerberos Plan}} is essentially composite {{Kerberos Descriptor}} consisting of details for an entire cluster (as opposed to just a single service).  The details that a Kerberos Plan encapsulates are as follows:
* Cluster-wide Kerberos Identities
* Cluster-wide Kerberos-related configurations
* Service-specific Kerberos Identities
* Service-specific Kerberos-related configurations

An (pseudo) Kerberos Plan may be as follows:
{code}
{
  ""identities"": [
    ... cluster-wide identity specifications ...
  ],
  ""configurations"": [
    ... cluster-wide configuration specifications ...
  ],
  ""services"": [
    {
      ... service-specific specifications ...
      ""components"": [
        ... component-specific specifications ...
     ]
    }
  ]
}
{code}",Create facility to get and set kerberos plans via REST API,1,,"['api', 'kerberos', 'kerberos_plan', 'resource']",rlevas,True,rlevas,rlevas
ambari,AMBARI-8247,2014-11-10T14:10:47.000+0000,2014-12-13T03:32:24.000+0000,2014-12-13T04:24:03.000+0000,2014-11-14,Fixed,New Feature,Major,['2.0.0'],['2.0.0'],,,,,,,['ambari-server'],['Ambari Server'],Provide a way for a caller via the REST API to get information about a service's Kerberos descriptor.  This information should probably be attached to a service resource response.,Provide a way to get service-specific Kerberos descriptor via REST API,5,,"['api', 'service', 'stack']",rlevas,True,rlevas,rlevas
ambari,AMBARI-8245,2014-11-10T11:18:15.000+0000,2014-11-10T12:02:25.000+0000,2014-11-10T12:49:25.000+0000,,Fixed,New Feature,Major,['2.0.0'],['2.0.0'],,,,,,,['ambari-web'],['Ambari Web UI'],"Add tab ""Versions"" to host detail page. Tab contains table with Stack Versions. 
Add ""Current Version"" field to host summary.",Host Details: Add Stack versions tab to host details page,2,,,atkach,True,atkach,atkach
ambari,AMBARI-8166,2014-11-05T19:01:37.000+0000,2015-01-14T20:37:43.000+0000,2015-01-14T20:37:43.000+0000,,Fixed,New Feature,Major,['2.0.0'],['2.0.0'],,,,,,,"['ambari-agent', 'ambari-server']","['Ambari Agent', 'Ambari Server']","There needs to be a way, given the details about a KDC to verify that Ambari and (optionally) the nodes in the existing cluster can connect to it. 

From the cluster hosts, this test should test that the address and port combinations are reachable. 

From the Ambari server, this test should make sure the administrator credentials allow at least read access to the KDC.

As an example of a similar action, see how Oozie does this for DB check pre install.

","Implement custom command for checking connectivity to KDC, via REST API",7,,"['connectivity_', 'kdc', 'kerberos']",rlevas,True,rpidva,rlevas
ambari,AMBARI-8163,2014-11-05T18:33:18.000+0000,2014-11-24T18:19:30.000+0000,2014-11-24T20:05:24.000+0000,2014-11-14,Fixed,New Feature,Major,['2.0.0'],['2.0.0'],,,,,,,['ambari-server'],['Ambari Server'],"Currently, it is possible to query Ambari (via the REST API) for details about _asynchronous_ requests and their related tasks. This useful when trying to obtain progress information.  However, some information necessary for the UI to indicate meaningful progress is not available.  This information is related to the stages that are generated. 

*NOTE:* Each _asynchronous_ request is broken down into 1 or more stages and each stage contains 1 or more tasks.

If stage information was available via the REST API, it would be possible for the caller (maybe a UI) to track high-level tasks (at the {{stage}} level) rather than each lower-level unit of work (at the {{task}} level).   

To allow for this, a new API resource (and associated handler) needs to be created.  The resource should be read-only (like {{requests}} and {{tasks}}), and should provide information stored in the {{stage}} table from the Ambari database.  

The following properties should be returned for each {{stage}}:

* stage_id
* request_id
* cluster_id
* request_context 
** _This should probably be renamed to something more appropriate, like stage_context, stage_name, or etc..._
* start_time
* end_time
* progress_percent
* status

It is expected that the resources would be queried using:

{code}
GET  /api/v1/clusters/{clusterid}/requests/{requestid}/stages
{code}

Also, some subset of the stage data should be provided when querying for details about a specific {{request}}, like in:

{code}
GET  /api/v1/clusters/{clusterid}/requests/{requestid}
{code}

See {{request}} and {{task}} resource for examples.


",Provide stage resource information via REST API,4,,"['api', 'resources', 'rest_api']",rlevas,True,tbeerbower,rlevas
ambari,AMBARI-7985,2014-10-27T14:28:29.000+0000,2014-11-19T21:46:26.000+0000,2015-06-26T00:03:28.000+0000,,Fixed,New Feature,Major,['2.0.0'],['2.0.0'],,,,,,,['ambari-server'],['Ambari Server'],"Ambari currently handles _client-/agent-side_ commands; however there is no ability to handle _server-side_ commands. Server-side commands should be specified as a task in a stage and managed along with the stage.

*Use Case:*  Generate principals and keytabs on the Ambari server before sending the keytabs to their relevant hosts.

*Implementation:*  To add the concept of a server-side task:
* update {{org.apache.ambari.server.serveraction.ServerAction}} to be an _abstract class_
** _server-side_ tasks must implement this class 
* reuse existing _host_role_command_ and _execution_command_ data
** _server-side_ tasks are to have a role of {{AMBARI_SERVER_ACTION}}
** _server-side_  execution command data should be encapsulated as JSON and specify the ServerAction implementation class and any needed payload data
* {{org.apache.ambari.server.actionmanager.ActionScheduler}} and {{org.apache.ambari.server.serveraction.ServerActionManagerImpl}} need to be updated to handle the execution of server-side tasks
** each _server-side_ task should be executed in its own thread.
*** _server_side_ tasks should be executed in (staged) order, serially - not in parallel
*** _server_side_ tasks should ensure not to mess up _stage_ ordering

",Allow for server-side commands,6,,"['ambari-server', 'commands', 'server', 'server-side', 'tasks']",rlevas,True,rlevas,rlevas
ambari,AMBARI-7940,2014-10-23T20:44:19.000+0000,,2017-04-20T18:53:11.000+0000,,,New Feature,Major,,,,,,,,,['ambari-server'],['Ambari Server'],"Support Removing Hosts via the API.  This would be a higher level api than the existing api's where a user would need to make multiple api invocations to decommission the host first, etc.  

 ",Blueprints: Provide support for Remove Host in the REST API,2,,,jspeidel,True,jspeidel,jspeidel
ambari,AMBARI-7811,2014-10-16T05:45:00.000+0000,,2017-04-01T04:34:40.000+0000,,,New Feature,Major,,,,,,,,,['ambari-server'],['Ambari Server'],"A hadoop cluster has many nodes, many services and many components, which generates many logs on different machines.  It would be a good idea to search and view those logs at a central place.  

The ELK stack (elastic search, logstash and kibana) is the one of the best open source log collection, storage and search solution.  By integrating ELK with Ambari, user could optionally install ELK when deploying a cluster.  This will enable user to search and view all the logs at free.","Integrate ElasticSearch, LogStash and Kibana with Ambari",9,4,,mjshi,True,,mjshi
ambari,AMBARI-7809,2014-10-16T04:49:57.000+0000,,2014-10-16T04:51:08.000+0000,,,New Feature,Major,,['1.7.0'],,,,,,,['ambari-web'],['Ambari Web UI'],"Currently what metrics graphs to display in Ambari-UI is fixed. Jira AMBARI-7808 is providing ability for each stack vendor to choose that default list by including these metrics list in the stack definition (currently in 1.7.0, it is hardcoded on UI side). 
This Jira should provide ability to users to change the metrics graphs on the UI side per their needs. This is very useful because, depending on situation Administrator or Users typically want to see different metrics graphs and correlate them. ",Ambari Web-UI should provide ability to users to select the list of metrics graphs to display,4,1,,vitthal_gogate,True,vitthal_gogate,vitthal_gogate
ambari,AMBARI-7767,2014-10-14T00:39:40.000+0000,2015-03-26T00:07:38.000+0000,2015-03-26T02:41:09.000+0000,,Fixed,New Feature,Major,['2.1.0'],['1.7.0'],,,,,,,['ambari-agent'],['Ambari Agent'],"Ambari agent relies on the following configuration in ambari-agent.ini to find the Ambari server hostname. 
{noformat}
[server]
hostname=localhost
{noformat}
We want to add a feature that let the hostname be discovered via a configurable script similar to how the agent can use 'hostname_script' to find its own hostname. ",Add an agent config to discover server hostname via a script,6,,,chuanliu,True,chuanliu,chuanliu
ambari,AMBARI-7656,2014-10-06T18:03:44.000+0000,,2018-10-02T23:53:36.000+0000,,,New Feature,Major,,,,,,,,,,,"There is currently no first-class Hbase REST API support out of the box for Ambari. It would be nice to have it install as part of the setup, so that it can be managed via Ganglia, and propagate to every node in a cluster.",Add support for Hbase REST API,8,,,267ada9d,True,,267ada9d
ambari,AMBARI-7450,2014-09-23T13:16:55.000+0000,2015-03-19T16:44:38.000+0000,2015-03-19T16:44:51.000+0000,,Fixed,New Feature,Major,['2.0.0'],['2.0.0'],,,,,,,['ambari-web'],['Ambari Web UI'],"Create a wizard in the web-base interface to help users configure Kerberos on the cluster. 

See [Ambari Cluster Kerberization Technical Document|https://issues.apache.org/jira/secure/attachment/12667192/AmbariClusterKerberization.pdf] for more information.",Add UI Wizard to facilitate configuring cluster to use Kerberos,2,,"['kerberos', 'ui', 'wizard']",rlevas,True,jaimin,rlevas
ambari,AMBARI-7449,2014-09-23T13:14:55.000+0000,2015-03-18T10:45:35.000+0000,2015-03-18T10:45:35.000+0000,,Fixed,New Feature,Major,['2.0.0'],['2.0.0'],,,,,,,['ambari-server'],['Ambari Server'],"Update API to enable the ability to configure services to use Kerberos.  The API call(s) should create necessary Kerberos identities (if necessary) and keytab files, distribute them to the relevant hosts, and update relevant service configurations. 

See [Ambari Cluster Kerberization Technical Document|https://issues.apache.org/jira/secure/attachment/12667192/AmbariClusterKerberization.pdf] for more information.
",Update API to enable configuring of services to use Kerberos,3,1,"['api', 'kerberos', 'stack']",rlevas,True,rlevas,rlevas
ambari,AMBARI-7448,2014-09-23T13:10:42.000+0000,2014-11-19T22:02:13.000+0000,2014-11-19T23:27:46.000+0000,,Fixed,New Feature,Major,['2.0.0'],['2.0.0'],,,,,,,['stacks'],['Ambari Stacks'],"Create a service to manage the (optional) Kerberos server (managed KDC) and client components.

See [Ambari Cluster Kerberization Technical Document|https://issues.apache.org/jira/secure/attachment/12671235/AmbariClusterKerberization.pdf] for more information.",Create Kerberos Service,3,,"['component', 'kdc', 'kerberos', 'stack']",rlevas,True,rlevas,rlevas
ambari,AMBARI-7202,2014-09-08T15:28:41.000+0000,2014-12-11T02:30:23.000+0000,2014-12-11T03:24:13.000+0000,,Fixed,New Feature,Major,,['2.0.0'],216000,216000,43200,259200,43200,83,['stacks'],['Ambari Stacks'],Implement logic in Ambari to support common services. See design discussed in AMBARI-7201.,Implement logic in Ambari to support common services.,3,,"['common-services', 'stack']",rlevas,True,jluniya,rlevas
ambari,AMBARI-7166,2014-09-05T10:12:20.000+0000,2014-09-05T10:21:49.000+0000,2014-09-05T10:58:46.000+0000,,Fixed,New Feature,Major,['1.7.0'],['1.7.0'],,,,,,,"['ambari-admin', 'ambari-web']","['Ambari Admin', 'Ambari Web UI']","Provide ability to change the cluster name ID. By changing that cluster name ID, the API calls for that cluster will change too (for example, changing the name from ClusterX to Cluster Y means /api/v1/clusters/ClusterX will then become /api/v1/clusters/ClusterY).",Support changing the cluster name of an existing cluster (Frontend part),2,,,atkach,True,atkach,atkach
ambari,AMBARI-7072,2014-08-29T09:49:23.000+0000,,2015-09-25T21:53:07.000+0000,,,New Feature,Major,,['1.7.0'],,,,,,,"['ambari-server', 'contrib']","['Ambari Server', 'Contributions under ""contrib""']","We'd like to retrieve metadata information (most of them are already persisted) through the API. We have open sourced a project called Periscope https://github.com/sequenceiq/periscope which brings SLA policy based autoscaling for Hadoop clusters installed with Ambari, and when up and downscaling clusters we need to know certain metadata information listed below - these features we have implemented in Periscope but would like to add in Ambari API, REST client and shell:

1. Ability to know the host-hostgroup relationship - from both directions
2. Ability to know the services/components-host relationship - from both directions
3. Ability to know the blueprint-cluster(s) relationship
4. Once Slider is supported, keep and retrieve the Slider apps related metadata


",Metadata information exposed through API,3,1,,matyix,True,matyix,matyix
ambari,AMBARI-6977,2014-08-21T16:19:11.000+0000,2014-08-21T20:04:46.000+0000,2014-08-21T20:04:46.000+0000,,Duplicate,New Feature,Major,['1.7.0'],['1.7.0'],,,,,,,,,"1) During cluster install (either via Blueprints or Wizard), user specifies the ""cluster name"".
2) This becomes the cluster name ID, used in the Ambari Web UI and in the API to uniquely identify the cluster.

Provide ability to change the cluster name ID. By changing that cluster name ID, the API calls for that cluster will change too (for example, changing the name from ClusterX to Cluster Y means /api/v1/clusters/ClusterX will then become /api/v1/clusters/ClusterY).
Expose this rename operation via Ambari API; this should not require direct SQL against the Ambari database
",Support changing the name of an existing cluster via API,1,,,shivanigupta,True,,shivanigupta
ambari,AMBARI-6819,2014-08-11T17:04:17.000+0000,,2014-08-11T17:04:45.000+0000,,,New Feature,Major,,['1.6.1'],,,,,,,,,"Add support for Fair Scheduler settings similar to the Capacity Scheduler dialog.

Also add settings validation similar to AMBARI-6817",add support for Fair Scheduler settings,4,,,harisekhon,True,,harisekhon
ambari,AMBARI-6818,2014-08-11T16:54:38.000+0000,2014-08-11T17:05:32.000+0000,2014-08-11T17:05:32.000+0000,,Duplicate,New Feature,Minor,,['1.6.1'],,,,,,,,,"Taking AMBARI-6815 a step further, add ability to revision control all Ambari configuration changes to allow for easy roll backs and auditing of what was set before.",revision control configuration changes in Ambari to allow roll backs,2,,,harisekhon,True,,harisekhon
ambari,AMBARI-6817,2014-08-11T16:50:35.000+0000,,2014-08-11T16:52:53.000+0000,,,New Feature,Minor,,['1.6.1'],,,,,,,,,"Add validation of capacity scheduler settings eg if queues do not amount to 100%.

Also see AMBARI-6815 for ability to revert settings to default to roll back misconfigurations more easily.",validate capacity scheduler settings,1,,,harisekhon,True,,harisekhon
ambari,AMBARI-6815,2014-08-11T16:43:32.000+0000,2015-01-18T23:31:25.000+0000,2015-01-18T23:31:25.000+0000,,Duplicate,New Feature,Minor,,['1.6.1'],,,,,,,,,"When changing a setting in Ambari it would be nice to have a ""revert to original"" option appear next to it after saving it. Similar to ""undo"" except that ""undo"" disappears after Save.

If you change something, save and restart and then later want to put it back to the default, you have to figure out what it was yourself and set it back manually.",Add revert to default config,2,,,harisekhon,True,,harisekhon
ambari,AMBARI-6762,2014-08-06T21:39:11.000+0000,2014-08-07T02:35:51.000+0000,2014-08-07T02:35:51.000+0000,,Fixed,New Feature,Major,['1.7.0'],,,,,,,,,,"Include both host group and cluster scoped configuration in an exported blueprint.

To export a blueprint, use the api:
AMBARI_HOST:8080/api/v1/clusters/:clustername?format=blueprint

The exported blueprint will contain the entire configuration for the associated cluster.  The only properties which are not included are those that are marked as ""input required"" in the stack.  All passwords are marked as required so they will not be exported.  Also, any hostnames in the configuration properties are replaced with a hostgroup token: %HOSTGROUP::group1%.  When a cluster is provisioned from the blueprint, the hostgroup tokens are resolved to host names for the target cluster. ",Include configuration in exported blueprint,1,,"['api', 'blueprints', 'configuration']",jspeidel,True,jspeidel,jspeidel
ambari,AMBARI-6706,2014-08-01T11:01:37.000+0000,,2015-03-05T15:49:08.000+0000,,,New Feature,Major,,"['1.6.1', '1.7.0']",,,,,,,['ambari-web'],['Ambari Web UI'],"Rolling restarts currently only work for slaves such as Datanodes in Web UI when doing Service Actions -> Restart Datanodes. When doing a Restart All it doesn't give any option to do a rolling restart across all components including masters.

I usually configure HDFS NameNode HA but this means that in Ambari there is downtime in reconfiguration even with NN HA enabled since it doesn't do rolling restarts of the NameNodes in order to keep the cluster up.

This is needed to actually make the service really HA including HA during routine maintenance and reconfiguration. Maybe I could workaround to shut master components one by one manually but would be nice if Ambari could support this directly.

Regards,

Hari Sekhon
http://www.linkedin.com/in/harisekhon",Rolling restarts for HA Masters eg HDFS NameNodes,2,,,harisekhon,True,,harisekhon
ambari,AMBARI-6619,2014-07-25T16:08:03.000+0000,,2014-07-25T16:08:03.000+0000,,,New Feature,Major,,,,,2419200,2419200,2419200,,['ambari-web'],['Ambari Web UI'],"I have limited disk space on my cluster's default mount ""/""
It will be great if I can choose which mount I want my service to be on.
Such as ""/grid/""",Choose to install services on different mounts,1,,['mount'],yixiaolin,True,,yixiaolin
ambari,AMBARI-6618,2014-07-25T16:04:13.000+0000,2014-07-30T20:56:16.000+0000,2014-07-30T20:56:16.000+0000,,Duplicate,New Feature,Major,['1.7.0'],,,,604800,604800,604800,,['ambari-web'],['Ambari Web UI'],I would like to switch between different JDKs to point HDFS service to use my custom JDK.,The ability to switch between different JDK,1,,,yixiaolin,True,,yixiaolin
ambari,AMBARI-6416,2014-07-08T13:47:43.000+0000,2014-08-30T06:44:41.000+0000,2014-08-30T06:45:12.000+0000,,Done,New Feature,Major,['1.7.0'],['1.6.1'],,,,,,,['ambari-client'],"['Ambari Client (Python, Groovy, and others)']",Add the functionality to the Groovy rest client to add a new host to an existing cluster and install components on it,Groovy client add new hosts and components to an existing cluster,2,,,keyki,True,keyki,keyki
ambari,AMBARI-6086,2014-06-11T16:55:36.000+0000,2014-06-16T18:14:13.000+0000,2014-06-23T16:53:31.000+0000,2014-06-18,Fixed,New Feature,Major,['1.6.1'],['1.6.1'],,,,,,,['infra'],"['Project infrastructure including builds, releases mechanics and automation']",Implement 2.1.GlusterFS stack for version 1.6.1 and above.,Add Gluster 2.1 Support for HDP,5,,['2.1.GlusterFS'],screeley,True,screeley,screeley
ambari,AMBARI-6058,2014-06-09T07:42:32.000+0000,,2014-06-11T06:13:46.000+0000,,,New Feature,Major,,,,,,,,,,,"Tthere is a host or more than one in a host_group. So if I want to set configurations for each host. How to do? 
I think it isn't convenient to set configurations to each host when a host_group has many hosts.For example,
{
  ""blueprint"": ""multi-node-hdfs-yarn"",
  ""host_groups"":[
    {
      ""name"": ""master"", 
      ""hosts"": [         
        {
          ""fqdn"": ""c6401.ambari.apache.org""
        }
      ]
    },
    {
      ""name"": ""slaves"", 
      ""hosts"": [
        {
          ""fqdn"": ""c6402.ambari.apache.org""
        },
        {
          ""fqdn"": ""c6403.ambari.apache.org""
        }
      ]
    }
  ]
}

The host_groups slaves has two hosts. In this case how to set configurations for each host?
",add configurations to specific host to create cluster using blueprint ?,2,,"['features', 'rest_api']",chiq,True,,chiq
ambari,AMBARI-5699,2014-05-07T15:21:20.000+0000,2014-06-19T21:05:14.000+0000,2014-06-19T21:05:14.000+0000,,Won't Fix,New Feature,Major,,['1.6.1'],,,,,,,,,,Only open necessary ports for Hadoop instead of disabling IPTABLES,2,,,erinaboyd,True,erinaboyd,erinaboyd
ambari,AMBARI-5660,2014-05-02T10:00:06.000+0000,2014-05-02T14:52:32.000+0000,2014-05-02T14:52:32.000+0000,,Duplicate,New Feature,Major,,['1.5.0'],,,,,,,,,"Can we add the ability to cancel / forcibly kill a background operation?

For example I restart the Falcon service, but it's taking ages and has failed before so I elect to just restart all services to see if that helps with  any dependencies, but the restart all command is stuck behind waiting for the restart Falcon service to fail or eventually timeout. Right now I can't kill the slow/hanging Falcon restart command (which I believe will fail anyway), in order to allow the new restart all services operation to commence.

Thanks

Hari Sekhon
http://www.linkedin.com/in/harisekhon",Ability to cancel background operation,2,,,harisekhon,True,,harisekhon
ambari,AMBARI-5553,2014-04-23T22:51:21.000+0000,2014-04-24T15:13:06.000+0000,2014-04-24T15:13:06.000+0000,,Fixed,New Feature,Major,['1.6.0'],,,,,,,,,,"Added dependency related information to stacks in the REST API.  This information describes dependencies between components which are part of a stack.  

A new dependencies resources will be added as a child to the stacksServices/serviceComponent (soon to be renamed to services/component) resource.

For example:
GET http://172.18.192.3:8080/api/v1/stacks/HDP/versions/1.3.3/stackServices/HBASE/serviceComponents/HBASE_MASTER?fields=dependencies/*

{code}
{
  ""href"" : ""http://172.18.192.3:8080/api/v1/stacks/HDP/versions/1.3.3/stackServices/HBASE/serviceComponents/HBASE_MASTER?fields=dependencies/*"",
  ""StackServiceComponents"" : {
    ""component_name"" : ""HBASE_MASTER"",
    ""service_name"" : ""HBASE"",
    ""stack_name"" : ""HDP"",
    ""stack_version"" : ""1.3.3""
  },
  ""dependencies"" : [
    {
      ""href"" : ""http://172.18.192.3:8080/api/v1/stacks/HDP/versions/1.3.3/stackServices/HBASE/serviceComponents/HBASE_MASTER/dependencies/HDFS_CLIENT"",
      ""Dependencies"" : {
        ""component_name"" : ""HDFS_CLIENT"",
        ""dependent_component_name"" : ""HBASE_MASTER"",
        ""dependent_service_name"" : ""HBASE"",
        ""scope"" : ""host"",
        ""service_name"" : ""HDFS"",
        ""stack_name"" : ""HDP"",
        ""stack_version"" : ""1.3.3""
      },
      ""auto_deploy"" : {
        ""enabled"" : true
      }
    },
    {
      ""href"" : ""http://172.18.192.3:8080/api/v1/stacks/HDP/versions/1.3.3/stackServices/HBASE/serviceComponents/HBASE_MASTER/dependencies/ZOOKEEPER_SERVER"",
      ""Dependencies"" : {
        ""component_name"" : ""ZOOKEEPER_SERVER"",
        ""dependent_component_name"" : ""HBASE_MASTER"",
        ""dependent_service_name"" : ""HBASE"",
        ""scope"" : ""cluster"",
        ""service_name"" : ""ZOOKEEPER"",
        ""stack_name"" : ""HDP"",
        ""stack_version"" : ""1.3.3""
      },
      ""auto_deploy"" : {
        ""enabled"" : true,
        ""location"" : ""HBASE/HBASE_MASTER""
      }
    }
  ]
}
{code}

",Add dependency related information to stacks in the REST API,2,,"['API', 'stack']",jspeidel,True,jspeidel,jspeidel
ambari,AMBARI-5550,2014-04-23T19:40:37.000+0000,2014-04-28T19:10:40.000+0000,2017-06-12T20:57:26.000+0000,,Fixed,New Feature,Blocker,,,,,,,,,['contrib'],"['Contributions under ""contrib""']",The MSI should support fresh install as well as upgrade.,Create MSI for Ambari-SCOM 2.0.0.0,3,,,imalamen,True,imalamen,imalamen
ambari,AMBARI-5482,2014-04-16T15:06:36.000+0000,2014-05-31T07:22:39.000+0000,2014-05-31T19:24:15.000+0000,,Fixed,New Feature,Major,['1.6.1'],['1.6.0'],,,,,,,,,"There is an open source effort to implement Ambari-Shell: https://github.com/sequenceiq/ambari-shell
It would make sense to integrate it into the official source tree.

The attached ambari-shell.md contains a detailed explanation of what Ambari shell is, but in broadly
it is a context aware, interactive command line tool which simplifies the usage of the Ambari's REST API without 
using the webui. The plan is to cover all the functionalities that Ambari provides to be able to automate
every necessary step needed to create and manage a cluster.",Integrate Ambari Shell,6,2,,lalyos,True,matyix,lalyos
ambari,AMBARI-5462,2014-04-14T16:15:31.000+0000,2014-04-15T18:36:36.000+0000,2014-04-15T18:36:36.000+0000,,Fixed,New Feature,Major,['1.6.0'],,,,,,,,,,"Allow configuration to be specified for host groups.  This configuration will provide overrides for all hosts assigned to the host group.  This configuration is specified inline within the host group. 

An example of a simple one host group blueprint with both cluster scoped and host group scoped configuration.

{code}
{
  ""configurations"" : [
    {
      ""core-site"" : {
        ""fs.trash.interval"" : ""480"",
        ""ipc.client.idlethreshold"" : ""8500"",
        ""my.awesome.property"" : ""excellent""
      }
    },
    {
      ""mapred-site"" : {
        ""tasktracker.http.threads"" : ""45""
      }
    }
  ],
  ""host_groups"" : [
    {
      ""name"" : ""host_group_1"",
      ""configurations"" : [
        {
          ""core-site"" : {
            ""fs.trash.interval"" : ""475""
          }
        }
      ],     
      ""components"" : [
        {
          ""name"" : ""HDFS_CLIENT""
        },
        {
          ""name"" : ""GANGLIA_SERVER""
        },
        {
          ""name"" : ""AMBARI_SERVER""
        },
        {
          ""name"" : ""MAPREDUCE_CLIENT""
        },
        {
          ""name"" : ""GANGLIA_MONITOR""
        },
        {
          ""name"" : ""DATANODE""
        },
        {
          ""name"" : ""NAMENODE""
        },
        {
          ""name"" : ""JOBTRACKER""
        },
        {
          ""name"" : ""HISTORYSERVER""
        },
        {
          ""name"" : ""SECONDARY_NAMENODE""
        },
        {
          ""name"" : ""NAGIOS_SERVER""
        },
        {
          ""name"" : ""TASKTRACKER""
        }
      ],
      ""cardinality"" : ""1""
    }
  ],
  ""Blueprints"" : {
    ""blueprint_name"" : ""single-node-test"",
    ""stack_name"" : ""HDP"",
    ""stack_version"" : ""1.3.3""
  }
}
{code} 

*Note:* Allowing service configurations which are specified external to the host group definition to allow for configuration reuse across host groups will be handled in a subsequent Jira.

 ",Allow host group scoped configuration to be specified in Blueprint,1,,"['api', 'blueprints']",jspeidel,True,jspeidel,jspeidel
ambari,AMBARI-23249,2018-03-15T17:42:37.000+0000,,2018-03-15T19:08:10.000+0000,,,New Feature,Major,['3.0.0'],['2.6.0'],,,,,,,['ambari-metrics'],['Ambari Metrics System'],"Currently Ambari does not support Grafana LDAP configuration (ldap.toml ) from the Ambari UI. Ambar should provide configuration option in the UI. few customers were looking for this feature.

",Grafana LDAP configuration from Ambari UI,4,,,apappu@hortonworks.com,True,,apappu@hortonworks.com
ambari,AMBARI-23168,2018-03-07T18:17:34.000+0000,,2018-03-07T18:17:34.000+0000,,,New Feature,Major,,['2.5.0'],,,,,,,"['ambari-agent', 'ambari-server', 'infra']","['Ambari Agent', 'Ambari Server', 'Project infrastructure including builds, releases mechanics and automation']","Feature Request for Kubernetes integration to deploy all components via Kubernetes.

Hadoop is targeting Docker integration in 3.1 and MapR is moving towards full Kubernetes deployment support. Also the market in general seems to be moving in the Kubernetes direction outside of the Hadoop ecosystem so they are going to converge eventually so may as well start working towards towards this convergence earlier.",Kubernetes integration,3,,,harisekhon,True,,harisekhon
ambari,AMBARI-23154,2018-03-06T15:01:48.000+0000,,2018-03-06T15:03:07.000+0000,,,New Feature,Major,,['2.5.0'],,,,,,,"['ambari-admin', 'ambari-agent', 'ambari-server', 'infra', 'stacks']","['Ambari Admin', 'Ambari Agent', 'Ambari Server', 'Project infrastructure including builds, releases mechanics and automation', 'Ambari Stacks']","Feature Request to port all service management to standard Linux Systemd service units. Ambari can then delegate calls to systemd which is better designed for service management anyway.

This will allow cluster admins to use standard Linux operating system commands, even when Ambari is unavailable (Ambari lacks true native HA at time of writing).

Delegating service management to Systemd improves the design to align with other standard software rather than being more esoteric as Hadoop has historically been (I'm also pushing MapR to do the same and drop their old Warden service management system which is also an outdated mechanism by modern systems engineering standards).",Ambari service management via standard Linux Systemd service units,1,,,harisekhon,True,,harisekhon
ambari,AMBARI-22708,2017-12-29T06:37:43.000+0000,,2017-12-29T06:37:43.000+0000,,,New Feature,Trivial,,,,,,,,,['alerts'],['Ambari Alerts System'],"First some background:

We were directed to retain audit/access records ""forever"" (technically 7 years but that is basically forever in electronic log time). 

Each Hadoop component generates local audit logs as per their log4j settings. In our production system these logs would frequently fill up the disk. At first we would just compress them in place but that only works for so long and there was no redundancy with local disk storage. In others words, no long term plan. 

We started to discuss moving them to HDFS or a different storage solution. One of our team members pointed out the Ranger plugins are already logging the ""same data"" into HDFS. 
Probably after several meeting with the higher-ups, using Ranger logs as the record truth was approved. Components log4j settings were updated to purge data automatically. 

Purging local logs felt like operating with out a safety net. 
Thought it we be good to check that Ranger was successful logging to HDFS each day. Should mention this is a kerberized cluster, not that anything ever goes wrong with kerberos.  
*Checking this would have certainly been possible with a shell script, but we have been pushing to centralize warning/alerts in Ambari. And so an Ambari alert python script to check on Ranger Logging Health was crafted. *

For the most part the alert was modeled after some of the hive alerts. 
At the moment it just checks that the daily /ranger/audit/<component> HDFS directory has been created. 

I am attaching the host script and the alert.json for HDFS and Knox components. 
In the alert.json, service_name and component_name should be set to local values. 
Everything else should ""work out of the box"". 
",Ranger HDFS logging health Ambari Alert,2,,['features'],quirogadf,True,,quirogadf
ambari,AMBARI-4786,2014-02-21T17:52:55.000+0000,2014-02-27T16:05:42.000+0000,2016-08-05T06:41:03.000+0000,,Fixed,New Feature,Major,['1.5.0'],,,,,,,,,,"Export a blueprint for a running cluster using an alternate rendering for the cluster resource.

api/v1/clusters/c1?format=blueprint

For this change, the blueprint will be very minimal and will only contain node groups and no configuration.  Subsequent patches will introduce configuration and other cluster data.

{code}
{
  ""host_groups"" : [
    {
      ""name"" : ""host_group_1"",
      ""components"" : [
        {
          ""name"" : ""HISTORYSERVER""
        },
        {
          ""name"" : ""OOZIE_CLIENT""
        },
        {
          ""name"" : ""JOBTRACKER""
        },
        {
          ""name"" : ""NAMENODE""
        },
        {
          ""name"" : ""OOZIE_SERVER""
        },
        {
          ""name"" : ""TASKTRACKER""
        },
        {
          ""name"" : ""NAGIOS_SERVER""
        },
        {
          ""name"" : ""SECONDARY_NAMENODE""
        },
        {
          ""name"" : ""MAPREDUCE_CLIENT""
        },
        {
          ""name"" : ""AMBARI_SERVER""
        },
        {
          ""name"" : ""GANGLIA_SERVER""
        },
        {
          ""name"" : ""HDFS_CLIENT""
        },
        {
          ""name"" : ""DATANODE""
        },
        {
          ""name"" : ""GANGLIA_MONITOR""
        }
      ],
      ""cardinality"" : ""1""
    }
  ],
  ""Blueprints"" : {
    ""blueprint_name"" : ""blueprint-c1"",
    ""stack_name"" : ""HDP"",
    ""stack_version"" : ""1.3.3""
  }
}
{code}",Add ability to export a blueprint from a running cluster ,2,,,jspeidel,True,jspeidel,jspeidel
ambari,AMBARI-4688,2014-02-14T21:39:52.000+0000,2014-04-09T23:14:16.000+0000,2014-04-09T23:15:17.000+0000,,Fixed,New Feature,Major,['1.6.0'],['1.5.0'],,,,,,,,,,Add GlusterFS to 2.1.1 Stack,2,,,erinaboyd,True,erinaboyd,erinaboyd
ambari,AMBARI-4671,2014-02-14T02:14:44.000+0000,2014-09-05T23:21:56.000+0000,2014-09-06T00:10:28.000+0000,,Fixed,New Feature,Critical,['1.7.0'],,,,604800,604800,604800,,['ambari-agent'],['Ambari Agent'],"Ambari should support the ability to change the cluster name.

One use case is the desire to relocate your cluster without removing the data. Perhaps there is a network conflict at the new location - it should be straightforward to change the cluster name via Ambari.
",Support changing the cluster name of an existing hadoop cluster,4,1,['features'],brian12346,True,mahadev,brian12346
ambari,AMBARI-4670,2014-02-14T02:11:34.000+0000,2016-04-04T22:49:49.000+0000,2016-04-04T22:49:49.000+0000,,Duplicate,New Feature,Critical,,,,,604800,604800,604800,,['ambari-agent'],['Ambari Agent'],"Ambari should support the ability to change the name of nodes in the cluster.

One use case is the desire to relocate your cluster without removing the data.  Perhaps there is a network conflict at the new location - it should be straightforward to change the name of the nodes via Ambari. ",Support changing hostname of installed hadoop nodes,3,1,['features'],brian12346,True,,brian12346
ambari,AMBARI-4657,2014-02-13T15:11:52.000+0000,,2014-02-23T07:14:27.000+0000,,,New Feature,Major,,,,,,,,,,,"Create a Juju Charm for Ambari in order for users to: 
do a one line install (juju quickstart bundle...) 
have a one line upgrade (juju upgrade-charm) 
instantly scale (juju add-unit) 
instantly integrate with other charms like Hadoop, Hive, HBase, Zookeeper, etc. (juju add-relation) 

More info on Juju Charms at http://juju.ubuntu.com/docs.",Create a Juju Charm for Ambari,2,,,mectors,True,,mectors
ambari,AMBARI-4467,2014-01-30T01:55:08.000+0000,2014-02-03T16:01:44.000+0000,2014-02-03T16:01:44.000+0000,,Fixed,New Feature,Major,['1.5.0'],,,,,,,,,,"Create a new /blueprints REST endpoint. This endpoint represents an 'abstract blueprint' or 'template' and doesn't contain cluster specific details such specific host information.

This initial jira will be limited to basic blueprint information and will not contain configuration elements.  These additional elements will be added in subsequent patches.

Available operations are get, create and delete.  Update is not supported because blueprints are immutable.

Example of a simple blueprint resource:
{code}
{
  ""href"" : ""http://172.18.193.10:8080/api/v1/blueprints/bp1"",
  ""host_groups"" : [
    {
      ""name"" : ""foo"",
      ""components"" : [
        {
          ""name"" : ""component2""
        },
        {
          ""name"" : ""component1""
        },
        {
          ""name"" : ""component4""
        },
        {
          ""name"" : ""component3""
        }
      ],
      ""cardinality"" : ""2""
    },
    {
      ""name"" : ""bar"",
      ""components"" : [
        {
          ""name"" : ""component5""
        }
      ],
      ""cardinality"" : ""1""
    }
  ],
  ""Blueprints"" : {
    ""blueprint_name"" : ""bp1"",
    ""stack_name"" : ""HDP"",
    ""stack_version"" : ""1.3.3""
  }
}
{code}



",Create new /blueprints REST endpoint,7,,"['api-addition', 'blueprints']",jspeidel,True,jspeidel,jspeidel
ambari,AMBARI-4465,2014-01-29T22:02:55.000+0000,2014-01-30T20:27:43.000+0000,2014-01-30T20:27:43.000+0000,,Fixed,New Feature,Major,['1.5.0'],['1.5.0'],,,,,,,['contrib'],"['Contributions under ""contrib""']","

The class org.apache.ambari.msi.ClusterDefinition defines the cluster created by the HDP MSI. It contains a component name mapping to account for differences in what is provided by the MSI and what is expected by the Ambari providers. The mapping should be updated to include the components from the HDP2 MSI.

The MSI provides the cluster definition in the clusterproperties.txt file. The ClusterDefinition class reads this file to define the cluster. The names provided in the file do not match what is expected by Ambari, so the mapping is required. See... ambari-server/src/main/resources/stacks.HDP for the expected service / component names.
",Update component name mapping in ClusterDefinition,2,,,abaranchuk,True,abaranchuk,abaranchuk
ambari,AMBARI-4366,2014-01-21T11:52:51.000+0000,2014-01-21T11:57:14.000+0000,2014-01-21T11:57:14.000+0000,,Fixed,New Feature,Major,['1.5.0'],['1.5.0'],,,,,,,['ambari-web'],['Ambari Web UI'],"h3. Install Wizard > Customize Services
Under ZooKeeper tab, there's a new section called ""Custom zoo.cfg"", just like ""Custom core-site.xml"" (with the ability to add / override properties).  Also, relevant properties exposed in General/Advanced sections should go into ""zoo.cfg"".

h3. Services > ZooKeeper > Config
Mirror the changes in Install Wizard > Customize Services.

h3. Add Services Wizard
Mirror the changes in Install Wizard > Customize Services.

h3. Add Hosts Wizard > Configurations
Make sure that the ZooKeeper config group can be selected.

h3. Misc
Config changes to zoo.cfg should trigger restart indicator, actual pushing of zoo.cfg file content on the servers, etc.",Manage ZooKeeper configs with zoo.cfg,2,,,atkach,True,atkach,atkach
ambari,AMBARI-4229,2014-01-06T17:01:49.000+0000,2014-01-29T15:54:06.000+0000,2014-01-29T15:54:06.000+0000,,Fixed,New Feature,Major,['1.5.0'],['1.5.0'],,,,,,,['ambari-web'],['Ambari Web UI'],Add ability to turn on/off maintenance mode for each component on host detail page,Maintenance Mode: Host Detail page (Components Section),3,,,ababiichuk,True,ababiichuk,ababiichuk
ambari,AMBARI-4205,2013-12-31T23:46:04.000+0000,2014-04-06T21:05:33.000+0000,2014-04-06T21:05:33.000+0000,,Fixed,New Feature,Major,['1.6.0'],,,,,,,,,,,Add Existing Postgres Database option for Hive and Oozie Database during Ambari cluster install,4,,['patch'],miharp,True,miharp,miharp
ambari,AMBARI-4204,2013-12-31T23:45:29.000+0000,2014-03-26T23:46:14.000+0000,2014-03-26T23:46:14.000+0000,,Duplicate,New Feature,Major,,,,,,,,,,,,Add Existing Postgres Database option for Oozie Database during Ambari cluster install,1,,,miharp,True,miharp,miharp
ambari,AMBARI-4032,2013-12-10T17:44:50.000+0000,2014-01-24T00:25:52.000+0000,2014-01-24T00:25:52.000+0000,,Fixed,New Feature,Major,['1.5.0'],['1.5.0'],,,,,,,['ambari-server'],['Ambari Server'],"*Definitions*
_Request Schedule Resource_

A request schedule resource captures all the information required to create a request for executing a “Custom Action” or a “Custom Command” or a “First Class Command: (Start, Stop, Restart)” with a recurring schedule or a point in time execution if no schedule is specified.

*Scenarios to support*

1. _Rolling restart of Host Components / Service components_

Restart services with zero/minimal downtime for upgrades or configuration changes. You can do a rolling restart on individual services (HDFS, MapReduce, HBase, ZooKeeper, etc). Rolling restart also lets you choose which service components (Namenode, etc.) to restart.

2. _Rebalancer_

Rebalancer is a sample of a custom action that can be executed to initiate HDFS block rebalance. This could be a one time task or a recurring task (e.g. rebalance once each week at a specific time).

",Ability to schedule a recurring request execution of an Action/Command,1,,,swagle,True,swagle,swagle
ambari,AMBARI-4016,2013-12-09T08:51:01.000+0000,,2018-06-07T09:37:31.000+0000,,,New Feature,Major,['2.3.0'],,,,,,,,['ambari-server'],['Ambari Server'],"Our team has a fork of ambari 1.4.1, we need ambari service to be HA. 
But seems currently not supported.

Whether we can implement ambari HA like this
1)	Deploy 2 ambari-server with it database in 2 machine, say primary and backup;
2)	Sync data from primary database to backup database(using postgresql streaming sync) when have data updated.
3)	Use heartbeat between the 2 ambari-server to discover server_lost event, and make backup server to be primary; and the new backup server stop communicate with ambari-agent
4)	Each agent have 2 ambari-server’s address configurations(or negotiate from zookeeper?), once primary server changed, agent should register to new one.
5)	Backup webui can redirect page to primary one. GET method REST api can access both ambari-server

My question
a.	Can this works?
b.	Any other information I need sync or restore other than information in database.
",Ambari Server HA,21,9,['HA'],shihaoliang,True,,shihaoliang
ambari,AMBARI-3883,2013-11-26T11:17:21.000+0000,2013-11-26T11:21:28.000+0000,2013-11-26T11:21:28.000+0000,,Fixed,New Feature,Critical,['1.4.3'],['1.4.1'],,,,,,,['ambari-web'],['Ambari Web UI'],"Installer needs config-groups management during install. The creation of groups etc. should not happen till Deploy is pressed.
",Provide config-groups capability in installer,2,,,atkach,True,atkach,atkach
ambari,AMBARI-3774,2013-11-15T15:56:38.000+0000,2013-11-15T15:59:44.000+0000,2013-11-21T08:47:00.000+0000,,Fixed,New Feature,Major,['1.4.3'],['1.4.2'],,,,,,,['ambari-web'],['Ambari Web UI'],"Need ability to add/remove hosts from various config-groups. Any host membership change will require the Save button to be clicked - at which point the changes are persisted to server.
In a service, hosts can belong to only 1 non-default group. Or to say another way, only hosts in the Default config-group can be added to other config-groups.
In the Default config-group you cannot 'add' or 'delete' a host. Its hosts are dynamically determined by adding/removing from non-default config-groups.
Refer to mockups.",Provide ability to add/remove hosts in manage-config-groups dialog,2,,,atkach,True,atkach,atkach
ambari,AMBARI-3767,2013-11-14T16:19:41.000+0000,2013-11-14T16:23:10.000+0000,2013-11-21T11:05:10.000+0000,,Fixed,New Feature,Major,['1.4.3'],['1.4.1'],,,,,,,['ambari-web'],['Ambari Web UI'],"For this issue we need to provide a basic config-group management dialog. The dialog should be launched from 'Manage Configuration Groups...' action in the 'Service Actions...' combo.

The dialog should at a minimum show the available config-groups in the left panel, and when selected should show the hosts in the right panel. Default is the only special config-group which should show all hosts not belonging to any config-groups.

The basic dialog should also show the properties link with hover and popup.
Mockups are provided in AMBARI-3531. ",Provide basic config-group management dialog,2,,,akovalenko,True,akovalenko,akovalenko
ambari,AMBARI-3765,2013-11-14T15:47:44.000+0000,2013-11-14T15:51:05.000+0000,2013-11-21T08:47:42.000+0000,,Fixed,New Feature,Major,['1.4.3'],['1.4.1'],,,,,,,['ambari-web'],['Ambari Web UI'],"We need an easy way for admins to filter and find properties in config-groups.
From the current filter we should remove the options 'Modified Properties' and 'Properties needing restart', as we will initially focus only on 'Overridden Properties'. When 'Overridden Properties' is selected, only overridden configs show in UI, and filtering applies only to them. When unselected all configs should show as normal.
Filtering should match on key or value.
This should be done in common configs so that all implementations (services, hosts, add-host, install, etc.) get it for free.",Create property filtering capability for config-groups,2,,,atkach,True,atkach,atkach
ambari,AMBARI-3753,2013-11-12T21:03:22.000+0000,2013-12-17T01:02:15.000+0000,2013-12-17T01:02:15.000+0000,,Fixed,New Feature,Major,['1.4.3'],['1.4.3'],,,,,,,,,Create a stack to include GlusterFS as Hadoop Compatible File System,Add GlusterFS to 2.x Stack,3,,,erinaboyd,True,erinaboyd,erinaboyd
ambari,AMBARI-3531,2013-10-16T22:39:25.000+0000,2013-11-19T20:33:08.000+0000,2013-11-19T20:33:08.000+0000,,Fixed,New Feature,Major,['1.4.3'],['1.4.1'],,,,,,,['ambari-server'],['Ambari Server'],"Ambari currently provides cluster level service configurations. 

This is a problem when the cluster has a mix of different hardwares requiring different configurations for a group of hosts of a service. For example, some DataNodes having bigger disks might need extra mount points in {{dfs_datanode_data_dir}}.  

Ambari needs to provide configuration override capability for a group of hosts per service. 

The primary use cases from UI perspective are the ability to
# Override 3 properties for 500 hosts
# Go to host and see effective configurations
# Save configuration without restarting
# Indicate which services/components need restarting
",Provide configuration host overrides capability in Ambari,4,,,srimanth.gunturi,True,swagle,srimanth.gunturi
ambari,AMBARI-3500,2013-10-11T14:30:36.000+0000,2013-10-11T14:34:23.000+0000,2013-10-11T14:34:23.000+0000,,Fixed,New Feature,Major,['1.4.2'],['1.4.2'],,,,,,,['ambari-web'],['Ambari Web UI'],,Reassign Master Wizard: move non-HA SNameNode in 2.x stack (non-secure),2,,,akovalenko,True,akovalenko,akovalenko
ambari,AMBARI-3492,2013-10-10T12:46:09.000+0000,2013-10-10T12:52:55.000+0000,2013-10-10T12:52:55.000+0000,,Fixed,New Feature,Major,['1.4.2'],['1.4.2'],,,,,,,['ambari-web'],['Ambari Web UI'],,Reassign Master Wizard: move HA NameNode in 2.x stack (non-secure),2,,,akovalenko,True,akovalenko,akovalenko
ambari,AMBARI-3472,2013-10-07T16:01:10.000+0000,2013-10-07T16:06:02.000+0000,2013-10-07T16:06:02.000+0000,,Fixed,New Feature,Major,['1.4.2'],['1.4.2'],,,,,,,['ambari-web'],['Ambari Web UI'],"HBase Service Summary:
List all the HBase Masters. Give appropriate label for the ""Active HBase Master"" and the ""Standby HBase Master""s (mirror what we do for HDFS service summary page, except there could be more than 2 HBase Masters).
Host Detail page:
Active HBase Master should be shown as ""Active HBase Master"".
Standby HBase Master (up but not Active) should be shown as ""Standby HBase Master"".
HBase Master that is stopped should be shown as ""HBase Master"".",HBase Service Summary and Host Detail page: add support for multiple HBase Masters,2,,,ababiichuk,True,ababiichuk,ababiichuk
ambari,AMBARI-3447,2013-10-03T15:10:01.000+0000,2013-10-03T15:18:29.000+0000,2013-10-03T15:18:29.000+0000,,Fixed,New Feature,Major,['1.4.2'],['1.4.2'],,,,,,,['ambari-web'],['Ambari Web UI'],,Reassign Master Wizard: move ResourceManager in 2.x stack (non-secure),2,,,akovalenko,True,akovalenko,akovalenko
ambari,AMBARI-3429,2013-10-02T15:47:03.000+0000,2013-10-02T15:53:06.000+0000,2013-10-02T15:53:06.000+0000,,Fixed,New Feature,Major,['1.4.2'],['1.4.2'],,,,,,,['ambari-web'],['Ambari Web UI'],,Reassign Master Wizard: move non-HA NameNode in 2.x stack (non-secure),2,,,akovalenko,True,akovalenko,akovalenko
ambari,AMBARI-3399,2013-10-01T14:10:42.000+0000,2013-10-01T14:23:06.000+0000,2013-10-01T14:23:29.000+0000,,Fixed,New Feature,Major,['1.4.2'],['1.4.2'],,,,,,,['ambari-web'],['Ambari Web UI'],,Reassign Master Wizard: move JobTracker in 1.x stack (non-secure),2,,,akovalenko,True,akovalenko,akovalenko
ambari,AMBARI-3390,2013-09-30T16:57:43.000+0000,2013-09-30T17:10:53.000+0000,2013-09-30T17:10:53.000+0000,,Fixed,New Feature,Major,['1.4.2'],['1.4.2'],,,,,,,['ambari-web'],['Ambari Web UI'],,Reassign Master Wizard: move Secondary NameNode in 1.x stack (non-secure),2,,,akovalenko,True,akovalenko,akovalenko
ambari,AMBARI-3388,2013-09-30T16:48:43.000+0000,2013-09-30T17:11:02.000+0000,2013-09-30T17:11:02.000+0000,,Fixed,New Feature,Major,['1.4.2'],['1.4.2'],,,,,,,['ambari-web'],['Ambari Web UI'],,Reassign Master Wizard: move NameNode in 1.x stack (non-secure),2,,,akovalenko,True,akovalenko,akovalenko
ambari,AMBARI-3387,2013-09-30T16:40:50.000+0000,2013-09-30T16:48:53.000+0000,2013-09-30T17:42:54.000+0000,,Fixed,New Feature,Major,['1.4.2'],['1.4.2'],,,,,,,['ambari-web'],['Ambari Web UI'],,Allow the user to add HBase Master from the Host Details page,3,,,ababiichuk,True,ababiichuk,ababiichuk
ambari,AMBARI-3319,2013-09-24T00:36:48.000+0000,2013-10-11T02:43:10.000+0000,2013-10-11T02:43:13.000+0000,,Fixed,New Feature,Major,['1.4.2'],['1.4.1'],,,,,,,['ambari-web'],['Ambari Web UI'],,Simplify Local Repo setup via UI,2,,,xiwang0309,True,xiwang0309,xiwang0309
ambari,AMBARI-3264,2013-09-17T19:03:13.000+0000,2014-07-02T18:08:21.000+0000,2014-07-02T18:08:21.000+0000,,Duplicate,New Feature,Major,,['1.6.1'],,,,,,,['ambari-web'],['Ambari Web UI'],"support Ambari-shell
Support a python shell to exceute amabri-client api with simple commandline commands

ambari>help
show_all_services
show_all_hosts",Ambari-client shell,5,,['client'],subin11,True,subin11,subin11
ambari,AMBARI-3253,2013-09-16T23:10:22.000+0000,2013-09-16T23:22:40.000+0000,2013-09-16T23:22:40.000+0000,,Fixed,New Feature,Major,['1.4.1'],['1.4.0'],,,,,,,['ambari-web'],['Ambari Web UI'],"We will add the ability to remove a host that belongs to an existing cluster.

The user can delete hosts that consists solely of:
* DataNodes
* TaskTrackers
* NodeManagers
* ZooKeeper Servers
* Ganglia Monitor
* Clients (need to add the ability to install client components as well)
* HBase RegionServer
",Provide UI to delete host from Ambari,2,,,srimanth.gunturi,True,srimanth.gunturi,srimanth.gunturi
ambari,AMBARI-3167,2013-09-10T17:50:05.000+0000,2013-12-16T23:40:58.000+0000,2015-01-20T22:11:23.000+0000,,Fixed,New Feature,Major,['1.4.3'],,,,,,,,,,"This feature extends the ability for Ambari to support an Hadoop Compatible File System outside of HDFS.  For this stack definition we are introducing the use of GlusterFS, but it provides a good road map for other Hadoop Compatible File Systems to also be able to leverage Ambari. The feature/patch does not remove the HDFS patch, but simply provides an alternative when selecting the services within the installer.",Create new stack with Gluster support for 1.3.2 HDP version,5,,['patch'],erinaboyd,True,erinaboyd,erinaboyd
ambari,AMBARI-3113,2013-09-05T17:07:03.000+0000,2013-09-05T17:21:43.000+0000,2013-09-05T17:21:43.000+0000,,Fixed,New Feature,Major,['1.4.1'],['1.4.0'],,,,,,,['ambari-server'],['Ambari Server'],"Ganglia metrics for JN:
https://issues.apache.org/jira/browse/HDFS-3870",Add Ganglia/JMX monitoring for Journal Node component,2,,,mkononenko@hortonworks.com,True,mkononenko@hortonworks.com,mkononenko@hortonworks.com
ambari,AMBARI-3054,2013-08-29T22:16:08.000+0000,2013-10-31T17:13:50.000+0000,2013-10-31T17:13:50.000+0000,,Fixed,New Feature,Major,['1.4.2'],['1.4.0'],,,,,,,['ambari-web'],['Ambari Web UI'],"1. In Admin section, add a User Settings section, and include the following:

[ X] Show background operations dialog when an operation is started

Default to true to show the dialog (for backwards compat)

2. On the operations dialog, add at the bottom:

[ ] Do not show this dialog when starting a background operation

Changing this setting will set the preference above.
=================================================================
These two checkboxes should always have opposite value.
This preference is related to login user. If a new user login, the default value is ""show background dialog"".","User Preference to show ""Background Operations""",2,,,xiwang0309,True,xiwang0309,xiwang0309
ambari,AMBARI-2963,2013-08-20T11:10:39.000+0000,2013-08-21T18:09:29.000+0000,2013-08-21T18:09:29.000+0000,,Fixed,New Feature,Major,['1.4.1'],['1.4.0'],,,,,,,['ambari-web'],['Ambari Web UI'],"Just like how one can add datanode component to a host, we should be able to node manager component.",Need 'Add NodeManager' action on host,3,,,ababiichuk,True,ababiichuk,ababiichuk
ambari,AMBARI-2956,2013-08-19T17:04:08.000+0000,2013-08-19T19:15:44.000+0000,2013-08-19T19:15:44.000+0000,,Fixed,New Feature,Major,['1.4.0'],['1.4.0'],,,,,,,['ambari-web'],['Ambari Web UI'],,NameNode HA: Show HA info in Services > HDFS > Summary,3,,,akovalenko,True,akovalenko,akovalenko
ambari,AMBARI-2853,2013-08-09T09:57:12.000+0000,2013-08-09T10:11:58.000+0000,2013-08-09T10:11:58.000+0000,,Fixed,New Feature,Major,['1.4.0'],['1.4.0'],,,,,,,['ambari-web'],['Ambari Web UI'],"When YARN was starting, I fired off 4 MR2 smoke tests. The background ops count did not increase. Once started I see the container count go up and apps stuck in ACCEPTED state.
We should not allow starting of MR2 smoke test till YARN is in STARTED state. We should show message 'MapReduce2 smoke test requires YARN service be started'.",MR2 smoke test should check YARN is started,2,,,atkach,True,atkach,atkach
ambari,AMBARI-2852,2013-08-08T23:46:25.000+0000,,2018-06-16T16:01:42.000+0000,,,New Feature,Major,,['1.4.0'],,,,,,,['ambari-web'],['Ambari Web UI'],"In case you want to use Ambari for management of a live Hadoop cluster (including monitoring and eco-system services), without deployment. Ambari assumes it has the control of Hadoop installation. Suppose there is an existing HDP Hadoop cluster and one wants to install Ambari to start managing/monitoring Hadoop. The proposal is to allow Ambari installation on the cluster but bypass the Hadoop (and eco-system) deployment. We should provide the ability to import the existing configuration without over writing existing setup. 
 
Proposed phases:
1. Provide on Ambari wiki documentation, leveraging Ambari API, all manual steps to configure Ambari without redeploying Hadoop and its eco-system
2. Provide a template configuration file and one Utility (leaning toward Python script) that automates the process of important an existing configuration
3. Provide User interface to allow step above via UI",Ambari installation and configuration to manage an existing deployed hadoop Cluster,22,9,,aagarwal,True,,aagarwal
ambari,AMBARI-2849,2013-08-08T21:12:04.000+0000,2013-08-09T01:13:19.000+0000,2013-08-09T01:13:19.000+0000,,Fixed,New Feature,Major,['1.4.0'],['1.4.0'],,,,,,,['ambari-web'],['Ambari Web UI'],"Create progress page after ""Initialize Metadata"" in NameNode HA Wizard","NameNode HA Wizard: progress page after ""Initialize Metadata""",2,,,aantonenko,True,aantonenko,aantonenko
ambari,AMBARI-2846,2013-08-08T19:34:50.000+0000,2013-08-09T01:11:26.000+0000,2013-08-09T01:11:26.000+0000,,Fixed,New Feature,Major,['1.4.0'],['1.4.0'],,,,,,,['ambari-web'],['Ambari Web UI'],"Implement the ""Initialize Metadata"" page. Skip the API calls and enable the Next button by default for now.
","NameNode HA Wizard: ""Initialize Metadata"" page",2,,,aantonenko,True,aantonenko,aantonenko
ambari,AMBARI-2845,2013-08-08T19:12:26.000+0000,2013-08-09T17:44:59.000+0000,2013-08-09T17:44:59.000+0000,,Fixed,New Feature,Major,['1.4.0'],['1.4.0'],,,,,,,['ambari-web'],['Ambari Web UI'],"Check API requests, fix found bugs.","NameNode HA Wizard: E2E integration for progress page after ""Initialize JournalNodes""",3,,,aantonenko,True,aantonenko,aantonenko
ambari,AMBARI-2833,2013-08-07T16:58:40.000+0000,2013-08-07T18:13:39.000+0000,2013-08-07T18:13:39.000+0000,,Fixed,New Feature,Major,['1.4.0'],['1.4.0'],,,,,,,['ambari-web'],['Ambari Web UI'],,"NameNode HA Wizard: progress page after ""Initialize JournalNodes""",3,,,akovalenko,True,akovalenko,akovalenko
ambari,AMBARI-2831,2013-08-07T15:46:14.000+0000,2013-08-07T16:52:54.000+0000,2013-08-07T16:53:08.000+0000,,Fixed,New Feature,Major,['1.4.0'],['1.4.0'],,,,,,,['ambari-web'],['Ambari Web UI'],* Resource used % (memory),Add YARN specific heatmaps,3,,,onechiporenko,True,onechiporenko,onechiporenko
ambari,AMBARI-2821,2013-08-06T18:33:10.000+0000,2013-08-06T21:01:47.000+0000,2013-08-06T21:01:47.000+0000,,Fixed,New Feature,Major,['1.4.0'],['1.4.0'],,,,,,,['ambari-web'],['Ambari Web UI'],"Implement the ""Initialize JournalNodes"" page. Skip the API calls and enable the Next button by default for now.","NameNode HA Wizard: ""Initialize JournalNodes"" page",3,,,aantonenko,True,aantonenko,aantonenko
ambari,AMBARI-2818,2013-08-06T17:07:38.000+0000,2013-08-06T21:00:05.000+0000,2013-08-06T21:00:05.000+0000,,Fixed,New Feature,Major,['1.4.0'],['1.4.0'],,,,,,,['ambari-web'],['Ambari Web UI'],"Implement the ""Create Checkpoint"" page.
Skip the API calls and enable the Next button by default for now.
","NameNode HA Wizard: ""Create Checkpoint"" page",3,,,aantonenko,True,aantonenko,aantonenko
ambari,AMBARI-2815,2013-08-06T12:33:04.000+0000,2013-08-06T21:05:10.000+0000,2013-08-06T21:05:10.000+0000,,Fixed,New Feature,Major,['1.4.0'],['1.4.0'],,,,,,,['ambari-web'],['Ambari Web UI'],,Add ability to add ZooKeeper Server from Host Details page,3,,,ababiichuk,True,ababiichuk,ababiichuk
ambari,AMBARI-2798,2013-08-02T15:57:59.000+0000,2013-08-02T17:36:11.000+0000,2013-08-02T18:02:08.000+0000,,Fixed,New Feature,Major,['1.4.0'],['1.2.5'],,,,,,,['ambari-web'],['Ambari Web UI'],"Apart from the 4 JVM graphs, we need 4 YARN specific graphs
Allocated containers (yarn.queuemetrics.allocatedcontainer)
Allocated memory (yarn.queuemetrics.allocatedMB)
Nodemanager statuses
Queue memory resource % full",YARN specific graphs should be created,3,,,ababiichuk,True,ababiichuk,ababiichuk
ambari,AMBARI-2797,2013-08-02T15:14:28.000+0000,2013-08-02T21:50:31.000+0000,2013-08-02T21:50:31.000+0000,,Fixed,New Feature,Major,['1.4.0'],['1.4.0'],,,,,,,['ambari-web'],['Ambari Web UI'],,"NameNode HA Wizard: progress page after ""Create Checkpoint"" page",3,,,akovalenko,True,akovalenko,akovalenko
ambari,AMBARI-2793,2013-08-01T22:55:24.000+0000,2013-08-06T00:21:44.000+0000,2013-08-06T00:21:44.000+0000,,Fixed,New Feature,Major,['1.4.0'],['1.4.0'],,,,,,,['ambari-web'],['Ambari Web UI'],,YARN Memory widget needed on dashboard,3,,,xiwang0309,True,xiwang0309,xiwang0309
ambari,AMBARI-2791,2013-08-01T16:51:09.000+0000,2013-08-01T17:23:33.000+0000,2013-08-01T17:23:33.000+0000,,Fixed,New Feature,Major,['1.4.0'],['1.4.0'],,,,,,,['ambari-web'],['Ambari Web UI'],"Step 3 asks the user to review the host assignments and to proceed ahead.

Ask the user to confirm the location of the current NameNode, NN2, and the 3 JNs.  The user clicks on “Proceed” to move ahead.
",NameNode HA Wizard: Review page,3,,,aantonenko,True,aantonenko,aantonenko
ambari,AMBARI-2790,2013-08-01T15:27:20.000+0000,2013-08-01T17:37:55.000+0000,2013-08-01T17:37:56.000+0000,,Fixed,New Feature,Major,['1.4.0'],['1.4.0'],,,,,,,['ambari-web'],['Ambari Web UI'],"Step 2 collects necessary host assignment information from the user (which host to place the second NameNode and 3 JournalNodes).

Ask the user which hosts to map NN2 and 3 JNs.  For the user’s info, we show where the existing NN is as well.
“Select a host that will be running an additional NameNode.  In addition, select 3 hosts that will be running JournalNodes to store NameNode edit logs in a fault tolerant manner.”
",NameNode HA Wizard: Assign Masters page,3,,,aantonenko,True,aantonenko,aantonenko
ambari,AMBARI-2775,2013-07-31T00:31:05.000+0000,2013-07-31T01:17:47.000+0000,2013-07-31T18:00:14.000+0000,,Fixed,New Feature,Major,['1.4.0'],['1.4.0'],,,,,,,['ambari-agent'],['Ambari Agent'],"Verifying that installing and starting namenode using current Ambari scripts does not reformat the NN 2.
This is because the bootstrap namenode should have formatted the namenode 2 already.
Also fix tests affected by NN HA changes.",Ability to add second Namenode to the cluster for 2.0.* stack,2,,,swagle,True,swagle,swagle
ambari,AMBARI-2772,2013-07-30T22:10:47.000+0000,2013-08-09T21:21:27.000+0000,2013-08-09T21:21:27.000+0000,,Fixed,New Feature,Major,['1.4.0'],['1.4.0'],,,,,,,['ambari-web'],['Ambari Web UI'],,YARN dashboard widgets needed,3,,,xiwang0309,True,xiwang0309,xiwang0309
ambari,AMBARI-2770,2013-07-30T16:12:18.000+0000,2013-07-30T20:34:54.000+0000,2013-07-30T20:34:54.000+0000,,Fixed,New Feature,Major,['1.4.0'],['1.4.0'],,,,,,,['ambari-web'],['Ambari Web UI'],"Prerequisite Checks
When the user clicks on ""Enable NameNode HA"", we will perform the following checks:
* If the cluster consists of less than 3 hosts, display the error message in a popup: ""You must have at least 3 hosts in your cluster to enable NameNode HA"".
* If the above check passes and if the cluster consists of less than 3 ZooKeeper Servers, display the error message in a popup: ""You must have at least 3 ZooKeeper Servers in your cluster to enable NameNode HA"".
* If the above check passes and if the cluster is already secure, display the error message in a popup: ""You cannot enable NameNode HA via this wizard as your cluster is already secured.  First, disable security by going to Admin > Security, and then run this Enable NameNode HA wizard again.  After NameNode HA is enabled, you can go back to Admin > Security to secure the cluster.""

The user cannot proceed forward in case any of the above errors have been encountered.
",NameNode HA Wizard: prerequisite checks,3,,,akovalenko,True,akovalenko,akovalenko
ambari,AMBARI-2756,2013-07-29T15:57:39.000+0000,2013-07-30T20:32:04.000+0000,2013-07-30T20:32:04.000+0000,,Fixed,New Feature,Major,['1.4.0'],['1.4.0'],,,,,,,['ambari-web'],['Ambari Web UI'],"The scope of this task is to implement Admin > High Availability page where the ""Enable NameNode HA"" button is exposed to launch ""Enable NameNode HA Wizard"", and to implement the skeleton pages for all the steps and basic routing. Mockup attached.",NameNode HA Wizard: Admin > High Availability page and wizard skeleton with basic routing,3,,,akovalenko,True,akovalenko,akovalenko
ambari,AMBARI-2410,2013-06-18T02:15:12.000+0000,,2014-08-05T03:33:00.000+0000,,,New Feature,Major,,['1.2.5'],,,,,,,"['ambari-agent', 'ambari-server']","['Ambari Agent', 'Ambari Server']","Since the architecture of sqoop2 is different from sqoop , we need a new service called sqoop2.",Sqoop2 support ,3,,,juby0710,True,,juby0710
ambari,AMBARI-2332,2013-06-09T03:01:12.000+0000,2013-06-11T23:59:38.000+0000,2013-06-11T23:59:38.000+0000,,Fixed,New Feature,Major,['1.4.0'],['1.4.0'],,,,,,,"['ambari-server', 'ambari-web']","['Ambari Server', 'Ambari Web UI']",,Support Jobs display for Tez and MR2 in Ambari,3,,,u39kun,True,billie.rinaldi,u39kun
ambari,AMBARI-2304,2013-06-06T21:08:47.000+0000,2013-06-07T18:22:08.000+0000,2013-06-07T18:22:08.000+0000,,Fixed,New Feature,Major,['1.4.0'],['1.4.0'],,,,,,,['ambari-web'],['Ambari Web UI'],"At a minimum we should provide the below 
YARN > Garbage Collection Time
YARN > JVM Heap Memory Used",Provide YARN heatmaps,2,,,srimanth.gunturi,True,srimanth.gunturi,srimanth.gunturi
ambari,AMBARI-2250,2013-06-03T10:50:58.000+0000,2013-06-03T20:15:35.000+0000,2013-06-03T20:15:35.000+0000,,Fixed,New Feature,Major,['1.4.0'],['1.4.0'],,,,,,,"['ambari-agent', 'ambari-server']","['Ambari Agent', 'Ambari Server']","Assuming we are running this off a node that has the yarn client jars, we can run the distributed shell application.
{code}
bin/hadoop jar <path to yarn-applications-distributedshell*.jar> -appname yarnservicecheck -master_memory 512 -container_memory 128 -num_containers 2 -shell_command ""ls"" -jar <path to yarn-applications-distributedshell*.jar>
{code}
Before running this, we can running basic sanity tests by verify that the resource manager is up by either using the webservice and also use the webservice to ensure that the required NMs have connected to the RM via <RM node:RM web port>/ws/v1/cluster and /ws/v1/cluster/nodes",Create a service check for YARN,3,,,odiachenko,True,odiachenko,odiachenko
ambari,AMBARI-2214,2013-05-29T01:48:54.000+0000,2013-05-30T21:48:20.000+0000,2013-05-30T21:48:20.000+0000,,Fixed,New Feature,Major,['1.4.0'],['1.2.4'],,,,,,,['ambari-web'],['Ambari Web UI'],"We need a place to summarize YARN along with configuration tab.
",Hadoop2 Monitoring: YARN does not have a service page,2,,,srimanth.gunturi,True,srimanth.gunturi,srimanth.gunturi
ambari,AMBARI-2168,2013-05-20T22:11:23.000+0000,2013-05-22T21:45:04.000+0000,2013-05-22T21:45:04.000+0000,,Fixed,New Feature,Major,['1.4.0'],['1.4.0'],,,,,,,['ambari-web'],['Ambari Web UI'],,Integrate installing and starting Hadoop 2.0 Services,2,,,u39kun,True,u39kun,u39kun
ambari,AMBARI-2167,2013-05-20T22:01:49.000+0000,2013-05-22T21:47:45.000+0000,2013-05-22T21:47:45.000+0000,,Fixed,New Feature,Major,['1.2.4'],['1.2.4'],,,,,,,['ambari-web'],['Ambari Web UI'],"Introduce a new step called ""Select Stack"".

* Rename the current step1* files to step0*.
* Create step1* files for the Select Stack step.
* Page title: ""Select Stack""
* Page callout: ""Please select the service stack that you want to use to install your Hadoop cluster.""
* Show all stacks available from /api/v1/stacks2, but mask out all the stacks whose stack_name ends in ""Local"".
* For each stack, the ""display name"" should be stack_name + ' ' + stack_version.
* The stacks should be sorted by ""display name"" DESC (e.g., HDP 1.3.0, HDP 1.2.1)
* The API will provide an additional property ""active"" to tell the frontend weather the stack should be displayed on this page or not.  App.defaultStackVersion is used to set the default selection.",Support for displaying various stacks and having the user select which stack to install,2,,,u39kun,True,u39kun,u39kun
ambari,AMBARI-2123,2013-05-14T00:37:49.000+0000,2013-05-14T01:11:55.000+0000,2013-05-14T01:11:55.000+0000,,Fixed,New Feature,Major,['1.2.4'],['1.2.3'],,,,,,,['ambari-web'],['Ambari Web UI'],,Allow the user to specify a non-root ssh user in Install Options,2,,,u39kun,True,u39kun,u39kun
ambari,AMBARI-2115,2013-05-13T13:52:47.000+0000,,2013-09-19T23:58:35.000+0000,,,New Feature,Major,,,,,,,,,"['ambari-agent', 'infra']","['Ambari Agent', 'Project infrastructure including builds, releases mechanics and automation']","Right now Ambari is unable to forget a host once it has been registered. This makes is tough to use Ambari in a dynamic cluster environment, where clusters are torn down and machines are reset to a clean state so they can be reused for the next customer.

If the machine is reset to a clean state without unregistering the host in Ambari, the new Ambari client won't be able to register since its cert won't match the one Ambari server already has for that host.",Support host unregistration,3,,,edrevo,True,edrevo,edrevo
ambari,AMBARI-1922,2013-04-12T22:43:40.000+0000,2013-04-13T00:46:33.000+0000,2013-06-11T03:28:13.000+0000,,Fixed,New Feature,Major,['1.2.4'],['1.2.3'],,,,,,,,,"Where the infrastructure team already has a process around datacenter management, it is very unlikely one would be able to get user 'root' access on the machines and its private key. Its much easier to get user accounts who can sudo.

From what i can see if ambari needs root private key, in theory you can accept any user who can sudo and the private key to do the same.",Support not root ssh via a user that can sudo in as root,3,,,sumitmohanty,True,sumitmohanty,sumitmohanty
ambari,AMBARI-1908,2013-04-12T05:17:29.000+0000,2013-04-16T02:48:35.000+0000,2013-08-13T20:40:12.000+0000,,Fixed,New Feature,Major,,,,,,,,,['ambari-web'],['Ambari Web UI'],,HDFS Mirroring: Add Bread Crumbs and Validation,2,,,akandregula,True,akandregula,akandregula
ambari,AMBARI-1829,2013-04-06T01:54:26.000+0000,2013-04-09T20:38:24.000+0000,2013-08-13T20:41:26.000+0000,,Fixed,New Feature,Major,,,,,,,,,['ambari-web'],['Ambari Web UI'],,"HDFS Mirroring: Display Status and handle maintenance operations like Stop, Suspend, Activate etc.",3,,,akandregula,True,akandregula,akandregula
ambari,AMBARI-1817,2013-04-05T17:49:16.000+0000,2013-07-24T15:41:18.000+0000,2013-07-24T15:41:18.000+0000,,Fixed,New Feature,Major,,['1.2.0'],,,1814400,1814400,1814400,,"['ambari-server', 'documentation', 'infra', 'test']","['Ambari Server', 'Documentation', 'Project infrastructure including builds, releases mechanics and automation', 'unit tests, functional tests, and test automation']","Create ability to add alternate storage system that uses Ambari managment tools - Stage 1
--Allow Hadoop applications to run on alternative compatible storage layer ",Create ability to add alternate storage system that uses Ambari management tools - Stage 1,8,,['features'],erinaboyd,True,erinaboyd,erinaboyd
ambari,AMBARI-1800,2013-04-05T01:55:43.000+0000,2013-04-06T02:02:18.000+0000,2013-05-08T19:09:14.000+0000,,Fixed,New Feature,Major,['1.2.3'],['1.2.3'],,,,,,,['ambari-web'],['Ambari Web UI'],,"Add ""Admin > Misc"" section to Ambari Web to show service user accounts",2,,,u39kun,True,u39kun,u39kun
ambari,AMBARI-1793,2013-04-04T20:20:16.000+0000,2014-06-10T16:07:28.000+0000,2014-06-10T16:07:28.000+0000,,Won't Fix,New Feature,Major,,,,,,,,,['ambari-web'],['Ambari Web UI'],"On the main apps page, I would like to introduce a timeline view of apps that changes as you filter the apps table.  To support arbitrary filtering of collections of jobs, I'd like to introduce a tags column to the apps table that picks up tags specified in a mapreduce.workflow.tags property of a job's configuration.  This will require patches to the job history logger, to the ambari-log4j contrib that parses the logs, to the jobhistory webservice and the UI.",App timeline with support for tagging,1,,,billie.rinaldi,True,billie.rinaldi,billie.rinaldi
ambari,AMBARI-1781,2013-04-03T22:25:27.000+0000,2013-04-03T22:37:55.000+0000,2013-06-11T03:27:53.000+0000,,Fixed,New Feature,Major,['1.2.4'],['1.2.3'],,,,,,,,,"Ambari Server should work with MySQL and Oracle where the Ambari Server data might be stored.
We need to create DDL scripts that will setup a already running MySQL server/Oracle Server. Ambari Server then needs to be configured to use the right adaptor for connecting to either MySQL or Oracle.",Ambari Server should work with MySQL and Oracle where the Ambari Server data might be stored,2,,,sumitmohanty,True,sumitmohanty,sumitmohanty
ambari,AMBARI-1777,2013-04-03T20:10:40.000+0000,2013-07-15T21:23:05.000+0000,2013-08-23T18:08:08.000+0000,,Fixed,New Feature,Major,['1.4.0'],['1.3.0'],,,,,,,,,"Support Delete host API in Ambari to be able to delete a host from a cluster. Also it would be best if the delete would only work if all the compoenents were in MAINTENANCE state, else it should fail to delete the node.",Support Delete host API in Ambari to be able to delete a host from a cluster.,4,1,,mahadev,True,ncole@hortonworks.com,mahadev
ambari,AMBARI-1769,2013-04-02T10:46:49.000+0000,2013-04-15T07:25:44.000+0000,2013-08-13T20:41:09.000+0000,,Fixed,New Feature,Major,['1.2.3'],['1.2.3'],,,1814400,1814400,1814400,,['ambari-server'],['Ambari Server'],"Ambari doesnt have a python REST client to invoke REST calls .Currently users have to depend upon curl command.

I have created the Ambari python client and attached as a zip.Please review and give your feedbacks

The zip contains a skeleton code with few of the resources supported.The features are :

Supported feature
===================
1)get_all_clusters
2)get cluster by name
3)get service by nmae
4)start/stop service

Once this skeleton code is reviewed i will checkin the rest of the resources/features
I currently support creation of cluster via this python client.



install python client on linux box
=============================='
1)unzip the attachment
2)execute 'python setup.py install  --record installation.txt'






example:
==========
from ambari_client.ambari_api import  AmbariClient 
client = AmbariClient(""localhost"", 8080, ""admin"",""admin"",version=1)
all_clusters = client.get_all_clusters()

   
cluster = client.get_cluster('test1')
   
serviceList = cluster.get_all_services()
  
    
for service in serviceList:
    print str(service.service_name)+"" = ""+str(service.state)
  

to start/stop service
-----------------------
ganglia = cluster.get_service(""GANGLIA"")       
ganglia.stop()
ganglia.start()

    


",Python REST client to invoke REST calls,3,2,['REST'],subin11,True,subin11,subin11
ambari,AMBARI-1756,2013-03-31T23:25:38.000+0000,2013-04-06T02:02:19.000+0000,2013-05-08T19:09:18.000+0000,,Fixed,New Feature,Major,['1.2.3'],['1.2.3'],,,,,,,['ambari-web'],['Ambari Web UI'],"This includes all changes needed to install HUE end-to-end via Install Wizard including but may not be limited to:
 * Showing HUE as a service that can be installed in Step 4 (Select Services)
 * Adding HUE Server host to Step 5 (Assign Masters)
 * Showing HUE configs in Step 7 (Customize Services); map all necessary config knobs - initially, just show all config parameter names and values from the stack definition as ""Advanced"".
 * Showing HUE as a service being added and the HUE master host in Step 8 (Review)
 * Creating a HUE service, creating a HUE Master as a service component, adding the HUE Master component to the HUE Master host, creating a config called ""hue-site"" and attaching it to the HUE service.",Add ability to install and edit HUE as a service,2,,,srimanth.gunturi,True,srimanth.gunturi,srimanth.gunturi
ambari,AMBARI-1742,2013-03-29T05:13:41.000+0000,2013-04-06T02:02:24.000+0000,2013-08-13T20:41:45.000+0000,,Fixed,New Feature,Major,,,,,,,,,['ambari-web'],['Ambari Web UI'],,HDFS Mirroring: Edit/Delete Cluster,3,,,akandregula,True,akandregula,akandregula
ambari,AMBARI-1723,2013-03-27T06:43:18.000+0000,2013-04-06T02:02:20.000+0000,2013-08-13T20:41:36.000+0000,,Fixed,New Feature,Major,,,,,,,,,['ambari-web'],['Ambari Web UI'],"Adding a data set is completed. Hence updating task should not be complex.
",HDFS Mirroring : Edit/Delete Data Set,2,,,srimanth.gunturi,True,srimanth.gunturi,srimanth.gunturi
ambari,AMBARI-1717,2013-03-26T23:12:10.000+0000,2013-04-06T02:02:20.000+0000,2013-05-08T19:09:14.000+0000,,Fixed,New Feature,Major,['1.2.3'],['1.2.3'],,,,,,,,,,Add ability to start and stop all services from Services page,2,,,xiwang0309,True,xiwang0309,xiwang0309
ambari,AMBARI-1716,2013-03-26T21:46:26.000+0000,2013-04-06T02:02:22.000+0000,2013-08-13T20:41:54.000+0000,,Fixed,New Feature,Major,,,,,,,,,['ambari-web'],['Ambari Web UI'],,HDFS Mirroring: Add a cluster,3,,,akandregula,True,akandregula,akandregula
ambari,AMBARI-1710,2013-03-26T05:56:52.000+0000,2013-04-06T02:02:25.000+0000,2013-08-13T20:42:03.000+0000,,Fixed,New Feature,Major,,,,,,,,,['ambari-web'],['Ambari Web UI'],"Adding a data set is completed. Hence updating/deleting task should not be complex.
",HDFS Mirroring : Edit/Delete Data Set,2,,,srimanth.gunturi,True,srimanth.gunturi,srimanth.gunturi
ambari,AMBARI-1699,2013-03-23T05:18:16.000+0000,2013-04-06T02:02:20.000+0000,2013-08-13T20:42:22.000+0000,,Fixed,New Feature,Major,,,,,,,,,['ambari-web'],['Ambari Web UI'],,HDFS Mirroring: Side Panel of individual jobs page,2,,,u39kun,True,u39kun,u39kun
ambari,AMBARI-1698,2013-03-23T05:14:27.000+0000,2013-04-06T02:02:19.000+0000,2013-05-08T19:09:26.000+0000,,Fixed,New Feature,Major,['1.2.3'],['1.2.3'],,,,,,,['ambari-web'],['Ambari Web UI'],,Host Detail page needs to allow upgrade for host components that failed to upgrade,3,,,u39kun,True,u39kun,u39kun
ambari,AMBARI-1696,2013-03-23T05:02:05.000+0000,2013-04-06T02:02:19.000+0000,2013-05-08T19:09:28.000+0000,,Fixed,New Feature,Major,['1.2.3'],['1.2.3'],,,,,,,['ambari-web'],['Ambari Web UI'],,Capacity Scheduler configuration UI,2,,,u39kun,True,u39kun,u39kun
ambari,AMBARI-1694,2013-03-23T04:55:48.000+0000,2013-04-06T02:02:21.000+0000,2013-08-13T20:42:30.000+0000,,Fixed,New Feature,Major,,,,,,,,,['ambari-web'],['Ambari Web UI'],,HDFS Mirroring: Side Panel of DataSet page,2,,,u39kun,True,u39kun,u39kun
ambari,AMBARI-1693,2013-03-23T04:52:41.000+0000,2013-04-06T02:02:21.000+0000,2013-08-13T20:42:38.000+0000,,Fixed,New Feature,Major,,,,,,,,,['ambari-web'],['Ambari Web UI'],,HDFS Mirroring: Display Jobs table,2,,,u39kun,True,u39kun,u39kun
ambari,AMBARI-1691,2013-03-22T23:10:08.000+0000,2013-04-06T02:02:23.000+0000,2013-05-08T19:09:31.000+0000,,Fixed,New Feature,Major,['1.2.3'],['1.2.3'],,,,,,,,,,Add filtering by host-level status on Step 9 of Installer,2,,,xiwang0309,True,xiwang0309,xiwang0309
ambari,AMBARI-1668,2013-03-19T23:11:03.000+0000,2013-04-06T02:02:25.000+0000,2013-08-13T20:42:47.000+0000,,Fixed,New Feature,Major,,,,,,,,,['ambari-web'],['Ambari Web UI'],,HDFS Mirroring: Add Data Set Popup,3,,,akandregula,True,akandregula,akandregula
ambari,AMBARI-1663,2013-03-19T02:20:18.000+0000,2013-03-23T04:50:44.000+0000,2013-05-08T19:09:45.000+0000,,Fixed,New Feature,Major,['1.2.3'],['1.2.3'],,,,,,,,,,Allow adding host components to existing hosts,2,,,xiwang0309,True,xiwang0309,xiwang0309
ambari,AMBARI-1653,2013-03-16T01:18:31.000+0000,2013-03-23T04:38:12.000+0000,2013-05-08T19:09:45.000+0000,,Fixed,New Feature,Major,['1.2.3'],['1.2.3'],,,,,,,['ambari-web'],['Ambari Web UI'],,HDFS Mirroring: Display DataSets table,3,,,u39kun,True,u39kun,u39kun
ambari,AMBARI-1650,2013-03-15T21:06:09.000+0000,2013-04-06T02:02:24.000+0000,2013-06-11T03:31:03.000+0000,,Fixed,New Feature,Major,['1.2.4'],['1.2.3'],,,,,,,,,,Add Oracle and MySQL option for Oozie during Ambari cluster install,2,,,xiwang0309,True,xiwang0309,xiwang0309
ambari,AMBARI-1639,2013-03-14T07:34:41.000+0000,2013-06-11T00:21:48.000+0000,2013-06-11T00:22:37.000+0000,,Fixed,New Feature,Major,['1.2.5'],['1.2.5'],,,,,,,['ambari-web'],['Ambari Web UI'],"It would be great if the Ambari Web Dashboard had an inventory of widgets for showing various metrics (point-in-time values, time-series graphs, pie/donut charts, dials, histograms, etc) and allowed the user to select which ones to display and where to display them.  Also, it would be useful if the user could specify the thresholds for highlighting these metrics in different colors to alert the user.",Customizable Dashboard for Ambari Web,1,,,u39kun,True,xiwang0309,u39kun
ambari,AMBARI-1618,2013-03-12T17:53:06.000+0000,2013-03-13T23:51:59.000+0000,2013-08-13T20:42:54.000+0000,,Fixed,New Feature,Major,,,,,,,,,['ambari-web'],['Ambari Web UI'],,"HDFS Mirroring: Create Mapper, Model, Mock Data for Cluster",2,,,akandregula,True,akandregula,akandregula
ambari,AMBARI-1611,2013-03-12T04:59:30.000+0000,2013-10-17T21:21:41.000+0000,2013-10-17T21:21:41.000+0000,,Fixed,New Feature,Major,['1.2.5'],['1.2.3'],,,,,,,['ambari-web'],['Ambari Web UI'],,Add ability to start and stop all services from Services page,2,,,u39kun,True,,u39kun
ambari,AMBARI-1558,2013-03-06T04:49:03.000+0000,2013-04-15T22:13:00.000+0000,2013-05-08T19:09:31.000+0000,,Fixed,New Feature,Major,['1.2.3'],['1.2.3'],,,,,,,['ambari-web'],['Ambari Web UI'],,Script to add host components to existing hosts,3,,,u39kun,True,u39kun,u39kun
ambari,AMBARI-1538,2013-03-01T19:24:00.000+0000,2013-03-01T22:13:14.000+0000,2013-05-08T19:09:22.000+0000,,Fixed,New Feature,Major,['1.2.3'],['1.2.3'],,,,,,,['ambari-web'],['Ambari Web UI'],Implement Step 3 of the Stack Upgrade Wizard to show progress and allow Retry on warning/failure.,Stack Upgrade Wizard - Step 3 (Progress and Retry),2,,,u39kun,True,u39kun,u39kun
ambari,AMBARI-1509,2013-02-27T00:54:16.000+0000,2013-02-27T19:55:56.000+0000,2013-05-14T15:19:42.000+0000,,Fixed,New Feature,Major,['1.3.0'],['1.2.2'],,,,,,,['ambari-web'],['Ambari Web UI'],"Currently service configurations are cluster wide. There is no ability to provide different configurations for different hosts, for a property. This would be very beneficial in providing different mount folders, or heap-size settings for a group of hosts.",Frontend: For service configurations provide ability to enter host level exceptions,2,,,srimanth.gunturi,True,srimanth.gunturi,srimanth.gunturi
ambari,AMBARI-1481,2013-02-22T20:41:02.000+0000,2013-02-22T21:37:08.000+0000,2013-05-08T19:09:38.000+0000,,Fixed,New Feature,Major,['1.2.3'],['1.2.3'],,,,,,,['ambari-web'],['Ambari Web UI'],,Stack Upgrade Wizard - Step 2 (confirm and check all master components are running) ,2,,,u39kun,True,u39kun,u39kun
ambari,AMBARI-1469,2013-02-22T01:24:52.000+0000,2013-02-22T08:07:27.000+0000,2013-05-08T19:09:25.000+0000,,Fixed,New Feature,Major,['1.2.3'],['1.2.3'],,,,,,,['ambari-web'],['Ambari Web UI'],,Allow user to add multiple HBase masters in Install Wizard,2,,,u39kun,True,u39kun,u39kun
ambari,AMBARI-1468,2013-02-22T01:24:23.000+0000,2013-02-22T08:05:57.000+0000,2013-05-08T19:09:31.000+0000,,Fixed,New Feature,Major,['1.2.3'],['1.2.3'],,,,,,,['ambari-web'],['Ambari Web UI'],,Stack Upgrade Wizard - Step 1 (show services and versions),2,,,u39kun,True,u39kun,u39kun
ambari,AMBARI-1459,2013-02-20T22:36:34.000+0000,2013-02-20T22:41:24.000+0000,2013-05-08T19:09:44.000+0000,,Fixed,New Feature,Major,['1.2.3'],['1.2.3'],,,,,,,['ambari-web'],['Ambari Web UI'],"Add Cluster page under the Admins tab.  This page is used to show the currently installed Stack version, services and their versions, and whether a stack upgrade is available.",Add Admin > Cluster page,2,,,u39kun,True,u39kun,u39kun
ambari,AMBARI-1422,2013-02-12T20:21:57.000+0000,2013-03-19T20:21:59.000+0000,2013-05-08T17:01:05.000+0000,,Fixed,New Feature,Major,['1.2.3'],,,,259200,259200,259200,,,,"This context value will be added to the associated ""request"" resource to give some context as to what the request was doing.  The context value will only have meaning for asynchronous requests and will be ignored for synchronous requests.  This is a request from the UI team.","Allow client to specify a ""context"" value for asynchronous requests",2,,,jspeidel,True,jspeidel,jspeidel
ambari,AMBARI-1377,2013-02-12T16:44:08.000+0000,2014-03-11T20:09:53.000+0000,2014-03-11T20:09:53.000+0000,,Done,New Feature,Major,['1.5.0'],,,,,,,,,,"Provide ability to move master components of services to different host after installation. For example, move HDFS NameNode to a different host in the cluster after the install or at some later date etc, etc.

This is the list of Master Components to support re-assign, grouped by priority order.

P0. Master Components to Support

HDFS: NameNode
HDFS: SecondaryNameNode
MapReduce: JobTracker + HistoryServer
HBase: MasterServer

P1. Master Components to Support

Oozie: Server

P2. Master Components to Support

Hive: Metastore
Hive: HiveServer2
WebHCat: Server

P3. Master Components to Support

Ganglia: Gangalia Collector
Nagios: Nagios Server


Support moving a single master component at a time. For example: Services > HDFS, under the ""Maintenance"" dropdown, add options ""Reassign NameNode"" and ""Reassign SNameNode"". After selecting the option, popup wizard walks user through the re-assign:

Step 1: List all the manual steps that the user must explicitly do after Step 2. ( that Ambari doesnt do for him. )

Step 2: Select the target host. User should be presented with a list of possible target hosts in the cluster.
TODO: Need to determine what it means to be a target. What criteria does that host need to meet? Can it be running other master components? Slave components? Which ones and/or how many? This answer might be master component dependent so need to define this for each of the Master Components listed above.

Step 3: Ask the user to reconfigure the component for this new host. [ Similar to, each Category Section of Step 7 of Install Wizard ] for example select mount points for the new host before reassigning the component.

Step 4: Ask user to confirm.

Step 5: Show the order and state of tasks with live update. For example, stopping services, installing component on target host, applying config, adjusting cluster settings for new master host, restarting services, removing old master component (from source host).

Step 6: Complete.",Add ability to move master components after initial setup,3,,,ncole@hortonworks.com,True,ncole@hortonworks.com,ncole@hortonworks.com
ambari,AMBARI-1349,2013-02-06T00:29:04.000+0000,2013-02-06T00:54:24.000+0000,2013-02-28T21:17:21.000+0000,,Fixed,New Feature,Major,['1.2.2'],['1.2.1'],,,,,,,['ambari-web'],['Ambari Web UI'],,Expose host-specific Nagios alerts in Ambari Web,2,,,u39kun,True,u39kun,u39kun
ambari,AMBARI-1294,2013-01-29T19:27:39.000+0000,2013-01-31T22:34:23.000+0000,2013-02-01T17:08:36.000+0000,,Fixed,New Feature,Major,,['1.2.1'],,,,,,,,,"Add additional query operators including 'isEmpty' to determine if a category contains any properties.

For example: ?category.isEmpty()
Should return true if there are no properties in the category named 'category'; otherwise it should return false.",Ambari API: Add additional query operator 'isEmpty' for categories,2,,,jspeidel,True,jspeidel,jspeidel
ambari,AMBARI-1280,2013-01-29T01:53:59.000+0000,2013-01-29T19:20:01.000+0000,2013-05-08T17:00:53.000+0000,,Fixed,New Feature,Major,['1.2.3'],['1.2.1'],,,,,,,,,"Provide support for explicit predicate grouping using brackets.

Example: a=1&(b=2|c=3)",Ambari API: Support explicit predicate grouping,2,,,jspeidel,True,jspeidel,jspeidel
ambari,AMBARI-1194,2013-01-15T18:29:48.000+0000,2013-01-24T01:44:19.000+0000,2013-05-07T23:07:37.000+0000,,Fixed,New Feature,Critical,"['1.2.1', '1.2.3']",['1.2.0'],,,,,,,,,"We need to be able to allow the user to reconfigure the cluster if installation fails.
It would greatly simplify UI logic if the API supported cascade delete of the cluster (delete the cluster and all of its sub-resources).
This way, the UI can simply issue DELETE on cluster and proceed as if performing a fresh cluster install. This minimizes code changes on the UI side to support AMBARI-1193.",API support for cascade delete of a specified cluster,2,,,u39kun,True,tbeerbower,u39kun
ambari,AMBARI-1180,2013-01-15T00:56:01.000+0000,2013-01-25T04:50:20.000+0000,2013-05-02T02:30:00.000+0000,,Fixed,New Feature,Critical,['1.2.1'],['1.2.0'],,,,,,,['ambari-web'],['Ambari Web UI'],"In the host registration results page (Step 3 of Install Wizard), display host check results performed by the agents (AMBARI-1163).
UI needs to show warnings in an actionable manner (which checks caused warnings and what needs to be done to correct them).
In case warnings are encountered, the user needs to be able to re-run the checks.
The user can choose to proceed to the next step even if there are warnings. ",Display host check status results given by the agent as part of host registration,2,,,u39kun,True,u39kun,u39kun
ambari,AMBARI-1147,2013-01-10T20:22:07.000+0000,2013-01-10T20:24:38.000+0000,2013-01-10T20:31:58.000+0000,,Fixed,New Feature,Major,['1.2.0'],['1.2.0'],,,,,,,['ambari-web'],['Ambari Web UI'],,Handling Hive/HCat/WebHCat configuration parameters with Ambari Web,1,,,u39kun,True,u39kun,u39kun
ambari,AMBARI-1018,2012-11-16T21:34:40.000+0000,2012-12-14T03:38:56.000+0000,2012-12-14T03:38:56.000+0000,,Fixed,New Feature,Major,,,,,,,,,,,"Add API functionality for a user to query a resource and add sub-resources(all of the same type) to the matching set of resources.

For example, to add the ""SECONDARY_NAMENODE"" and ""HDFS_CLIENT"" host components where the service name is HDFS:

POST http://localhost/api/v1/clusters/mycluster/services?ServiceInfo/service_name=HDFS
{
""components"" : [
{""ServiceComponentInfo"" : {
        ""component_name"" : ""SECONDARY_NAMENODE""
      }
},
{""ServiceComponentInfo"" : {
        ""component_name"" : ""HDFS_CLIENT""
      }
}
] }

The msg body must start with the name of the sub-resource collection that the new resources are to be added to, in this case ""components"".  This name can be determined by doing a get on the resource that is being queried.  For each resource being added, provide a new array element and provide all of the properties that are required.  It is not necessary(or possible) to provide the values of all foreign key properties for ancestors, they are added implicitly.  So, in this case there is no need to provide the clusterId, or serviceId.  

The resources defined in the body are added as sub-resources to all resources that match the provided query.  In this example, only one service will match the query, but any valid query can be used to match any number of resources.

At the time of this checkin, it is necessary to provide a query for this functionality.  Therefore, to add the sub-resources to all resources, you need to provide a query that will match all resources(It can be a bugs query such as name!=bogus).  This is temporary and will be fixed in a subsequent commit.  ",Add API support for creating multiple sub-resources to multiple resources in a single request,2,,,jspeidel,True,jspeidel,jspeidel
ambari,AMBARI-971,2012-11-06T18:48:44.000+0000,2012-11-07T08:26:53.000+0000,2012-11-07T08:26:53.000+0000,,Fixed,New Feature,Major,['ambari-666'],,,,,,,,,,"Add the ability to create multiple resources of the same type by providing an array of resource properties in the http body.

For example, to create multiple service resources for the cluster named ""mycluster"":

POST htp://myHost:8080/api/v1/clusters/mycluster/services
[ 
{""ServiceInfo"" : {
        ""service_name"" : ""PIG""
      }
},
{""ServiceInfo"" : {
        ""service_name"" : ""OOZIE""
      }
},
{""ServiceInfo"" : {
        ""service_name"" : ""HIVE""
      }
}
]
",Add api support for creating multiple resources in a single request,2,,,jspeidel,True,jspeidel,jspeidel
ambari,AMBARI-649,2012-07-18T22:57:43.000+0000,,2012-08-31T23:52:06.000+0000,,,New Feature,Major,,,,,,,,,,,"To round out the trifecta (the other 2 of which are https://issues.apache.org/jira/browse/AMBARI-645 and https://issues.apache.org/jira/browse/AMBARI-646), we need to ensure that our Puppet layer queries the rack info stored in the Ambari DB and generates (and propagates) the appropriate rack-related config files needed by the various Hadoop components.",Propagate Rack Info From Ambari DB To Hadoop Components,1,,['AMBARI-666'],reznor,True,,reznor
ambari,AMBARI-631,2012-07-12T01:07:22.000+0000,2012-07-12T01:11:24.000+0000,2012-07-12T01:11:24.000+0000,,Fixed,New Feature,Major,['1.0'],['1.0'],,,,,,,,,,Set the new Hadoop stack version in the database upon successful upgrade,1,,,u39kun,True,u39kun,u39kun
ambari,AMBARI-623,2012-07-11T17:08:56.000+0000,2012-07-11T17:15:07.000+0000,2012-07-11T17:16:00.000+0000,,Fixed,New Feature,Major,['1.0'],['1.0'],,,,,,,,,"Instead of requiring 3 separate inputs from the user for upgrading (for uninstall, reconfigure, and deploy), just have the user click on the ""Upgrade Now"" button after specifying the SSH private key file.  Ask the user for confirmation (as it will first uninstall the stack as part of the upgrade process).  Once the user confirms, the SSH private key file is uploaded and the upgrade process is triggered; there's no further input required from the user until the upgrade completes successfully, or if there's an issue that requires the user to fix (after fixing the problem, the user can click on ""Retry"" to continue with the upgrade process).",Streamline UI flow for Hadoop stack upgrade,1,,,u39kun,True,u39kun,u39kun
ambari,AMBARI-621,2012-07-11T02:36:26.000+0000,2012-07-11T02:41:07.000+0000,2012-07-11T02:41:07.000+0000,,Fixed,New Feature,Major,['1.0'],['0.9.0'],,,,,,,,,,"On Cluster Summary page, show Hadoop stack version information and ""Upgrade available"" link if a newer version of the stack is available",1,,,u39kun,True,u39kun,u39kun
ambari,AMBARI-616,2012-07-09T20:22:23.000+0000,2012-07-16T23:48:05.000+0000,2012-07-16T23:48:05.000+0000,,Fixed,New Feature,Major,['1.0'],,,,,,,,,,"Currently, tasktrackers are started with heapsize set to a hardcoded value i.e. -Xmx1024. ",Enable configuration of heapsize for TaskTracker,1,,,hitesh,True,hitesh,hitesh
ambari,AMBARI-605,2012-07-06T03:32:07.000+0000,2012-07-06T04:55:26.000+0000,2012-07-06T04:55:36.000+0000,,Fixed,New Feature,Major,['1.0'],['0.9.0'],,,,,,,,,"Add the UI flow for handling Ambari and Hadoop stack version upgrades.
The assumption is that the Ambari RPM has been upgraded and the necessary database schema upgrade has been applied already (this is to be implemented).
Upon hitting any Ambari URL, the user is taken to the Upgrade wizard to complete upgrading the Hadoop stack bits (if this is necessary). 
To upgrade the stack, first we uninstall the stack while retaining data, have the user reconfigure services, and then install/start the new version of the stack.
This is still preliminary and to be improved upon and merged with actual backend calls (yet to be implemented).
Currently the front-end code is mocked with Sinon.JS so that XHR (AJAX) calls can be intercepted to return hard-coded values.

Also, to ease development in general, a new parameter was added to php/conf/Config.inc called $GLOBALS[""BYPASS_ROUTER""].  Setting this to TRUE, the router (hmc/html/_router.php - used to be called hmc/html/head.inc - the interceptor that is invoked upon hitting all front-end PHP pages) will not cause any forced redirects/forwards regardless of the application state.  For production, this is to be set to FALSE to protect the users from performing actions that could lead the cluster to be in an inconsistent state.


",Add UI flow/groundwork for handling Ambari / Hadoop stack version upgrades,1,,,u39kun,True,u39kun,u39kun
ambari,AMBARI-545,2012-06-11T22:22:15.000+0000,2012-08-25T17:19:56.000+0000,2012-08-25T17:19:56.000+0000,,Fixed,New Feature,Major,,,,,,,,,,,Umbrella jira to cover all issues related to getting Ambari to work for RHEL6/CentOS6 clusters.,Support RHEL6 installs,2,1,,hitesh,True,hitesh,hitesh
ambari,AMBARI-231,2012-05-11T20:52:21.000+0000,2012-05-15T02:41:09.000+0000,2012-07-04T21:54:54.000+0000,,Fixed,New Feature,Major,['0.9.0'],['0.9.0'],,,,,,,,,"Currently there is no option to cleanup hadoop data i.e. name dirs, data dirs, log dirs, pid dirs and rpms",Support hadoop cleanup,1,,,rramya,True,rramya,rramya
ambari,AMBARI-174,2011-12-24T16:00:11.000+0000,2011-12-25T23:36:31.000+0000,2011-12-25T23:36:31.000+0000,,Fixed,New Feature,Major,,,,,,,,,,,"AMBARI-171 handles retries of commands on the agent side. This jira makes the controller aware of nodes where repeated tries of any command execution failed, and marks such nodes unhealthy. The nodes are put back in the healthy state when the agent is restarted.",Controller marks nodes unhealthy upon command execution failures,,,,devaraj,True,devaraj,devaraj
ambari,AMBARI-171,2011-12-22T02:15:30.000+0000,2011-12-23T03:01:03.000+0000,2011-12-23T03:01:04.000+0000,,Fixed,New Feature,Major,,,,,,,,,,,,Agents should retry failed actions,,,,devaraj,True,devaraj,devaraj
ambari,AMBARI-119,2011-10-31T21:01:22.000+0000,2012-06-21T02:14:27.000+0000,2012-06-21T02:14:28.000+0000,,Invalid,New Feature,Major,['0.1.0'],['0.1.0'],,,,,,,['ambari-agent'],['Ambari Agent'],"Proposed directory structure layout:

{noformat}
$prefix/$cluster-$role/stacks
                      /config
                      /logs/
                      /data/disk-*/
                      /pkgs/hadoop-*.tar.gz
                      /config/hadoop/core-site.xml
{noformat}

One optimization that Owen proposed is to make the directory structure shorter for client.

{noformat}
$prefix/$cwd/stacks
            /config
            /logs
            /data/disk-*/
            /pkgs/hadoop-*.tar.gz
            /config/hadoop/core-site.xml
{noformat}

Action send by controller will contain ""current working directory"", which agent will record the cluster and role mapping to current working directory.",Add database to map cluster-role to current working directory,1,,,eyang,True,eyang,eyang
ambari,AMBARI-107,2011-10-26T22:17:56.000+0000,2011-11-01T17:05:55.000+0000,2011-11-01T17:05:55.000+0000,,Fixed,New Feature,Major,['0.1.0'],['0.1.0'],,,,,,,['infra'],"['Project infrastructure including builds, releases mechanics and automation']",Javadoc and rest api is current generated from a maven profile.  They should be generated as part of site.,Improve javadoc and site generation,,,,eyang,True,eyang,eyang
ambari,AMBARI-99,2011-10-25T00:44:07.000+0000,2011-11-01T17:13:25.000+0000,2011-11-01T17:13:25.000+0000,,Fixed,New Feature,Major,['0.1.0'],['0.1.0'],,,,,,,"['ambari-server', 'ambari-web', 'documentation', 'infra', 'site']","['Ambari Server', 'Ambari Web UI', 'Documentation', 'Project infrastructure including builds, releases mechanics and automation', 'Project site http://ambari.apache.org/']","Jersey can automatically generate REST API documentation from javadoc.  The trick is generating application.wadl then apply wadl.xslt stylesheet to generate human readable html file and integrate it as part of site generation.  The last part is to automatically generate schema.xsd from Java class, and link it to the generated application html.",Add REST API and schema.xsd document to site generation,,,,eyang,True,eyang,eyang
ambari,AMBARI-84,2011-10-19T00:02:50.000+0000,2011-10-21T21:46:53.000+0000,2011-10-21T21:46:53.000+0000,,Fixed,New Feature,Major,['0.1.0'],['0.1.0'],,,,,,,['ambari-agent'],['Ambari Agent'],"In Owen's acd document (Ambari Configuration Definition), it describes three different ways of writing config files.

copyProperties
copyXml
copySh

Those feature should be implemented in ambari_component",Implement config file write in ambari_component as described in acd,,,,eyang,True,eyang,eyang
ambari,AMBARI-69,2011-10-13T17:56:40.000+0000,2011-10-13T20:43:06.000+0000,2011-10-13T20:43:06.000+0000,,Fixed,New Feature,Major,['0.1.0'],['0.1.0'],,,,,,,['ambari-agent'],['Ambari Agent'],"For plugin to invoke python script, there is a predefined set of libraries.  The supported functions are:

{noformat}
copyXml(config, options)
copySh(config, options)
install(cluster, role, packages)
{noformat}

This jira tracks the skeleton setup for ambari component.",Add python library skeleton for ambari component,,,,eyang,True,eyang,eyang
ambari,AMBARI-66,2011-10-13T01:42:29.000+0000,2011-10-13T20:43:34.000+0000,2011-10-13T20:43:34.000+0000,,Fixed,New Feature,Major,,,,,,,,,,,Need a package install action that is compatible with heartbeat based controller.,Add PACKAGE install action,,,,eyang,True,eyang,eyang
ambari,AMBARI-65,2011-10-12T22:43:34.000+0000,2011-10-13T20:26:20.000+0000,2011-10-13T20:26:21.000+0000,,Fixed,New Feature,Major,['0.1.0'],['0.1.0'],,,,,,,['ambari-agent'],['Ambari Agent'],"Proposed cluster structure layout is:

{noformat}
$prefix/clusters/$cluster-$role/stack/
                               /logs/
                               /data/disk-*/
                               /pkgs/hadoop-*.tar.gz
                               /config/hadoop/core-site.xml
{noformat}

CREATE_STRUCTURE_ACTION and DELETE_STRUCTURE_ACTION are used to create layout for a given cluster.",Create/remove directory structure for cluster,,,,eyang,True,eyang,eyang
ambari,AMBARI-39,2011-10-05T00:56:05.000+0000,2011-10-07T20:51:59.000+0000,2011-10-07T20:51:59.000+0000,,Fixed,New Feature,Major,['0.1.0'],,,,,,,,,,"org.apache.ambari.controller.Clusters stores the goal state of the cluster, and org.apache.ambari.resource.statemachine.Cluster stores the transition states.  There is a gap that needs to be bridged between the two APIs.",Integration of REST API create cluster with controller state machine,,,,eyang,True,eyang,eyang
ambari,AMBARI-18,2011-09-29T17:10:22.000+0000,2011-09-30T01:07:35.000+0000,2011-09-30T01:07:35.000+0000,,Fixed,New Feature,Major,['0.1.0'],['0.1.0'],,,,,,,['ambari-agent'],['Ambari Agent'],"Add special command to write configuration file with proper ownership and permission setup.

The required fields are:

* Username
* Group
* Permission
* Filename
* Content

Example:

{noformat}
cmd = [ 'ambari-write-file', 'eyang', 'staff', 0777, ""/tmp/_file_write_test"", ""This is content of the file"" ]
FileUtil.writeFile(cmd)
{noformat}",Add special command to write config file,,,,eyang,True,eyang,eyang
ambari,AMBARI-15,2011-09-27T21:46:24.000+0000,2011-10-01T01:43:07.000+0000,2011-10-01T01:43:07.000+0000,,Fixed,New Feature,Major,['0.1.0'],['0.1.0'],,,,,,,"['ambari-agent', 'ambari-server']","['Ambari Agent', 'Ambari Server']","Agent communicates with controller via REST API.  Controller resource rest api should setup authentication mechanism, and agent should call controller resource with the same credential.
",Implement agent to controller authentication via shared secrets,,,,eyang,True,eyang,eyang
ambari,AMBARI-11,2011-09-24T21:09:02.000+0000,2011-10-01T01:43:18.000+0000,2011-10-01T01:43:18.000+0000,,Fixed,New Feature,Major,['0.1.0'],['0.1.0'],,,,,,,['ambari-agent'],['Ambari Agent'],Implement periodic heartbeat from agent calling to controller.,Implement Agent to Controller Heartbeat,,,,eyang,True,eyang,eyang
ambari,AMBARI-6,2011-09-23T01:20:20.000+0000,2011-09-23T13:12:10.000+0000,2011-09-23T13:12:10.000+0000,,Fixed,New Feature,Major,['0.1.0'],['0.1.0'],,,,,,,,,Moving Clusters and Nodes container objects into controller. Currently they are part of client/entities. ,Moving Clusters and Nodes container objects into controller,,,,vitthal_gogate,True,vitthal_gogate,vitthal_gogate
ambari,AMBARI-5,2011-09-22T23:30:33.000+0000,2011-09-23T13:11:02.000+0000,2011-09-23T13:11:02.000+0000,,Fixed,New Feature,Major,['0.1.0'],['0.1.0'],,,,,,,['ambari-server'],['Ambari Server'],Adding some left over changes to svn after initial import of Ambari code from git. ,Added some left over changes from git repository for Ambari REST APIs,,,,vitthal_gogate,True,vitthal_gogate,vitthal_gogate
ambari,AMBARI-4,2011-09-22T23:17:57.000+0000,2011-09-22T23:25:07.000+0000,2011-09-22T23:25:07.000+0000,,Fixed,New Feature,Major,['0.1.0'],,,,,,,,,,We should have a plugin interface for components.,Create interface for component plugins,,,,owen.omalley,True,owen.omalley,owen.omalley
ambari,AMBARI-2,2011-09-22T22:57:33.000+0000,2011-09-22T23:07:38.000+0000,2012-05-18T03:47:31.000+0000,,Fixed,New Feature,Major,['0.1.0'],['0.1.0'],,,,,,,['ambari-server'],['Ambari Server'],"Define the data structure for heartbeat from Ambari Agent to controller:

The heartbeat message is composed of:

{noformat}
responseId
clusterId
timestamp
hostname
stackId
hardwareProfile
actionResults
serversStatus
{noformat}

The controller response is composed of:

{noformat}
responseId
actions
  commands
  cleanUpCommands
{noformat}
",Define data model for agent controller communication,,,,eyang,True,eyang,eyang
ambari,AMBARI-22182,2017-10-10T05:59:26.000+0000,,2017-12-08T21:49:23.000+0000,,,New Feature,Major,['3.0.0'],"['2.5.0', '2.4.2', '2.5.2']",,,2419200,2419200,2419200,,['ambari-web'],['Ambari Web UI'],"With the development of the community, the use of ambari is becoming more and more popular, and more and more developers of different languages are also involved.At present, only the English version has been unable to meet the requirements of users.For example, in China, many companies face the problem of localization.
So the next version, hoping to add new language support features,users can switch to other languages, such as Chinese, Japanese, and so on.","In order to achieve internationalization, provide multi-language version support, such as Chinese support",2,,"['newbie', 'performance', 'usability']",rila121,True,,rila121
ambari,AMBARI-21805,2017-08-24T16:35:29.000+0000,,2017-08-24T16:35:29.000+0000,,,New Feature,Major,,,,,,,,,['ambari-server'],['Ambari Server'],"New feature in YARN 3 enables to deploy docker containers in Hadoop cluster.  Ambari can automate Docker installation to make the integrate seamless.  Docker-ce can be installed on all slave nodes, and master node can install Docker Registry, and run Docker Registry2 in a docker container.

Installation instruction for docker:
https://docs.docker.com/engine/installation/linux/docker-ce/centos/

Docker Registry can mount HDFS as storage for docker registry data:
{code}
docker run -d -p 5000:5000 --name registry -v /mnt/hdfs/apps:/var/lib/registry registry:2
{code}",Add Docker as a managed service in HDP stack,6,,,eyang,True,,eyang
ambari,AMBARI-21731,2017-08-16T06:57:57.000+0000,,2017-08-17T09:16:43.000+0000,,,New Feature,Major,,['2.5.1'],,,,,,,['ambari-server'],['Ambari Server'],"Implement feature of Rolling Decommission for HDFS Datanodes.

Use ""Under-Replicated Blocks"" as parameter for Rolling Decommission, Instead of Wait Time.


E.g. - To decommission 100 HDFS Datanodes - with rolling decommission of 3 nodes at a time, a batch of decommission should trigger when a threshold of - ""Minimum number of Under-Replicated Blocks"" has reached.

*Note - Minimum Threshold of - ""Under-Replicated Blocks"" has to be taken care so that we don't end up in a situation with huge load on HDFS Namenode and high number of Under Replicated Blocks - Specially with Datanodes of huge Data size ( ~ TB )",Rolling Decommission of HDFS Datanodes,4,,"['datanode', 'decommision', 'hdfs', 'rolling', 'rolling_decommision']",shahrukhkhan489,True,,shahrukhkhan489
ambari,AMBARI-21716,2017-08-14T11:46:54.000+0000,,2017-08-14T11:46:54.000+0000,,,New Feature,Minor,,,,,,,,,['ambari-web'],['Ambari Web UI'],"Currently, clients can be removed from a node only via REST API.
The web UI should let the user do this as well.",Deleting installed clients via UI is not possible,1,,,mgaido,True,,mgaido
ambari,AMBARI-21599,2017-07-28T14:02:17.000+0000,,2017-07-28T14:31:09.000+0000,,,New Feature,Major,,['2.5.1'],,,,,,,"['alerts', 'ambari-server', 'ambari-web']","['Ambari Alerts System', 'Ambari Server', 'Ambari Web UI']","Feature Request to add an Ambari Alert check on Ranger Plugins policy sync for each component.

There are occasions when this sync can get broken and new policies not are applied, this has happened to a previous client, and current it doesn't appear there is any monitoring on this without somebody proactively looking at the Ranger Admin UI -> Audit -> Plugins page for which there is no alerting.",Ambari Alerts check on Ranger Plugins policy sync,1,,,harisekhon,True,,harisekhon
ambari,AMBARI-21597,2017-07-28T13:05:40.000+0000,,2017-07-28T14:31:48.000+0000,,,New Feature,Major,,['2.5.1'],,,,,,,"['alerts', 'ambari-server', 'documentation']","['Ambari Alerts System', 'Ambari Server', 'Documentation']","Request to add online documentation listing all Ambari Alerts checks, script locations and description around what each one does.

You can get most of this information from an Ambari instance's Alerts section, but I think this should still be documented online so that people can see this information if they don't have an Ambari instance available. This will allow one to see what is and isn't covered at any time, which is useful when advising on what additional enterprising monitoring to supplement or when raising requests for additional Ambari checks by cross checking what is already there.",Document Ambari Alert checks,1,,,harisekhon,True,,harisekhon
ambari,AMBARI-21536,2017-07-20T13:19:18.000+0000,,2017-12-27T02:26:23.000+0000,,,New Feature,Major,,['2.5.1'],,,,,,,['ambari-web'],['Ambari Web UI'],"Feature Request add UI ability to trigger an alert check.

For example, Ranger admin password check is critical showing out of sync but since the check is run only every 30 mins it would be nice to be able to instantly trigger the alert check after fixing the issue to test and clear it quicker rather than waiting half an hour. Running a Ranger service check doesn't seem to do this, it just runs the health check for the login page.

It appears this can be triggered via an API call to the alert definition with run_now=true so this simply needs be to exposed via the UI.",Trigger alert check via Ambari UI,2,,,harisekhon,True,,harisekhon
ambari,AMBARI-21513,2017-07-19T05:48:46.000+0000,,2017-07-19T05:48:46.000+0000,,,New Feature,Major,,,,,,,,,['ambari-views'],['Ambari Views framework and Views themselves.  Specific Views that are built on the framework will be handled with labels.'],,WFM-Add a view to list all the unique jobs within a WFM instance.,1,,,Shikhakp12,True,,Shikhakp12
ambari,AMBARI-21508,2017-07-18T17:10:35.000+0000,2017-11-02T16:37:49.000+0000,2017-11-02T16:48:29.000+0000,,Fixed,New Feature,Major,"['trunk', '3.0.0']",['3.0.0'],,,,,,,['ambari-web'],['Ambari Web UI'],"Today Start, or Start All buttons for an individual service are greyed out if all masters are started. The issue is if you have a lot of workers that are not started, you still want to start/start all to get them started. We should change the behavior so it's more like this:
Any component Stopped -> ""Start/Start All"" is available
All components Started -> ""Start/Start All"" is not available
All components (that are not in maintenance mode or on hosts in maintenance mode) are Started -> ""Start/Start All"" is not available
That way if any component is stopped we can start it with a Start/Start All. The behavior should be that Ambari looks at all components for the service and if any are in the stopped state, it will start those components. Maintenance mode should be respected and if a component or host is in maintenance mode, they will be ignored from this.","Lifecycle: Start,Start All Visibility",2,,,ishanbha,True,ishanbha,ishanbha
ambari,AMBARI-21506,2017-07-18T10:41:07.000+0000,,2017-07-18T10:41:07.000+0000,,,New Feature,Blocker,,['2.5.1'],,,,,,,['ambari-server'],['Ambari Server'],"Feature Request to add user/group mapping rules similar to Hadoop's auth_to_local.

This will allow munging users/groups and rule based remappings to differentiate duplicate users in multi-domain Active Directory forests where the LDAP results returned from the global catalog include duplicate usernames which need to be translated with a prefix/suffix in order to differentiate between domains to prevent users from different domains sharing logins, permissions etc.",User/group mapping rules similar to Hadoop's auth_to_local,1,,,harisekhon,True,,harisekhon
ambari,AMBARI-21503,2017-07-17T19:38:48.000+0000,,2018-09-12T05:35:12.000+0000,,,New Feature,Major,,,,,,,,,,,"Would be nice to have JanusGraph as a common-service in Ambari.

Information for JanusGraph can be found at: http://janusgraph.org/

Required Services: HBASE, SOLR (create SOLR service as well, or leverage AMBARI_INFRA?)

JanusGraph is a fork from Titan, IOP (IBM Open Platform) supported Titan 1.0.0 as a service in the BigInsights 4.2.5.0 stack. This work can most likely be used as a starting point for creating a janus service. Similarly BigInsights 4.2.5.0 had a standalone SOLR (6.3.0) service.
",Add JanusGraph Service,2,,,jackson,True,,jackson
ambari,AMBARI-21485,2017-07-17T09:00:22.000+0000,,2018-07-27T16:47:28.000+0000,,,New Feature,Major,,['2.5.0'],,,,,,,['ambari-server'],['Ambari Server'],"Feature Request to use Git inside Ambari for revision controlling the configurations.

This would allow excellent integration with countless other git tools and provide many new capabilities such as instantly replicating the configurations to other git repos for DR, hosting in sophisticated Git repo servers with more features and governance, and would automatically solve missing features around the current revision control system like being able to do global diffs of changes across all services in the entire cluster as requested in AMBARI-12761 a couple years ago.

Several queries across version history are simply not possible or very difficult to do right now and using Git underneath would solve all of them in one go.",Use Git for Ambari Revision Control,1,,,harisekhon,True,,harisekhon
ambari,AMBARI-21433,2017-07-10T09:47:22.000+0000,,2017-07-10T09:47:22.000+0000,,,New Feature,Major,,['2.4.0'],,,,,,,['ambari-server'],['Ambari Server'],"I've been receiving alerts every 2 minutes all weekend from a client's cluster.

Request feature to add alert notification throttling to Ambari Alerts similar to enterprise monitoring systems like Nagios have - they only email once per hour unless there is a state change. They also have flapping detection and suppression if the state changes too frequently, which is related and very useful to minimize duplicate alerts filling up one's inbox or being filtered to a huge folder and ignored because they happen too often. Cluster administrators cannot be expected to wade through a lot of emails to see if something has changed between alerts.",Ambari Alerts notification throttling,1,,,harisekhon,True,,harisekhon
ambari,AMBARI-21307,2017-06-22T10:44:41.000+0000,2017-10-30T15:40:03.000+0000,2017-11-13T18:53:36.000+0000,,Fixed,New Feature,Major,['3.0.0'],,,,2419200,2419200,2419200,,,,Develop support for handling LDAP (and possibly other ambari ralated) configuration as a rest resource.,Create a new rest resource for handling LDAP (and possibly other) ambari configuration,3,,,lpuskas,True,lpuskas,lpuskas
ambari,AMBARI-21186,2017-06-06T14:41:46.000+0000,,2017-07-05T23:28:27.000+0000,,,New Feature,Major,"['trunk', '3.0.0']",,,,,,,,['ambari-web'],['Ambari Web UI'],"Many customers do not want to install all of the clients on their edge nodes, just a subset. We should give users the ability to pick and choose which clients are installed, instead of making it an all or nothing scenario. Likewise if a client installation fails, they want to be able to re-install that single client, or remove a single client. The issue occurs today as we force all clients to be installed or re-installed. We need to provide more fine granularity to allow people to pick single clients, like just HDFS, or just HBase.

After Installation:
On a specific host you should be able to add (All Clients, or Specific Clients)
On a specific host you should be able to remove (All Clients, or Specific Clients)
If an individual client install fails you should be able to (Retry Client Install)",Install: Selective Client Install/Delete for Hosts Page,1,,,ishanbha,True,ishanbha,ishanbha
ambari,AMBARI-21178,2017-06-05T16:34:36.000+0000,,2018-07-27T16:47:27.000+0000,,,New Feature,Major,,['2.5.0'],,,,,,,['ambari-server'],['Ambari Server'],"Feature request for a global view of all service configuration revision history.

When there are changes to multiple services pending restarts for a change maintenance window it would be nice to be able to see the global log of revision control changes so we can see what will be applied until cluster restart. This is especially important when more than one person has scheduled changes.",Revision Control - Global change log - view across all cluster services,1,,,harisekhon,True,,harisekhon
ambari,AMBARI-21143,2017-05-29T19:12:18.000+0000,,2017-09-14T19:03:47.000+0000,,,New Feature,Minor,,,,,,,,,,,"Allow for optional deployment of Confluent Kafka REST Proxy along with Kafka.   This would be similar to the HBase REST proxy on top of HBase.

https://github.com/confluentinc/kafka-rest

",Deployment of Confluent Kafka REST Proxy,1,,,rkellogg,True,,rkellogg
ambari,AMBARI-20993,2017-05-11T16:24:36.000+0000,2017-06-26T14:21:48.000+0000,2017-06-26T14:21:48.000+0000,,Invalid,New Feature,Major,,['trunk'],,,,,,,['ambari-server'],['Ambari Server'],"AMBARI-19685 started the work but is only enforcing non-conditional dependencies for slaves and masters.  This will add in the ability to validate the layout and the configuration for conditional dependencies both for ""property exists"" and ""property equals"" type of dependencies.",Stack advisor needs to enforce conditional component dependency for slaves and masters,1,,,Tim Thorpe,True,Tim Thorpe,Tim Thorpe
ambari,AMBARI-20984,2017-05-10T21:38:09.000+0000,2017-05-23T16:04:44.000+0000,2017-05-23T18:01:38.000+0000,,Fixed,New Feature,Major,['ambari-server'],['trunk'],,,,,,,['ambari-server'],['Ambari Server'],"Currently, if a user wants to use custom HDP repos during a blueprint deployment, he has to update the repo URLs before hand via a REST API. The JIRA here proposes a way to include stack repos in the blueprint as optional fields so that when the fields exist, Ambari server will use the repos to deploy a cluster. This eliminates the need for user to run a separate rest api call before the actual blueprint deployment.",Be able to include stack repos in blueprint,2,,,dili,True,dili,dili
ambari,AMBARI-20944,2017-05-05T16:42:07.000+0000,,2017-06-01T20:08:24.000+0000,,,New Feature,Major,['trunk'],['trunk'],,,,,,,['ambari-web'],['Ambari Web UI'],"Currently, blueprint based install is a REST API operation, the JIRA here contains a series of UI-based improvements for better user experience with blueprints.",Ambari installer UI support for blueprint,2,,,dili,True,arborkar,dili
ambari,AMBARI-20927,2017-05-04T04:16:40.000+0000,,2017-05-04T04:16:40.000+0000,,,New Feature,Major,,['2.5.0'],,,,,,,['ambari-server'],['Ambari Server'],"Allow Ambari Agents to be managed by Ambari Server:

1. A tab for Agents or include in Host tab from where we should be able to maintain and monitor the Agents
2. Once Ambari Sever is upgraded, have an option to upgrade the agents through Ambari dashboard",Allow Ambari Agents to be Managed by Ambari Server,1,,,venkatramanp,True,,venkatramanp
ambari,AMBARI-20891,2017-04-28T15:05:12.000+0000,2017-05-09T14:54:49.000+0000,2017-11-24T20:32:06.000+0000,,Fixed,New Feature,Major,"['trunk', '2.6.1']",['trunk'],,,,,,,['ambari-server'],['Ambari Server'],"It would possible to link extensions to supported stack versions while parsing the stacks, extensions and common-services directories.

This would allow extensions to avoid making rest API calls to set up the link.",Allow extensions to auto-link with supported stack versions,2,,,Tim Thorpe,True,Tim Thorpe,Tim Thorpe
ambari,AMBARI-20848,2017-04-25T18:00:45.000+0000,,2017-04-25T18:08:35.000+0000,,,New Feature,Major,['3.0.0'],['2.5.0'],,,,,,,['ambari-server'],['Ambari Server'],"Many times upon troubleshooting kerberos related issues we have to remember or research how each component enables kerberos logging and apply to the env, etc.  I would like to see it as easy as a check box to enable kerberos debugging on each component.

Code example:

{code}
if [ {{debug_kerberos}} == ""true"" ]]; then 
  export HADOOP_OPTS=""-Dsun.security.krb5.debug=true $HADOOP_OPTS""
if
{code}",UI switch to turn on kerberos debug logging for each component.,1,,,dvillarreal,True,rlevas,dvillarreal
ambari,AMBARI-20767,2017-04-14T03:32:21.000+0000,,2017-06-13T07:12:15.000+0000,,,New Feature,Major,['2.4.0'],['2.4.0'],,,,,,,"['ambari-admin', 'ambari-web']","['Ambari Admin', 'Ambari Web UI']","Need chinese translation files , include ambari-admin\src\main\resources\ui\admin-web\app\scripts\i18n.config.js and ambari-web\app\locales\zh\messages.js",Add the Chinese translation files,3,,['features'],1925432244@qq.com,True,,1925432244@qq.com
ambari,AMBARI-20454,2017-03-15T01:08:45.000+0000,,2017-04-27T22:11:26.000+0000,,,New Feature,Major,,,,,,,,,"['ambari-server', 'ambari-web']","['Ambari Server', 'Ambari Web UI']","Currently, Ambari has a number of configuration in ambari.properties and quite a few other such as service check time out, directory permission that are hard coded.     This make it hard to manage Ambari customization to better work with the cluster.    Many of these properties should be managed in Ambari similar to service configuration for better version control, and upgrading from release to release.",Some Ambari configuration should be managed similar to other configuration ,2,,,tctruong213,True,jmarron,tctruong213
ambari,AMBARI-19928,2017-02-08T19:23:25.000+0000,2017-02-10T00:50:33.000+0000,2017-03-15T21:21:49.000+0000,,Fixed,New Feature,Major,['2.5.0'],['2.5.0'],,,,,,,,,- Add solr grafana dashboards for the solr stack available here: https://github.com/lucidworks/solr-stack,Solr grafana dashboards,4,,,willy.solaligue,True,willy.solaligue,willy.solaligue
ambari,AMBARI-19925,2017-02-08T16:33:56.000+0000,,2017-02-08T16:33:56.000+0000,,,New Feature,Major,,['2.4.1'],,,,,,,['ambari-server'],['Ambari Server'],"Feature Request to add action ""Rolling Restart All Affected"" when making configuration changes.

Current option ""Restart All Affected"" causes a service outage and wastes the HA configuration of dual NameNode / ResourceManager etc.

Current ""Rolling Restart"" only handles slave roles as raised in AMBARI-6706.

This leaves the admin having to rolling restart all slaves and then manual restart all masters components such as namenodes, resource managers, journal nodes, zkfc etc",Rolling Restart All Affected,1,,,harisekhon,True,,harisekhon
ambari,AMBARI-19413,2017-01-07T08:38:59.000+0000,,2018-08-15T09:46:06.000+0000,,,New Feature,Major,['2.2.1'],['2.2.1'],,,,,,,['ambari-server'],['Ambari Server'],"Allow Ceph to be installed via Ambari
I'm trying to do
ceph-repo addr: http://archive.redoop.com/crh4/redhat/6/x86_64/crh/4.9/",Support Ceph for Ambari Service,3,,"['ambari', 'ceph']",Gaving,True,,Gaving
ambari,AMBARI-19248,2016-12-20T02:35:09.000+0000,2017-01-11T00:45:50.000+0000,2017-01-13T20:01:01.000+0000,,Fixed,New Feature,Major,['2.5.0'],['2.5.0'],,,,,,,['stacks'],['Ambari Stacks'],,Add Livy to HDP 2.6 as slave component of Spark2,3,,,merlin,True,merlin,merlin
ambari,AMBARI-19241,2016-12-19T17:01:30.000+0000,2017-01-12T13:49:24.000+0000,2017-01-12T14:41:42.000+0000,,Fixed,New Feature,Major,"['trunk', '2.5.0']","['trunk', '2.5.0']",,,691200,691200,691200,,"['ambari-agent', 'contrib']","['Ambari Agent', 'Contributions under ""contrib""']","Currently the hdfs_resources.py supports 
1) create files/directories in HDFS
2) delete files/directories in HDFS
3) upload files/directories to HDFS

We should also support download of files/directories from HDFS.  This will help particularly in cloud environments where the users of the cluster don't necessarily have write access to the local file system.",Ambari python scripts should support hdfs download,2,,,Tim Thorpe,True,Tim Thorpe,Tim Thorpe
ambari,AMBARI-19187,2016-12-13T10:23:04.000+0000,2016-12-23T18:48:27.000+0000,2016-12-23T18:48:27.000+0000,,Fixed,New Feature,Major,['2.5.0'],['2.5.0'],,,,,,,"['ambari-agent', 'ambari-server']","['Ambari Agent', 'Ambari Server']","Hadoop components need to establish a secure connection with ZooKeeper when Kerberos is enabled. This involves the setup of the correct authentication (JAAS config file) and authorization (per-component Kerberos-backed ACLs on the znodes) between the service and ZooKeeper. Most services are able to set these ACLs based on their config when the user enable kerberos.

When we disable kerberos again, the sasl ACL should be removed otherwise the services won't be able to access their znodes.

This issue is about introducing a new command (DISABLE_SECURITY) that will be sent by the ambari server to the services upon the dekerberiztion process. When a service receives this command it will be able to do the zookeeper secure to unsecure migration process (e.g. removing sasl ACLs).

",Disable security hook,2,,,amagyar,True,amagyar,amagyar
ambari,AMBARI-19066,2016-12-02T17:09:02.000+0000,2016-12-09T14:14:17.000+0000,2016-12-09T14:14:17.000+0000,,Fixed,New Feature,Major,['2.5.0'],,,,,,,,['ambari-server'],['Ambari Server'],"Add more INFO level logging in HeartbeatProcessor.processStatusReports, AgentRequests.setExecutionDetailsRequest to be able to trace status command report processing and agent request for execution command details.",Add more logging around status command report processing on server side,2,,,smagyari,True,smagyari,smagyari
ambari,AMBARI-19044,2016-12-01T07:19:42.000+0000,2017-01-19T10:23:32.000+0000,2017-01-19T11:08:52.000+0000,,Fixed,New Feature,Major,"['3.0.0', '2.5.0']",,,,,,,,['ambari-server'],['Ambari Server'],"Currently, Ambari provides a single toggle button to enable Ranger plugins for a component. Plugin enabled results in a bunch of configuration which are created on backend which are tighly coupled with Ranger admin installed on the same cluster.

Need to make sure plugins communicate to Ranger admin which is installed on separate cluster.",Install & configure Ranger plugin components independently of Ranger admin components,2,,,mugdha.varadkar,True,mugdha.varadkar,mugdha.varadkar
ambari,AMBARI-19019,2016-11-29T16:43:42.000+0000,,2017-03-20T19:00:09.000+0000,,,New Feature,Major,['3.0.0'],['2.5.0'],,,,,,,['ambari-server'],['Ambari Server'],"This feature is to  implement a Knox HA Wizard. (Model after existing HA wizards).
1. Add HA Knox instances through Knox HA Wizard.
   - Sync up the topologies keystores for all Knox nodes. (to the initial Knox topology).
   - Hook either HA Proxy or Apache server + ssl_mod + balancer_mod + proxy_mod ( we could provide Wizard instructions).
   - Service check for Knox HA.
   - Restart Knox server.
 
 ",Knox HA Wizard,3,,,jeffreyr97,True,jeffreyr97,jeffreyr97
ambari,AMBARI-19018,2016-11-29T14:47:22.000+0000,2016-12-02T16:21:47.000+0000,2016-12-02T20:47:29.000+0000,,Fixed,New Feature,Major,"['trunk', '2.5.0']","['trunk', '2.5.0']",,,,,,,"['ambari-agent', 'ambari-server']","['Ambari Agent', 'Ambari Server']","A component should be able to specify a custom folder to sync to the agents in the metainfo.xml.

For example:

        <component>
          <name>METRICS_GRAFANA</name>
          <displayName>Grafana</displayName>
          <category>MASTER</category>
          <customFolder>dashboards</customFolder>
          <commandScript>
            <script>scripts/metrics_grafana.py</script>

This will sync the resources/dashboards directory to the agents if the directory exists on the server.
",Services should be able to specify their own resources subdirs for sync'ing to agents,2,,,Tim Thorpe,True,Tim Thorpe,Tim Thorpe
ambari,AMBARI-19007,2016-11-29T08:58:59.000+0000,2016-12-05T04:50:22.000+0000,2016-12-05T05:31:55.000+0000,,Fixed,New Feature,Blocker,"['3.0.0', '2.5.0']",,,,,,,,['ambari-server'],['Ambari Server'],"*Need support in ambari to allow atlas to communicate with hooks installed on separate clusters*

# Ambari Stack definition changes (for all four hooks being installed on Separate Cluster) - this information will be stored in blueprints.
# Need to be able to sync with Atlas to leverage tag based security via tagsync service
# Ambari driven configuration and addition of flags to support the feature.",Atlas to support configuration of hooks from separate cluster,3,,,mugdha.varadkar,True,mugdha.varadkar,mugdha.varadkar
ambari,AMBARI-18987,2016-11-24T20:08:35.000+0000,2016-11-28T21:04:08.000+0000,2016-11-30T20:18:46.000+0000,,Fixed,New Feature,Major,"['trunk', '2.5.0']","['trunk', '2.5.0']",,,,,,,['ambari-server'],['Ambari Server'],"A general check that takes list of services defined in upgrade XML and check if they are installed, fail the check if there is at least one hit.

Displaying different error messages depending on if a service does not support upgrade or if a service is removed in the new release ( the two types are specified via different properties in the upgrade XML files.",A general preupgrade check on if services cannot be upgrade are installed,2,,,dili,True,dili,dili
ambari,AMBARI-18852,2016-11-10T22:03:50.000+0000,2016-11-15T21:47:54.000+0000,2016-11-16T02:03:15.000+0000,,Fixed,New Feature,Major,['trunk'],['trunk'],,,,,,,['ambari-agent'],['Ambari Agent'],a new feature to support stopping processes owned by users or java processes.,HostCleanup.py to be able to stop processes owned by users or java processes ,2,,,dili,True,dili,dili
ambari,AMBARI-18818,2016-11-08T06:32:27.000+0000,2016-11-22T13:19:00.000+0000,2016-11-22T13:53:39.000+0000,,Fixed,New Feature,Blocker,['2.5.0'],['2.5.0'],,,,,,,['ambari-server'],['Ambari Server'],"Following properties are need to be added in atlas-application.properties from Ambari  Configs to enable Knox SSO in Atlas

{noformat}
atlas.sso.knox.enabled
atlas.sso.knox.providerurl
atlas.sso.knox.publicKey
atlas.sso.knox.browser.useragent
{noformat}",Config changes for Atlas to Support KnoxSSO Authentication,2,,,mugdha.varadkar,True,mugdha.varadkar,mugdha.varadkar
ambari,AMBARI-18587,2016-10-13T13:29:01.000+0000,2016-11-14T17:48:01.000+0000,2016-12-07T11:01:31.000+0000,,Fixed,New Feature,Major,['2.5.0'],,864000,864000,172800,1209600,172800,83,['ambari-server'],['Ambari Server'],"Ability to execute post user creation script.


committed to trunk:

commit 95233eb3aa57027601072440f6c7b9b757c28692
Author: Laszlo Puskas <lpuskas@hortonworks.com>
Date:   Mon Nov 14 18:40:11 2016 +0100

AMBARI-18587. Post user creation hook. (Laszlo Puskas via stoader)


committed to branch-2.5:

commit a5fdae802210ae1f8d4fed2234f1651cbe61c2b5
Author: Laszlo Puskas <lpuskas@hortonworks.com>
Date:   Mon Nov 14 18:43:53 2016 +0100

AMBARI-18587. Post user creation hook. (Laszlo Puskas via stoader)",Post user creation hook,2,,,lpuskas,True,lpuskas,lpuskas
ambari,AMBARI-18297,2016-09-01T18:35:26.000+0000,2016-09-03T01:13:14.000+0000,2016-11-03T01:16:53.000+0000,,Fixed,New Feature,Major,"['trunk', '2.4.1', '2.5.0']",['trunk'],,,,,,,['contrib'],"['Contributions under ""contrib""']",Add initial code for HAWQ View under contrib/views,Add View for Apache HAWQ,7,,,mithmatt,True,spollock,mithmatt
ambari,AMBARI-18281,2016-08-30T21:00:38.000+0000,2016-09-13T20:34:02.000+0000,2016-09-13T22:20:12.000+0000,,Fixed,New Feature,Major,['3.0.0'],['3.0.0'],,,,,,,['ambari-web'],['Ambari Web UI'],Add an option to Enable/ Disable alert notifications in Manage Notifications popup.,Expose Disabling of Alert Targets in Web Client,3,,,vivekratnavel,True,vivekratnavel,vivekratnavel
ambari,AMBARI-18141,2016-08-12T18:26:08.000+0000,2016-08-15T14:54:43.000+0000,2016-08-15T16:42:37.000+0000,,Fixed,New Feature,Major,['trunk'],['trunk'],,,,,,,['ambari-admin'],['Ambari Admin'],"This jira proposes a read-only view of the blueprint used by the current cluster so that if user wants to deploy a cluster with the same (or similar) configurations, they can save the blueprint directly from the Ambari admin view UI instead of issuing rest api calls via command line.",Display and export current cluster's blueprint from Ambari Admin view UI,2,,,dili,True,dili,dili
ambari,AMBARI-18102,2016-08-10T14:37:40.000+0000,,2017-04-07T15:44:37.000+0000,,,New Feature,Major,['trunk'],['trunk'],,,,,,,['ambari-server'],['Ambari Server'],"There are limited validations against blueprint and the cluster creation template when a user deploys a cluster via blueprint. 

More host level checks before actually deploying the cluster would be beneficial as users do not need to sit thru the deploy only to find out errors at the end. Users can rerun the checks multiple times after fixing issues on given hosts or correcting configuration values in the blueprint.

This JIRA proposes a Python script based design including a central script on the Ambari server and a Custom Action script on each Ambari Agent node to run checks based on property values defined in the blueprint. Right now the checks are checking ports and directories.",Preinstall check for blueprint-based cluster deployment,2,,,dili,True,dili,dili
ambari,AMBARI-18065,2016-08-08T15:49:51.000+0000,2016-12-05T15:41:46.000+0000,2016-12-05T19:07:24.000+0000,,Fixed,New Feature,Major,['2.5.0'],['2.4.0'],,,,,,,['ambari-server'],['Ambari Server'],"When using option 'blueprint.skip_install_tasks' to skip install tasks on a preconfigured environment, Ranger Admin start is failing due to missing ranger db configs because Ranger db is only configured in INSTALL phase. DB setup should be moved to configure method so that start request would setup db as well.",Change Ranger Admin start script to setup db for ranger,2,,,smagyari,True,smagyari,smagyari
ambari,AMBARI-18051,2016-08-05T19:17:15.000+0000,2016-09-28T16:31:28.000+0000,2016-10-14T16:21:06.000+0000,,Fixed,New Feature,Major,"['trunk', '2.5.0', '2.4.2']",['trunk'],,,,,,,['ambari-server'],['Ambari Server'],"Services should be able to provide their own pre-req checks by supplying a jar file.

This would allow custom services to supply their own jar files to handle pre-req checks rather than forcing third party developers to make changes to ambari-server code.",Services should be able to provide their own pre-req checks by supplying a jar file,4,,,Tim Thorpe,True,Tim Thorpe,Tim Thorpe
ambari,AMBARI-18050,2016-08-05T19:14:46.000+0000,2016-09-12T14:27:58.000+0000,2016-09-28T16:32:56.000+0000,,Fixed,New Feature,Major,"['trunk', '2.5.0', '2.4.2']",['trunk'],,,,,,,['ambari-server'],['Ambari Server'],"Currently each pre-req check must add an entry in the CheckDescription enum.  This limits the ability for third party stacks and extensions to provide their own pre-req checks.  

The CheckDescription enum should be rewritten as a class and each pre-req check class should create an instance of it.  This will allow stacks and extensions to include their own pre-req checks in separate jar files without requiring changes to ambari-server java code.",Upgrade pre-req check code needs to be decoupled from CheckDescription class,2,,,Tim Thorpe,True,Tim Thorpe,Tim Thorpe
ambari,AMBARI-18049,2016-08-05T19:07:03.000+0000,2016-09-28T16:35:35.000+0000,2016-11-03T15:17:37.000+0000,,Won't Fix,New Feature,Major,,['trunk'],,,,,,,['ambari-server'],['Ambari Server'],"Extensions generally contain custom services.  These custom services are usually not included in the stack's repositories.  This means that when you link an extension version to a stack version and want to install the extension's custom services, you first need to add a repositories.  

Extensions should include their own repos/repoinfo.xml.  Then ambari can handle the new repositories automatically.

Here are the two possible scenarios:

A) Install cluster using HDP-2.5
B) Add extension EXT-1.0
C) Link EXT-1.0 to HDP-2.5
D) Install services from EXT-1.0

1) Add extension EXT-1.0
2) Link EXT-1.0 to HDP-2.5
3) Install cluster using HDP-2.5 including services from EXT-1.0

In the first case during step A, HDP-2.5's repoinfo.xml is used to populate the repositories in the repo_version table. During step C, EXT-1.0's repoinfo.xml is used to add additional repositories to the repo_version table.

In the second case during step 3, both HDP-2.5's and EXT-1.0's repoinfo.xml files are used to populate the repositories in the repo_version table.

Unlinking EXT-1.0 from HDP-2.5 would need to remove the extension's repositories from the repo_version table. This would use the repo_id because the repo_url may have been changed by the user.

During the linking process (extension version to stack version), there would be a check to ensure that the extension does not duplicate any repo_ids or repo_urls that exist within the stack.
",Extensions should support repositories,1,,,Tim Thorpe,True,Tim Thorpe,Tim Thorpe
ambari,AMBARI-17995,2016-08-02T23:58:09.000+0000,,2016-08-02T23:58:09.000+0000,,,New Feature,Major,,,,,,,,,['ambari-views'],['Ambari Views framework and Views themselves.  Specific Views that are built on the framework will be handled with labels.'],"Business analysts require an interface that supports their typical workflow for adhoc data lake information requests:

import necessary transactional data from RDBMS ----> model queries based on data from the data like combined with the imported transactional data ---->  create recurring job.

Such a workflow requires the ability to use a datasource built by a DBA or other authorized user to build a Sqoop job for the data import.  Ideally, this view connects to the RDBMS and executes a list-tables command.  Once the results are returned by Sqoop, a modified version of Hive Query view would open with two database explorers:  an RDBMS explorer and the Hive explorer.

",Hive View integration with Sqoop and Oozie,1,,,cnormile,True,,cnormile
ambari,AMBARI-17994,2016-08-02T23:45:03.000+0000,,2016-08-16T23:57:17.000+0000,,,New Feature,Major,['trunk'],,,,,,,,['ambari-views'],['Ambari Views framework and Views themselves.  Specific Views that are built on the framework will be handled with labels.'],,Workflow Designer,2,,,cnormile,True,,cnormile
ambari,AMBARI-17985,2016-08-01T23:36:16.000+0000,,2016-08-01T23:36:16.000+0000,,,New Feature,Major,,,,,,,,,"['ambari-server', 'stacks']","['Ambari Server', 'Ambari Stacks']","Integrate Elasticsearch with Ambari

Custom service for Ambari already exists which can be integrated into ambari.

https://github.com/Symantec/ambari-elasticsearch-service",Integrate Elasticsearch with Ambari,2,1,"['ambari', 'elasticsearch']",saurabh_mishra,True,,saurabh_mishra
ambari,AMBARI-17984,2016-08-01T23:33:37.000+0000,,2016-08-12T21:20:57.000+0000,,,New Feature,Major,,,,,,,,,['ambari-server'],['Ambari Server'],"Integrate Cassandra with Ambari

Custom service for Cassandra already exists which can be integrated into ambari.

https://github.com/Symantec/ambari-cassandra-service",Integrate Cassandra with Ambari,2,,['cassandra'],saurabh_mishra,True,,saurabh_mishra
ambari,AMBARI-17752,2016-07-16T00:04:01.000+0000,,2016-07-16T00:04:01.000+0000,,,New Feature,Major,,,,,4838400,4838400,4838400,,"['2.2.1', 'ambari-web']","['', 'Ambari Web UI']","Ambari Localization Redesign

The Content of Em.I18n.translations part in app.js should be taken out and named as EN-US.js and other hardcoded content needs to be taken out from the web pages and merge into the EN-US.js as well. 

Ambari should have a feature to allow user to select UI Language, and langurage registration should be easy for users to use. 
",Ambari Localization Redesign,1,,['patch'],liuyan,True,,liuyan
ambari,AMBARI-17708,2016-07-14T12:21:42.000+0000,2016-07-18T07:56:11.000+0000,2016-07-18T07:56:11.000+0000,,Duplicate,New Feature,Major,,['trunk'],,,,,,,['ambari-server'],['Ambari Server'],"LDAP is complicated and needs careful configuration especially if synchronizing with a local users repository. It can even get more complex, when trying to support users from multiple domains, which is not supported by Ambari right now.

Tools like SSSD, Winbind, Quest, Centrify, ... do a good job of integrating complex LDAP/AD environments to Unix/Linux based systems using PAM.

Using PAM in Ambari could potentials simplify user authentication a lot.

As users synchronization would not be required anymore, users would need to be created at first log in. This can be borrowed from the newly implemented JWT authentication.

Other projects using PAM authentication:
(In Hadoop Knox) https://issues.apache.org/jira/browse/KNOX-537
(With Spring Auth) https://github.com/ImmobilienScout24/yum-repo-server/blob/master/src/main/java/de/is24/infrastructure/gridfs/http/security/PamAuthenticationProvider.java",Support PAM Authentication,3,,['security'],hkropp,True,hkropp,hkropp
ambari,AMBARI-17706,2016-07-14T08:23:43.000+0000,2016-07-15T13:55:12.000+0000,2016-07-15T16:58:27.000+0000,,Fixed,New Feature,Major,['2.4.0'],['2.4.0'],,,,,,,['ambari-logsearch'],[''],Event history implementation without using 3rd party libraries.,LogSearch Portal: Event history implementation.,3,,,mdharmesh,True,mdharmesh,mdharmesh
ambari,AMBARI-17352,2016-06-21T19:59:25.000+0000,2016-06-30T14:00:57.000+0000,2016-06-30T16:03:21.000+0000,,Fixed,New Feature,Major,['trunk'],['trunk'],,,,,,,['contrib'],"['Contributions under ""contrib""']","Ambari web UI runs host checks during host (ambari agent) registration. For users that run blueprint install, it would be nice if Ambari ships with a command line script that runs host checks and summarize the results. ",A command line script to run pre-install checks and summarize the results,2,,,dili,True,dili,dili
ambari,AMBARI-17241,2016-06-14T22:55:40.000+0000,,2016-07-20T18:26:52.000+0000,,,New Feature,Minor,['trunk'],,,,,,,,,,"The current implementation forces users to restart the service if any of the configurations have been updated. 

However some of the services support reloading the configuration parameters during run-time which does not require restart of the service.

This behavior should be driven by metadata defined in configuration files.
{code}
  <property>
    <name>foo-bar</name>
    <value>0</value>
    <supports-reload>true</supports-reload>
  </property>
{code}",Services should be able to reload configs without requiring restart,11,1,,mithmatt,True,mithmatt,mithmatt
ambari,AMBARI-17092,2016-06-07T12:14:29.000+0000,2016-06-07T16:55:09.000+0000,2016-06-07T20:25:14.000+0000,,Fixed,New Feature,Major,['2.4.0'],['2.4.0'],,,,,,,['ambari-logsearch'],[''],LogSearch portal should support HTTPS to ensure secure communication,LogSearch https support.,3,,,mdharmesh,True,mdharmesh,mdharmesh
ambari,AMBARI-17041,2016-06-04T00:05:16.000+0000,2016-11-11T17:48:31.000+0000,2016-11-11T19:05:55.000+0000,,Fixed,New Feature,Major,['2.5.0'],['2.2.2'],,,,,,,['ambari-server'],['Ambari Server'],"Currently, services can define properties in the XML configuration files that is flagged as type password:
  <property>
        <name>my.special.password</name>
        <value></value>
        <property-type>PASSWORD</property-type>
        <description>Password to be masked</description>
    </property> 
and it will be masked properly in the UI as well as blueprint.

Custom property should also support this option so that password can be added as custom property and treat accordingly.


==========================================
Proposed Design for the fix:
==========================================
At present only the key-value information of the service properties is stored in the DB (""clusterconfig"" table in the ""config_data"" column). 
The ""config_attributes"" column stores only certain attributes like ""final"" indicating the list of properties set with the Final flag = true. 
The information about the property-type (i.e PASSWORD, USER, GROUP, ADDITIONAL_USER_PROPERTY, VALUE_FROM_PROPERTY_FILE, NOT_MANAGED_HDFS_PATH, etc) is extracted from the corresponding service's property file (e.g. hive-site.xml, core-site.xml, webhcat-env.xml, etc). These files contain information of the existing properties only. Custom Properties added by ambari user have no provision to store their additional attributes. 

Since, for this Jira we are concerned with only <property-type> attribute for Custom Properties, we could add an additional field called ""Property Type"" in the ""Add Property"" pop-up which shows up on clicking ""Add Property ..."" in the Custom property section for a service. For now, only 2 options are shown in the drop-down list: NONE and PASSWORD .
A few sample test properties are created using the new ""Add Property"" pop-up as can be seen in the following attachments. 
Attachments: 
""add_property_pop_up.tiff""
""custom_property_password_type.tiff""
""custom_property_regular_type.tiff""
""custom_properties_after_save.tiff""

The <property-type> information for these Custom properties is stored in the DB in ""clusterconfig"" table, ""config_attributes"" column.
The schema for ""clusterconfig"" table can be seen in the attachment:
""schema_of_clusterconfig_table.tiff""

The content of the ""config_attributes"" column with the <property-type> information from the new Custom properties can be seen in the attachment:
""cluster_config_with_password_type_in_config_attributes_column.tiff""


Note: The fix so far is performed only for new Custom properties. The <property-type> information for existing properties is extracted from the corresponding property xml files for the service.",Support password type for custom properties,6,,,tctruong213,True,patelket@us.ibm.com,tctruong213
ambari,AMBARI-17024,2016-06-03T01:05:49.000+0000,2016-06-03T19:26:17.000+0000,2016-07-05T22:23:48.000+0000,,Fixed,New Feature,Major,['2.4.0'],['2.4.0'],,,,,,,['stacks'],['Ambari Stacks'],,Update name of PXF component to PXF Agent,2,,,bhuvnesh2703,True,bhuvnesh2703,bhuvnesh2703
ambari,AMBARI-17023,2016-06-02T23:42:07.000+0000,2016-06-13T22:38:19.000+0000,2016-06-13T22:38:19.000+0000,,Fixed,New Feature,Major,['2.4.0'],['2.4.0'],,,,,,,['stacks'],['Ambari Stacks'],"Show only relevant properties based on the status of HAWQ Resource Manager.

When Yarn mode is enable, only two parameters should be invisible.
hawq_rm_memory_limit_perseg
hawq_rm_nvcore_limit_perseg

When standalone mode is enabled, four parameters should be invisible. what you list are correct.
hawq_rm_yarn_app_name
hawq_rm_yarn_queue_name
hawq_rm_yarn_scheduler_address
hawq_rm_yarn_address",Show only relevant properties based on the status of HAWQ Resource Manager,2,,,bhuvnesh2703,True,bhuvnesh2703,bhuvnesh2703
ambari,AMBARI-16937,2016-05-27T20:01:01.000+0000,,2016-05-27T20:01:01.000+0000,,,New Feature,Major,['3.0.0'],,,,,,,,,,"Components like HBase, and Storm have additional master component additions available from the Service Actions menu.  For consistency purposes, the suggestion to add the ability to add an additional Ooze Server.  Suggestion  is ""+ Add Oozie Server"", just like we have for HBase ""+ Add HBase Master"".",Make adding an Oozie Server available from Service Actions,1,,,paulcodding,True,,paulcodding
ambari,AMBARI-16884,2016-05-25T17:52:07.000+0000,,2016-05-25T17:53:12.000+0000,,,New Feature,Major,['3.0.0'],,,,,,,,,,"When configurations are changed and components require restarting, having a button to easily view the changes that will be applied to the configuration during the restart operation is required.  Simply displaying the configuration diff screen that we have for manually comparing versions would be helpful so the operator knows exactly what changes will be applied.",Quickly show configuration changes when stale configuration is detected,2,,,paulcodding,True,,paulcodding
ambari,AMBARI-16882,2016-05-25T16:43:13.000+0000,,2016-05-25T16:57:34.000+0000,,,New Feature,Major,['3.0.0'],,,,,,,,,,"Each stack version takes up ~2GB of space on a host, and after a version is successfully upgraded to and fully checked out, it would be nice to be able to manually remove old stack versions that are no longer in use.  Adding a (- Remove) button from the Cluster -> Versions table of installed versions to erase the packages and /usr/hdp entries is desired.",Manually remove previous stack versions after upgrade is completed,2,,,paulcodding,True,,paulcodding
ambari,AMBARI-16880,2016-05-25T16:14:15.000+0000,2017-03-06T22:11:34.000+0000,2017-03-06T22:11:34.000+0000,,Fixed,New Feature,Major,['2.5.0'],,,,,,,,,,"Common log4j configurations for the rolling file appender used by components like Hive, HDFS, Kafka, etc. should be easily configurable as Smart Config fields.  

Specifically configurations like:

* MaxBackupIndex
* MaxFileSize

These fields should be exposed in each component in a Logging section that has input fields such as:

* Maximum Backup File Size: 100 MB
* Maximum Number of Backup Files: 10",Add common log rotation settings to Smart Config,3,,,paulcodding,True,mradhakrishnan,paulcodding
ambari,AMBARI-16879,2016-05-25T15:53:28.000+0000,,2016-05-25T15:53:28.000+0000,,,New Feature,Major,['3.0.0'],,,,,,,,,,"It would be preferable to have a choice besides MB when editing ""slider"" fields that deal with sizes that can range from MB's to GB's such as:

* NameNode Java heap size
* DataNode maximum Java heap size

What would be preferable would be two things:

# Round the slider stops to the largest default unit (MB)
# When editing the field, make the unit field a drop-down so that (bytes,MB,GB,TB) can be chosen.

With these two enhancements, the sliders will be more usable, and when editing it will be more convenient to chose the unit that makes the most sense to the operator, for some 2048MB, others 2GB.",Allow for alternate units to be selected when editting smart config fields,2,,,paulcodding,True,,paulcodding
ambari,AMBARI-16878,2016-05-25T15:26:54.000+0000,2016-05-25T15:36:59.000+0000,2016-05-25T15:36:59.000+0000,,Duplicate,New Feature,Major,['3.0.0'],,,,,,,,,,"In order to provide more flexibility for diverse cluster sizes, we need the ability to allow SCRIPT type alerts to expose thresholds that can be modified by operators.  For example SCRIPT alerts like:

* Host Disk Usage
* NameNode Last Checkpoint
* NameNode Client RPC Queue Latency (Daily)

Each need to be modifiable for a specific environment if necessary.  This capability already exists for Metric, Aggregate, and other alert types, and extending this functionality to SCRIPT alert types is highly beneficial.","Allow for easy, and persistent editing of SCRIPT alert thresholds ",1,,,paulcodding,True,,paulcodding
ambari,AMBARI-16877,2016-05-25T14:58:59.000+0000,,2016-05-25T14:58:59.000+0000,,,New Feature,Major,['3.0.0'],,,,,,,,,,"In an effort to expedite component lifecycle operations like start and restart, being able to skip the yum install operations that proceed the component start command would be preferred.  It makes sense to do this on first starting a component to ensure that the proper packages are available, but once a component has been successfully started, and there are no upgrade related activities, the day to day management of that component shouldn't have to pay the tax of waiting for the yum install command to complete on every single start/restart operation.",Skip yum install operations on component start,2,,,paulcodding,True,,paulcodding
ambari,AMBARI-16864,2016-05-25T08:56:25.000+0000,,2017-03-21T18:29:31.000+0000,,,New Feature,Major,['trunk'],['2.4.0'],,,,,,,['stacks'],['Ambari Stacks'],Propose to add Spark2 in Ambari as a standalone service in parallel with Spark1.,Add unit tests for Spark2 service definition,3,,,jluniya,True,jerryshao,jluniya
ambari,AMBARI-16753,2016-05-19T01:17:49.000+0000,2016-05-25T08:58:12.000+0000,2016-05-25T11:37:25.000+0000,,Fixed,New Feature,Major,['2.4.0'],['2.4.0'],,,,,,,['stacks'],['Ambari Stacks'],Propose to add Spark2 in Ambari as a standalone service in parallel with Spark1.,Add new Spark2 service definition to Ambari,3,,,jerryshao,True,jerryshao,jerryshao
ambari,AMBARI-16745,2016-05-18T19:02:14.000+0000,2016-05-19T19:15:12.000+0000,2016-07-05T22:25:10.000+0000,,Fixed,New Feature,Major,['2.4.0'],['2.4.0'],,,,,,,,,Recommend value for hawq_rm_memory_limit_perseg for HAWQ,Recommend value for hawq_rm_memory_limit_perseg for HAWQ,2,,,bhuvnesh2703,True,bhuvnesh2703,bhuvnesh2703
ambari,AMBARI-16153,2016-04-28T10:41:45.000+0000,2016-05-10T10:06:05.000+0000,2016-05-10T13:17:57.000+0000,,Fixed,New Feature,Critical,['2.4.0'],['2.4.0'],,,,,,,['ambari-server'],['Ambari Server'],"Add one more authentication provider in LogSearch using credential server of Ambari Server.
Ambari Authentication ref : https://github.com/apache/ambari/blob/trunk/ambari-server/docs/api/v1/index.md#authentication",Use Ambari Server as credential server,4,,['feature'],mdharmesh,True,mdharmesh,mdharmesh
ambari,AMBARI-16074,2016-04-22T21:04:05.000+0000,2016-04-26T04:43:12.000+0000,2016-04-26T14:20:35.000+0000,,Fixed,New Feature,Minor,['2.2-next'],"['trunk', '2.2.0']",,,,,,,['ambari-web'],['Ambari Web UI'],,Show number of PXFs Live widget on main dashboard,2,,,mithmatt,True,mithmatt,mithmatt
ambari,AMBARI-15933,2016-04-16T06:10:59.000+0000,2016-04-26T17:26:21.000+0000,2017-06-12T20:37:50.000+0000,,Fixed,New Feature,Critical,['2.4.0'],['2.4.0'],,,3024000,3024000,3024000,,['ambari-views'],['Ambari Views framework and Views themselves.  Specific Views that are built on the framework will be handled with labels.'],"Problem: Hue is planned to be deprecated, hence need a way for customers to migrate over artifacts from Hue to the equivalent Views.
Link to the PRD Doc https://docs.google.com/a/hortonworks.com/document/d/13m-Ioxamq4oF3yYHASkVjJ3mhpRlKuM12BCgCAJaap4/edit?usp=sharing
This RMP is a clone specifically to cover the work on the Views Framework side.",Views work for Hue to Views Migration Tool,3,,"['ambari-views', 'dev-test', 'scoping']",pradarttana@gmail.com,True,pradarttana,pradarttana@gmail.com
ambari,AMBARI-15875,2016-04-14T02:06:44.000+0000,,2016-04-14T02:06:44.000+0000,,,New Feature,Major,,,,,,,,,['ambari-server'],['Ambari Server'],"Instead of only do a curl to spark history server. At least, spark service check should run a cluster mode job such as MapReduce service check.",Ambari spark service check should submit at least a spark job ,1,,,airbots,True,,airbots
ambari,AMBARI-15752,2016-04-07T04:47:25.000+0000,2016-04-21T13:56:10.000+0000,2016-06-20T19:12:31.000+0000,,Fixed,New Feature,Critical,['2.4.0'],,,,,,,,['ambari-server'],['Ambari Server'],Add HSM support configuration properties for Ranger KMS.,Ambari support for additional config params for Ranger KMS to support HSM,3,,,mugdha.varadkar,True,mugdha.varadkar,mugdha.varadkar
ambari,AMBARI-15612,2016-03-29T07:58:51.000+0000,2016-06-02T17:41:32.000+0000,2016-06-06T00:14:06.000+0000,,Fixed,New Feature,Major,,,,,,,,,,,,Add Livy to HDP 2.5 as slave component of Spark,4,,,zjffdu,True,zjffdu,zjffdu
ambari,AMBARI-15554,2016-03-24T08:02:23.000+0000,2016-03-29T08:21:28.000+0000,2016-03-29T08:21:28.000+0000,,Fixed,New Feature,Major,['2.4.0'],['2.1.1'],,,,,,,"['ambari-server', 'ambari-web']","['Ambari Server', 'Ambari Web UI']","*Problem:*
In case LDAP set up with multiple Domains which are joined into a Forrest with trusts between the different Domains  users may appear in different locations in  LDAP.

Since users who wants to access Ambari can be in any domain Ambari has to search the whole forrest, and as the users appearing in multiple domains are identical Ambari cannot filter out all but one of the user entries.

This leads to the following error message when they try to login to Ambari with one of the users that has multiple entries:
{code}
ServletHandler:563 - /api/v1/users/USERNAME 
org.springframework.dao.IncorrectResultSizeDataAccessException: Incorrect result size: expected 1, actual 2 
at org.springframework.security.ldap.SpringSecurityLdapTemplate.searchForSingleEntryInternal(SpringSecurityLdapTemplate.java:243) 
at org.springframework.security.ldap.SpringSecurityLdapTemplate$3.executeWithContext(SpringSecurityLdapTemplate.java:198) 
at org.springframework.ldap.core.LdapTemplate.executeWithContext(LdapTemplate.java:807) 
at org.springframework.ldap.core.LdapTemplate.executeReadOnly(LdapTemplate.java:793) 
at org.springframework.security.ldap.SpringSecurityLdapTemplate.searchForSingleEntry(SpringSecurityLdapTemplate.java:196) 
at org.springframework.security.ldap.search.FilterBasedLdapUserSearch.searchForUser(FilterBasedLdapUserSearch.java:116) 
at org.springframework.security.ldap.authentication.BindAuthenticator.authenticate(BindAuthenticator.java:90) 
at org.apache.ambari.server.security.authorization.AmbariLdapBindAuthenticator.authenticate(AmbariLdapBindAuthenticator.java:53) 
at org.springframework.security.ldap.authentication.LdapAuthenticationProvider.doAuthentication(LdapAuthenticationProvider.java:178) 
at org.springframework.security.ldap.authentication.AbstractLdapAuthenticationProvider.authenticate(AbstractLdapAuthenticationProvider.java:61) 
at org.apache.ambari.server.security.authorization.AmbariLdapAuthenticationProvider.authenticate(AmbariLdapAuthenticationProvider.java:60) 
at org.springframework.security.authentication.ProviderManager.authenticate(ProviderManager.java:156) 
at org.springframework.security.authentication.ProviderManager.authenticate(ProviderManager.java:174) 
at org.springframework.security.web.authentication.www.BasicAuthenticationFilter.doFilter(BasicAuthenticationFilter.java:168) 
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342) 
at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:87) 
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342) 
at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:192) 
at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160) 
at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:237) 
at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:167) 
at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467) 
at org.apache.ambari.server.api.MethodOverrideFilter.doFilter(MethodOverrideFilter.java:72) 
at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467) 
at org.apache.ambari.server.api.AmbariPersistFilter.doFilter(AmbariPersistFilter.java:47) 
at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467) 
at org.eclipse.jetty.servlets.UserAgentFilter.doFilter(UserAgentFilter.java:82) 
at org.eclipse.jetty.servlets.GzipFilter.doFilter(GzipFilter.java:294) 
at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467) 
at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:501) 
at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137) 
at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557) 
at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231) 
at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086) 
at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:429) 
at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193) 
at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020) 
at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135) 
at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:209) 
at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:198) 
at org.apache.ambari.server.controller.AmbariHandlerList.handle(AmbariHandlerList.java:132) 
at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116) 
at org.eclipse.jetty.server.Server.handle(Server.java:370) 
at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494) 
at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:971) 
at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1033) 
at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644) 
at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235) 
at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82) 
at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:696) 
at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:53) 
at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608) 
at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543) 
at java.lang.Thread.run(Thread.java:745)
{code}

*Solution:*
If the LDAP search upon login to Ambari leads to multiple match user match due to the user appears in multiple domains show an error message to user prompting for providing domain as well to log-in. (e.g. _Login Failed: Please append your domain to your username and try again. Example: username@domain_)

When user provides domain information at login as well Ambari looks up the user in LDAP using different filter which is configurable. If this configuration is not set Ambari defaults to filter by _userPrincipalName_",Ambari LDAP integration cannot handle LDAP directories with multiple entries for the same user,2,,,stoader,True,stoader,stoader
ambari,AMBARI-15265,2016-03-02T16:38:54.000+0000,2016-04-25T18:53:06.000+0000,2017-06-12T20:39:36.000+0000,,Fixed,New Feature,Major,['2.4.0'],,,,,,,,,,"Ambari to support the following scenario for Zeppelin Service
* Install of Zeppelin
* Provide start/stop of Zeppelin component
* Configure Zeppelin with Ambari including Zeppelin LDAP Authentication
* Ambari to provide Quicklink from Ambari to Zeppelin
* Ambari to Kerberize a cluster with Zeppelin in it.
",Install & Manage Zeppelin with Ambari,3,,,rkamath,True,rkamath,rkamath
ambari,AMBARI-15241,2016-03-01T09:01:09.000+0000,2016-03-30T21:47:45.000+0000,2016-03-30T21:47:45.000+0000,,Fixed,New Feature,Critical,['2.4.0'],,,,,,,,['ambari-server'],['Ambari Server'],"Ambari should audit operational events (including user, timestamp, etc) such as: start, stop, restart, move, add/delete service, add/delete component, enable/disable kerberos, enter/leave maintenance mode, create/edit/enable/disable alerts. This should also include user/group role changes (including Ambari Admin flag).

This information should be available in an operational log.
When an operation is executed in Ambari, append an entry to a history log showing:
The timestamp is when the operation is started
The user is the logged in user
The operation is what is currently displayed in the operations UI
The success/fail is what is displayed in the UI when the operation is completed
Comment is an optional field the user can supply
",Basic Operational Audit Logging,2,,,stoader,True,dgergely,stoader
ambari,AMBARI-15133,2016-02-22T16:42:35.000+0000,2016-03-02T13:17:09.000+0000,2016-03-02T13:17:09.000+0000,,Fixed,New Feature,Major,['2.2.2'],['2.2.1'],,,604800,604800,604800,,['ambari-server'],['Ambari Server'],"Ambari operational data may significantly grow over time and as a result the database operations may degrade.

This feature is about allowing operators to purge related tables based on configurable cleaning policies. (As a first step only time based cleanup is to be supported)",Functionality to purge operational logs from the ambari database,4,,['features'],lpuskas,True,lpuskas,lpuskas
ambari,AMBARI-15105,2016-02-19T00:53:09.000+0000,2016-02-25T23:47:40.000+0000,2016-07-14T18:11:55.000+0000,,Fixed,New Feature,Major,['2.2.0'],"['trunk', '2.2.0']",,,,,,,['alerts'],['Ambari Alerts System'],"Add alerts for HAWQ components based on PORT. The below components are covered.
- Master
- Standby
- Segment",Add alerts for HAWQ components status,3,,,bhuvnesh2703,True,bhuvnesh2703,bhuvnesh2703
atlas,ATLAS-3324,2019-07-10T12:09:00.000+0000,,2019-07-16T09:53:27.000+0000,,,New Feature,Major,,,,,,,,,,,"Right now Incremental export is supported if it is for hive_db.
 This patch will enable Incremental export with hive_table.
 This will export hive_table & its connected entities provided qualifies changeMarker.",Export API - incremental export with hive_table,1,,,nikhilbonte,True,nikhilbonte,nikhilbonte
atlas,ATLAS-3290,2019-06-18T20:32:31.000+0000,2019-06-19T07:11:13.000+0000,2019-06-26T03:24:47.000+0000,,Fixed,New Feature,Major,,['2.1.0'],,,,,,,[' atlas-core'],['Atlas Core'],"The column name in Impala lineage record may not contain its database name and its table name. 

To get its  its database name and its table name, we should use the metadata in a vertex, not assuming column name contains its database name and its table name. 

When assuming that column name always contains its database name and its table name, we run into the following exception

{code}
I0618 19:16:02.415920 209817 QueryEventHookManager.java:212] Initiating onQueryComplete: org.apache.atlas.impala.hook.ImpalaLineageHook
E0618 19:16:02.418964 210738 ImpalaLineageHook.java:126] ImpalaLineageHook.process(): failed to process query create table sales_sg as select * from sales_asia
Java exception follows:
java.lang.IllegalArgumentException: fullColumnName {} does not contain database name or table name
        at org.apache.atlas.impala.hook.AtlasImpalaHookContext.getQualifiedNameForColumn(AtlasImpalaHookContext.java:115)
        at org.apache.atlas.impala.hook.events.BaseImpalaEvent.getQualifiedName(BaseImpalaEvent.java:164)
        at org.apache.atlas.impala.hook.events.BaseImpalaEvent.getQualifiedName(BaseImpalaEvent.java:134)
        at org.apache.atlas.impala.hook.events.BaseImpalaEvent.getColumnEntities(BaseImpalaEvent.java:495)
        at org.apache.atlas.impala.hook.events.BaseImpalaEvent.toTableEntity(BaseImpalaEvent.java:430)
        at org.apache.atlas.impala.hook.events.BaseImpalaEvent.toTableEntity(BaseImpalaEvent.java:393)
        at org.apache.atlas.impala.hook.events.BaseImpalaEvent.toAtlasEntity(BaseImpalaEvent.java:315)
        at org.apache.atlas.impala.hook.events.BaseImpalaEvent.getInputOutputEntity(BaseImpalaEvent.java:297)
        at org.apache.atlas.impala.hook.events.CreateImpalaProcess.getEntities(CreateImpalaProcess.java:103)
        at org.apache.atlas.impala.hook.events.CreateImpalaProcess.getNotificationMessages(CreateImpalaProcess.java:54)
        at org.apache.atlas.impala.hook.ImpalaLineageHook.process(ImpalaLineageHook.java:122)
        at org.apache.atlas.impala.hook.ImpalaLineageHook.process(ImpalaLineageHook.java:79)
        at org.apache.atlas.impala.hook.ImpalaHook.onQueryComplete(ImpalaHook.java:36)
        at org.apache.atlas.impala.hook.ImpalaLineageHook.onQueryComplete(ImpalaLineageHook.java:52)
        at org.apache.impala.hooks.QueryEventHookManager.lambda$null$1(QueryEventHookManager.java:215)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
{code}

The lineage record from Impala is
{code}
{  
   ""queryText"":""create table sales_china as select * from sales_asia"",
   ""queryId"":""2940d0b242de53ea:e82ba8d300000000"",
   ""hash"":""a705a9ec851a5440afca0dfb8df86cd5"",
   ""user"":""root"",
   ""timestamp"":1560885032,
   ""endTime"":1560885040,
   ""edges"":[  
      {  
         ""sources"":[  
            1
         ],
         ""targets"":[  
            0
         ],
         ""edgeType"":""PROJECTION""
      },
      {  
         ""sources"":[  
            3
         ],
         ""targets"":[  
            2
         ],
         ""edgeType"":""PROJECTION""
      }
   ],
   ""vertices"":[  
      {  
         ""id"":0,
         ""vertexType"":""COLUMN"",
         ""vertexId"":""id"",
         ""metadata"":{  
            ""tableName"":""sales_db.sales_china"",
            ""tableCreateTime"":1560885039
         }
      },
      {  
         ""id"":1,
         ""vertexType"":""COLUMN"",
         ""vertexId"":""sales_db.sales_asia.id"",
         ""metadata"":{  
            ""tableName"":""sales_db.sales_asia"",
            ""tableCreateTime"":1560884919
         }
      },
      {  
         ""id"":2,
         ""vertexType"":""COLUMN"",
         ""vertexId"":""name"",
         ""metadata"":{  
            ""tableName"":""sales_db.sales_china"",
            ""tableCreateTime"":1560885039
         }
      },
      {  
         ""id"":3,
         ""vertexType"":""COLUMN"",
         ""vertexId"":""sales_db.sales_asia.name"",
         ""metadata"":{  
            ""tableName"":""sales_db.sales_asia"",
            ""tableCreateTime"":1560884919
         }
      }
   ]
}

{code}
 ",Impala Hook should get database name and table name from vertex metadata,2,,,LinaAtAustin,True,LinaAtAustin,LinaAtAustin
atlas,ATLAS-3284,2019-06-17T23:09:36.000+0000,,2019-06-17T23:09:58.000+0000,,,New Feature,Major,,,,,,,5097600,,,,This task aims at computing dynamic attributes for an entity. The first one we are going to take up in qualifiedName,Adding functionality to compute dynamic attributes for an entity.,1,,,django,True,merrylewang,django
atlas,ATLAS-3257,2019-05-31T22:44:49.000+0000,,2019-06-27T22:50:06.000+0000,,,New Feature,Major,,,,,,,,,,,"As of now we can search by classification with full name only, atlas should provide wildcard character support.",Enable atlas search classification with wildcard character *,1,,,weixia,True,lma,weixia
atlas,ATLAS-3236,2019-05-25T03:08:18.000+0000,,2019-05-27T08:08:41.000+0000,,,New Feature,Major,,['trunk'],,,,,,,[' atlas-core'],['Atlas Core'],"Enhance Atlas DSL grammar to support relationship traversal.

 

Examples:

from hive_db as db where db.name=""hive_db_g_test_2"" hasr hive_table as tb where tb.name=""hive_table_g_test_2"" hasr hive_column as c where  c.name=""hive_column_g_test_2"" and c is realm

 

query should be transformed into gremlin query as below

g.V().has('__typeName', 'hive_db').as('db').has('Asset.name', eq(""hive_db_g_test_2"")).bothE().bothV().has('__typeName','hive_table').as('tb').has('Asset.name', eq(""hive_table_g_test_2"")).bothE().bothV().has('__typeName','hive_column').as('c').and(__.has('Asset.name', eq(""hive_column_g_test_2"")),__.outE('classifiedAs').has('__name', within('realm')).outV()).dedup().limit(25).toList(

 ",Relationship traversal through DSL search queries,1,,,kirankumardg,True,kirankumardg,kirankumardg
atlas,ATLAS-3230,2019-05-24T01:35:08.000+0000,2019-05-24T21:09:16.000+0000,2019-05-24T21:09:16.000+0000,,Fixed,New Feature,Major,,,,,,,,,[' atlas-core'],['Atlas Core'],"Impala has made the following changes:

1) ""createTime"" to ""tableCreateTime""
2) ""PostQueryHookContext"" to ""QueryCompleteContext""
3) ""QueryExecHook"" to ""QueryEventHook""

Atlas needs to make corresponding changes to keep end-to-end work.",Change Impala Hook API Names,2,,,LinaAtAustin,True,LinaAtAustin,LinaAtAustin
atlas,ATLAS-3226,2019-05-22T02:52:14.000+0000,2019-06-03T20:20:51.000+0000,2019-06-11T18:09:10.000+0000,,Fixed,New Feature,Major,,,,,,,,,[' atlas-core'],['Atlas Core'],"ATLAS-3197 introduced a new feature on Hive integration. It introduced new model called Hive_table_ddl and hive_db_ddl which are used to track commands that are executed either on a table entity or bd entity. DDL entities will be added to a table/db entity through relationship Attribute call hive_db_ddl_queries and hive_table_ddl_queries. Every time when a new command is executed, a new ddl entity will be created, it will append to the existing ddlQueries field if a table/db has already been created. Once a table/db is deleted, all these relationship attributes will be deleted as well.

We need to support this new feature on Impala integration.",Add QueryText for hive_table and hive_db for Impala integration,2,,,LinaAtAustin,True,LinaAtAustin,LinaAtAustin
atlas,ATLAS-3214,2019-05-16T21:36:40.000+0000,2019-05-20T17:17:16.000+0000,2019-05-20T17:24:04.000+0000,,Fixed,New Feature,Major,"['2.0.0', 'trunk']",,,,,,,,,,,Create Spark Models in Atlas to support Spark Lineage,2,,,django,True,django,django
atlas,ATLAS-3207,2019-05-10T20:28:19.000+0000,,2019-05-10T20:28:19.000+0000,,,New Feature,Major,,,,,,,,,,,"It would be nice to have the automation that could run pre-commit job once a patch is submitted to Jira. By adding this, it could be easier for contributors to validate their patch and once is this part of committing process, we could commit more bug-free code. I've heard that Hive use this approach for commit.

Thankd

 ",Run pre-commit job once a patch is submitted on jira,1,,,lma,True,,lma
atlas,ATLAS-3197,2019-05-07T16:59:57.000+0000,2019-05-09T01:23:09.000+0000,2019-05-13T19:01:55.000+0000,,Fixed,New Feature,Major,['trunk'],,,,,,,,,,"This patch introduced a model called Hive_table_ddl and hive_db_ddl which are used to track commands that are executed either on a table entity or bd entity. DDL entities will be added to a table/db entity through relationshipAttribute call HIVE_DB_TO_DDL and hive_tb_to_ddl. Every time when a new command is executed, a new ddl entity will be created, it will append to the existing ddlQueries field if a table/db has already been created. Once a table/db is deleted, all these relationship attributes will be deleted as well.",Add QueryText for hive_table and hive_db,3,,,lma,True,lma,lma
atlas,ATLAS-3185,2019-05-02T00:04:58.000+0000,2019-05-29T19:48:09.000+0000,2019-05-29T19:48:09.000+0000,,Fixed,New Feature,Major,,['1.1.0'],,,,,,,[' atlas-core'],['Atlas Core'],[ATLAS-3133|https://issues.apache.org/jira/browse/ATLAS-3133] adds a new feature to track metadata for different executions of the same process in Atlas. Need to add this in Impala integration,Create process execution for Impala integration,2,,,LinaAtAustin,True,LinaAtAustin,LinaAtAustin
atlas,ATLAS-3184,2019-05-02T00:00:37.000+0000,2019-05-20T22:09:09.000+0000,2019-05-20T22:09:09.000+0000,,Fixed,New Feature,Major,,['1.1.0'],,,,,,,[' atlas-core'],['Atlas Core'],"[ATLAS-3183|https://issues.apache.org/jira/browse/ATLAS-3183] creates the Impala integration with Atlas. It only supports ""create view"" command.

We need to add support for all other Impala commands that generate lineage records.",Add support of ineage integration for more Impala commands,2,,,LinaAtAustin,True,LinaAtAustin,LinaAtAustin
atlas,ATLAS-3183,2019-05-01T20:07:59.000+0000,2019-05-09T01:39:23.000+0000,2019-05-09T06:25:56.000+0000,,Fixed,New Feature,Major,,['1.1.0'],,,,,,,[' atlas-core'],['Atlas Core'],"Impala generates lineage records for its commands. This new feature will read Impala lineage file, convert the lineage record to Atlas entities and send them to Atlas. In this way, Atlas can get lineage of Impala operation.

The metadata referred in the lineage are captured in Hive Metastore hook and sent to Atlas. This work is done in [ATLAS-3148|https://issues.apache.org/jira/browse/ATLAS-3148]

This jira only supports the Impala command ""create view"". Following jira will add support for more Impala commands.",Read Impala lineage record for creating view and send to Atlas,3,,,LinaAtAustin,True,LinaAtAustin,LinaAtAustin
atlas,ATLAS-3148,2019-04-17T23:17:52.000+0000,2019-04-17T23:50:20.000+0000,2019-05-24T18:02:56.000+0000,,Fixed,New Feature,Major,"['2.0.0', 'trunk']",['1.1.0'],,,,,,,[' atlas-core'],['Atlas Core'],"Atlas has hive hook ([http://atlas.apache.org/Hook-Hive.html]) which registers with HiveServer2 process to listen for create/update/delete operations and updates the metadata in Atlas.

If hive metastore is accessed using other clients like Impala shell, Hue or other JDBC/ODBC client apps there is no way to capture the events in Atlas.

This Jira will create a new atlas hook for Hive Metastore - stores the metadata for Hive tables and partitions in a relational database, and provides clients (including Hive) access to this information using the metastore service API. 

This hook is registered as post-listener (*hive.metastore.event.listeners*) and DDL operations are captured and send to atlas kafka topic for processing by Atlas server.

The following DDL operations are captured:
 * *CreateDatabaseEvent*
 * *DropDatabaseEvent*
 * *AlterDatabaseEvent*
 * *CreateTableEvent*
 * *DropTableEvent*
 * *AlterTableEvent*

 ",Implement Hive Metastore hook for Atlas,2,,,sarath.kum4r@gmail.com,True,sarath.kum4r@gmail.com,sarath.kum4r@gmail.com
atlas,ATLAS-3133,2019-04-12T00:34:59.000+0000,2019-04-24T23:26:55.000+0000,2019-07-12T01:33:15.000+0000,,Fixed,New Feature,Critical,,,,,,,,,"[' atlas-core', 'atlas-webui']","['Atlas Core', 'Atlas UI']","Background: The current process metadata model within Atlas does not track multiple instances of execution of the same process. For example if we run same DDL (e.g. insert into table A select * from table B, C) multiple times Atlas does not capture the multiple instances of execution of the same logical process. 

User story: As a compliance admin or data steward, I need to be able to track multiple executions of the same process or pipeline that were done at different times and relate them to the logical process so that I can understand traceability and how different instances of my data pipelining operations performed over time.

Acceptance Criteria:

1) Every new instance of process execution is captured with the appropriate metadata for the process along with context (who, what, when)

2) One can navigate between process and process execution in Atlas UI and explore relevant metadata of each entity

3) Process nodes in lineage view display high level information about number of process executions associated with a particular process node.

 

 

 

 ",Capture metadata for different executions of the same process in Atlas,2,,,srikvenk,True,django,srikvenk
atlas,ATLAS-3110,2019-04-01T06:52:41.000+0000,2019-04-11T08:11:22.000+0000,2019-04-19T18:09:21.000+0000,,Fixed,New Feature,Minor,"['2.0.0', 'trunk']",,,,,,,,,,"Add a bulk api to get entities by unique attributes,currently there is bulk api for guids only",Add a bulk api to get entities by unique attributes,2,,,ayushmnnit,True,ayushmnnit,ayushmnnit
atlas,ATLAS-3081,2019-03-19T15:48:21.000+0000,,2019-04-25T19:55:58.000+0000,,,New Feature,Minor,,,,,,,,,,,Expose Gremlin Search API to solve more complex usecases.,Expose Gremlin Search API,4,,,ayushmnnit,True,ayushmnnit,ayushmnnit
atlas,ATLAS-3071,2019-03-11T20:39:14.000+0000,2019-04-12T00:50:27.000+0000,2019-06-19T10:50:06.000+0000,,Fixed,New Feature,Major,['2.0.0'],,8400,8400,,,,100,,,"Extend api/atlas/admin/metrics to provide metrics for notifications/entity lifycycle:                           

                                                         # notification processed 

                                                         # notification failed 

today/thisHour/pastHour/total   {      # entity created 

                                                         # entity updated

                                                         # entity deleted",Add Functionalities to Collect Notification Metrics/Entity Lifecyle,3,,,lma,True,lma,lma
atlas,ATLAS-3057,2019-02-22T20:33:11.000+0000,2019-03-13T08:52:45.000+0000,2019-04-19T18:20:27.000+0000,,Fixed,New Feature,Major,"['2.0.0', 'trunk']",,,,,,,,,,"*Background*

For Atlas version that uses _HBase_ and _Solr_, occasionally if Solr is down and entities get created, then there is no record of the created entities within Solr. Basic search does not indicate presence of such entities.

*Solution*

Index Repair Tool (which was present in branch-0.8) should be implemented for JanusGraph.

*Implementation Guidance*
 * Create Java-based implementation rather than Groovy script that needs graphdb-specific shell to be installed. 
 * Use JanusGraph APIs for restore instead of custom per-vertex logic.
 * Investigate possibility of using MapReduce for higher throughput.",Index Repair Tool: Add JanusGraph-Specific Index Repair Tool,3,,,ashutoshm,True,nikhilbonte,ashutoshm
atlas,ATLAS-3047,2019-02-07T18:48:15.000+0000,,2019-02-20T18:55:32.000+0000,,,New Feature,Major,,,,,,,,,,,"As part of our use case we have developed Avro Parser that can parse fully nested avro schemas and persist them to atlas.

Also we have added capability to add field classifications and have done enhancements on already existing avro modle to accomodate new types such as avro_map,avro_union,logical types etc",Ability to parse and persist avro schema in Atlas,2,,['AVRO'],ayushmnnit,True,ayushmnnit,ayushmnnit
atlas,ATLAS-2973,2018-11-19T10:47:27.000+0000,,2018-11-19T10:47:27.000+0000,,,New Feature,Minor,,,,,,,,,['atlas-webui'],['Atlas UI'],"Similar to setting Tags to all objects in the list , it should be possible to set all to all objects a given attribute. As a precondition,the list may only contain objects of a single type

Example: Set the ""owner"" attribute of all hive_table objects that match a specific search function",UI - Attribute mass update,1,,,carlosdiogo,True,,carlosdiogo
atlas,ATLAS-2972,2018-11-19T10:32:49.000+0000,,2018-11-19T10:32:49.000+0000,,,New Feature,Major,,,,,,,,,['atlas-webui'],['Atlas UI'],"Lineage and processes are an important part of the structure . As such it makes sense to be able to invoke them in DSL to find objects. Some use cases:
 * return all ""asset"" which are not inputToProcess / ouputFromProcess
 * return all asset which are part of the lineage/impact of an asset be n depth - this is very important as navigating the lineage graph can be impossible in complex scenarios

 ",Allow lineage and processes to be used in DSL,1,,,carlosdiogo,True,,carlosdiogo
atlas,ATLAS-2915,2018-10-08T15:33:14.000+0000,,2018-10-08T16:11:34.000+0000,,,New Feature,Major,,,,,,,,,,,"Currently the base types in Atlas do not include AWS Kinesis Stream objects. It would be nice to add a typedef for a kinesis stream, inheriting from DataSet.  Attributes would include:
 * streamType string, eg """"Single Region Stream"".
 * awsRegion string: the AWS region in which the kinesis stream endpoint is deployed
 * shardCount int:  number of shards (uniquely identified sequence of data records) in the stream
 * streamEnvironment enum.  Valid values are ""unknown"", ""production"", ""staging"", ""QA"" and ""development""
 * containsPII boolean: does this stream's data contain Personally Identifiable Information?
 * aggregationFormat enum. Indicates if/how the records are aggregated within a single kinesis record. Valid values are ""none"" or ""kpl"".
 * contentType enum: serialization format used by the producer of the stream.  Valid values are ""unknown"", ""avro"", ""bson"", ""csv"", ""json"", ""key-value"", ""kryo"", ""protobuf"", ""raw"" [ie no consistent schema], ""sdp"" [confluent-style avro with envelope that specifies schema id surrounding the payload], ""thrift"", ""tlv"", ""xml"", ""other"".
 * schemaURL string: A URL to the data schema used by the producer, to facilitate consumption.
 * avroSchemas: array of avro schema objects (see ATLAS-2694) associated with the kinesis stream.

 ",AWS Kinesis Stream Typedef for Atlas,1,,,barbara,True,barbara,barbara
atlas,ATLAS-2896,2018-09-26T20:39:36.000+0000,,2019-06-27T13:57:18.000+0000,,,New Feature,Major,,,,,,,,,[' atlas-core'],['Atlas Core'],"Enhance V2 Java client to add glossary terms,categories",Enhance Atlas Client V2 java api to add glossary terms,1,,['patch'],ayushmnnit,True,ayushmnnit,ayushmnnit
atlas,ATLAS-2889,2018-09-25T00:00:50.000+0000,,2018-09-26T05:47:16.000+0000,,,New Feature,Major,,['1.1.0'],,,,,,,"[' atlas-core', 'atlas-intg']","['Atlas Core', 'Atlas Integration Components']","https://issues.apache.org/jira/browse/ATLAS-2708 introduced support for s3 type.

 

From the comments in the jira, there are some external steps needed to import object tags. This Jira is about creating the E2E functionality within Atlas. ie functionality in UI or an API that when supplied with argument of an s3 bucket/slice then Atlas automatically imports all the object names/tags recursively from under that s3 bucket/slice.

 

""It doesn't do it automatically through a listener like the hive hook.  We do it via lambda functions, triggered, say, on the creation of S3 object or pseudodirectory or bucket.  We package up the info into AtlasEntities and then publish to the ATLAS_HOOK kafka topic.

You have to create your own Lambda code that creates AtlasEntities on the fly as trigerred by Lambda Function(on changes made to s3 object) and then push to Kafka Queue. This particular functionality is not part of Atlas tool as of now.""

cc: [~barbara] [~ayushmnnit]",S3 object tag import hook,2,,,toopt4,True,,toopt4
atlas,ATLAS-2881,2018-09-19T13:18:24.000+0000,,2019-06-27T13:58:09.000+0000,,,New Feature,Major,,,,,,,,,,,"Cassandra metadata ingestion to Atlas

 

Ability to import keyspace metadata from  Cassandra into Atlas",Cassandra metadata ingestion to Atlas,3,1,,toopt4,True,,toopt4
atlas,ATLAS-2822,2018-08-14T22:18:24.000+0000,2018-08-31T12:45:43.000+0000,2018-08-31T12:46:58.000+0000,,Fixed,New Feature,Major,"['1.0.0', '2.0.0']","['1.0.0', '1.1.0']",,,,,,,[' atlas-core'],['Atlas Core'],"By default, when an entity is deleted - its associated classifications which have been propagated to downstream entities will be retained.

This JIRA provides a boolean option when adding a new classification to an entity - *""Remove Propagations on Entity Delete""*

When this flag is set to :

*TRUE* - Propagated classifications are removed during entity delete

*FALSE* - Propagated classifications are retained during entity delete",Provide option whether to delete propagated classification on entity delete during add classification,2,,,sarath.kum4r@gmail.com,True,sarath.kum4r@gmail.com,sarath.kum4r@gmail.com
atlas,ATLAS-2790,2018-07-17T16:51:35.000+0000,,2018-07-17T16:51:35.000+0000,,,New Feature,Critical,,,,,,,,,[' atlas-core'],['Atlas Core'],"It would be nice if a search on a complex entity could perform a search on attributes of nested sub-entities or structs, as well as just top-level attributes.  

Here is an example of an Atlas basic search request object:
{
 ""typeName"": ""hive_table"",
 ""excludeDeletedEntities"": true,
 ""classification"" : """",
 ""query"": """",
 ""limit"": 25,
 ""offset"": 0,
 ""entityFilters"": {
 ""attributeName"": ""name"",
 ""operator"": ""contains"",
 ""attributeValue"": ""testtable""
 },
 ""tagFilters"": null,
 ""attributes"": [""""]
}

Here, attributeName must be one of the top-level attributes of the hive table entity.

This ticket requests that path expressions to attributes of sub-entities or structs also be allowed as attributeNames, eg column.name  (similar to DSL search).   This would execute a search on the name attribute of columns that are associated with the hive table.",Allow path expressions as attributeNames in Atlas basic search entityFilters,1,,,barbara,True,,barbara
atlas,ATLAS-2789,2018-07-17T13:50:30.000+0000,,2018-07-19T10:28:58.000+0000,,,New Feature,Major,,['0.8.2'],,,,,,,"[' atlas-core', 'atlas-intg']","['Atlas Core', 'Atlas Integration Components']","Feature Request to add Prometheus /metrics http endpoint for monitoring integration:

[https://prometheus.io/docs/prometheus/latest/configuration/configuration/#%3Cscrape_config%3E]

Prometheus metrics format for that endpoint:

[https://github.com/prometheus/docs/blob/master/content/docs/instrumenting/exposition_formats.md]

 ",Prometheus /metrics http endpoint for monitoring integration,1,,,harisekhon,True,,harisekhon
atlas,ATLAS-2774,2018-07-03T15:37:24.000+0000,2018-11-15T08:00:45.000+0000,2018-11-15T08:00:45.000+0000,,Fixed,New Feature,Major,"['0.8.4', '1.2.0', '2.0.0']",,,,,,,,,,"*Overview*

For Entities and Relationships, the following delete behaviors are desired.

Atlas can be configured to offer hard delete (only). Alternatively it can be configured to offer soft-delete. Configuration is achieved by setting the delete handler configuration property (see atlas-application.properties) to either the SoftDeleteHandlerV1 or the HardDeleteHandlerV1. The default (for when the property is not set) is to use the SoftDeleteHandlerV1.

The AtlasEntityStoreV2 deleteById() and deleteByIds() and methods and the AtlasRelationshipStoreV2 deleteById() method are sensitive to the above configuration. When the configuration is soft these methods will perform a soft delete; when it is hard they will perform a hard delete.

In addition to the above methods, the AtlasEntityStoreV2 and AtlasRelationshipStoreV2 offer a (new) purgeById() method that ALWAYS performs a hard delete. This is true regardless of which delete handler has been configured. When the configuration is hard, the purgeById() methods and deleteById() methods are essentially equivalent. When the configuration is soft, the purgeById() methods continue to provide a hard delete capability.

The AtlasEntityStoreV2 deleteById(), deleteByIds() and purgeById() methods will delete the specified entities and any relationships to which they are connected. The AtlasRelationshipStoreV2 deleteById() and purgeById() methods will delete the specified relationship. Deletion of a relationship may cause an upate to an entity to which the relationhsip is connected, if it changes the propagation of classifications, for example.

In a hard delete or purge operation, an affected entity and relationship will be removed from the graph and will not be returned in response to any future queries. This is true regardless of whether the affected entity or relationship has already been soft deleted or not. i.e. it's status could initially be ACTIVE or DELETED. Following the operation it will not exist.

In a soft delete, the affected entities and relationships (that initially have status ACTIVE) are updated to set the status to DELETED. These instances can still be returned from queries and searches if the appropriate control is selected (to include deleted instances). Without that control selected they will not be returned.

*Use Case for soft delete*

Soft delete provides a 'safe' means of deleting instances from the repository whilst preserving an audit trail and (if supported by the repository) enabling a deleted instance to be restored (to ACTIVE status).

*Use Cases for hard delete*

One use case for hard delete is to permanently remove soft-deleted items some period of time after they were soft deleted.
 Another use case for hard delete is to is to correct (clean up after) a mistake in which a potentially large batch of incomplete/corrupted/wrong metadata is loaded and needs to be fully removed.

*Use Scenario for an OMAS user*

An OMAS provides a delete function that does NOT offer the user a choice of hard/soft. The OMAS's delete function will invoke the relevant soft delete method provided by the OMRS - deleteEntity or deleteRelationship. These are both soft delete methods.

The Atlas OMRS Connector's deleteEntity/deleteRelationship methods will check whether Atlas is configured for hard or soft delete. If Atlas is configured for hard delete then soft-delete is not possible, and the method will throw a FunctionNotSupported exception which is caught by the OMAS.

On catching this exception the OMAS author should automatically escalate the soft delete to a hard delete by calling either of the mandatory OMRS purgeEntity() or purgeRelationship() methods. In the Atlas OMRS Connector these methods will invoke AtlasEntityStoreV2 purgeById() or AtlasRelationshipStoreV2 purgeById(). The effect of this is that if soft delete is not possible, a hard delete is used instead.

An OMAS does NOT provide a purge capability to a non-privileged user, but a suitably expert OMRS or Atlas repository administrator can explicitly issue a purge call at the OMRS or Atlas interface, by calling purgeEntity() or purgeRelationship() (or the relevant Atlas store's purgeById() method).

*Use Scenario for an Administrator*

As highlighted above, a suitably expert repository Administrator can invoke the purgeEntity(), purgeRelationship() methods directly. They may need to do this to permanently remove instances following an earlier soft-delete and the timeout of a period of grace, or following a failed import or batch update.

In addition to the above direct use by a repository administrator of the 'purge' methods, an Administrator could alternatively use the deleteEntity(), deleteRelationship() methods to perform soft deletes. The invoked method will behave as described above for an OMAS - performing a soft-delete if possible and throwing a FunctionNotSupported exception otherwise. In the latter case (no support for soft-delete), the Administrator COULD then choose to issue a purgeEntity(), purgeRelationship() call.

*Restore of an entity or relationship*

If an instance has been deleted using a soft-delete, the object still exists but has a status of 'DELETED'. Whilst in the DELETED state, the object can be:
 * optionally included/excluded from search results
 * restored using the OMRS method for restoreEntity or restoreRelationship.
 * purged from the repository using the purgeEntity() or purgeRelationship() method (or the underlying Atlas store purgeById() method).

The use case for restoring a soft-deleted entity or relationship is to correct an accidental deletion, or possibly a deliberately deletion that was performed for impact analysis.

To restore an entity or relationship that has been soft-deleted, an OMAS user or repository administrator can use the restoreEntity() or restoreRealtionship() method of the OMRS connector.

In the case of the Atlas OMRS Connector, the restoreEntity() and restoreRealtionship() methods will invoke (new) Atlas store methods to update the affected entities and relationships, leaving them in ACTIVE state and with classifications that reflect the propagation options between all the resulting active entities and relationships.

*Within the Atlas Repository*

The AtlasEntityStoreV2 and AtlasRelationshipStoreV2 provide additional methods for purgeById() that perform a hard delete, regardless of the choice of configured delete handler.

The stores also provide additional methods to enable restore of an entity or relationship that has been soft-deleted.

*Within the Atlas OMRS Connector*

The Atlas OMRS Connector will interrogate the Atlas repository configuration to determine whether it is configuraed for soft or hard deletes. The Atlas OMRS Connector implementations of deleteEntity() and deleteRelationship() methods will therefore have prior knowledge as to whether it is feasible to attempt to perform a soft-delete. If feasible, the soft-delete will be issued to the relevant Atlas store. If not feasible, the methods will throw a FunctionNotSupported exception.

Also within the Atlas OMRS Connector, the mandatory OMRS purgeEntity(), purgeRelationship() methods will invoke the Atlas stores' purgeById() methods to perform permanent (hard) removal of an entity or relationship from the repository. As outline above, these methods may be called either by an OMAS that has attempted a soft-delete and been 'bounced' or called directly by an Adminstrator.

 ",Options for hard and soft delete of instances,5,,,grahamwallis,True,ashutoshm,grahamwallis
atlas,ATLAS-2724,2018-05-29T15:39:11.000+0000,2018-08-27T08:56:03.000+0000,2018-08-31T14:09:59.000+0000,,Fixed,New Feature,Critical,"['0.8.3', '1.1.0', '2.0.0']",['0.8-incubating'],,,,,,,,,"Currently JSON-valued attributes are fully displayed in-line with other attributes, not pretty-printed, cluttering the display.  To support a better display, we can display JSON-valued attributes in a one-line box that can be scrolled down, or fully expanded with a mouse click that pretty-prints the JSON. ",UI enhancement for Avro schemas and other JSON-valued attributes,5,,,barbara,True,kevalbhatt,barbara
atlas,ATLAS-2718,2018-05-24T14:49:47.000+0000,2018-05-24T21:02:02.000+0000,2018-05-24T21:02:02.000+0000,,Fixed,New Feature,Major,['1.0.0'],,,,,,,,,,,Documentation for Atlas Ranger authorization.,1,,,nixonrodrigues,True,nixonrodrigues,nixonrodrigues
atlas,ATLAS-2709,2018-05-21T20:32:08.000+0000,2018-05-26T06:25:34.000+0000,2018-05-29T14:31:38.000+0000,,Fixed,New Feature,Critical,['1.0.0'],,,,,,,,[' atlas-core'],['Atlas Core'],"Currently the base types in Atlas do not include RDMBS objects. It would be nice to add generic typedefs for the basic objects found in virtually any RDBMS.  From this, one can subclass types specific to Oracle, MS SQL Server, etc if desired.  For example:
 
 * rdbms_instance represents the host platform that the database is running on. It supports:
 ** rdbms_type (eg Oracle, mysql) 
 ** hostname
 ** port
 ** protocol
 ** platform
 ** contact_info for the instance owner
 ** array of databases (schemas) associated with the instance

 
 * rdbms_db represents a database (schema) running on an rdbms_instance. It supports:
 ** inverse reference to the rdbms_instance
 ** contact_info for the database owner
 ** prodOrOther: a self-documenting attribute name representing whether the database is production, development, staging, etc
 ** array of tables in the database

 
 * rdbms_table represents a table in a database (schema). It supports:
 ** inverse reference to the rdbms_db
 ** time of creation
 ** comment
 ** type (e.g., table or view)
 ** contact_info for the table owner
 ** array of columns in the table
 ** array of indexes on the table
 ** array of foreign keys defined on the table

 
 * rdbms_column represents a column in a table. It supports:
 ** data_type of the column
 ** length
 ** default_value
 ** comment
 ** inverse reference to the rdbms_table
 ** isNullable boolean
 ** isPrimaryKey boolean

 * rdbms_index represents an index on a set of columns in a table. It supports:
 ** inverse reference to the rdbms_table
 ** index_type (e.g., ""NORMAL"", ""BITMAP"", ""DOMAIN"")
 ** isUnique boolean
 ** ordered list of columns in the index
 ** comment

 
 * rdbms_foreign_key represents a foreign key relationship between columns in source and referenced tables.  It supports:
 ** inverse reference to the source table
 ** key_columns: ordered list of columns in the source table
 ** references_table: table that the foreign key references
 ** references_columns: ordered list of columns in the referenced table
 ** comment

 ",RDBMS typedefs for Atlas,2,,,barbara,True,barbara,barbara
atlas,ATLAS-2708,2018-05-21T20:04:49.000+0000,2018-06-19T00:38:14.000+0000,2018-09-25T00:02:56.000+0000,,Fixed,New Feature,Critical,"['1.1.0', '2.0.0']",,,,,,,,[' atlas-core'],['Atlas Core'],"Currently the base types in Atlas do not include AWS data lake objects. It would be nice to add typedefs for AWS data lake objects (buckets and pseudo-directories) and lineage processes that move the data from another source (e.g., kafka topic) to the data lake.  For example:
 * AWSS3PseudoDir type represents the pseudo-directory “prefix” of objects in an S3 bucket.  For example, in the case of an object with key “myWork/Development/Projects1.xls”, “myWork/Development” is the pseudo-directory.  It supports:
 ** Array of avro schemas that are associated with the data in the pseudo-directory (based on Avro schema extensions outlined in ATLAS-2694)
 ** what type of data it contains, e.g., avro, json, unstructured
 ** time of creation

 * AWSS3BucketLifeCycleRule type represents a rule specifying a transition of the data in a bucket to a storageClass after a specific time interval, or expiration.  For example, transition to GLACIER after 60 days, or expire (i.e. be deleted) after 90 days:
 ** ruleType (e.g., transition or expiration)
 ** time interval in days before rule is executed  
 ** storageClass to which the data is transitioned (null if ruleType is expiration)

 * AWSTag type represents a tag-value pair created by the user and associated with an AWS object.
 **  tag
 ** value

 * AWSCloudWatchMetric type represents a storage or request metric that is monitored by AWS CloudWatch and can be configured for a bucket
 ** metricName, for example, “AllRequests”, “GetRequests”, TotalRequestLatency, BucketSizeBytes
 ** scope: null if entire bucket; otherwise, the prefixes/tags that filter or limit the monitoring of the metric.

 * AWSS3Bucket type represents a bucket in an S3 instance.  It supports:
 ** Array of AWSS3PseudoDirectories that are associated with objects stored in the bucket 
 ** AWS region
 ** IsEncrypted (boolean) 
 ** encryptionType, e.g., AES-256
 ** S3AccessPolicy, a JSON object expressing access policies, eg GetObject, PutObject
 ** time of creation
 ** Array of AWSS3BucketLifeCycleRules that are associated with the bucket 
 ** Array of AWSS3CloudWatchMetrics that are associated with the bucket or its tags or prefixes
 ** Array of AWSTags that are associated with the bucket

 * Generic dataset2Dataset process to represent movement of data from one dataset to another.  It supports:
 ** array of transforms performed by the process 
 ** map of tag/value pairs representing configurationParameters of the process
 ** inputs and outputs are arrays of dataset objects, e.g., kafka topic and S3 pseudo-directory.

 ",AWS S3 data lake typedefs for Atlas,8,,,barbara,True,barbara,barbara
atlas,ATLAS-2694,2018-05-16T21:21:55.000+0000,2018-05-25T21:28:24.000+0000,2018-06-14T18:17:30.000+0000,,Fixed,New Feature,Critical,['1.0.0'],,,,,,,,[' atlas-core'],['Atlas Core'],"Currently the base types in Atlas do not include Avro schemas. It would be nice to add typedef for Avro schema and any associated metadata to support schema evolution.
 * For example, Avro_schema type supports:
 ** All avro types, both primitive and complex, including union types, as fields of schema
 ** All types have doc strings and defaults
 ** A field of a schema can be another schema
 ** Indefinite nesting of records, arrays.
 ** Associated entities array attribute contains pointers to all datasets that reflect the avro schema
 ** Fully expanded avroNotation for use in serDe
 ** Schema evolution features such as isLatest (Boolean) and version number
 * Schema evolution Process
 ** Input: avro schema
 ** Output: new version of avro schema
 ** Compatibility: FULL, BACKWARD, FORWARD, NONE
 ** IsBreakingChange (Boolean): does the change produce an incompatible schema? (ie its compatibility is not “FULL”)
 *",Avro schema typedef and support for Avro schema evolution  in Atlas,3,,,srikvenk,True,barbara,srikvenk
atlas,ATLAS-2563,2018-04-13T13:27:28.000+0000,,2018-09-10T07:33:32.000+0000,,,New Feature,Minor,['1.0.0-alpha'],['1.0.0-alpha'],,,,,,,['atlas-intg'],['Atlas Integration Components'],"Hello 

Is there any chance that ""AWS GLUE CATALOG"" is supported as a source of metadata or bridges?

Hcatalog already supports ""Aws Glue Catalog"" so what level of integration can we expect about now and in the future?",Metadata Sources (Bridge) support for AWS GLUE CATALOG ,4,1,,damdr,True,,damdr
atlas,ATLAS-2534,2018-04-05T19:04:12.000+0000,2018-04-18T15:06:11.000+0000,2018-05-16T23:59:26.000+0000,,Fixed,New Feature,Major,['1.0.0'],['1.0.0-alpha'],,,,,,,,,,Atlas Glossary support,3,,,apoorvnaik,True,apoorvnaik,apoorvnaik
atlas,ATLAS-2488,2018-03-08T10:38:29.000+0000,2018-03-17T17:57:45.000+0000,2018-05-10T07:52:23.000+0000,,Fixed,New Feature,Major,['1.0.0'],,,,,,,,,,"RelationshipREST allows one to create, delete & get relationships

However the AtlasClientV2 API only has support for get & delete, not create

This is an omission (bug arguably?) , since the client should reflect what is available via the rest API in a more java-usable form

 ",AtlasClientV2 should support create of relationship,2,,,jonesn,True,jonesn,jonesn
atlas,ATLAS-2469,2018-03-01T18:59:56.000+0000,2018-03-05T17:46:21.000+0000,2018-03-05T17:46:21.000+0000,,Fixed,New Feature,Blocker,['1.0.0'],['1.0.0'],,,,,,,,,"User Story:

As a data steward i need a scalable way to quickly and efficiently propagate tags for efficient searches and tag based security. Likewise tags for derivative dataset should be inherited from the parent. For example, if an entity is tagged ""Secret"" then resulting entity created from a CTAS operation should also be tagged ""secret"" to maintain the classification of the parent. In the case where 2 or more datasets are aggregated the derivative dataset should be a union of all parent tags.


Terms:

* Child business terms should inherit the tags associated with the parent term.
* the option to propagate tags to child business terms in a hierarchy should be provided
* Ability to update the propagated tags manually via UI or through the API
* Tagging a term should propagate to data assets that are already attached to that business term as well

Data assets

* For supported components in HDP 2.6,if a derivative asset is created it should inherit the tags and attributes from the original asset.
* the option to propagate tags to child entities should be provided (e.g. if you tag a hdfs folder optionally tag all the files within it)
* Ability to update the propagated tags manually via UI or through the API
* Tagging a parent object should be inherited even after child creation dynamically (unless a flag is set not to do this)
* Derived data assets should have the tags of the original data asset.

conflict resolution - if the different values for attributes then a UX dialog that prompts user for action needs to be provided. Once resolved, the resolved value will be carry forth to derivative dataset.",Tag propagation from object to child object or derivative asset,1,,,pratik24mac,True,pratik24mac,pratik24mac
atlas,ATLAS-2457,2018-02-21T20:37:52.000+0000,2018-04-12T20:07:18.000+0000,2018-04-12T20:07:18.000+0000,,Fixed,New Feature,Major,['1.0.0'],,,,,,,,[' atlas-core'],['Atlas Core'],"To address use cases where an entity-classification association needs to be time bound, for example an earnings report is sensitive and restricted only until its public release, Atlas classifications should support attributes startTime and endTime. These new attributes will enable applications that consume the classification association, like Apache Ranger, to make authorization decisions based on the specified time-bounds.

- the new attributes should be optional; when no value is provided, the current behavior will be retained i.e. no time-bounds for the classification association
- the association will be effective from the given startTime - if a value is provided; if no value is present for startTime, the association will be effective immediately
- the association will be effective until the given endTime - if a value is provided; if no value is present for endTime, the association will not have an end time

",Entity-Classification association to support time bounds,1,,,madhan.neethiraj,True,madhan.neethiraj,srikvenk
atlas,ATLAS-2298,2017-12-05T08:51:46.000+0000,2018-08-23T14:24:59.000+0000,2018-08-23T14:25:04.000+0000,,Fixed,New Feature,Major,,['1.0.0'],,,,,,,,,"Code review is available in https://reviews.apache.org/r/65435/

This Jira is focused on development of the OCF Database Connector. 

The OCF Database Connector is the subclass of OCF Connector and it is designed especially for connection to database to retrieve data. It provides the OCFDatabaseConnector as a basic class for other implementations for various databases.

Here we implement a connector for Gaian (GaianOCFConnector) as an example for using OCF Database Connector.",OCF Database Connector,3,,['VirtualDataConnector'],mstrelchuk,True,Yao 22,mstrelchuk
atlas,ATLAS-2135,2017-09-15T00:13:01.000+0000,,2017-11-02T17:43:22.000+0000,,,New Feature,Major,,,,,,,,,,,Parent ticket to track all development work for integration with IGC.,Atlas OMRS integration with IBM Information Governance Catalog (IGC) ,2,,,jinghe,True,jinghe,jinghe
atlas,ATLAS-2123,2017-09-08T11:01:51.000+0000,,2018-07-07T06:45:49.000+0000,,,New Feature,Major,,,,,,,,,,,This Jira implemented a new open metadata and governance server called the Stewardship Server.  This server supports a workflow engine and the ability to manage and remediate exceptions.,Stewardship Server and Toolkit,1,,,mandy_chessell,True,,mandy_chessell
atlas,ATLAS-2122,2017-09-08T10:45:56.000+0000,2018-04-06T08:35:27.000+0000,2018-05-08T13:39:15.000+0000,,Fixed,New Feature,Major,['1.0.0'],['1.0.0'],,,,,,,,,"This Jira Assembles the Open Metadata and Governance components into a configurable server to support the OMAS Access Layer, Repository or native metadata repository.

The OMAG server is a Spring Boot Application with a simple REST API. It takes the user Id of the administrator, the server name and cohort name as path variables, other parameters are passed as request parameters. These are examples of the commands to set up the server using defaults. 

Query configuration 
GET http://localhost:8080/omag/admin/{userId}/{serverName}/configuration/ 

Set server type name: 
POST http://localhost:8080/omag/admin/{userId}/{serverName}/server-type?typeName={name} 

Set organization name: 
POST http://localhost:8080/omag/admin//{userId}/{serverName}/organization-name?name={organizationName} 

Enable the access services: 
POST http://localhost:8080/omag/admin//{userId}/{serverName}/access-services/mode?serviceMode=ENABLED 

Enable the Atlas graph repository: 
POST http://localhost:8080/omag/admin//{userId}/{serverName}/local-repository/mode?repositoryMode=LOCAL_GRAPH_REPOSITORY 

Enable the in-memory repository: 
POST http://localhost:8080/omag/admin//{userId}/{serverName}/local-repository/mode?repositoryMode=IN_MEMORY_REPOSITORY 

Enable server as a repository proxy: 
POST http://localhost:8080/omag/admin//{userId}/{serverName}/local-repository/mode?repositoryMode=REPOSITORY_PROXY 

To add the local repository connection for the repository proxy 
POST http://localhost:8080/omag/admin//{userId}/{serverName}/local-repository/proxy-details?connectorProvider={javaClassName}&url={nativeServerURL} 

To add the local repository's event mapper: 
POST http://localhost:8080/omag/admin//{userId}/{serverName}/local-repository/event-mapper-details?connectorProvider={javaClassName}&eventSource={resourceName} 

To enable access to a cohort 
POST http://localhost:8080/omag/admin//{userId}/{serverName}/cohort/{cohortName}/mode?serviceMode=ENABLED 

To remove the local repository 
POST http://localhost:8080/omag/admin//{userId}/{serverName}/local-repository/mode?repositoryMode=NO_LOCAL_REPOSITORY 

To disable the access services 
POST http://localhost:8080/omag/admin//{userId}/{serverName}/access-services/mode?serviceMode=DISABLED 

To disconnect from a cohort 
POST http://localhost:8080/omag/admin//{userId}/{serverName}/cohort/{cohortName}/mode?serviceMode=DISABLED 

To start up OMRS/OMAS services 
POST http://localhost:8080/omag/admin//{userId}/{serverName}/instance 

To shutdown OMRS/OMAS services 
DELETE http://localhost:8080/omag/admin//{userId}/{serverName}/instance?permanent=false ",OMAG Server,5,,,mandy_chessell,True,mandy_chessell,mandy_chessell
atlas,ATLAS-2116,2017-09-06T05:52:39.000+0000,2018-02-17T05:01:54.000+0000,2018-02-17T05:08:36.000+0000,,Fixed,New Feature,Major,['1.0.0'],['1.0.0'],,,,,,,[' atlas-core'],['Atlas Core'],"Implement propagation of classifications using relationships. When a classification is associated with an entity the tag is propagated to all its associated entities using the tagPropagation property in the edges. Also propagate tags, if tag is deleted or updated.

tagPropagation properties can be NONE, BOTH, ONE_TO_TWO or TWO_TO_ONE

propagate tags only if relationships tagPropagation property is BOTH, ONE_TO_TWO or TWO_TO_ONE.",Implement tag propagation using relationships,1,,,sarath.kum4r@gmail.com,True,sarath.kum4r@gmail.com,sarath.kum4r@gmail.com
atlas,ATLAS-2095,2017-08-29T17:32:35.000+0000,,2017-08-29T17:32:35.000+0000,,,New Feature,Major,,,,,,,,,,,"In ATLAS-2012, ATLAS-2093 & ATLAS-2094 we have created docker images for Atlas.

However this is 'just' Atlas. It would be interesting to have a proper data engine like hive contributing metadata, maybe even perhaps an enforcement engine like Ranger, enforcing access to the data based on business terms, classifications & real data. This makes it much easier to understand the scope of how Atlas can help.

Since this is cross component we should consider if Atlas is the right home, or if there are other external projects which might handle this. Also whilst we could build a single monolithic image, I think it would be preferable instead to orchestrate a number of docker containers. docker-compose is rather primitive, but is simple & could possibly suffice, whilst a kubernetes approach is more real, but requires more setup work to run ","Docker - add other interesting pieces like hive, ranger",2,,,jonesn,True,,jonesn
atlas,ATLAS-2094,2017-08-29T17:27:51.000+0000,,2017-11-15T06:18:37.000+0000,,,New Feature,Major,,,,,,,,,,,"In ATLAS-2012 we introduced a basic docker image for Atlas which is created in the build.
In ATLAS-2093 this can be extended with some basic sample data

This Jira will take things a step further. We can make Atlas available on the docker hub or similar, so that just as with many technologies like Cassandra, mysql, someone interested in taking a peek at what atlas has to offer can quickly obtain and run atlas in little more than a single line.

We need to check licensing (I imagine it's ok but need to validate), and consider what other documentation might be needed, and any extra sample data, so that someone is given a good impression of what the project offers.",Make Atlas available in docker hub,3,1,,jonesn,True,jonesn,jonesn
atlas,ATLAS-2093,2017-08-29T17:23:11.000+0000,,2017-08-29T17:23:11.000+0000,,,New Feature,Major,,,,,,,,,,,"The docker image added in ATLAS-2012 is an empty Atlas server only. Probably more of use to developers than someone wanting to experiment.

As discussed on community call & suggested my Madhan - As an enhancement it would be useful to run the quick-start so that the server is populated with data (Thought: Perhaps this could be configurable, default to enabled) ",Docker: Add sample data,2,,,jonesn,True,jonesn,jonesn
atlas,ATLAS-2059,2017-08-18T17:42:01.000+0000,2017-08-18T18:44:22.000+0000,2017-08-18T18:44:22.000+0000,,Fixed,New Feature,Major,"['0.8-incubating', '1.0.0']",['1.0.0'],,,,,,,[' atlas-core'],['Atlas Core'],"Update test-patch.sh script for PreCommit-ATLAS-Build jenkins job and fix for one failed IT

https://builds.apache.org/view/A/view/Atlas/job/PreCommit-ATLAS-Build-Test/

We can test patches using jenkins job (Build with parameters) using:
* Review Board ID  
* JIRA ID
* Local Atlas Patch",Update test-patch.sh script for PreCommit-ATLAS-Build jenkins job,1,,,sarath.kum4r@gmail.com,True,sarath.kum4r@gmail.com,sarath.kum4r@gmail.com
atlas,ATLAS-2013,2017-08-01T15:07:20.000+0000,,2017-08-01T15:07:20.000+0000,,,New Feature,Major,,,,,,,,,,,"As per mailing list post on 2017-07-24:


I was just reading ATLAS-1870 which related to default audit repositories. I see the default is HBaseBasedAuditRepository, and that there is also NoopEntityAuditRepository, and InMemoryEntityAuditRepository.

I'm thinking that when Atlas is used in a non-hadoop oriented environment it would be useful to have a non-hbase, persistent (helps a little with compliance ) audit respository. Perhaps another RDB, or indeed in solr/elastic search alongside Ranger's audit events.

I'll raise a JIRA on this if it's felt useful & my understanding is correct

Nigel.


","Audit - add non-hbase, persistent audit repository",1,,,jonesn,True,,jonesn
atlas,ATLAS-2012,2017-08-01T15:02:03.000+0000,2017-08-26T22:09:32.000+0000,2018-07-20T12:11:18.000+0000,,Fixed,New Feature,Major,['1.0.0'],,,,,,,,,,"Docker is increasingly become a standard way of easily running components in a flexible manner, whether for development, production, or test

I feel there are a few things we can do with docker that will aid Atlas's appeal

1. We could provide a simple example of how to create a docker image from the Atlas build. This could be published on the wiki & it would make it easier for developers to use Atlas within a docker environment - mostly by addressing how to configure & start up
2. We could automatically generate a docker image as part of the build process. This builds on #1 by automating the creation of the image & making it ""just part of the build""
3. We could publish the docker image from #2, for releases, to dockerhub, making it near trivial for any developer to easily pull down and experiment with Atlas. Full source of course would be provided/documented so that a user could customize as needed for their environment plus of course improve what is provided with the core project
4. We could work with other teams especially ranger, to allow similar efforts in other teams to easily work together & be orchestrated ",Docker - image & hub - for Atlas,9,2,,jonesn,True,jonesn,jonesn
atlas,ATLAS-1962,2017-07-18T10:41:11.000+0000,,2017-08-16T08:30:56.000+0000,,,New Feature,Blocker,,['0.8-incubating'],,,,,,,"[' atlas-core', 'atlas-intg']","['Atlas Core', 'Atlas Integration Components']","Feature Request to add user/group mapping rules similar to Hadoop's auth_to_local.

This will allow munging users/groups and rule based remappings to differentiate duplicate users in multi-domain Active Directory forests where the LDAP results returned from the global catalog include duplicate usernames which need to be translated with a prefix/suffix in order to differentiate between domains to prevent users from different domains sharing logins, permissions etc.",User/group mapping rules similar to Hadoop's auth_to_local,1,,,harisekhon,True,,harisekhon
atlas,ATLAS-1959,2017-07-18T06:21:05.000+0000,2017-08-22T22:50:59.000+0000,2018-05-17T05:23:04.000+0000,,Fixed,New Feature,Major,['1.0.0'],"['1.0.0', 'trunk']",,,,,,,[' atlas-core'],['Atlas Core'],"1. Improve relationship model to support create/update operations and the following cardinalities (previously supported using inverseReference):
* 1 to 1
* 1 to many
* many to 1
* many to many

2. Change legacyLabel flag in AtlasRelationshipEndDef to boolean flag.
3. Add unit tests for the above cases.",Enhance relationship attributes to support different cardinality mappings,1,,,sarath.kum4r@gmail.com,True,sarath.kum4r@gmail.com,sarath.kum4r@gmail.com
atlas,ATLAS-1955,2017-07-17T17:22:53.000+0000,,2017-10-13T20:45:55.000+0000,,,New Feature,Major,['1.0.0'],['1.0.0'],,,,,,,[' atlas-core'],['Atlas Core'],"It would be very nice that Atlas model could contain a way to represent attribute validation. 

A simple example is that we would like to model a Person, with attributes Name, Email and Country. Now we would like to specify that Email has to follow a specific regular expression, so it would be nice if we could set Email -> hasValidation -> EmailRegex, with EmailRegex having:

Name: Email Regular Expresion
Expression: /[0-9a-z]+@[0-9a-z]+.[0-9a-z]+/

For more complex types of validation, e.g. checking card number validity, it could be added some external validator function/service.

Name: Credit Card Number Validator
Validator: org.apache.atlas.validators.creditcard or https://host:port/creditCardValidator

For validations from a reference table, for example a country name, it could be:

Name: Country Name Ref Validator
Reference Column: <country_name_column>

where <country_name_column> would be an instance of type Hive_Column or HBase_Column.

Since this is a kind of Standarization, it could be placed in [Area 5|https://cwiki.apache.org/confluence/display/ATLAS/Area+5+-+Standards].

A similar approach is followed in software [Kylo|https://github.com/Teradata/kylo/tree/master/integrations/spark/spark-validate-cleanse]",Validation for Attributes,4,,,ivarea,True,rding,ivarea
atlas,ATLAS-1907,2017-06-30T19:46:52.000+0000,2017-08-22T22:52:14.000+0000,2018-05-17T05:12:02.000+0000,,Fixed,New Feature,Major,['1.0.0'],"['0.8.1', '1.0.0']",,,,,,,[' atlas-core'],['Atlas Core'],"Create relationshipDefs for all out of the box models. Use relationshipDef information when creating edges for composite attributes in entities.
",Create RelationshipDefs for all base models and use this definitions when creating edges for entities,1,,,sarath.kum4r@gmail.com,True,sarath.kum4r@gmail.com,sarath.kum4r@gmail.com
atlas,ATLAS-1892,2017-06-21T19:06:37.000+0000,2017-08-22T22:52:45.000+0000,2018-05-17T05:58:03.000+0000,,Fixed,New Feature,Major,['1.0.0'],['1.0.0'],,,,,,,[' atlas-core'],['Atlas Core'],"When a new relationshipDef is created, relation attributes needs to be populated in AtlasEntityType during resolveReference stage.

This JIRA also adds UT and IT and addresses review comments in https://reviews.apache.org/r/59769/",Implement logic to create relationship attributes in AtlasEntityType,1,,,sarath.kum4r@gmail.com,True,sarath.kum4r@gmail.com,sarath.kum4r@gmail.com
atlas,ATLAS-1869,2017-06-09T13:18:53.000+0000,,2017-08-03T12:50:49.000+0000,,,New Feature,Major,,,,,,,,,,,"With a variety of data processing engines in Hadoop such as Hive, we have an Atlas plugin & hook that captures new & updated metadata from those engines and pushes it to Atlas. This can then be used to support governance including lineage.

We already have a ""ranger plugin"" for Atlas which allows ranger to control access to metadata in atlas - this is NOT the subject of this Jira, but rather ""the other way around""

Examples might include
 * Capture information about the policies that are deployed in a ranger server - the types of assets they refer to, the classifications that are used. 
 * Capture information about the topology of ranger - by this I mean the plugins that are deployed and active, the nodes they run on, and feed this back into an operational model in Atlas

In each case the information could be published by Ranger, consumed by Atlas, and stewardship activities around the atlas metadata could help in tying things together

The benefit would be
 - better end to end view (since we know the endpoints, identifiers in audit logs)
 - optimizing the interfaces (rest & kafka) by being able to better targer useful information - ie if only a hive plugin is being used & configured for tags, let's just worry the tags it needs.

I see this of use around our open metadata work & specifically VDC, though not essential for an initial MVP

Placeholder for now... will elaborate further



At the same time the coupling would be loose, and shouldn't hinder any existing integrations, or decisions as to what is done in Atlas vs Ranger","Atlas ""plugin"" for Ranger (metadata capture)",1,,,jonesn,True,jonesn,jonesn
atlas,ATLAS-1856,2017-06-02T20:38:22.000+0000,2017-08-22T22:53:13.000+0000,2018-05-17T15:19:53.000+0000,,Fixed,New Feature,Major,['1.0.0'],"['1.0.0', 'trunk']",,,,,,,[' atlas-core'],['Atlas Core'],Based on the relationDef implementation in ATLAS-1852. Implement instance API and REST for entity relationships,Relationship instance API Java & REST implementation,1,,,sarath.kum4r@gmail.com,True,sarath.kum4r@gmail.com,sarath.kum4r@gmail.com
atlas,ATLAS-1849,2017-05-31T08:43:37.000+0000,2018-04-23T09:52:06.000+0000,2018-05-17T15:22:38.000+0000,,Fixed,New Feature,Major,['1.0.0'],,,,,,,,,,"Atlas should move to assume Java 8 in order to remain current

Java 7's last official release was in April 2015
Java 8 was released in March 2014 and is the current release
Java 9 is currently targetted for release in July 2017

Various new components we wish to integrate, such as Janus Graph now quite sensibly depend on, and can take advantage of, Java 8. Whilst Atlas does build cleanly under java 8, the new capabilities cannot be used and more crucially any components requiring it can't be so easily integrated into the source. With Java 7 way out of focus there are more likely to be security exposures too.

One of the current main environments Atlas is supported in is Hadoop. Hadoop 2.x is build with Java 7, though the very latest releases do at least support build with Java 8. Hadoop 3 which prereqs java 8 is due for release in August 2017

However other environments in the enterprise need not be constrained by this dependency

We need to decide how we will tackle this dependency change & mitigate the impacts - for example
 * we stay as-is until hadoop 3 is current/into 2018 (in my view too long)
 * we go all out on java 8 in the current trunk, and branches are cut for java 7 approach (I worry this is too much work to dual-fix)
 * we create multiple build profiles and carefully allow java8 code in as needed, but in such a way as a java7 version can still be built, albeit missing out on selective functionality - this could still be tricky, and has run-time dependency differences
 * if profiles we decide on default (once working I'd suggest 8...)

Thoughts welcome :-)",Atlas should prereq Java 8,3,,,jonesn,True,,jonesn
atlas,ATLAS-1847,2017-05-30T11:13:51.000+0000,2018-07-20T17:43:05.000+0000,2018-08-23T13:56:09.000+0000,,Fixed,New Feature,Major,,,,,,,,,,,"This JIRA covers the architecture and tracking of the open metadata and governance initiative for Apache Atlas.

Open metadata and governance is a moon-shot type of project to create a open set of APIs, types and interchange protocols to allow all metadata repositories to share and exchange metadata.  From this common base, it adds governance, discovery and access frameworks to automate the collection, management and use of metadata across an enterprise.  The result is an enterprise catalog of data resources that are transparently assessed, governed and used in order to deliver maximum value to the enterprise.

Delivering this capability as open source is a critical part of the project since multiple vendors must buy into this ecosystem.  They are not going to do this if one organization dominates the technology base.  Thus the open metadata and governance technology must be freely available with an open source governance model that allows a community of organizations and practitioners to develop and evolve the base and then use it in their offerings and deployments.

The proposal is to use Apache Atlas as the open source reference implementation for open metadata and governance.  Apache Alas will support an open metadata and governance compliant repository plus provide the adapters and interchange formats to allow other metadata repositories to connect into the ecosystem.",Open Metadata and Governance,2,,"['OpenMetadata', 'open-metadata']",mandy_chessell,True,mandy_chessell,mandy_chessell
atlas,ATLAS-1846,2017-05-27T21:14:55.000+0000,2018-08-23T13:56:00.000+0000,2018-08-23T13:56:16.000+0000,,Fixed,New Feature,Major,,,,,,,,,[' atlas-core'],['Atlas Core'],"In governance there are many consumers of lineage.  This Jira looks to provide:
* Open APIs for registering and querying lineage from metadata repositories and data movement engines that maintain their own lineage.
* Collection and correlation information about the design and expectation of data flows that is linked to the operational logs to provide business context to lineage.
* Lifecycle management of lineage data so lineage of key resources is preserved while lineage of everything else is removed after a short period of time.
",Design and business lineage for Atlas,1,,,mandy_chessell,True,mandy_chessell,mandy_chessell
atlas,ATLAS-1845,2017-05-27T21:06:16.000+0000,2018-08-23T13:57:50.000+0000,2018-08-23T13:57:57.000+0000,,Fixed,New Feature,Major,,,,,,,,,,,This JIRA is for tracking the design and implementation of the OMAS interfaces for defining a governance program.,Governance Definitions OMAS,1,,,mandy_chessell,True,mandy_chessell,mandy_chessell
atlas,ATLAS-1838,2017-05-27T10:24:48.000+0000,2018-03-30T13:13:32.000+0000,2018-08-23T14:26:06.000+0000,,Fixed,New Feature,Major,,['1.0.0'],,,,,,,[' atlas-core'],['Atlas Core'],This task delivers the JSON files for the new models that describe types for Area 1 in the open metadata model. This area covers Asset and Connector types.,Area 1 of the open metadata model,1,,"['OpenMetadata', 'VirtualDataConnector']",mandy_chessell,True,mandy_chessell,mandy_chessell
atlas,ATLAS-1831,2017-05-25T21:33:38.000+0000,2018-08-23T14:34:22.000+0000,2018-08-23T14:34:27.000+0000,2017-06-30,Fixed,New Feature,Major,,['1.0.0'],,,,,,,['atlas-intg'],['Atlas Integration Components'],"Code review is available in [https://reviews.apache.org/r/66174/]

This component listens for events from the Information View OMAS event publisher built by ATLAS-1711 (https://issues.apache.org/jira/browse/ATLAS-1711) to detect changes to relational data assets and configures an information virtualization tool (Gaian in the first instance) to create/maintain a view that maps to the physical data asset. It also populates Information View metadata (using the Information View OMAS API) for each of the views it creates.

RANGER-1485 (https://issues.apache.org/jira/browse/RANGER-1485) will also configure Gaian with enforcement logic to apply data protection rules to the data when queries are made to the views.",Virtualiser to dynamically configure an information virtualization layer,2,,['VirtualDataConnector'],mandy_chessell,True,Yao 22,mandy_chessell
atlas,ATLAS-1829,2017-05-25T21:14:17.000+0000,2018-08-23T14:01:34.000+0000,2018-08-23T14:01:41.000+0000,2017-06-30,Fixed,New Feature,Major,,['1.0.0'],,,,,,,[' atlas-core'],['Atlas Core'],"This JIRA covers the event support for OMRS.  There is an OMRS Event Publisher that publishes events relating to changing metadata in the Atlas repository.  These events follow the OMRS message payloads that are derived from the Atlas type model.

OMRS Messages are published to the OMRS Topic in Kafka.

There is also an OMRS Event Listener that takes events from the OMRS Topic and pushes them to an OMRS Connector if they are from a different repository instance.

This JIRA complements ATLAS-1773 (https://issues.apache.org/jira/browse/ATLAS-1773) OMRS REST Connector",OMRS Message Payloads with Event Listener and Publisher,1,,['VirtualDataConnector'],mandy_chessell,True,mandy_chessell,mandy_chessell
atlas,ATLAS-1828,2017-05-25T20:27:57.000+0000,2018-08-23T14:05:15.000+0000,2018-08-23T14:05:20.000+0000,2017-06-30,Fixed,New Feature,Major,,['1.0.0'],,,,,,,[' atlas-core'],['Atlas Core'],"This JIRA delivers the External API for catalog search applications.  This API is used by ATLAS-1765 (https://issues.apache.org/jira/browse/ATLAS-1765) that provides the Self-Service Catalog Search and Data Preview UI.

This JIRA is dependent on ATLAS-1773 (https://issues.apache.org/jira/browse/ATLAS-1773) that implements the OMRS REST Connector",Asset Catalog OMAS,2,,['VirtualDataConnector'],mandy_chessell,True,mandy_chessell,mandy_chessell
atlas,ATLAS-1807,2017-05-16T19:51:35.000+0000,2017-05-17T00:37:35.000+0000,2017-05-17T17:16:31.000+0000,,Fixed,New Feature,Major,"['0.8.1', '1.0.0']",,,,,,,,[' atlas-core'],['Atlas Core'],"Current DSL search cannot do wildcard searches, this JIRA will introduce a new operator in DSL for wildcard - ""like""

eg. 
* hive_table where name like ""employee*""
* hive_column where qualifiedName like ""defa?lt*""
* hive_column where name like ""loc*"" and position=5 and qualifiedName like ""default.employee2*""","Enhance DSL query to support ""like"" operator for wildcard searches",1,,,sarath.kum4r@gmail.com,True,sarath.kum4r@gmail.com,sarath.kum4r@gmail.com
atlas,ATLAS-1775,2017-05-09T18:58:16.000+0000,2018-04-17T11:02:00.000+0000,2018-08-23T14:00:53.000+0000,2017-09-30,Fixed,New Feature,Major,['1.0.0'],['1.0.0'],,,,,,,[' atlas-core'],['Atlas Core'],The Enterprise OMRS Connector provides a connector that implements the OMRS Connector API defined in JIRA ATLAS-1773 that is able to aggregate the metadata from multiple metadata repositories in response to metadata requests.,Create Enterprise OMRS Connector,2,,,mandy_chessell,True,mandy_chessell,mandy_chessell
atlas,ATLAS-1774,2017-05-09T18:52:05.000+0000,2018-08-23T14:13:56.000+0000,2018-08-23T14:14:04.000+0000,2017-09-30,Fixed,New Feature,Major,,['1.0.0'],,,,,,,[' atlas-core'],['Atlas Core'],This JIRA provides a connector for IBM’s Information Governance Catalog that implements the OMRS Connector API defined in JIRA ATLAS-1773.,Create the OMRS Connector for IBM's Information Governance Catalog (IGC),2,,,mandy_chessell,True,mandy_chessell,mandy_chessell
atlas,ATLAS-1773,2017-05-09T18:48:09.000+0000,,2019-01-10T16:55:50.000+0000,2017-09-30,,New Feature,Major,,['1.0.0'],,,,,,,[' atlas-core'],['Atlas Core'],"This JIRA provides the definition of the OMRS Connector API and an implementation of this API for a local Apache Atlas metadata repository and for the OMRS REST API.

The OMRS Connector has 3 API groups
* The types API - this is the metadata API for a metadata repository
* The entity and relationships APIs that provide the type-agnostic interfaces that can access any type - even those added dynamically
* The fine-grained type-safe APIs that are generated from the addons models in the build.
",Create the OMRS Connector for Atlas,9,,,mandy_chessell,True,grahamwallis,mandy_chessell
atlas,ATLAS-1772,2017-05-09T18:37:54.000+0000,2018-08-23T14:11:26.000+0000,2018-08-23T14:11:32.000+0000,2017-09-30,Fixed,New Feature,Major,,['1.0.0'],,,,,,,[' atlas-core'],['Atlas Core'],The Data Set Connector provides an OCF provider and connector implementation for collections of files.  These files may be structured or unstructured data.  The data set may be a set of files physically co-located (such as in the same folder on disk) or files that have similar characteristics in a larger collection.,Create a data set connector,2,,,mandy_chessell,True,mandy_chessell,mandy_chessell
atlas,ATLAS-1768,2017-05-05T15:07:18.000+0000,2018-08-23T07:36:12.000+0000,2018-08-23T07:36:12.000+0000,2017-07-31,Fixed,New Feature,Major,,['1.0.0'],,,,,,,[' atlas-core'],['Atlas Core'],"This JIRA describes a proposal for standard types for open metadata entities and relationships.  For example, glossaries, database definitions, rules, policies, ...

The value of having standard definitions for metadata is to enable type safe APIs and business level UIs plus be able to exchange metadata between different instances of metadata repositories.

The implementation of these common types is divided into 8 areas:
* Area 0 - for extensions to Apache Atlas's base model
* Area 1 - for definitions of the data-related assets we are governing and using
* Area 2 - for a glossary of meanings and semantic relationships
* Area 3 - for information about asset use, crowd-sourced definitions and collaboration around the data-related assets
* Area 4 - for governance such as policies, rules and classifications
* Area 5 - for reference models and reference data
* Area 6 - for metadata discovery processes (see https://issues.apache.org/jira/browse/ATLAS-1748)
* Area 7 - for lineage

Adaptation and flexibility are key in metadata environments so these common definitions must be extensible - and we still need to support the ad hoc definition of new types in Atlas.

Apache Atlas supports meta-types that are used in the definition of new types.  These are currently enumeration, struct, classification and entity.  JIRA https://issues.apache.org/jira/browse/ATLAS-1690 adds relationships to this list.  The open metadata models make use of all of these meta-types.  These are represented by sterotypes on the classes of the open metadata definitions.

The Atlas wiki has the models as a set of linked pages which are probably the easiest way to view the models.
Start here: https://cwiki.apache.org/confluence/display/ATLAS/Building+out+the+Apache+Atlas+Typesystem
",Create common types for Open Metadata,10,1,['OpenMetadata'],mandy_chessell,True,mandy_chessell,mandy_chessell
atlas,ATLAS-1765,2017-05-03T07:20:15.000+0000,2018-07-13T10:40:14.000+0000,2018-07-13T11:20:35.000+0000,2019-07-31,Fixed,New Feature,Major,,['1.0.0'],,,,,,,['atlas-webui'],['Atlas UI'],"This JIRA covers the development of the catalog search and preview of data for data scientists and business users.  It supports the search of the Atlas metadata repository, display of search results, additional filtering and drill down into details of the data sources, including a data preview option if the end user has access permission.",Self-Service Catalog Search and Data Preview,6,,"['BusinessUserUI', 'Self-Service-UIs', 'VirtualDataConnector']",mandy_chessell,True,mandy_chessell,mandy_chessell
atlas,ATLAS-1764,2017-05-02T21:24:51.000+0000,,2017-08-23T18:58:03.000+0000,,,New Feature,Major,,['1.0.0'],,,,,,,[' atlas-core'],['Atlas Core'],"Design and implement Atlas Collections - A first class element in Atlas to group related entities together and perform operations on these collections - associate tags, add/remove entities...

Operations:
---------------
    a. Create collection(s) with attributes, constraints
    b. Update existing collection(s)
    c. Delete collection(s) - soft delete, hard delete
    d. Retrieve collections by id, name
    e. Add entity(s) to collection(s)
    f. Remove entity(s) from collection(s)
    g. Associate classification(s) to collection(s)
    h. Disassociate classification(s) from collection(s)",Design and implement Atlas Collections,4,2,['features'],sarath.kum4r@gmail.com,True,sarath.kum4r@gmail.com,sarath.kum4r@gmail.com
atlas,ATLAS-1754,2017-04-26T08:02:42.000+0000,2018-07-08T06:20:11.000+0000,2018-07-13T10:37:50.000+0000,,Duplicate,New Feature,Major,,['1.0.0'],,,,,,,['atlas-webui'],['Atlas UI'],"Apache Atlas collects information about an organization's data assets.  This JIRA provides a role-based user interface to enable people in the organization to locate, use, manage and govern these assets.  The personas that it will support are:
 - Data scientists and citizen analysts (business people working with data) wishing to locate and use data.
 - Application developers wanting to use standard structures in their APIs and to understand the rules that apply to the data they are processing with the API.
 - Data engineers and information architects supporting the data platforms and integrations that support the business.  These people use metadata to create strong information supply chains that deliver data to the organization. They also deploy analytics created by data scientists and citizen analytics into production and build analytical processes themselves.
 - Data owners (any role) wanting to improve the description of their data asset, to classify their asset and to understand how their data assets are being used.
 - Governance team members from CDO, to data owners to data stewards and custodians that define and monitor the metadata used to manage and govern the data.
",Self-Service User Interfaces for Atlas,4,,['BusinessUserUI'],mandy_chessell,True,mandy_chessell,mandy_chessell
atlas,ATLAS-1748,2017-04-24T11:58:10.000+0000,2018-07-20T13:33:12.000+0000,2018-07-20T13:33:12.000+0000,,Won't Fix,New Feature,Major,,['1.0.0'],,,,,86400,,['atlas-intg'],['Atlas Integration Components'],This JIRA proposes a framework for managing automated metadata discovery analytics.  The framework is called when Atlas receives a notification about new or (significantly) updated data.  It selects a discovery pipeline and runs it.  The discovery pipeline is a list of pluggable discovery components that are called in sequence.  The later components in the pipeline can access the results of the previous components.  The results of the pipeline are added to the Apache Atlas metadata entry for the data source.,Automated Metadata Discovery,4,,,mandy_chessell,True,,mandy_chessell
atlas,ATLAS-1712,2017-04-03T21:48:08.000+0000,2018-08-23T14:29:53.000+0000,2018-08-23T14:30:05.000+0000,,Fixed,New Feature,Major,,,,,,,,,,,Asset OMAS is part of the API for a connector that returns the asset metadata. Asset OMAS used by the virtualizer component when constructing the view metadata.,Asset OMAS,1,,['VirtualDataConnector'],mstrelchuk,True,,mstrelchuk
atlas,ATLAS-1711,2017-04-03T21:44:15.000+0000,2018-08-23T14:30:26.000+0000,2018-08-23T14:31:21.000+0000,,Fixed,New Feature,Major,,,,,,,,,,,Information View OMAS manages the creation and maintenance of View metadata. and is called by the Virtualizer.,Information View OMAS,1,,['VirtualDataConnector'],mstrelchuk,True,mstrelchuk,mstrelchuk
atlas,ATLAS-1696,2017-03-29T21:26:10.000+0000,2018-07-11T11:35:04.000+0000,2018-07-11T11:35:04.000+0000,,Won't Do,New Feature,Major,,,,,,,,,,,"Governance Engine OMAS is one of multiple consumer-centric based interfaces that will be added to Apache Atlas, & provides the API (REST and messaging) to support policy enforcement frameworks such as Apache Ranger. Detailed knowledge of the Atlas data models and structure can then be hidden from these consumers.

The functionality of gaf includes
 - ability to retrieve classifications associated to assets
  - restricted to ""interesting"" classifications 
  - restricted to interesting assets being managed by the requesting endpoint
 - to retrieve a list of interesting roles that relate to enforcement
 - to retrieve any template rule definitions/lookup tables that might be used to construct executable rules

The scoping constructs supported in the API will include
 - Only get classifications that are relevant for security enforcement (ie: only those inheriting from a specified supertype? Verify in ATLAS-1839)
 - only get information about assets (resources) in a certain part of the datalake (Q: HOW. By zone? How to specify? by asset type? By associated endpoint?)
 - pagination
 
See ATLAS-1839 for more information on the model and classifications

In the Atlas data model classifications propagate - for example
 * An database column DOB has no explicit classification
 * It's containing table CDB  is classified as ""customer personal details""
 * The ""SPI"" classification is attached to this table with the value ""sensitive""

At enforcement time all that an engine such as ranger cares about is that the column ""DOB"" is sensitive, how we got there isn't important.  In the example above the propogation occurs

 * Along the assigned term relationship 
 * along the structural containment relationship (table->column)

Therefore gaf omas will ""flatten"" the structure - so in this case we'll see
 table/CDB - SPI:sensitive
 column/DOB - SPI:sensitive

There will be cases where multiple classifications (of the same type) can be navigated to from an asset like DOB. This may not make logical sense, however, Until precedence is resolved in ATLAS-1839 & related Jiras, OMAS will pass through multiple classifications

This interface will also support message notifications of changes to managed resources such as a new role, classification. A single kafka topic will be used. 
 <tbd>



A first pass swagger can be found at https://app.swaggerhub.com/apis/planetf1/GovernanceActionOMAS/0.1


NOTE: Updated 23 Aug with new name of GOVERNANCE ENGINE OMAS",Governance Engine OMAS,2,,['VirtualDataConnector'],jonesn,True,jonesn,jonesn
atlas,ATLAS-1695,2017-03-29T21:23:37.000+0000,,2017-05-27T21:49:03.000+0000,,,New Feature,Major,,,,,,,,,,,This Jira covers the business user interface for the governance team to define the governance program.,Governance Definitions and Rules Management UI,1,,"['BusinessUserUI', 'OpenMetadata']",florac,True,,florac
atlas,ATLAS-1694,2017-03-29T21:22:52.000+0000,2018-07-11T11:34:27.000+0000,2018-07-11T11:34:28.000+0000,,Won't Do,New Feature,Major,,,,,,,,,,,"Assets to support the virtual data connector project as samples

for example
 - sample data
 - sample rules/policies/classifications
 - tools
 - integration for GaianDB (open source virtualization engine)",Sample assets to support Virtual Connector Project,1,,['VirtualDataConnector'],jonesn,True,jonesn,jonesn
atlas,ATLAS-1662,2017-03-14T13:39:47.000+0000,2018-05-17T05:51:11.000+0000,2018-05-17T05:51:11.000+0000,,Duplicate,New Feature,Major,,,,,,,,,,,"Today Ranger obtains classification (""tags"") from atlas via a tagsync process which
 - looks at interesting types
 - going through entities of those types
 - pulling traits/trait instances to get tags
 - formulating these into a map
 - persisting in ranger

ATLAS-1410 proposes a new enterprise ready glossary which allows for a more sophticated approach to classification.

To support this we will develop a new consumer-centric (""omas"") API to support Ranger or other enforcement engines . The implementation behind the first function of this API will navigate the new glossary structure to present ranger with the simplified entity:classification map it needs rather than the more elaborate structure of the glossary. 

A new ranger tagsync process will then use this api to push tags into ranger without needing changes 

More generally this API will support anything an enforcement engine needs



more detail to follow
(Note to observers: could you assign this one to me or grant permission)
",New governance action API to support Ranger tags from v2 glossary,1,,,jonesn,True,jonesn,jonesn
atlas,ATLAS-1503,2017-01-27T02:40:45.000+0000,2017-03-06T03:28:15.000+0000,2017-05-17T16:18:24.000+0000,,Fixed,New Feature,Major,"['0.8.1', '1.0.0']",['0.8-incubating'],,,,,,,[' atlas-core'],['Atlas Core'],"One of the often asked feature is the ability to export data stored in Atlas (entity, lineage, tag-association, types) and import the exported data into another Atlas instance. This feature should support the ability to export specified list of objects, along with their associated objects, tags and type. For example, if a hive_db named ""finance"" is specified, export should include the following:
- hive_db entity along with its tags
- all tables in this database and their tags
- all columns in above tables and their tags
- the lineage information associated with above entities
- type-defintion of all above entities

During import, if an exported entity is not present in the target Atlas, it should be created. If the entity already exists at the target (by guid or by unique attribute value), the entity should be updated.",Export/import support to copy data between Atlas instances,2,,['patch'],madhan.neethiraj,True,ashutoshm,madhan.neethiraj
atlas,ATLAS-1390,2016-12-15T22:15:45.000+0000,,2017-05-30T17:54:11.000+0000,,,New Feature,Major,,,,,,,,,,,Support running Atlas against IBM Graph as the graph backend via IBM Graph implementation of the graph DB abstraction.,IBM Graph implementation of graph DB abstraction,2,,,dkantor,True,jnhagelb,dkantor
atlas,ATLAS-1389,2016-12-15T21:56:46.000+0000,,2016-12-16T20:14:53.000+0000,,,New Feature,Major,,,,,,,,,,,Provide a lightweight ping API to allow clients to check status of Atlas server.,Ping REST API entrypoint,3,,,dkantor,True,jnhagelb,dkantor
atlas,ATLAS-1385,2016-12-15T21:14:33.000+0000,2017-02-08T19:33:59.000+0000,2018-05-17T15:37:58.000+0000,,Fixed,New Feature,Major,['0.8-incubating'],,,,,,,,,,"Add configuration property to disable full text mapper. Populating the full text fields adds considerable overhead, and this allows applications to avoid this overhead if they do not need full text search capability.",Add configuration property to disable full text mapper,4,,,dkantor,True,wwojcik,dkantor
atlas,ATLAS-1300,2016-11-16T20:01:14.000+0000,2016-11-29T10:07:20.000+0000,2018-05-17T15:46:01.000+0000,,Fixed,New Feature,Major,['0.8-incubating'],['0.8-incubating'],,,,,,,,,"The current v1 Lineage API (LineageResource) needs to route to the new v2 Lineage API (LineageREST).
1. Inputs Graph
2. Outputs Graph",LineageResource API needs to map to the new LineageREST API,2,,,sarath.kum4r@gmail.com,True,sarath.kum4r@gmail.com,sarath.kum4r@gmail.com
atlas,ATLAS-1277,2016-11-09T03:45:35.000+0000,2017-01-05T19:30:05.000+0000,2017-01-05T19:30:06.000+0000,2016-11-09,Fixed,New Feature,Major,['0.8-incubating'],['0.8-incubating'],,,,,,,,,"in the Search using DSL,sort with 'orderby' ,but with :'order by ' is  a more customary way ,so consider add the 'order by '  is more desirable.",Add  feather use 'order by ' in the DSL search ,3,,,zhangqiang2,True,zhangqiang2,zhangqiang2
atlas,ATLAS-1254,2016-10-31T14:05:25.000+0000,,2016-12-14T10:15:54.000+0000,,,New Feature,Major,,,,,,,,,,,"Ability to add has relationships to a term. I suggest that to do this consistently , terms should be implemented as entities - thereby picking up the ability to have attributes, relationships (including has)  and guids (non-name) identity. ",Ability to add relationships to a term,1,,,davidrad,True,davidrad,davidrad
atlas,ATLAS-1252,2016-10-28T10:53:33.000+0000,,2016-10-28T10:53:33.000+0000,,,New Feature,Major,,,,,,,,,,,"I notice that GraphBackedMetadataRepository implements  MetadataRepository. It has @GraphTransaction annotations around its methods. This means that each one of the metadataService operations will run in its own transaction. There are cases in the code where an API call issue more than one update / creation orientated call to the Metadata service. This means that in the event of failures- the graph could be left partially updated. For example creating a term creates , a trait type and trait instance but each have their own transaction. Transactions should be scoped around all of the graph calls a create,update or delete API invocation makes. 

Note if an update / create / delete API call only issues one call to the metadata service then the existing mechanism works.",Transactions should be scoped at the API level not at the individual graph operations   ,1,,,davidrad,True,,davidrad
atlas,ATLAS-1244,2016-10-26T13:05:46.000+0000,2016-11-16T05:24:44.000+0000,2016-11-16T05:24:46.000+0000,,Fixed,New Feature,Major,['0.8-incubating'],['0.8-incubating'],,,,,,,,,Atlas to support KnoxSSO Authentication method.,Atlas to Support KnoxSSO Authentication,1,,,nixonrodrigues,True,nixonrodrigues,nixonrodrigues
atlas,ATLAS-1242,2016-10-24T23:50:04.000+0000,2016-11-18T19:27:42.000+0000,2016-11-18T19:27:43.000+0000,,Done,New Feature,Major,,['0.8-incubating'],,,,,,,,,"The current Types API (TypesResource) needs to route to the new Types API (TypesREST). 

",TypesResource API needs to use the new TypesREST API,1,,,sarath.kum4r@gmail.com,True,sarath.kum4r@gmail.com,sarath.kum4r@gmail.com
atlas,ATLAS-1234,2016-10-20T06:06:58.000+0000,2016-11-14T18:38:38.000+0000,2016-11-14T18:38:38.000+0000,,Done,New Feature,Major,,['0.8-incubating'],,,,,,,,,"Implement new Lineage REST API's to capture detailed information of lineage between DataSets for:

* Input Lineage
* Output Lineage
* Full Lineage",Lineage REST API v2 implementation,1,,,sarath.kum4r@gmail.com,True,sarath.kum4r@gmail.com,sarath.kum4r@gmail.com
atlas,ATLAS-1213,2016-10-05T11:06:43.000+0000,,2017-03-30T15:30:30.000+0000,,,New Feature,Major,,,,,,,,,,,"Whilst the Atlas docs have significant information on the API & type system, for Atlas to be seen as a foundation block in implementing information governance it's crucial we clarify some of the key terms that are used on the atlas web site/docs:


This should cover the definitions of

 - classification 
 - classification scheme
 - classification hierarchy
 - business term
 - business category
 - policies
 - rules
 - governance actions

There may be others too (perhaps post thoughts to this JIRA)

Where Atlas currently uses a different term that mapping should be explained, and indeed where a capability isn't addressed that should be mentioned too.

Consistent terminology will aid
 - adoption - readers will be more confident of the Atlas project direction
 - consistenct- we'll use consistent terms in presentations/discussions/APIs
 - new feature discussions - better domain understanding should make it easier to fill functional gaps


Many of these terms first came up in the mailing list discussion on ""Rename Trait to classification"", which relates to ATLAS-1187 - also viewable on gmane at news://news.gmane.org:119/nt0jdp$ml0$1@blaine.gmane.org",Doc: Explain key terms used in information governance,1,,,jonesn,True,,jonesn
atlas,ATLAS-1211,2016-10-05T09:03:53.000+0000,2018-05-17T15:57:20.000+0000,2018-05-17T15:57:20.000+0000,,Duplicate,New Feature,Major,['1.0.0'],,,,,,,,,,"Governance classifications may be ordered and contain sub-classifications.

For example a classification ""confidentiality"" may contain an *ordered* set of values such as
 - unclassified
 - internal use
 - confidential
 - sensitive
 - restricted
 - top secret

The ordering is a logical progression -- going from the least confidential to the most confidential. 

When these governance classifications are retrieved or checked against we may wish to performance range checks. For example in a ranger policy we may wish to do the equivilent of ""if confidentiality >= confidential then..""

Furthermore these classifications may be hierarchical. The mail thread below uses the example ""confidential/business confidential"" and ""confidential/personal confidential"". these are at an equivilent level, but different

Atlas should be extended to support this.(further JIRAs would be needed to update ranger)

In terms of implementation, For existing apis/tag integration we may consider a form of flattening or overloading the name to ease migration.

Note that this will prereq the anticipated work on ATLAS-1187 to introduce classifications, but I've split this further development out so that we can get a first pass done in 1187


This suggestion first came up in the mailing list discussion on ""Rename Trait to classification"", which relates to ATLAS-1187 - also viewable on gmane at news://news.gmane.org:119/nt0jdp$ml0$1@blaine.gmane.org",Classifications : Support subclasifications & ordering,1,,,jonesn,True,,jonesn
atlas,ATLAS-1202,2016-09-29T08:54:56.000+0000,,2016-09-29T08:54:56.000+0000,,,New Feature,Major,,,,,,,,,,,Currently Falcon hook captures only Submit entity . Update and Delete events should also be detected.,"Falcon hook to capture Falcon entity update,delete events.",1,,,ssainath,True,,ssainath
atlas,ATLAS-1186,2016-09-21T13:12:25.000+0000,,2019-06-27T14:12:41.000+0000,,,New Feature,Major,,['0.8-incubating'],,,,,,,,,"Add Glossary Category, which would have a name and description and be hung off a taxonomy. The category would contain (composition type) sub categories. Categories can contain terms.  ",Add Glossary Category,3,,['features'],davidrad,True,davidrad,davidrad
atlas,ATLAS-1174,2016-09-15T20:54:35.000+0000,2016-09-28T11:22:34.000+0000,2016-09-28T11:22:35.000+0000,,Fixed,New Feature,Major,['0.8-incubating'],,,,,,,,,,"1. Introduce ""version"" attribute to all types in the type-system, this helps to track changes made to the default types (hive, sqoop, falcon and storm types) and user created types. If version is not mentioned during creation of a type, default version ""1.0"" is assigned (optional attribute).

2. Using the version attributed for types, introduce a patch framework for type system. This framework applies patches to a type using its version number and can be used during upgrade - add new attributes to an existing types and it will be run during atlas startup. 

    The sequence of steps:

    a. During atlas startup, check $ATLAS_HOME/models/patches directory for any available patch files (json files). If there any patch files handle them.
    b. Sample patch json file looks like:
    {
      ""patches"": [
        { 
          ""action"": ""ADD_ATTRIBUTE"",
          ""typeName"": ""hive_column"",
          ""applyToVersion"": ""1.0"",
          ""updateToVersion"": ""2.0"",
          ""actionParams"": [
            { ""name"": ""position"",
              ""dataTypeName"": ""int"",
              ""multiplicity"": ""optional"",
              ""isComposite"": false,
              ""isUnique"": false,
              ""isIndexable"": false,
              ""reverseAttributeName"": null
            } ]
        } ]
    }

    c. The framework updates the type in ""typeName"" for the matching version number and after applying the patch, update the version to the one mentioned in ""updateToVersion""
    d. The json file can have more than one action (array of actions).
    e. There can be multiple patch json files in the directory and are applied in the sort order of the filename. eg:
        001-hive_column_add_position.json
        002-hive_column_add_anotherattribute.json",Framework to apply updates to types in the type-system,3,,"['feature', 'patch']",sarath.kum4r@gmail.com,True,sarath.kum4r@gmail.com,sarath.kum4r@gmail.com
atlas,ATLAS-1158,2016-09-06T16:48:16.000+0000,2017-11-22T07:17:11.000+0000,2018-04-23T09:54:58.000+0000,,Won't Fix,New Feature,Major,,['0.8-incubating'],,,,,,,,,"In RANGER-1137 ( https://issues.apache.org/jira/browse/RANGER-1137 ) there is a proposal to build ranger using docker in order to avoid environment issues. (though note it's a unix based shell script)

Some new users to Atlas have had difficulties building the code, so it could be a cool idea to do something similar for Atlas?

Happy to give it a go.... though I suspect some of the unit test issues with timeouts may still be hit.",Build ATLAS using Docker,2,,['VirtualDataConnector'],jonesn,True,zhangqiang2,jonesn
atlas,ATLAS-1109,2016-08-09T14:23:41.000+0000,,2016-11-21T06:24:36.000+0000,,,New Feature,Major,,,,,,,,,,,"Add DSL support for searching array and map attributes.  Perhaps there could be a contains predicate for searching array attributes for specific value(s), or searching a map attribute for specific key(s).",DSL support for Map and Array property search,3,,,guptaneeru,True,,guptaneeru
atlas,ATLAS-1101,2016-08-05T16:36:58.000+0000,2017-03-16T21:38:32.000+0000,2017-03-16T21:38:32.000+0000,,Fixed,New Feature,Major,"['0.8-incubating', '1.0.0']",,,,,,,,,,"The current style of the REST API has references everywhere there can be. I think that when we use this style of REST API we would need to constantly issue more RESt calls to find the referred to object.

For UIs a less chatty style of Rest API would be easier to use and more performant. I suggest having subobjects included in the responses rather than the reference (href) . I think we would need a paging capability to ensure that we do not get swamped with lots of data.     ",A less chatty REST API style required for UIs,2,,,davidrad,True,sarath.kum4r@gmail.com,davidrad
atlas,ATLAS-1095,2016-08-04T18:33:42.000+0000,2018-08-23T13:54:05.000+0000,2018-08-23T13:54:05.000+0000,,Fixed,New Feature,Major,,['1.0.0'],,,,,,,,,"Atlas provides a common approach to metadata management and governance across all systems and data within an organization. Today Atlas provides access to metadata. A connector provides access to a data source. As connectors are the proxy of all data, they can also be explicit providers of metadata.

This JIRA proposes an open connector framework to manage connectors that provide access to both data and the metadata Atlas provides together through a single connector interface.

This will help data tools to to better the exchange of information between platforms. It also offers new opportunities for the consistent enforcement of the governance policies and rules (e.g., rules of visibility). Source connector/connection metadata provides the nucleus around which all other metadata describing the data builds.

Full details of the OCF can be found on the Atlas wiki - [https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=69408729.]  This information superceeds the information in the design documents attached below.",Open connector framework,8,,['VirtualDataConnector'],stephanie,True,mandy_chessell,stephanie
atlas,ATLAS-1061,2016-07-27T10:09:47.000+0000,,2016-07-27T10:09:47.000+0000,,,New Feature,Major,,,,,,,,,,,Allow Terms to have Synonyms - other words by which they are known. This helps people refer to the same term but still have flexibility in what they like to refer to it as. ,Allow Terms to have Synonyms,1,,,davidrad,True,,davidrad
atlas,ATLAS-1055,2016-07-26T08:29:40.000+0000,,2016-07-26T08:29:40.000+0000,,,New Feature,Major,,,,,,,,,,,"It should be possible to create relationships between terms. This should include:
- ""has a"" relationships ( like the composition ones between assets)
- aggregation relationships 
- directional relationships (a reference)
-bidirectional relationships (for me this is the lowest priority as it is likely to more involved)

Like ALTAS-1054 is should be possible to map relationships to assets starting with the simplest case of the directional relationship. 
  ",Allow Term relationships to be defined ,1,,,davidrad,True,,davidrad
atlas,ATLAS-1054,2016-07-26T08:18:08.000+0000,,2016-07-26T16:24:04.000+0000,,,New Feature,Major,,,,,,,,,,,"I see that ATLAS-812 allows the user to associate terms with Assets. 

For governance use cases a natural way is to work with terms and assets is to work with the business glossary terms as the primary entities both in the Ui and in the REST API. 

I suggest the user be allowed :
- find the term of interest and then associate it with an asset
- be able to view terms and their associated assets in one REST GET term call. I suggest returning the assets as a subobjects (I suspect we will want to have a inquirylevel where 1 would just return the terms , and 2 would include subobjects.
- allow classifications to be specified against the terms.  I would like to see more intuitive words like term, asset and classification rather than tag , trait or trait name in the UI and API. 


  ",Allow Terms rather than assets to be the primary entities in the API and UI.,2,,,davidrad,True,,davidrad
atlas,ATLAS-872,2016-06-07T18:58:19.000+0000,,2019-03-29T14:27:18.000+0000,,,New Feature,Major,,['0.7-incubating'],,,,,,,,,"Atlas currently does not support multi tenancy.  As part of this feature, will add support to honor requests coming from multiple tenants. Individual Tenant data should remain isolated from one another. 

All the unique constraints should be applied per tenant and not globally. 
",Add Multitenancy support to Atlas,7,2,,guptaneeru,True,guptaneeru,guptaneeru
atlas,ATLAS-856,2016-06-03T00:54:09.000+0000,2016-06-22T05:27:59.000+0000,2016-06-22T06:00:29.000+0000,,Fixed,New Feature,Major,['0.7-incubating'],,,,,,,,,,"A modified version of the default local cache provider which will run a lookup on the backend storage if a requested type is not found in the cache, and load it in the cache and return it if it's found. This lazy-loading cache would cover the case where new types are registered, not the case where existing types are modified, while avoiding the deployment/maintenance and performance impact of a distributed cache in an HA environment.",Lazy-load type cache provider,3,,,dkantor,True,dkantor,dkantor
atlas,ATLAS-835,2016-05-27T18:48:44.000+0000,2016-06-20T06:52:29.000+0000,2016-06-20T06:52:30.000+0000,,Fixed,New Feature,Major,['0.7-incubating'],,,,,,,,,,"Currently only Falcon process registration is supported in Atlas. Extend this to support Falcon feed, replication feed, cluster entity addition and updates.

Here are the types to be defined:
{noformat}
falcon_cluster - models falcon cluster
falcon_feed - extends Dataset - models falcon feed
falcon_process - extends Process - models falcon process with inputs and outputs as falcon_feed
falcon_feed_creation - extends Process - input as hive_table or hdfs_path, output as falcon_feed
falcon_replication - extends Process - input as falcon_feed in source cluster, output as falcon_feed in target cluster
{noformat}",Falcon Integration with Atlas,5,,,sowmyaramesh,True,sowmyaramesh,sowmyaramesh
atlas,ATLAS-737,2016-05-02T21:19:08.000+0000,2016-12-06T21:07:28.000+0000,2016-12-06T21:07:29.000+0000,,Fixed,New Feature,Major,['0.8-incubating'],,,,,,,,,,"This can be implemented analogously to a ""select count *""  in SQL and is more relevant given the added ability to paginate a search result.

This also covers adding support for aggregate methods like max, min, count and sum with and without group by.","Add support for Sum, Max, Min and count operations with and without group by.",4,,,cassiodossantos,True,guptaneeru,cassiodossantos
atlas,ATLAS-638,2016-04-05T20:29:14.000+0000,,2016-06-14T10:03:38.000+0000,,,New Feature,Major,,,,,,,,,,,Need to differentiate between tables and HDFS paths,UI : Recognize and display HDFS Paths with a different notation,2,,,suma.shivaprasad,True,,suma.shivaprasad
atlas,ATLAS-599,2016-03-29T19:52:15.000+0000,2016-04-01T19:06:25.000+0000,2016-04-01T19:06:25.000+0000,,Fixed,New Feature,Major,['0.7-incubating'],['0.7-incubating'],,,,,,,,,Model HDFS paths as a DataSet so that lineage could be tracked for them,HDFS Path Model ,5,,,suma.shivaprasad,True,suma.shivaprasad,suma.shivaprasad
atlas,ATLAS-569,2016-03-15T20:25:21.000+0000,,2016-03-15T20:25:21.000+0000,,,New Feature,Major,,,,,,,,,,,"Model HDFS Paths needed for Hive, Falcon and storm hooks to inject the HDFS paths in lineage.",Model HDFS Paths,2,,,suma.shivaprasad,True,,suma.shivaprasad
atlas,ATLAS-561,2016-03-14T07:30:53.000+0000,,2016-06-02T05:13:04.000+0000,,,New Feature,Major,,,,,,,,,,,"We want to be able to measure the performance of the Atlas system that will help us evaluate different things:

* Performance of different storage backend choices of Titan
* Sizing & configuration tuning of storage backends
* Impact of new features coming in (any regressions caused)
* Impact of bug fixes or improvements (any regressions caused)

In order to do this, we need a simple way of testing Atlas end to end for various operations, primarily
* Creates and Updates of metadata entities.
* Queries of metadata entities.
* Combination of these activities.

What we need from a test data perspective could include the following things:
* Test data to generate metadata entities of different types - should cover basic entity creation, lineage being built, tags etc.
* A set of queries that can be considered typical of user behavior

This JIRA is to track support we can build into Atlas for the same.",Simple tools to add performance test support to Atlas,2,,,yhemanth,True,,yhemanth
atlas,ATLAS-541,2016-02-26T14:58:02.000+0000,2016-11-03T08:31:14.000+0000,2018-03-13T16:51:59.000+0000,,Fixed,New Feature,Major,['0.7-incubating'],,,,,,,,,,"We don't have graph versioning currently and hard deletes are not acceptable for data governance. This jira tracks the proposal for soft deletes which can mark an entity as deleted and by default search should return only active entities. However, there should be an option to retrieve deleted entities",Soft deletes,7,,,shwethags,True,shwethags,shwethags
atlas,ATLAS-510,2016-02-23T06:52:55.000+0000,,2016-02-23T11:11:42.000+0000,,,New Feature,Major,,,,,,,,,,,"There has been some work done at various levels to improve availability of Atlas. Architecturally, using highly available dependent components like HBase, Solr and Kafka allows Atlas to rely on the HA capabilities of these components (like replicas) to ensure it is not impacted by single machine failures. Integrating components (like Hive, Sqoop, Falcon, Storm) use Kafka to integrate with Atlas. In principle, this allows the integrating components to not be impacted due to loss of connectivity with Atlas while ensuring no loss of data.

However, there are still some important changes that the system requires for improving availability guarantees of Atlas. This JIRA is a master JIRA to track this work. I expect to open several child JIRAs to elaborate on the tasks and make it more manageable from a development / review perspective.",High availability of Atlas,7,,,yhemanth,True,,yhemanth
atlas,ATLAS-502,2016-02-16T20:46:07.000+0000,2016-02-25T08:38:22.000+0000,2016-02-25T08:38:22.000+0000,,Fixed,New Feature,Major,['0.7-incubating'],['0.7-incubating'],,,,,,,,,"In the left hand navigation of Atlas, where tags are displayed, there needs to be an ability to search for tags (in the event that there are too many tags to display). The functionality should be similar to what is found in the Ambari Hive view; see: https://monosnap.com/file/1aQ1uRwikvnjdhqKOSVvDfTv1ui2zl 

A user should be able to type in a partial query (no wildcards required) and the list of tags displayed should dynamically update based on search box contents.

The query should be executed against what is cached (vs. querying the backend). 





",UI: Provide the ability to search for tags,4,,,bergenholtz,True,Anilg,bergenholtz
atlas,ATLAS-501,2016-02-16T20:41:31.000+0000,,2016-06-14T07:42:42.000+0000,,,New Feature,Major,,['0.7-incubating'],,,,,,,,,There should be a question mark that provides a link to a document that explains the syntax. ,UI: Add link to document that explains the DSL syntax,4,,,bergenholtz,True,rohitl,bergenholtz
atlas,ATLAS-498,2016-02-16T14:46:26.000+0000,2016-04-08T05:09:02.000+0000,2016-04-11T18:46:43.000+0000,,Fixed,New Feature,Major,['0.7-incubating'],['0.7-incubating'],,,,,,,,,"If possible, Atlas should switch over to use embedded HBase. Initially, this is an investigative task which it if proves successful with result in switching default Atlas configuration/deployment to use embedded HBase.",Support Embedded HBase,6,1,,bergenholtz,True,tbeerbower,bergenholtz
atlas,ATLAS-497,2016-02-16T14:43:47.000+0000,2016-05-09T17:11:10.000+0000,2016-05-10T23:24:19.000+0000,,Fixed,New Feature,Major,['0.7-incubating'],['0.7-incubating'],,,,,,,,,"Atlas needs to support a simple (out of box) authorization mechanism.

Defined Roles:
- Data Scientist: provides a read only view (GET)
- Data Steward: provides a read/edit view (PUT, POST, DELETE)
- Admin (can do anything)

All can comment on entity

Requirements
- Atlas will implement a simple file based store for providing user to role mapping
- The out of box experience will be this file based mechanism for authorization",Simple Authorization,9,,['incompatibleChange'],bergenholtz,True,saqeeb.s,bergenholtz
atlas,ATLAS-495,2016-02-16T14:42:04.000+0000,2016-05-20T17:57:23.000+0000,2016-05-22T15:04:04.000+0000,,Fixed,New Feature,Major,['0.7-incubating'],['0.7-incubating'],,,,,,,,,"Atlas must support an Authorization Plugin that leverage Ranger:
Requirements
- The Ranger Plugin will authorize all user actions against security policies defined in Ranger
Example of Policies include:
- Restricting Access to specific actions based on user role i.e.
- Ability to Tag
- Ability to Delete
- Ability to View
- Ability to Modify / Update",Atlas to support pluggable authorization,5,,,bergenholtz,True,nixonrodrigues,bergenholtz
atlas,ATLAS-494,2016-02-16T14:40:47.000+0000,2016-04-14T11:24:30.000+0000,2016-04-14T11:24:30.000+0000,,Fixed,New Feature,Major,['0.7-incubating'],['0.7-incubating'],,,,,,,,,"Atlas needs an authentication mechanism besides kerberos.

Requirements
- Provide ability to authenticate against local file
- Provide Authentication that supports LDAP
- For consistency, configuration should use exactly the same configuration (properties) as Ranger & Knox

",UI Authentication ,6,,,bergenholtz,True,nixonrodrigues,bergenholtz
atlas,ATLAS-493,2016-02-16T14:38:47.000+0000,,2016-11-03T08:18:55.000+0000,,,New Feature,Major,,['0.7-incubating'],,,,,,,,,"General Use Case
Atlas must be able to track the evolution of assets as they change. Each evolution (or version) of each asset must be recorded in order to track lineage

Delete Use Case
If an asset is deleted, the asset should not show up searches unless explicitly searched for.

Requirements
Be able to show changes over time to be implemented through Audit Log
Requires API support to retrieve history",Entity Versioning Support - Audit only,3,,,bergenholtz,True,shwethags,bergenholtz
atlas,ATLAS-492,2016-02-16T14:37:44.000+0000,,2017-07-12T23:37:14.000+0000,,,New Feature,Major,,['0.7-incubating'],,,,,,,,,"h2. Requirements:
* Support ALTER statements (add/replace/rename cols)
* Support DROP statements ( support drop database, table, view)
* Support LOAD statements
* Remodelling changes
",Hive Hook Improvements,5,,,bergenholtz,True,,bergenholtz
atlas,ATLAS-491,2016-02-16T14:36:24.000+0000,2016-05-18T13:04:45.000+0000,2016-05-18T13:04:45.000+0000,,Fixed,New Feature,Major,['0.7-incubating'],['0.7-incubating'],,,,,,,,,"s a Atlas UX user, I need to:
Browse business taxonomy hieracharlically through tree
Search by Tags
Search by combination of keyword, tag, free text in search field
All search from view context of taxonomy. If 3 levels deep, then the search should be narrow to that scope.
expose search ordering capabilities available through SOLR.
As a Data Steward, I need to:
Manage the taxonomy of objects including
Objects Management:
Creating new objects
Modify the object details - annotations, external links, tags, acceptable use policy, etc
Delete objects (in UI - changing status in backend,and versioning changes)
Relationship management:
creating relationship of objects as a parent or child
assigning tagging to taxonomy
verify tag inheritance
Verify tag impact on tag based policy - ranger API call
Tag Management:
List
Search
Edit
Delete (change state to archive)
All user should be able to comment on any business catalog entity.",Business Catalog / Taxonomy,8,,,bergenholtz,True,jspeidel,bergenholtz
atlas,ATLAS-248,2015-10-23T14:31:04.000+0000,2015-10-29T08:10:06.000+0000,2015-10-29T08:10:06.000+0000,,Duplicate,New Feature,Major,,['0.5-incubating'],,,,,,,,,"We have a model implemented, with entities/Tags created for different classType definitions, is it possible to enhance the model (e.g. update classType definitions) without recreating those entities/tags? It seems to changes are only addictive for now.

Is an import/export feature going to help? export to JSON and then modify/update JSON and then import back?

This might be related to ATLAS-171, which was asking for the ability of updating exiting classType Definitions.",Enhance an implemented model,3,,,teeupdata,True,,teeupdata
atlas,ATLAS-247,2015-10-23T14:24:30.000+0000,2016-09-30T07:21:55.000+0000,2016-09-30T07:21:57.000+0000,,Fixed,New Feature,Major,['0.8-incubating'],['0.5-incubating'],,,,,,,,,"hive_column is not inherited from DataSet, thus can't be using hive_process to track column level lineages

Is there specific reason that hive_column is not inheriting from Data Set? ",Hive Column level lineage,6,,,teeupdata,True,rhbutani,teeupdata
atlas,ATLAS-172,2015-09-16T16:47:20.000+0000,2016-06-15T08:14:08.000+0000,2016-11-10T09:13:06.000+0000,,Fixed,New Feature,Critical,['0.7-incubating'],['0.6-incubating'],,,,,,,,,"It should be possible to be able to specify a description of a trait definition at creation time of the that trait; not just as the time the trait is associated with an entity.

The use case is:
A user creates multiple PII tags:
- PII-foo
- PII-bar
- PII-baz

Each tag should have an associated description with it so that the data curator (associating the tag to the entity) will know which is the appropriate tag to use. The name of the tag is often enough not enough information to go on.


",Be able to specify a description with a Trait definition,3,,,bergenholtz,True,guptaneeru,bergenholtz
atlas,ATLAS-164,2015-09-15T00:49:05.000+0000,2017-10-27T10:48:05.000+0000,2017-10-27T10:48:05.000+0000,,Won't Fix,New Feature,Major,,['0.6-incubating'],,,,,,,,,"Hi,
I have wrote an addon for sending DFS metadata into Atlas.
The patch is attached.
However, I have a hard time getting the unit tests working properly thus some advices would be welcome.
Thanks.
",DFS addon for Atlas,5,2,,rémy,True,rémy,rémy
atlas,ATLAS-156,2015-09-14T02:54:36.000+0000,,2016-11-10T09:08:09.000+0000,,,New Feature,Major,,['0.6-incubating'],,,,,,,,,Login Screen,Login Screen,6,,['patch-available'],Anilg,True,Anilg,Anilg
atlas,ATLAS-151,2015-09-10T19:18:24.000+0000,2015-11-30T18:20:04.000+0000,2015-11-30T18:20:04.000+0000,,Fixed,New Feature,Major,['0.6-incubating'],['0.6-incubating'],,,,,,,,,"UI: Make lineage graph Fit the window size 

All the node's to be visible on initial load",UI: Make lineage graph Fit the window size ,1,,,Anilg,True,,Anilg
atlas,ATLAS-150,2015-09-10T19:14:51.000+0000,,2016-11-10T09:08:19.000+0000,,,New Feature,Major,,['0.6-incubating'],,,,,,,,,"UI: Make lineage graph Reset Button

On the top right , On click Reset the graph to it's origin state",UI: Make lineage graph Reset Button,1,,,Anilg,True,,Anilg
atlas,ATLAS-149,2015-09-10T18:58:48.000+0000,2016-01-29T06:44:00.000+0000,2016-11-10T09:11:07.000+0000,,Duplicate,New Feature,Major,,['0.6-incubating'],,,,,,,,,"UI: Make lineage graph Zoom on Zoom-out on click of button on 

+

-

As google map's",UI: Make lineage graph Zoom on Zoom-out on click of button on [+ & - ],1,,,Anilg,True,,Anilg
atlas,ATLAS-138,2015-09-10T03:48:40.000+0000,2015-10-06T06:17:06.000+0000,2015-10-06T06:17:06.000+0000,,Fixed,New Feature,Major,['0.6-incubating'],['0.6-incubating'],,,,,,,,,"Combine the i/p and o/p graph so that there is only lineage graph. 
",Combine Input/Output graph,6,,['patch-available'],babanb,True,Anilg,babanb
atlas,ATLAS-127,2015-08-27T10:35:26.000+0000,,2015-09-01T09:48:47.000+0000,,,New Feature,Major,,,,,,,,,,,,Handle complex types in DSL queries,4,,,suma.shivaprasad,True,,suma.shivaprasad
atlas,ATLAS-123,2015-08-24T06:59:47.000+0000,,2015-08-24T07:00:01.000+0000,,,New Feature,Major,,,,,,,,,,,,"Add support for nested map, arrays in Entity",2,,,suma.shivaprasad,True,,suma.shivaprasad
atlas,ATLAS-122,2015-08-24T06:13:57.000+0000,2016-05-23T14:22:51.000+0000,2016-10-20T17:35:46.000+0000,,Fixed,New Feature,Major,['0.7-incubating'],,,,,,,,,,,Support for Deletion of Entities,8,,,suma.shivaprasad,True,dkantor,suma.shivaprasad
atlas,ATLAS-112,2015-08-20T01:31:24.000+0000,2015-09-11T22:53:33.000+0000,2015-09-11T22:53:33.000+0000,,Fixed,New Feature,Major,['0.6-incubating'],['0.6-incubating'],,,,,,,,,"The lineage should be extensible and provide ability to:
- Zoom
- Collapse Nodes",UI: Make lineage graph extensible for multiple nodes,3,,,bergenholtz,True,vkadam,bergenholtz
atlas,ATLAS-111,2015-08-20T01:27:44.000+0000,2015-09-11T22:53:16.000+0000,2015-09-11T22:53:16.000+0000,,Fixed,New Feature,Major,['0.6-incubating'],,,,,,,,,,"Create a help link for ATLAS that links to this document
https://cwiki.apache.org/confluence/display/ATLAS/Atlas+Home
",UI: Create Help Link,3,,,bergenholtz,True,vkadam,bergenholtz
atlas,ATLAS-108,2015-08-20T01:23:01.000+0000,,2016-11-10T09:08:23.000+0000,,,New Feature,Major,,['0.6-incubating'],,,,,,,,,To be detailed.,UI: integrate ng-inputs-tag library to select super types from types available for autocomeplete,1,,['ATLAS-UI-BUGS'],bergenholtz,True,rohitl,bergenholtz
atlas,ATLAS-107,2015-08-20T01:21:26.000+0000,,2016-11-10T09:08:21.000+0000,,,New Feature,Major,,['0.5-incubating'],,,,,,,,,"The Atlas UI needs to provide a means by which new Entities can be created associated with known SuperTypes such as DB, Table, LoadProcess and Column.",UI: Create Entities,3,,,bergenholtz,True,darshankumar89,bergenholtz
atlas,ATLAS-77,2015-07-16T18:15:29.000+0000,2015-07-21T16:53:11.000+0000,2016-01-05T05:50:08.000+0000,2015-07-17,Fixed,New Feature,Major,,['0.5-incubating'],,,,,,,,,"Create REST API to access all functions in Atlas.   Specifically the capabilities should include:

1    Create new types
2    Modify existing types
3    Create new relationships for all types (data and trait)
4    Remove relationships -- disassociate
5    Allow assignment of properties to all types
6    Allow search for type, entity, properties
7    Return search results - detail, lineage (relations)

These API's will be used for UI, HDP component and 3rd party interfaces.",REST API  to access all functions in Atlas,2,,,lgeorge,True,svenkat,lgeorge
atlas,ATLAS-76,2015-07-16T18:14:14.000+0000,2015-07-21T16:53:04.000+0000,2016-01-05T05:50:08.000+0000,2015-07-17,Fixed,New Feature,Major,,['0.5-incubating'],,,,,,,,,"Create an agile facility to model a business organizational taxonomy.   This logical model would include the following capabilities:

1   Allow the creation a hierarchical model 
2   Allow the creation of properties to any object in the model
3   Have inheritance capabilites so that child objects will get the same attributes (association) as the parent object.
4   Allow relating business logical model to data objects such as a Hive table, Kafka topic or Storm topology.
5   Be able to create (import) and report (export) on structures in bulk or ad hoc.


 
",Atlas business classification,2,,,lgeorge,True,svenkat,lgeorge
atlas,ATLAS-75,2015-07-16T18:11:42.000+0000,2015-07-21T16:52:57.000+0000,2016-01-05T05:50:09.000+0000,2015-07-17,Fixed,New Feature,Major,,['0.5-incubating'],,,,,,,,,"""Native"" Atlas connector for Hive.

This should be a jar file - or a simple install- to add to Hive to allow complete capture for Hive metadata will Atlas.   

1) Should require no configuration from either Hive or Altas
2) Have bootstrapping capability to get an initial synch. Should be able to re-run in the event of DR situations.
3) Post - Execution Hive Hook to capture incremental changes.   This should not impact Hive performance or reliability.
",Atlas Hive integration,1,,,lgeorge,True,svenkat,lgeorge
atlas,ATLAS-73,2015-07-16T06:45:14.000+0000,2015-12-14T03:18:12.000+0000,2015-12-14T03:18:12.000+0000,,Fixed,New Feature,Major,['0.6-incubating'],,,,,,,,,,"We need notification framework for two purposes:
1. The atlas hook in the external components can post the entities to be registered as a message so that the processing in hook is minimal and the entity registration is reliable
2. Need notifications on type and entity mutations which can be used in policy engines",Notification framework,1,,,shwethags,True,shwethags,shwethags
atlas,ATLAS-69,2015-07-14T16:08:00.000+0000,2015-12-09T04:59:47.000+0000,2015-12-09T04:59:48.000+0000,,Duplicate,New Feature,Major,,,,,,,,,,,"Add a falcon plugin to consume falcon entities like cluster, feed, process into ATLAS.",Add Falcon Bridge and Hook for Atlas,3,,,suma.shivaprasad,True,,suma.shivaprasad
atlas,ATLAS-55,2015-07-08T09:57:33.000+0000,,2015-07-08T09:59:31.000+0000,,,New Feature,Major,,,,,,,,,,,"To make it easier for users to do a DSL query,  we could add support for a query editor which does syntax highlighting and autocomplete for types, traits ..for eg: if user types from, we could display autocomplete list of types and when he types ""is"", could do an autocomplete for traits.

Syntax highligting will help users to detect syntax errors while typing instead of firing the query and then finding some issue with the query.

",Search Query Editor with syntax highlighting and autocomplete,1,,,suma.shivaprasad,True,,suma.shivaprasad
atlas,ATLAS-47,2015-07-06T13:28:52.000+0000,2015-12-08T06:42:11.000+0000,2015-12-09T14:24:57.000+0000,,Fixed,New Feature,Major,['0.6-incubating'],['0.6-incubating'],,,,,,,,,,Entity mutations for complex types,3,,,suma.shivaprasad,True,suma.shivaprasad,suma.shivaprasad
atlas,ATLAS-28,2015-06-18T14:28:33.000+0000,2015-06-18T14:35:53.000+0000,2015-06-18T14:35:53.000+0000,,Invalid,New Feature,Minor,,,,,,,,,,,"It would be nice to have a way to manage client interactions (install clients, refresh client configurations, etc.) at a host level grouping.  Currently you are limited to installing/refreshing clients at the individual host view.  Providing the ability to do this at the Host menu by selecting some or all hosts would be very beneficial. ",Ambari - Client Host Actions,,,,mbsharp85,True,,mbsharp85
atlas,ATLAS-5,2015-06-15T06:27:09.000+0000,,2015-08-19T23:15:36.000+0000,,,New Feature,Major,,,,,,,,,,,MS SQL Adventure works exposes a relational sample data model which would be great to showcase on ATLAS examples. ,Atlas - Create Sample Data Model from MS SQL Adventure Works,1,,,suma.shivaprasad,True,tbeerbower,suma.shivaprasad
batik,BATIK-1104,2015-01-31T14:55:58.000+0000,,2018-05-16T09:45:53.000+0000,,,New Feature,Major,,['1.7'],,,,,,,['SVG Rasterizer'],['Utility application to transfor SVG images into raster images.'],"PNGTranscoder p = new PNGTranscoder();
p.transcode(input, output);

suggest the ability to suppress errors so if the transcoder was not able to convert it only throws an exception without reporting the error in the output stream i.e  the system.out",ability to suppress validation errors or redirect any output from validation error reporting to a different stream,1,,,aosama,True,,aosama
batik,BATIK-1093,2014-11-13T04:29:27.000+0000,2017-04-11T13:33:14.000+0000,2017-04-11T13:33:22.000+0000,,Duplicate,New Feature,Major,,,,,,,,,,,"An oft requested feature in SVG is the ability to draw a background color on text and tspan. While it has been suggested that this can be done using a separate rect element, it is either impractical or highly inconvenient to do this because the bounding box and/or line height geometry of this rect is not available at authoring time.

Given the greater use of CSS features in SVG 2.0 and the general desire to support CSS properties, it is proposed that the 'background-color' [1] property be supported on both text and tspan elements.

[1] http://www.w3.org/TR/CSS2/colors.html#background-properties

In addition, it is desirable to allow the author to customize the presentation of the background decoration by distinguishing between the use of bounding box versus line height in order to control the block progression dimension. Further, it is desirable to allow the author to outset or inset the edges of the background decoration.

In order to support these latter two features, it is proposed that the following additional properties be supported:

* background-mode, with values 'bbox' or 'line-height', default of line-height

* background-outline-{top,right,bottom,left} takes <length> value, where positive is outset and negative is inset, and default is 0

* background-outline, a shorthand for the above, with value syntax <length>{1,4}, where 1 value expresses out(in)set on all four edges, 2 values express out(in)set on top/bottom and right/left, 3 values express out(in)set on top, left/right, bottom, and 4 values express out(in)set on top, right, bottom, left

This background outline operates similarly to the CSS outline-width property [2], except that (1) the outline color is set to the background color, (2) the outline style is solid, and (3) the outline widths can be independently specified for each edge (instead of the same value applying to all edges). In addition, this outline is drawn adjacent to the edges of the background rectangle determined by the 'background-mode' property, i.e., bounding box or line height.

[2] http://www.w3.org/TR/CSS21/ui.html#propdef-outline-width

It might be argued that this background-outline is equivalent to the CSS 'border' property semantics. We do not take that position here because it may be desirable to add border or padding support to the SVG text and tspan elements in the future, and we don't wish to influence the possible semantics of doing so. If border and padding were to be added in the future, then the background-outline would effectively intersect with the border or padding regions, but be drawn before, not after (as is done with the CSS outline property); i.e., the drawing order should be background with background outline, text with decorations, then borders (if added in future), then CSS outline (if added in future).
",Add support for background on text content elements,1,,,gadams,True,gadams,gadams
batik,BATIK-1092,2014-11-13T04:27:18.000+0000,,2018-05-16T09:45:53.000+0000,,,New Feature,Major,,,,,,,,,,,"An oft requested feature in SVG is the ability to draw a background color on text and tspan. While it has been suggested that this can be done using a separate rect element, it is either impractical or highly inconvenient to do this because the bounding box and/or line height geometry of this rect is not available at authoring time.

Given the greater use of CSS features in SVG 2.0 and the general desire to support CSS properties, it is proposed that the 'background-color' [1] property be supported on both text and tspan elements.

[1] http://www.w3.org/TR/CSS2/colors.html#background-properties

In addition, it is desirable to allow the author to customize the presentation of the background decoration by distinguishing between the use of bounding box versus line height in order to control the block progression dimension. Further, it is desirable to allow the author to outset or inset the edges of the background decoration.

In order to support these latter two features, it is proposed that the following additional properties be supported:

* background-mode, with values 'bbox' or 'line-height', default of line-height

* background-outline-{top,right,bottom,left} takes <length> value, where positive is outset and negative is inset, and default is 0

* background-outline, a shorthand for the above, with value syntax <length>{1,4}, where 1 value expresses out(in)set on all four edges, 2 values express out(in)set on top/bottom and right/left, 3 values express out(in)set on top, left/right, bottom, and 4 values express out(in)set on top, right, bottom, left

This background outline operates similarly to the CSS outline-width property [2], except that (1) the outline color is set to the background color, (2) the outline style is solid, and (3) the outline widths can be independently specified for each edge (instead of the same value applying to all edges). In addition, this outline is drawn adjacent to the edges of the background rectangle determined by the 'background-mode' property, i.e., bounding box or line height.

[2] http://www.w3.org/TR/CSS21/ui.html#propdef-outline-width

It might be argued that this background-outline is equivalent to the CSS 'border' property semantics. We do not take that position here because it may be desirable to add border or padding support to the SVG text and tspan elements in the future, and we don't wish to influence the possible semantics of doing so. If border and padding were to be added in the future, then the background-outline would effectively intersect with the border or padding regions, but be drawn before, not after (as is done with the CSS outline property); i.e., the drawing order should be background with background outline, text with decorations, then borders (if added in future), then CSS outline (if added in future).
",Add support for background on text content elements,3,1,,gadams,True,gadams,gadams
batik,BATIK-1066,2014-02-12T13:30:01.000+0000,,2014-08-05T20:38:04.000+0000,,,New Feature,Minor,,,,,,,,,,,"I am migrating an application from Websphere V6 to Websphere V7 on a z/OS plateform.
In this application i use the batik- all-1.5b2.jar in websphere V6.
But in Websphere V7, during deployment i get an error message:
NestedJarException: IWAE0008E An error occurred reading WEB-         
INF/lib/batik- all-1.5b2.jar from infosphere.war                     
 Stack trace of nested exception:                                    
 java.util.zip.ZipException: invalid entry size (expected 2472738816 but got 48 bytes)                                                    

My question is: is my batik- all-1.5b2.jar compatible with SDK 1.6?
If not what is the minimum level for SDK 1.6 available on Websphere V7?
Thanks for your help. ",Batik / SDk compatibility,3,1,['features'],dpoyet,True,,dpoyet
batik,BATIK-1044,2013-05-06T21:28:59.000+0000,,2013-05-08T07:58:14.000+0000,,,New Feature,Major,,['1.8'],,,,,,,,,A patch will follow...,SVGGraphics2D does not implement getDeviceConfiguration(),1,,,luc.girardin,True,,luc.girardin
commons-bcel,BCEL-322,2019-07-06T14:27:09.000+0000,2019-07-06T14:31:14.000+0000,2019-07-06T14:31:14.000+0000,,Fixed,New Feature,Major,['6.4.0'],,,,,,,,,,"Add constants to {{org.apache.bcel.Const}} for Java 14:

- org.apache.bcel.Const.MAJOR_14
- org.apache.bcel.Const.MINOR_14 

Checked with:
{quote}openjdk version ""14-ea"" 2020-03-17
OpenJDK Runtime Environment (build 14-ea+3-65)
OpenJDK 64-Bit Server VM (build 14-ea+3-65, mixed mode, sharing){quote}",Add constants to org.apache.bcel.Const for Java 14 ,1,,,ggregory,True,,ggregory
commons-bcel,BCEL-318,2019-06-03T11:40:59.000+0000,2019-07-06T14:53:00.000+0000,2019-07-06T14:53:00.000+0000,,Fixed,New Feature,Major,['6.4.0'],,,,,,,,,,"Add {{org.apache.bcel.classfile.ConstantUtf8.clearCache()}}.

There was previously no way to clear the cache.",Add org.apache.bcel.classfile.ConstantUtf8.clearCache().,1,,,ggregory,True,ggregory,ggregory
commons-bcel,BCEL-305,2018-07-28T05:02:27.000+0000,2018-07-28T15:36:23.000+0000,2018-07-28T15:36:23.000+0000,,Fixed,New Feature,Major,['6.3'],,,,,,,,,,See BCEL-304,ClassPath.getClassFile() and friends do not work with JRE 9 and higher,1,,,ggregory,True,ggregory,ggregory
commons-bcel,BCEL-301,2017-11-27T18:53:20.000+0000,2017-11-27T18:58:41.000+0000,2017-11-27T18:59:11.000+0000,,Fixed,New Feature,Major,['6.2'],['6.1'],,,,,,,,,"The method {{org.apache.bcel.classfile.Constant.readConstant()}} should not blow up on a Java 9 package reference.
{noformat}
org.apache.bcel.classfile.ClassFormatException: Invalid byte tag in constant pool: 20
    at org.apache.bcel.classfile.Constant.readConstant (Constant.java:169)
    at org.apache.bcel.classfile.ConstantPool.<init> (ConstantPool.java:66)
    at org.apache.bcel.classfile.ClassParser.readConstantPool (ClassParser.java:239)
    at org.apache.bcel.classfile.ClassParser.parse (ClassParser.java:144)
    at org.apache.maven.shared.jar.classes.JarClassesAnalysis.analyze (JarClassesAnalysis.java:96)
    at org.apache.maven.report.projectinfo.dependencies.Dependencies.getJarDependencyDetails (Dependencies.java:259)
    at org.apache.maven.report.projectinfo.dependencies.renderer.DependenciesRenderer.hasSealed (DependenciesRenderer.java:1542)
    at org.apache.maven.report.projectinfo.dependencies.renderer.DependenciesRenderer.renderSectionDependencyFileDetails (DependenciesRenderer.java:545)
    at org.apache.maven.report.projectinfo.dependencies.renderer.DependenciesRenderer.renderBody (DependenciesRenderer.java:240)
    at org.apache.maven.reporting.AbstractMavenReportRenderer.render (AbstractMavenReportRenderer.java:83)
    at org.apache.maven.report.projectinfo.DependenciesReport.executeReport (DependenciesReport.java:201)
    at org.apache.maven.reporting.AbstractMavenReport.generate (AbstractMavenReport.java:255)
    at org.apache.maven.plugins.site.render.ReportDocumentRenderer.renderDocument (ReportDocumentRenderer.java:229)
    at org.apache.maven.doxia.siterenderer.DefaultSiteRenderer.render (DefaultSiteRenderer.java:337)
    at org.apache.maven.plugins.site.render.SiteMojo.renderLocale (SiteMojo.java:178)
    at org.apache.maven.plugins.site.render.SiteMojo.execute (SiteMojo.java:132)
    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:134)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:208)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:154)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:146)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:117)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:81)
    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:51)
    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:128)
    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:309)
    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:194)
    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:107)
    at org.apache.maven.cli.MavenCli.execute (MavenCli.java:955)
    at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:290)
    at org.apache.maven.cli.MavenCli.main (MavenCli.java:194)
    at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke (Method.java:498)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:289)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:229)
    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:415)
    at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:356)
{noformat}",org.apache.bcel.classfile.ClassFormatException: Invalid byte tag in constant pool: 20,1,,,ggregory,True,,ggregory
commons-bcel,BCEL-300,2017-11-27T18:50:47.000+0000,2017-11-27T18:53:09.000+0000,2017-11-27T18:59:11.000+0000,,Fixed,New Feature,Major,['6.2'],['6.1'],,,,,,,,,"The method {{org.apache.bcel.classfile.Constant.readConstant()}} should not blow up on a Java 9 module reference.
{noformat}
org.apache.bcel.classfile.ClassFormatException: Invalid byte tag in constant pool: 19
    at org.apache.bcel.classfile.Constant.readConstant (Constant.java:169)
    at org.apache.bcel.classfile.ConstantPool.<init> (ConstantPool.java:66)
    at org.apache.bcel.classfile.ClassParser.readConstantPool (ClassParser.java:239)
    at org.apache.bcel.classfile.ClassParser.parse (ClassParser.java:144)
    at org.apache.maven.shared.jar.classes.JarClassesAnalysis.analyze (JarClassesAnalysis.java:96)
    at org.apache.maven.report.projectinfo.dependencies.Dependencies.getJarDependencyDetails (Dependencies.java:259)
    at org.apache.maven.report.projectinfo.dependencies.renderer.DependenciesRenderer.hasSealed (DependenciesRenderer.java:1542)
    at org.apache.maven.report.projectinfo.dependencies.renderer.DependenciesRenderer.renderSectionDependencyFileDetails (DependenciesRenderer.java:545)
    at org.apache.maven.report.projectinfo.dependencies.renderer.DependenciesRenderer.renderBody (DependenciesRenderer.java:240)
    at org.apache.maven.reporting.AbstractMavenReportRenderer.render (AbstractMavenReportRenderer.java:83)
    at org.apache.maven.report.projectinfo.DependenciesReport.executeReport (DependenciesReport.java:201)
    at org.apache.maven.reporting.AbstractMavenReport.generate (AbstractMavenReport.java:255)
    at org.apache.maven.plugins.site.render.ReportDocumentRenderer.renderDocument (ReportDocumentRenderer.java:229)
    at org.apache.maven.doxia.siterenderer.DefaultSiteRenderer.render (DefaultSiteRenderer.java:337)
    at org.apache.maven.plugins.site.render.SiteMojo.renderLocale (SiteMojo.java:178)
    at org.apache.maven.plugins.site.render.SiteMojo.execute (SiteMojo.java:132)
    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:134)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:208)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:154)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:146)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:117)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:81)
    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:51)
    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:128)
    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:309)
    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:194)
    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:107)
    at org.apache.maven.cli.MavenCli.execute (MavenCli.java:955)
    at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:290)
    at org.apache.maven.cli.MavenCli.main (MavenCli.java:194)
    at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke (Method.java:498)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:289)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:229)
    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:415)
    at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:356)
{noformat}",org.apache.bcel.classfile.ClassFormatException: Invalid byte tag in constant pool: 19,1,,,ggregory,True,,ggregory
commons-bcel,BCEL-292,2017-09-12T16:39:55.000+0000,2017-09-12T16:54:44.000+0000,2017-09-18T09:12:28.000+0000,,Fixed,New Feature,Major,['6.1'],,,,,,,,,,"First pass at Java 9 support based on EA releases and http://cr.openjdk.java.net/~mr/jigsaw/spec/lang-vm.html#jigsaw-2.6

The motivation is primarily to provide the minimum required by Tomcat to parse Java 9 class files.  

I have assumed (early access) Java 9 support will make the next release 6.1",Add minimal Java 9 support,1,,,britter,True,,britter
commons-bcel,BCEL-224,2015-07-26T23:18:03.000+0000,,2015-07-26T23:18:03.000+0000,,,New Feature,Minor,,['6.0'],,,,,,,,,BCELifier does not support BootstrapMethod Annotations.  Nor is there a BootstrapMethodGen class.,BCELifier does not support BootstrapMethods,1,,,chonton,True,,chonton
commons-bcel,BCEL-223,2015-07-26T23:16:36.000+0000,,2015-07-26T23:16:36.000+0000,,,New Feature,Minor,,,,,,,,,,,BCELifier ignores both RuntimeVisibleAnnotations and RUntimeInvisibleAnnotations.,BCELifier does not support Annotations,1,,,chonton,True,,chonton
beam,BEAM-7746,2019-07-15T18:42:59.000+0000,,2019-07-15T18:44:43.000+0000,,,New Feature,Major,,,600,600,,,,100,['sdk-py-core'],['SDKs: Python'],"As a developer of the beam source code, I would like the code to use pep484 type hints so that I can clearly see what types are required, get completion in my IDE, and enforce code correctness via a static analyzer like mypy.

This may be considered a precursor to BEAM-7060

Work has been started here:  [https://github.com/apache/beam/pull/9056]

 

 ",Add type hints to python code,1,,,chadrik,True,,chadrik
beam,BEAM-7741,2019-07-14T07:19:53.000+0000,,2019-07-15T12:39:55.000+0000,,,New Feature,Major,,,,,,,,,['sdk-py-core'],['SDKs: Python'],Currently SetState is missing from Python Sdks but it is existing in Java sdks. ,Add SetState in Python sdk,1,,,rakeshkumar,True,rakeshkumar,rakeshkumar
beam,BEAM-7740,2019-07-14T07:18:29.000+0000,,2019-07-15T12:35:56.000+0000,,,New Feature,Major,,,,,,,,,['sdk-py-core'],['SDKs: Python'],Currently MapState is missing from Python Sdks but it is existing in Java sdks. ,Add MapState in Python sdk,1,,,rakeshkumar,True,rakeshkumar,rakeshkumar
beam,BEAM-7739,2019-07-14T06:11:26.000+0000,,2019-07-16T03:27:02.000+0000,,,New Feature,Major,,,3000,3000,,,,100,['sdk-py-core'],['SDKs: Python'],Currently ValueState is missing from Python Sdks but it is existing in Java sdks. ,Add ValueState in Python sdk,1,,,rakeshkumar,True,rakeshkumar,rakeshkumar
beam,BEAM-7738,2019-07-13T20:31:12.000+0000,,2019-07-15T12:40:25.000+0000,,,New Feature,Major,,,,,,,,,"['io-java-gcp', 'runner-flink', 'sdk-py-core']","['IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)', 'Runners: Flink runner', 'SDKs: Python']",Now that KafkaIO is supported via the external transform API (BEAM-7029) we should add support for PubSub.,Support PubSubIO to be configured externally for use with other SDKs,1,,['portability'],chadrik,True,,chadrik
beam,BEAM-7730,2019-07-12T02:32:47.000+0000,,2019-07-12T20:07:46.000+0000,,,New Feature,Major,['2.15.0'],,,,,,,,['runner-flink'],['Runners: Flink runner'],"Apache Flink 1.9 will coming and it's better to add Flink 1.9 build target and make Flink Runner compatible with Flink 1.9.
I will add the brief changes after the Flink 1.9.0 released. 

And I appreciate it if you can leave your suggestions or comments!",Add Flink 1.9 build target and Make FlinkRunner compatible with Flink 1.9,3,,,sunjincheng121,True,sunjincheng121,sunjincheng121
beam,BEAM-7728,2019-07-11T23:19:21.000+0000,,2019-07-15T12:35:48.000+0000,,,New Feature,Major,,,3600,3600,,,,100,['dsl-sql'],['DSLs: SQL'],,Support ParquetTable in SQL,1,,,vectorijk,True,vectorijk,vectorijk
beam,BEAM-7718,2019-07-10T20:19:31.000+0000,2019-07-12T14:01:30.000+0000,2019-07-12T14:01:30.000+0000,,Done,New Feature,Major,['2.14.0'],,,,,,,,['io-java-gcp'],"['IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)']","{quote}The default implementation uses the HTTP REST API, which seems to be much less performant than the gRPC implementation.  Is there a reason that the gRPC implementation is essentially unavailable from the public API?  PubsubIO.Read.withClientFactory is package private.  I worked around this by making it public and rebuilding{quote}",PubsubIO to use gRPC API instead of JSON REST API,1,,,kenn,True,,SteveNiemitz
beam,BEAM-7711,2019-07-09T19:18:02.000+0000,,2019-07-15T18:56:16.000+0000,,,New Feature,Major,,,5400,5400,,,,100,['dsl-sql'],['DSLs: SQL'],"DATETIME as a type represents a year, month, day, hour, minute, second, and subsecond(millis)

it ranges from 0001-01-01 00:00:00 to 9999-12-31 23:59:59.999.",Support DATETIME as a logical type in BeamSQL,1,,,amaliujia,True,,amaliujia
beam,BEAM-7692,2019-07-04T19:16:58.000+0000,,2019-07-15T13:31:32.000+0000,,,New Feature,Major,,['2.13.0'],,,,,,,"['io-ideas', 'io-java-text']","['IO: ideas (proposals for new connectors, new designs, etc)', 'IO: Java: TextIO']","As a beam user

when using any of the existing beams I/O transforms

I would like to add an index to each line read as part of the transform.

 

As spark has zipWithIndex to assign an index when reading files with the beam being abstraction layer for may runners. I would expect this feature should be added to beam ",Create a transform to assign an index when reading data from built in I/O transforms,1,,['beam'],saidivi,True,,saidivi
beam,BEAM-7685,2019-07-03T17:14:27.000+0000,,2019-07-11T17:05:13.000+0000,,,New Feature,Major,,,,,,,,,['dsl-sql'],['DSLs: SQL'],"Query Parser does not reorder joins based on their costs. The fix is simple we need to include the rules that are related to reordering joins such as JoinCommuteRule. However, reordering joins may produce plans that has Cross Join or other not supported types of join. We should either rewrite those rules to consider that or return infinite cost for those types of joins so that they will not be selected",Join Reordering,1,,,riazela,True,riazela,riazela
beam,BEAM-7626,2019-06-24T19:46:46.000+0000,,2019-07-02T09:11:03.000+0000,,,New Feature,Major,,,,,,,,,['java-fn-execution'],[''],Current implementation of ExecutableStage only accepts one input PColletion: https://github.com/apache/beam/blob/5ee4cf4e4880782492ec26f2b454a6df9b25f1e2/model/pipeline/src/main/proto/beam_runner_api.proto#L1247. But it's possible that a ExecutableStage has multiple inputs.,ExecutableStage should be able to accept multiple input PCollection ,2,,,boyuanz,True,,boyuanz
beam,BEAM-7577,2019-06-17T14:58:52.000+0000,,2019-07-04T11:16:19.000+0000,,,New Feature,Minor,,['2.13.0'],15000,15000,,,,100,['io-python-gcp'],['IO: Python: Google Cloud Platform'],"The current implementation of ReadFromDatastore does not support specifying the query parameter at runtime. This could potentially be fixed through the usage of a ValueProvider to specify and build the Datastore query.

Allowing specifying the query at runtime makes it easier to use dynamic queries in Dataflow templates. Currently, there is no way to have a Dataflow template that includes a dynamic query (such as filtering by a timestamp or similar).",Allow the use of ValueProviders in datastore.v1new.datastoreio.ReadFromDatastore query,1,,,EDjur,True,EDjur,EDjur
beam,BEAM-7576,2019-06-17T13:06:20.000+0000,,2019-06-19T05:38:03.000+0000,,,New Feature,Major,,,,,,,,,['runner-flink'],['Runners: Flink runner'],"Hey all,

 

It will be really helpful if beam automatically push the flink job-server-container to hub.docker

 

Thanks!.",Automatically push flink job-server-container to hub.docker,1,,,chethanuk,True,,chethanuk
beam,BEAM-7555,2019-06-13T22:29:59.000+0000,,2019-06-26T12:39:58.000+0000,,,New Feature,Major,,,,20400,,,,,['io-java-aws'],['IO: Java: Amazon Web Services'],"[AWS SDK for Java 2 was released|https://github.com/aws/aws-sdk-java-v2/blob/master/docs/LaunchChangelog.md] last october and has many interesting features including cleaner APIs and support for HTTP client sharing and HTTP/2 in some APIs. So worth checking a migration path for AWS IOs.

Due to incompatibility APIs + Binary objects the best strategy is to incrementally migrate each existing IO into a new module e.g. `sdks/java/io/amazon-web-services2` and then start migrating services one by one. Once we achieve feature completeness we can deprecate the IOs based in the 'old' AWS SDK for Java version 1 and eventually remove them once the other are in a good shape.",Migrate AWS IOs to AWS SDK for Java 2,2,,,iemejia,True,cmach,iemejia
beam,BEAM-7545,2019-06-12T16:40:59.000+0000,2019-07-11T23:23:58.000+0000,2019-07-16T05:45:41.000+0000,,Done,New Feature,Major,['Not applicable'],,28800,28800,,,,100,['dsl-sql'],['DSLs: SQL'],Implementing Row Count Estimation for CSV Tables by reading the first few lines of the file and estimating the number of records based on the length of these lines and the total length of the file.,Row Count Estimation for CSV TextTable,1,,,riazela,True,riazela,riazela
beam,BEAM-7538,2019-06-12T10:01:45.000+0000,,2019-06-12T10:11:37.000+0000,,,New Feature,Minor,,,,,,,,,['io-java-hbase'],['IO: Java: HBase'],,Make HBaseIO write return PCollection<Void> so it can be used with Wait.on,1,,,iemejia,True,,iemejia
beam,BEAM-7513,2019-06-07T23:21:15.000+0000,2019-06-20T16:23:48.000+0000,2019-06-20T16:23:48.000+0000,,Implemented,New Feature,Major,['2.14.0'],,36600,36600,,,,100,"['dsl-sql', 'io-java-gcp']","['DSLs: SQL', 'IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)']","Calcite tables (org.apache.calcite.schema.Table) should implement the method org.apache.calcite.schema.Statistic getStatistic(). The Statistic instance returned by this method is used for the Volcano optimizer in Calcite. 

Currently, org.apache.beam.sdk.extensions.sql.impl.BeamCalciteTable has not implemented getStatistic() which means it uses the implementation in org.apache.calcite.schema.impl.AbstractTable and that implementation just returns Statistics.UNKNOWN for all sources.

 

Things needed to be implemented:

1- Implementing getStatistic in BeamCalciteTable such that it calls a row count estimation method from BeamSqlTable and adding this method to BeamSqlTable.

2- Implementing the row count estimation method for BigQueryTable. ",Row Estimation for BigQueryTable,1,,,riazela,True,riazela,riazela
beam,BEAM-7495,2019-06-05T17:03:28.000+0000,,2019-06-25T18:33:27.000+0000,,,New Feature,Major,,,12600,12600,1801800,1814400,1801800,,['io-java-gcp'],"['IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)']","Currently, the BigQuery connector for reading data using the BigQuery Storage API does not support any of the facilities on the source for Dataflow to split streams.

 

On the server side, the BigQuery Storage API supports splitting streams at a fraction. By adding support to the connector, we enable Dataflow to split streams, which unlocks dynamic worker re-balancing.",Add support for dynamic worker re-balancing when reading BigQuery data using Cloud Dataflow,1,,,aryann,True,aryann,aryann
beam,BEAM-7492,2019-06-05T00:58:02.000+0000,2019-06-07T08:15:50.000+0000,2019-06-07T09:07:00.000+0000,,Fixed,New Feature,Major,['2.14.0'],,5400,5400,,,,100,['sdk-go'],['SDKs: Go'],"I imagine this should be as easy as copying [1] and s/flink/spark.

[1] [https://github.com/apache/beam/blob/master/sdks/go/pkg/beam/runners/flink/flink.go]",Add Spark runner to Go SDK,1,,['portability-spark'],ibzib,True,ibzib,ibzib
beam,BEAM-7488,2019-06-04T17:21:11.000+0000,2019-06-20T17:57:30.000+0000,2019-06-20T17:57:30.000+0000,,Fixed,New Feature,Minor,['Not applicable'],,,,,,,,"['project-management', 'testing']","['Project Management: monthly reports, administrative tasks', 'Testing: general infrastructure']","Similar to [BEAM-7259|https://issues.apache.org/jira/browse/BEAM-7259], was hoping to get this metric in a view that's aggregated per month. In this case averaging all the queue times seems like it would work well. Even better if you could average only the 95th percentile.","Grafana request: Add monthly aggregated view for ""Precommit Test Latency - Time In Queue""",1,,,danoliveira,True,Ardagan,danoliveira
beam,BEAM-7484,2019-06-04T11:50:00.000+0000,,2019-07-15T10:51:12.000+0000,,,New Feature,Major,,,16800,16800,,,,100,['testing'],['Testing: general infrastructure'],The goal is to collect bytes/time and messages/time metrics in BQ read and write tests in Python SDK.,Throughput collection in BigQuery performance tests,1,,,kamilwu,True,kamilwu,kamilwu
beam,BEAM-7462,2019-05-30T21:53:26.000+0000,,2019-06-12T07:57:12.000+0000,,,New Feature,Major,,,600,600,,,,100,['java-fn-execution'],[''],,Add Sampled Byte Count Metric to the Java SDK,1,,['portable-metrics-bugs'],ajamato@google.com,True,lcwik,ajamato@google.com
beam,BEAM-7452,2019-05-29T22:33:48.000+0000,,2019-06-12T08:11:39.000+0000,,,New Feature,Major,,,,,,,,,['runner-core'],['Runners: shared functionality for all runners'],"Fixing this requires updating 4 locations.
 * Dataflow RunnerHarness
 * FNAPDoFnRunner
 * UnifiedWorker
 * Shared libraries for this proto generation, which should cover OSS runners
+ Remove the workaround in ProcessBundleHandler.java which will assume that all PCollections are bounded, if not set.

See PCollectionTranslation.fromProto which should be always passed a valid value and not default to error or assume the PCollection is bounded.

 

Context

===

When I was updating the java SDK to conditionally serialize some elements to reported a sampled byte size metric, I encountered this.

 
Its due to to the refactoring in my [PR/8416|https://github.com/apache/beam/pull/8416], the RehydratedComponents was pulled up a level, and shared now among all the calls to createRunnerForPTransform in the various PtransfomRunnerFactories.
 
I is now triggering some code paths which were not previously triggered for all types of PTransforms/PCollections, causing this error to occur.
 
 jsonPayload: {
  exception:  ""org.apache.beam.vendor.guava.v20_0.com.google.common.util.concurrent.UncheckedExecutionException: java.lang.IllegalArgumentException: Cannot convert unknown org.apache.beam.model.pipeline.v1.RunnerApi.IsBounded to org.apache.beam.sdk.values.PCollection.IsBounded: UNSPECIFIED
 at org.apache.beam.vendor.guava.v20_0.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2214)
 at org.apache.beam.vendor.guava.v20_0.com.google.common.cache.LocalCache.get(LocalCache.java:4053)
 at org.apache.beam.vendor.guava.v20_0.com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:4057)
 at org.apache.beam.vendor.guava.v20_0.com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4986)
 at org.apache.beam.runners.core.construction.RehydratedComponents.getPCollection(RehydratedComponents.java:144)
 at org.apache.beam.fn.harness.data.PCollectionConsumerRegistry.getMultiplexingConsumer(PCollectionConsumerRegistry.java:145)
 at org.apache.beam.fn.harness.DoFnPTransformRunnerFactory$Context.<init>(DoFnPTransformRunnerFactory.java:284)
 at org.apache.beam.fn.harness.DoFnPTransformRunnerFactory.createRunnerForPTransform(DoFnPTransformRunnerFactory.java:97)
 at org.apache.beam.fn.harness.DoFnPTransformRunnerFactory.createRunnerForPTransform(DoFnPTransformRunnerFactory.java:63)
 at org.apache.beam.fn.harness.control.ProcessBundleHandler.createRunnerAndConsumersForPTransformRecursively(ProcessBundleHandler.java:198)
 at org.apache.beam.fn.harness.control.ProcessBundleHandler.createRunnerAndConsumersForPTransformRecursively(ProcessBundleHandler.java:166)
 at org.apache.beam.fn.harness.control.ProcessBundleHandler.createRunnerAndConsumersForPTransformRecursively(ProcessBundleHandler.java:166)
 at org.apache.beam.fn.harness.control.ProcessBundleHandler.processBundle(ProcessBundleHandler.java:306)
 at org.apache.beam.fn.harness.control.BeamFnControlClient.delegateOnInstructionRequestType(BeamFnControlClient.java:160)
 at org.apache.beam.fn.harness.control.BeamFnControlClient.lambda$processInstructionRequests$0(BeamFnControlClient.java:144)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: Cannot convert unknown org.apache.beam.model.pipeline.v1.RunnerApi.IsBounded to org.apache.beam.sdk.values.PCollection.IsBounded: UNSPECIFIED
 at org.apache.beam.runners.core.construction.PCollectionTranslation.fromProto(PCollectionTranslation.java:88)
 at org.apache.beam.runners.core.construction.PCollectionTranslation.fromProto(PCollectionTranslation.java:56)
 at org.apache.beam.runners.core.construction.RehydratedComponents$3.load(RehydratedComponents.java:103)
 at org.apache.beam.runners.core.construction.RehydratedComponents$3.load(RehydratedComponents.java:93)
 at org.apache.beam.vendor.guava.v20_0.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3628)
 at org.apache.beam.vendor.guava.v20_0.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2336)
 at org.apache.beam.vendor.guava.v20_0.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2295)
 at org.apache.beam.vendor.guava.v20_0.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2208)
 ... 17 more
""   
  job:  ""2019-05-29_03_31_14-4799355109250203557""   
  logger:  ""org.apache.beam.fn.harness.control.BeamFnControlClient""   
  *message:  ""Exception while trying to handle InstructionRequest -28""*   
  portability_worker_id:  ""1""   
  thread:  ""16""   
  worker:  ""testpipeline-pabloem-0529-05290331-75o8-harness-htz8""   
 }
 
The root of the issue is that the ProcessBundleDescriptors are invalid. The RunnerHarnesses are not setting the org.apache.beam.model.pipeline.v1.RunnerApi.IsBounded which breaks the specification and leads to this error.
 
 ",Ensure all RunnerHarnesses provide a valid RunnerApi.IsBounded value on all PCollections,1,,['portable-metrics-bugs'],ajamato@google.com,True,,ajamato@google.com
beam,BEAM-7450,2019-05-29T20:22:13.000+0000,2019-06-14T20:31:45.000+0000,2019-06-14T20:31:46.000+0000,,Fixed,New Feature,Minor,['2.14.0'],,45600,45600,,,,100,['io-java-hcatalog'],['IO: Java: HCatalog'],"# Current version of HcatalogIO is a bounded source.
 # While migrating our jobs to aws, we realized that it would be helpful to have an unbounded hcat reader that can behave as an unbounded source and polls for new partitions as and when they become available.
 # I have used splittable pardo(s) to do this. There is a flag that can be set to treat this as a bounded source which will terminate if that flag is set.",Support unbounded reads with HCatalogIO,1,,,jhalarua,True,jhalarua,jhalarua
beam,BEAM-7445,2019-05-28T22:56:12.000+0000,,2019-06-17T11:59:19.000+0000,,,New Feature,Major,,['Not applicable'],5400,5400,,,,100,['website'],['Website: content and bugs'],"Beam website has a lot of information and it is not always easy to find what you're looking for.

In case of [documentation|https://beam.apache.org/documentation/] it often happens so that user knows what he's looking for, but can't can't find a way to navigate to relevant section.

Adding a search bar to website will make it much easier to navigate and search for relevant information.",Add search field to Beam website,3,1,"['documentation', 'usability']",Ardagan,True,aromanenko,Ardagan
beam,BEAM-7443,2019-05-28T18:11:27.000+0000,2019-06-03T20:49:58.000+0000,2019-06-03T20:49:58.000+0000,,Fixed,New Feature,Major,['2.14.0'],,17400,17400,,,,100,['sdk-py-core'],['SDKs: Python'],, BoundedSource->SDF needs a wrapper in Python SDK,1,,,boyuanz,True,boyuanz,boyuanz
beam,BEAM-7425,2019-05-25T07:40:39.000+0000,,2019-06-11T16:42:55.000+0000,,,New Feature,Major,,['2.12.0'],,,,,,,"['io-java-avro', 'io-java-gcp']","['IO: Java: Avro', 'IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)']","While Developing my code I used the below code snippet to read the table data from BigQuery.

 
{code:java}
PCollection<ReasonCode> gpseEftReasonCodes = input
      .apply(""Reading xxyyzz"", 
          BigQueryIO
                  .read(new ReadTable<ReasonCode>(ReasonCode.class))
                  .withoutValidation()
                  .withTemplateCompatibility()
                  .fromQuery(""Select * from dataset.xxyyzz"")
                  .usingStandardSql()
                  .withCoder(SerializableCoder.of(xxyyzz.class))
{code}

Read Table Class:

{code:java}

@DefaultSchema(JavaBeanSchema.class)
public class ReadTable<T> implements SerializableFunction<SchemaAndRecord, T> {
  private static final long serialVersionUID = 1L;
  private static Gson gson = new Gson();
  public static final Logger LOG = LoggerFactory.getLogger(ReadTable.class); private final Counter countingRecords = 
  Metrics.counter(ReadTable.class, ""Reading Records EFT Report"");
  private Class<T> class1;
  
  public ReadTable(Class<T> class1) { this.class1 = class1; }
 
  public T apply(SchemaAndRecord schemaAndRecord) {
    Map<String, String> mapping = new HashMap<>();
    int counter = 0;
    try {
      GenericRecord s = schemaAndRecord.getRecord();
      org.apache.avro.Schema s1 = s.getSchema();
      for (Field f : s1.getFields()) {
        counter++;
        mapping.put(f.name(), null==s.get(f.name()) ? null : String.valueOf(s.get(counter)));
      }
      countingRecords.inc();
      JsonElement jsonElement = gson.toJsonTree(mapping);
      return gson.fromJson(jsonElement, class1);
    } catch (Exception mp) {
      LOG.error(""Found Wrong Mapping for the Record: ""+mapping); mp.printStackTrace(); return null; }
    }
}

{code}

So After Reading the data from Bigquery I was mapping data from SchemaAndRecord to pojo I was getting value for columns whose Data type is Numeric mention below.

{code}
last_update_amount=java.nio.HeapByteBuffer[pos=0 lim=16 cap=16]
{code}

My Expectation was I will get exact value but getting the HyperByte Buffer the version I am using is Apache beam 2.12.0. If any more information is needed then please let me know.

Way 2 Tried:

{code:java}
GenericRecord s = schemaAndRecord.getRecord();
org.apache.avro.Schema s1 = s.getSchema();
for (Field f : s1.getFields()) {
  counter++;
  mapping.put(f.name(), null==s.get(f.name()) ? null : String.valueOf(s.get(counter)));
  if(f.name().equalsIgnoreCase(""reason_code_id"")) {
    BigDecimal numericValue = new Conversions.DecimalConversion()
       .fromBytes((ByteBuffer) s.get(f.name()), Schema.create(s1.getType()), s1.getLogicalType());
       System.out.println(""Numeric Con""+numericValue);
} else {
  System.out.println(""Else Condition ""+f.name());
}
{code}

Facing Issue:

{code}
2019-05-24 (14:10:37) org.apache.avro.AvroRuntimeException: Can't create a: RECORD
{code}
 

It would be Great if we have a method which maps all the BigQuery Data with Pojo Schema which Means if I have 10 Columns in BQ and in my Pojo I need only 5 Column then, in that case, BigQueryIO should map only that 5 Data values into Java Class and Rest will be Rejected As I am Doing After So much Effort. 
 Numeric Data Type must be Deserialize by itself while fetching data like TableRow.

 ",Reading BigQuery Table Data into Java Classes(Pojo) Directly,3,,,KishanK,True,,KishanK
beam,BEAM-7395,2019-05-23T01:26:30.000+0000,,2019-06-04T23:13:11.000+0000,,,New Feature,Minor,,,,,,,,,['sdk-py-core'],['SDKs: Python'],Users are going to mutate inputs and outputs of DoFn inappropriately. We should help their tests fail to catch such mistakes. (Similar to the DirectPipelineRunner in Java SDK) This should be offered as an option and worked based on sampling because of the cost of these checks.,Check immutability violations in Dataflow Runner (as an option),2,,"['newbie', 'starter']",altay,True,evindj,altay
beam,BEAM-7383,2019-05-21T21:01:21.000+0000,2019-05-29T22:48:26.000+0000,2019-05-29T22:48:26.000+0000,,Fixed,New Feature,Major,['Not applicable'],,3600,3600,,,,100,['sdk-go'],['SDKs: Go'],"With the Vet Runner added, add the ability to use it to verify the user's pipeline while using the direct or universal runner by enabling some kind of flag or option.",Add flag enabling vet runner verification for Universal and Direct runners.,1,,,danoliveira,True,danoliveira,danoliveira
beam,BEAM-7373,2019-05-20T22:39:53.000+0000,2019-05-21T18:43:26.000+0000,2019-05-21T18:43:26.000+0000,,Fixed,New Feature,Major,['Not applicable'],,1800,1800,,,,100,['sdk-go'],['SDKs: Go'],"The ""vet runner"" is a tool written by [~lostluck] internally for Google that is used to evaluate Beam pipelines to check if they are fulfilling the requirements needed to perform well. The following are some of the more straightforward things that the vet runner checks:
 * If types or functions need to be registered.
 * If shims have been generated for types/functions that need them.
 * Whether all identifiers in the pipeline were exported.
 * Based on the above, whether a pipeline is performant and, if not, what needs to be done to make it performant.

I did not write the original tool but I will be migrating it to the Beam repo so that it can be integrated into the universal runner and direct runner, probably with a flag.","Migrate Google code for Go SDK ""vet runner""",2,,,danoliveira,True,danoliveira,danoliveira
beam,BEAM-7364,2019-05-20T09:31:29.000+0000,2019-06-06T23:01:59.000+0000,2019-06-06T23:01:59.000+0000,,Fixed,New Feature,Major,['2.14.0'],,3000,3000,,,,100,['sdk-py-core'],['SDKs: Python'],,Possible signed overflow for WindowedValue.__hash__,1,,,robertwb,True,robertwb,robertwb
beam,BEAM-7358,2019-05-20T04:42:51.000+0000,,2019-06-07T09:42:44.000+0000,,,New Feature,Major,,,,,,,,,"['beam-model', 'sdk-ideas']","['Beam Model: general programming model concepts, semantics', 'SDKs: Coordination on new language-specific SDKs']","GroupByKey is useful, but it is difficult to express complex grouping conditions.

I propose the development of the function to group if there is a common term in even one of multiple keys.

Like this....

 

usage

 
{code:java}
List io = Arrays.asList(
KV.of(new MultiKey(""A"",""C"", ""E""),""0""),
 KV.of(new MultiKey(""A"",""D"", ""F""),""1""),
 KV.of(new MultiKey(""B"",""D"", ""G""),""2""),
 KV.of(new MultiKey(""H"",""J"", ""L""),""3""),
 KV.of(new MultiKey(""I"",""K"", ""L""),""4""),
 KV.of(new MultiKey(""M"",""C"", ""O""),""5""),
 KV.of(new MultiKey(null,null, ""O""),""6""),
 KV.of(new MultiKey(null,null, null),""7"")
);
p.apply(Create.of(io))
.apply(GroupByMultiKey.create(3))
.apply(new DebugPrintln());
{code}
 

 out put this
{quote} 

[KV\{[""A"",""C"",""E""],""0""},KV\{[""A"",""D"",""F""],""1""},KV\{[""B"",""D"",""G""],""2""},KV\{[""M"",""C"",""O""],""5""},KV\{[null,null,""O""],""6""}]

[KV\{[""H"",""J"",""L""],""3""},KV\{[""I"",""K"",""L""],""4""}]

[KV\{[null,null,null],""7""}]


 
{quote}","Developing  ""GroupByMultiKey""",1,,,naoki,True,naoki,naoki
beam,BEAM-7355,2019-05-18T00:19:21.000+0000,,2019-05-21T22:08:31.000+0000,,,New Feature,Major,,,,,,,,,['build-system'],"['Build, CI, release systems and processes']","[https://builds.apache.org/job/beam_PreCommit_Java_Commit/5989/testReport/junit/org.apache.beam.sdk.io.cassandra/CassandraIOTest/classMethod/]

 

[https://builds.apache.org/job/beam_PreCommit_Java_Commit/5989/testReport/junit/org.apache.beam.sdk.io.cassandra/CassandraIOTest/classMethod_2/]
h3. Error Message

com.datastax.driver.core.exceptions.NoHostAvailableException: All host(s) tried for query failed (tried: localhost/127.0.0.1:9042 (com.datastax.driver.core.exceptions.TransportException: [localhost/127.0.0.1:9042] Cannot connect))
h3. Stacktrace

com.datastax.driver.core.exceptions.NoHostAvailableException: All host(s) tried for query failed (tried: localhost/127.0.0.1:9042 (com.datastax.driver.core.exceptions.TransportException: [localhost/127.0.0.1:9042] Cannot connect)) at com.datastax.driver.core.ControlConnection.reconnectInternal(ControlConnection.java:268) at com.datastax.driver.core.ControlConnection.connect(ControlConnection.java:107) at com.datastax.driver.core.Cluster$Manager.negotiateProtocolVersionAndConnect(Cluster.java:1652) at com.datastax.driver.core.Cluster$Manager.init(Cluster.java:1571) at com.datastax.driver.core.Cluster.init(Cluster.java:208) at com.datastax.driver.core.Cluster.connectAsync(Cluster.java:376) at com.datastax.driver.core.Cluster.connectAsync(Cluster.java:355) at com.datastax.driver.core.Cluster.connect(Cluster.java:305) at info.archinnov.achilles.embedded.AchillesInitializer.initializeFromParameters(AchillesInitializer.java:63) at info.archinnov.achilles.embedded.CassandraEmbeddedServer.<init>(CassandraEmbeddedServer.java:64) at info.archinnov.achilles.embedded.CassandraEmbeddedServerBuilder.buildNativeCluster(CassandraEmbeddedServerBuilder.java:535) at org.apache.beam.sdk.io.cassandra.CassandraIOTest.beforeClass(CassandraIOTest.java:131) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.run(ParentRunner.java:396) at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.runTestClass(JUnitTestClassExecutor.java:110) at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:58) at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:38) at org.gradle.api.internal.tasks.testing.junit.AbstractJUnitTestClassProcessor.processTestClass(AbstractJUnitTestClassProcessor.java:62) at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:51) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24) at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32) at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93) at com.sun.proxy.$Proxy2.processTestClass(Unknown Source) at org.gradle.api.internal.tasks.testing.worker.TestWorker.processTestClass(TestWorker.java:118) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24) at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:175) at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:157) at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:404) at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63) at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:46) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:55) at java.lang.Thread.run(Thread.java:748)
h3. Standard Error

May 17, 2019 11:04:15 PM info.archinnov.achilles.embedded.ServerStarter cleanCassandraDataFiles INFO: Cleaning up embedded Cassandra data directory '/tmp/junit4479425309042224224/embedded-cassandra/data' May 17, 2019 11:04:15 PM info.archinnov.achilles.embedded.ServerStarter cleanCassandraDataFiles INFO: Cleaning up embedded Cassandra data directory '/tmp/junit4479425309042224224/embedded-cassandra/commit-log' May 17, 2019 11:04:15 PM info.archinnov.achilles.embedded.ServerStarter cleanCassandraDataFiles INFO: Cleaning up embedded Cassandra data directory '/tmp/junit4479425309042224224/embedded-cassandra/saved-cache' May 17, 2019 11:04:16 PM info.archinnov.achilles.embedded.ServerStarter start INFO: Cassandra listen address = localhost May 17, 2019 11:04:16 PM info.archinnov.achilles.embedded.ServerStarter start INFO: Cassandra RPC address = localhost May 17, 2019 11:04:16 PM info.archinnov.achilles.embedded.ServerStarter start INFO: Cassandra broadcast address = May 17, 2019 11:04:16 PM info.archinnov.achilles.embedded.ServerStarter start INFO: Cassandra RPC broadcast address = May 17, 2019 11:04:16 PM info.archinnov.achilles.embedded.ServerStarter start INFO: Random embedded Cassandra RPC port/Thrift port = 9160 May 17, 2019 11:04:16 PM info.archinnov.achilles.embedded.ServerStarter start INFO: Random embedded Cassandra Native port/CQL port = 9042 May 17, 2019 11:04:16 PM info.archinnov.achilles.embedded.ServerStarter start INFO: Random embedded Cassandra Storage port = 7402 May 17, 2019 11:04:16 PM info.archinnov.achilles.embedded.ServerStarter start INFO: Random embedded Cassandra Storage SSL port = 7656 May 17, 2019 11:04:16 PM info.archinnov.achilles.embedded.ServerStarter start INFO: Random embedded Cassandra Remote JMX port = null May 17, 2019 11:04:16 PM info.archinnov.achilles.embedded.ServerStarter start INFO: Embedded Cassandra triggers directory = /home/jenkins/jenkins-slave/workspace/beam_PreCommit_Java_Commit/src/sdks/java/io/cassandra/target/cassandra_embedded/cassandra_triggers May 17, 2019 11:04:16 PM info.archinnov.achilles.embedded.ServerStarter start INFO: Starting Cassandra... May 17, 2019 11:04:19 PM org.apache.cassandra.config.Config log INFO: Node configuration:[allocate_tokens_for_keyspace=null; authenticator=org.apache.cassandra.auth.AllowAllAuthenticator; authorizer=org.apache.cassandra.auth.AllowAllAuthorizer; auto_bootstrap=true; auto_snapshot=false; back_pressure_enabled=false; back_pressure_strategy=null; batch_size_fail_threshold_in_kb=50; batch_size_warn_threshold_in_kb=5; batchlog_replay_throttle_in_kb=1024; broadcast_address=null; broadcast_rpc_address=null; buffer_pool_use_heap_if_exhausted=true; cas_contention_timeout_in_ms=1000; cdc_enabled=false; cdc_free_space_check_interval_ms=250; cdc_raw_directory=/tmp/junit4479425309042224224/embedded-cassandra/cdc-raw; cdc_total_space_in_mb=0; client_encryption_options=<REDACTED>; cluster_name=Test Cluster; column_index_cache_size_in_kb=2; column_index_size_in_kb=64; commit_failure_policy=stop; commitlog_compression=null; commitlog_directory=/tmp/junit4479425309042224224/embedded-cassandra/commit-log; commitlog_max_compression_buffers_in_pool=3; commitlog_periodic_queue_size=-1; commitlog_segment_size_in_mb=32; commitlog_sync=periodic; commitlog_sync_batch_window_in_ms=NaN; commitlog_sync_period_in_ms=10000; commitlog_total_space_in_mb=32; compaction_large_partition_warning_threshold_mb=100; compaction_throughput_mb_per_sec=16; concurrent_compactors=null; concurrent_counter_writes=32; concurrent_materialized_view_writes=32; concurrent_reads=32; concurrent_replicates=null; concurrent_writes=32; counter_cache_keys_to_save=2147483647; counter_cache_save_period=7200; counter_cache_size_in_mb=null; counter_write_request_timeout_in_ms=5000; credentials_cache_max_entries=1000; credentials_update_interval_in_ms=-1; credentials_validity_in_ms=2000; cross_node_timeout=false; data_file_directories=[Ljava.lang.String;@6a6a3f22; disk_access_mode=auto; disk_failure_policy=stop_paranoid; disk_optimization_estimate_percentile=0.95; disk_optimization_page_cross_chance=0.1; disk_optimization_strategy=ssd; dynamic_snitch=true; dynamic_snitch_badness_threshold=0.1; dynamic_snitch_reset_interval_in_ms=600000; dynamic_snitch_update_interval_in_ms=100; enable_materialized_views=true; enable_scripted_user_defined_functions=false; enable_user_defined_functions=true; enable_user_defined_functions_threads=true; encryption_options=null; endpoint_snitch=SimpleSnitch; file_cache_round_up=null; file_cache_size_in_mb=null; gc_log_threshold_in_ms=200; gc_warn_threshold_in_ms=0; hinted_handoff_disabled_datacenters=[]; hinted_handoff_enabled=false; hinted_handoff_throttle_in_kb=1024; hints_compression=null; hints_directory=/tmp/junit4479425309042224224/embedded-cassandra/hints; hints_flush_period_in_ms=10000; incremental_backups=false; index_interval=null; index_summary_capacity_in_mb=null; index_summary_resize_interval_in_minutes=60; initial_token=null; inter_dc_stream_throughput_outbound_megabits_per_sec=200; inter_dc_tcp_nodelay=true; internode_authenticator=null; internode_compression=all; internode_recv_buff_size_in_bytes=0; internode_send_buff_size_in_bytes=0; key_cache_keys_to_save=2147483647; key_cache_save_period=14400; key_cache_size_in_mb=null; listen_address=localhost; listen_interface=null; listen_interface_prefer_ipv6=false; listen_on_broadcast_address=false; max_hint_window_in_ms=10800000; max_hints_delivery_threads=2; max_hints_file_size_in_mb=128; max_mutation_size_in_kb=null; max_streaming_retries=3; max_value_size_in_mb=256; memtable_allocation_type=heap_buffers; memtable_cleanup_threshold=null; memtable_flush_writers=0; memtable_heap_space_in_mb=64; memtable_offheap_space_in_mb=null; min_free_space_per_drive_in_mb=50; native_transport_max_concurrent_connections=-1; native_transport_max_concurrent_connections_per_ip=-1; native_transport_max_frame_size_in_mb=256; native_transport_max_threads=128; native_transport_port=9042; native_transport_port_ssl=null; num_tokens=256; otc_backlog_expiration_interval_ms=200; otc_coalescing_enough_coalesced_messages=8; otc_coalescing_strategy=DISABLED; otc_coalescing_window_us=200; partitioner=org.apache.cassandra.dht.Murmur3Partitioner; permissions_cache_max_entries=1000; permissions_update_interval_in_ms=-1; permissions_validity_in_ms=2000; phi_convict_threshold=8.0; prepared_statements_cache_size_mb=null; range_request_timeout_in_ms=10000; read_request_timeout_in_ms=10000; request_scheduler=org.apache.cassandra.scheduler.NoScheduler; request_scheduler_id=null; request_scheduler_options=null; request_timeout_in_ms=10000; role_manager=null; roles_cache_max_entries=1000; roles_update_interval_in_ms=-1; roles_validity_in_ms=2000; row_cache_class_name=org.apache.cassandra.cache.OHCProvider; row_cache_keys_to_save=2147483647; row_cache_save_period=0; row_cache_size_in_mb=0; rpc_address=localhost; rpc_interface=null; rpc_interface_prefer_ipv6=false; rpc_keepalive=true; rpc_listen_backlog=50; rpc_max_threads=2147483647; rpc_min_threads=16; rpc_port=9160; rpc_recv_buff_size_in_bytes=null; rpc_send_buff_size_in_bytes=null; rpc_server_type=sync; saved_caches_directory=/tmp/junit4479425309042224224/embedded-cassandra/saved-cache; seed_provider=org.apache.cassandra.locator.SimpleSeedProvider\{seeds=localhost}; server_encryption_options=<REDACTED>; slow_query_log_timeout_in_ms=500; snapshot_before_compaction=false; ssl_storage_port=7656; sstable_preemptive_open_interval_in_mb=50; start_native_transport=true; start_rpc=false; storage_port=7402; stream_throughput_outbound_megabits_per_sec=200; streaming_keep_alive_period_in_secs=300; streaming_socket_timeout_in_ms=86400000; thrift_framed_transport_size_in_mb=15; thrift_max_message_length_in_mb=16; thrift_prepared_statements_cache_size_mb=null; tombstone_failure_threshold=100000; tombstone_warn_threshold=1000; tracetype_query_ttl=86400; tracetype_repair_ttl=604800; transparent_data_encryption_options=org.apache.cassandra.config.TransparentDataEncryptionOptions@65bb02df; trickle_fsync=false; trickle_fsync_interval_in_kb=10240; truncate_request_timeout_in_ms=60000; unlogged_batch_across_partitions_warn_threshold=10; user_defined_function_fail_timeout=1500; user_defined_function_warn_timeout=500; user_function_timeout_policy=die; windows_timer_interval=0; write_request_timeout_in_ms=10000] May 17, 2019 11:04:19 PM org.apache.cassandra.config.DatabaseDescriptor applySimpleConfig INFO: DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap May 17, 2019 11:04:19 PM org.apache.cassandra.config.DatabaseDescriptor applySimpleConfig INFO: Global memtable on-heap threshold is enabled at 64MB May 17, 2019 11:04:19 PM org.apache.cassandra.config.DatabaseDescriptor applySimpleConfig INFO: Global memtable off-heap threshold is enabled at 455MB May 17, 2019 11:04:20 PM org.apache.cassandra.net.RateBasedBackPressure <init> INFO: Initialized back-pressure with high ratio: 0.9, factor: 5, flow: FAST, window size: 10000. May 17, 2019 11:04:20 PM org.apache.cassandra.config.DatabaseDescriptor applySimpleConfig INFO: Back-pressure is disabled with strategy null. May 17, 2019 11:04:21 PM org.apache.cassandra.utils.JMXServerUtils logJmxServiceUrl INFO: Configured JMX server at: service:jmx:rmi://127.0.0.1/jndi/rmi://127.0.0.1:42017/jmxrmi May 17, 2019 11:04:21 PM org.apache.cassandra.utils.logging.LoggingSupportFactory getLoggingSupport WARNING: You are using Cassandra with an unsupported deployment. The intended logging implementation library logback is not used by slf4j. Detected slf4j logger factory: org.slf4j.impl.JDK14LoggerFactory. You will not be able to dynamically manage log levels via JMX and may have performance or other issues. May 17, 2019 11:04:21 PM org.apache.cassandra.service.CassandraDaemon logSystemInfo INFO: Hostname: apache-beam-jenkins-8 May 17, 2019 11:04:21 PM org.apache.cassandra.service.CassandraDaemon logSystemInfo INFO: JVM vendor/version: OpenJDK 64-Bit Server VM/1.8.0_212 May 17, 2019 11:04:21 PM org.apache.cassandra.service.CassandraDaemon logSystemInfo INFO: Heap size: 1.533GiB/1.778GiB May 17, 2019 11:04:21 PM org.apache.cassandra.service.CassandraDaemon logSystemInfo INFO: Code Cache Non-heap memory: init = 2555904(2496K) used = 9887744(9656K) committed = 9961472(9728K) max = 251658240(245760K) May 17, 2019 11:04:21 PM org.apache.cassandra.service.CassandraDaemon logSystemInfo INFO: Metaspace Non-heap memory: init = 0(0K) used = 20099216(19628K) committed = 20840448(20352K) max = -1(-1K) May 17, 2019 11:04:21 PM org.apache.cassandra.service.CassandraDaemon logSystemInfo INFO: Compressed Class Space Non-heap memory: init = 0(0K) used = 2281968(2228K) committed = 2490368(2432K) max = 1073741824(1048576K) May 17, 2019 11:04:21 PM org.apache.cassandra.service.CassandraDaemon logSystemInfo INFO: PS Eden Space Heap memory: init = 429916160(419840K) used = 300944272(293890K) committed = 429916160(419840K) max = 573046784(559616K) May 17, 2019 11:04:21 PM org.apache.cassandra.service.CassandraDaemon logSystemInfo INFO: PS Survivor Space Heap memory: init = 71303168(69632K) used = 0(0K) committed = 71303168(69632K) max = 71303168(69632K) May 17, 2019 11:04:21 PM org.apache.cassandra.service.CassandraDaemon logSystemInfo INFO: PS Old Gen Heap memory: init = 1145044992(1118208K) used = 0(0K) committed = 1145044992(1118208K) max = 1431830528(1398272K) May 17, 2019 11:04:21 PM org.apache.cassandra.service.CassandraDaemon logSystemInfo INFO: Classpath: /home/jenkins/jenkins-slave/workspace/beam_PreCommit_Java_Commit/src/sdks/java/io/cassandra/build/classes/java/test:/home/jenkins/jenkins-slave/workspace/beam_PreCommit_Java_Commit/src/sdks/java/io/cassandra/build/resources/test:/home/jenkins/jenkins-slave/workspace/beam_PreCommit_Java_Commit/src/sdks/java/io/cassandra/build/classes/java/main:/home/jenkins/jenkins-slave/workspace/beam_PreCommit_Java_Commit/src/sdks/java/io/cassandra/build/resources/main:/home/jenkins/jenkins-slave/workspace/beam_PreCommit_Java_Commit/src/sdks/java/io/common/build/libs/beam-sdks-java-io-common-2.14.0-SNAPSHOT-tests.jar:/home/jenkins/jenkins-slave/workspace/beam_PreCommit_Java_Commit/src/sdks/java/io/common/build/libs/beam-sdks-java-io-common-2.14.0-SNAPSHOT.jar:/home/jenkins/jenkins-slave/workspace/beam_PreCommit_Java_Commit/src/runners/direct-java/build/libs/beam-runners-direct-java-2.14.0-SNAPSHOT-unshaded.jar:/home/jenkins/jenkins-slave/workspace/beam_PreCommit_Java_Commit/src/runners/direct-java/build/libs/beam-runners-direct-java-2.14.0-SNAPSHOT.jar:/home/jenkins/jenkins-slave/workspace/beam_PreCommit_Java_Commit/src/runners/core-java/build/libs/beam-runners-core-java-2.14.0-SNAPSHOT.jar:/home/jenkins/jenkins-slave/workspace/beam_PreCommit_Java_Commit/src/runners/java-fn-execution/build/libs/beam-runners-java-fn-execution-2.14.0-SNAPSHOT.jar:/home/jenkins/jenkins-slave/workspace/beam_PreCommit_Java_Commit/src/runners/core-construction-java/build/libs/beam-runners-core-construction-java-2.14.0-SNAPSHOT.jar:/home/jenkins/jenkins-slave/workspace/beam_PreCommit_Java_Commit/src/runners/local-java/build/libs/beam-runners-local-java-core-2.14.0-SNAPSHOT.jar:/home/jenkins/jenkins-slave/workspace/beam_PreCommit_Java_Commit/src/sdks/java/fn-execution/build/libs/beam-sdks-java-fn-execution-2.14.0-SNAPSHOT.jar:/home/jenkins/jenkins-slave/workspace/beam_PreCommit_Java_Commit/src/vendor/sdks-java-extensions-protobuf/build/libs/beam-vendor-sdks-java-extensions-protobuf-2.14.0-SNAPSHOT.jar:/home/jenkins/jenkins-slave/workspace/beam_PreCommit_Java_Commit/src/sdks/java/core/build/libs/beam-sdks-java-core-2.14.0-SNAPSHOT.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.apache.beam/beam-vendor-guava-20_0/0.1/863c39fdde1257c0d30650102e73f7e07f7da610/beam-vendor-guava-20_0-0.1.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.datastax.cassandra/cassandra-driver-mapping/3.6.0/830fbfb1555d4d04ea32219c0f5eb24d086dcd5d/cassandra-driver-mapping-3.6.0.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/info.archinnov/achilles-junit/6.0.2/d394bf43eeddc817f1041369893967641eca7a34/achilles-junit-6.0.2.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/info.archinnov/achilles-embedded/6.0.2/bc1123a617f346171741da8e1cb212a78296b85f/achilles-embedded-6.0.2.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/info.archinnov/achilles-core/6.0.2/a3c0f6308003458c6d6d92cbc41cbe3eef94c78/achilles-core-6.0.2.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.datastax.cassandra/cassandra-driver-extras/3.6.0/e8acb6973070f1bd26448b2cbc3a22e89fed8e36/cassandra-driver-extras-3.6.0.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/info.archinnov/achilles-common/6.0.2/a852a3c4477d66e37221f8ef696043ac9ca62308/achilles-common-6.0.2.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/info.archinnov/achilles-model/6.0.2/524724e3ba928f7800ffbefdc080dd147dffeb23/achilles-model-6.0.2.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.datastax.cassandra/cassandra-driver-core/3.6.0/1d689ae757862f7c497dd6b186793d1bf921fd28/cassandra-driver-core-3.6.0.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.datastax.cassandra/cassandra-driver-core/3.6.0/78a1ab6540dd48b6761a06c8a043d525ef336b83/cassandra-driver-core-3.6.0-shaded.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.slf4j/slf4j-jdk14/1.7.25/bccda40ebc8067491b32a88f49615a747d20082d/slf4j-jdk14-1.7.25.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.apache.avro/avro/1.8.2/91e3146dfff4bd510181032c8276a3a0130c0697/avro-1.8.2.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.apache.cassandra/cassandra-all/3.11.3/4480b8b3601e1e2a73ccf6979d6932b13e5659e8/cassandra-all-3.11.3.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/io.dropwizard.metrics/metrics-jvm/3.1.5/42fe531a5873bd56dbca6e4b7678912b5df2a19/metrics-jvm-3.1.5.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.addthis.metrics/reporter-config3/3.0.3/5fd930067d25f0e8ecd005d7c3e40f31d60f2395/reporter-config3-3.0.3.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/io.dropwizard.metrics/metrics-core/3.2.2/cd9886f498ee2ab2d994f0c779e5553b2c450416/metrics-core-3.2.2.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.slf4j/slf4j-api/1.7.25/da76ca59f6a57ee3102f8f9bd9cee742973efa8a/slf4j-api-1.7.25.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/junit/junit/4.13-beta-1/1bc4a3b4a2d01a08c3a2cc8143666565b846ed17/junit-4.13-beta-1.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.hamcrest/hamcrest-library/2.1/42edeb45e615925992d79263e9e744a857cfbcd0/hamcrest-library-2.1.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.mockito/mockito-core/1.10.19/e8546f5bef4e061d8dd73895b4e8f40e3fe6effe/mockito-core-1.10.19.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.hamcrest/hamcrest-core/2.1/70dd49dea4507315b1fa3bc1aa59b1442d55957a/hamcrest-core-2.1.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.module/jackson-module-jaxb-annotations/2.9.8/da08815ba1c7f7b435e7df02b7bf98327bad2fd4/jackson-module-jaxb-annotations-2.9.8.jar:/home/jenkins/jenkins-slave/workspace/beam_PreCommit_Java_Commit/src/model/pipeline/build/libs/beam-model-pipeline-2.14.0-SNAPSHOT.jar:/home/jenkins/jenkins-slave/workspace/beam_PreCommit_Java_Commit/src/model/job-management/build/libs/beam-model-job-management-2.14.0-SNAPSHOT.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-databind/2.9.8/11283f21cc480aa86c4df7a0a3243ec508372ed2/jackson-databind-2.9.8.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-core/2.9.8/f5a654e4675769c716e5b387830d19b501ca191/jackson-core-2.9.8.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-annotations/2.9.8/ba7f0e6f8f1b28d251eeff2a5604bed34c53ff35/jackson-annotations-2.9.8.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.xerial.snappy/snappy-java/1.1.4/d94ae6d7d27242eaa4b6c323f881edbb98e48da6/snappy-java-1.1.4.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/joda-time/joda-time/2.10.1/9ac3dbf89dbf2ee385185dd0cd3064fe789efee0/joda-time-2.10.1.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.tukaani/xz/1.8/c4f7d054303948eb6a4066194253886c8af07128/xz-1.8.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/io.netty/netty-handler/4.1.30.Final/ecc076332ed103411347f4806a44ee32d9d9cb5f/netty-handler-4.1.30.Final.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.google.auto.service/auto-service/1.0-rc2/51033a5b8fcf7039159e35b6878f106ccd5fb35f/auto-service-1.0-rc2.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.google.auto/auto-common/0.4/ae410a6b2aa49c33bc0745178dd2826d8b9d5f31/auto-common-0.4.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.google.guava/guava/20.0/89507701249388e1ed5ddcf8c41f4ce1be7831ef/guava-20.0.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.github.jnr/jnr-posix/3.0.44/1f8e4551454e613c04f6d4045ed9d5b98e21980f/jnr-posix-3.0.44.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.github.jnr/jnr-ffi/2.1.7/31a7391a212069303935a1df29566b7372d3ef9f/jnr-ffi-2.1.7.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.hamcrest/hamcrest/2.1/9420ba32c29217b54eebd26ff7f9234d31c3fbb2/hamcrest-2.1.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.objenesis/objenesis/2.1/87c0ea803b69252868d09308b4618f766f135a96/objenesis-2.1.jar:/home/jenkins/jenkins-slave/workspace/beam_PreCommit_Java_Commit/src/model/fn-execution/build/libs/beam-model-fn-execution-2.14.0-SNAPSHOT.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.apache.beam/beam-vendor-grpc-1_13_1/0.2/249a9a576fc6541d87972e2dc24cf11ea8129b8f/beam-vendor-grpc-1_13_1-0.2.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/args4j/args4j/2.33/bd87a75374a6d6523de82fef51fc3cfe9baf9fc9/args4j-2.33.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.google.errorprone/error_prone_annotations/2.0.15/822652ed7196d119b35d2e22eb9cd4ffda11e640/error_prone_annotations-2.0.15.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.codehaus.jackson/jackson-mapper-asl/1.9.13/1ee2f2bed0e5dd29d1cb155a166e6f8d50bbddb7/jackson-mapper-asl-1.9.13.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.codehaus.jackson/jackson-core-asl/1.9.13/3c304d70f42f832e0a86d45bd437f692129299a4/jackson-core-asl-1.9.13.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.thoughtworks.paranamer/paranamer/2.7/3ed64c69e882a324a75e890024c32a28aff0ade8/paranamer-2.7.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.apache.commons/commons-compress/1.18/1191f9f2bc0c47a8cce69193feb1ff0a8bcb37d5/commons-compress-1.18.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/io.netty/netty-codec/4.1.30.Final/515c8f609aaca28a94f984d89a9667dd3359c1b1/netty-codec-4.1.30.Final.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/io.netty/netty-transport/4.1.30.Final/3d27bb432a3b125167ac161b26415ad29ec17f02/netty-transport-4.1.30.Final.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/io.netty/netty-buffer/4.1.30.Final/597adb653306470fb3ec1af3c0f3f30a37b1310a/netty-buffer-4.1.30.Final.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.github.jnr/jffi/1.2.16/5c1149dfcc9a16f85c8d9b8797f03806667cb9f1/jffi-1.2.16.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.github.jnr/jffi/1.2.16/3c1f0edf2df2c6e0419d60d0baa59659211624cb/jffi-1.2.16-native.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.ow2.asm/asm-commons/5.0.3/a7111830132c7f87d08fe48cb0ca07630f8cb91c/asm-commons-5.0.3.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.ow2.asm/asm-analysis/5.0.3/c7126aded0e8e13fed5f913559a0dd7b770a10f3/asm-analysis-5.0.3.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.ow2.asm/asm-util/5.0.3/1512e5571325854b05fb1efce1db75fcced54389/asm-util-5.0.3.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.ow2.asm/asm-tree/5.0.3/287749b48ba7162fb67c93a026d690b29f410bed/asm-tree-5.0.3.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.ow2.asm/asm/5.0.4/da08b8cce7bbf903602a25a3a163ae252435795/asm-5.0.4.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.github.jnr/jnr-x86asm/1.0.2/6936bbd6c5b235665d87bd450f5e13b52d4b48/jnr-x86asm-1.0.2.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.github.jnr/jnr-constants/0.9.9/33f23994e09aeb49880aa01e12e8e9eff058c14c/jnr-constants-0.9.9.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/commons-io/commons-io/2.5/2852e6e05fbb95076fc091f6d1780f1f8fe35e0f/commons-io-2.5.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.apache.cassandra/cassandra-thrift/3.11.3/28e94bd4147bf21fb2913cbe18c608adaf191f/cassandra-thrift-3.11.3.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.addthis.metrics/reporter-config-base/3.0.3/18602360f5e606cbca07ded5ecfeda6121789c9f/reporter-config-base-3.0.3.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.apache.commons/commons-lang3/3.6/9d28a6b23650e8a7e9063c04588ace6cf7012c17/commons-lang3-3.6.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/commons-collections/commons-collections/3.2.2/8ad72fe39fa8c91eaaf12aadb21e0c3661fe26d5/commons-collections-3.2.2.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.eclipse.jdt.core.compiler/ecj/4.4.2/71d67f5bab9465ec844596ef844f40902ae25392/ecj-4.4.2.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.squareup/javapoet/1.5.1/1d36b86b8fecbe64ea38aea741599720cb07b7d2/javapoet-1.5.1.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.hibernate/hibernate-validator/4.3.0.Final/f2d0447bcdb27a48398215a33eb351b8a594e3a4/hibernate-validator-4.3.0.Final.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/javax.validation/validation-api/1.1.0.Final/8613ae82954779d518631e05daa73a6a954817d5/validation-api-1.1.0.Final.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/io.netty/netty-resolver/4.1.30.Final/5106fd687066ffd712e5295d32af4e2ac6482613/netty-resolver-4.1.30.Final.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/io.netty/netty-common/4.1.30.Final/5dca0c34d8f38af51a2398614e81888f51cf811a/netty-common-4.1.30.Final.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/net.jpountz.lz4/lz4/1.3.0/c708bb2590c0652a642236ef45d9f99ff842a2ce/lz4-1.3.0.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.ning/compress-lzf/0.8.4/994da6bddb79a40c368d3040f2aa06b94faf6b1f/compress-lzf-0.8.4.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/commons-cli/commons-cli/1.1/11c98b99ad538f2f67633afd4d7f4d98ecfbb408/commons-cli-1.1.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.apache.commons/commons-math3/3.6.1/e4ba98f1d4b3c80ec46392f25e094a6a2e58fcbf/commons-math3-3.6.1.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.googlecode.concurrentlinkedhashmap/concurrentlinkedhashmap-lru/1.4/d4b42ed76a166a81dbac613339eed3343be6bd2d/concurrentlinkedhashmap-lru-1.4.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.antlr/antlr/3.5.2/c4a65c950bfc3e7d04309c515b2177c00baf7764/antlr-3.5.2.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.antlr/ST4/4.0.8/a1c55e974f8a94d78e2348fa6ff63f4fa1fae64/ST4-4.0.8.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.antlr/antlr-runtime/3.5.2/cd9cd41361c155f3af0f653009dcecb08d8b4afd/antlr-runtime-3.5.2.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.slf4j/jcl-over-slf4j/1.7.7/56003dcd0a31deea6391b9e2ef2f2dc90b205a92/jcl-over-slf4j-1.7.7.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.googlecode.json-simple/json-simple/1.1/5e303a03d04e6788dddfa3655272580ae0fc13bb/json-simple-1.1.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.boundary/high-scale-lib/1.0.6/7b44147cb2729e1724d2d46d7b932c56b65087f0/high-scale-lib-1.0.6.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.yaml/snakeyaml/1.12/ebe66a6b88caab31d7a19571ad23656377523545/snakeyaml-1.12.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.mindrot/jbcrypt/0.3m/fe2d9c5f23767d681a7e38fc8986b812400ec583/jbcrypt-0.3m.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/io.airlift/airline/0.6/a545c95d96a4081698777b240574983b7ed39187/airline-0.6.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.thinkaurelius.thrift/thrift-server/0.3.7/e8182774da1b1dde3704f450837c79997b5d7025/thrift-server-0.3.7.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.clearspring.analytics/stream/2.5.2/8ec983689f4cc5463d012f415129b0c9fe296b7a/stream-2.5.2.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.apache.thrift/libthrift/0.9.2/9b067e2e2c5291e9f0d8b3561b1654286e6d81ee/libthrift-0.9.2.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.caffinitas.ohc/ohc-core-j8/0.4.4/7c127ba21aa8adb4562bb3821ac3556f2620ce6b/ohc-core-j8-0.4.4.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.caffinitas.ohc/ohc-core/0.4.4/437cd3ae562daf1d2d47b126ba5ba520c8f156bb/ohc-core-0.4.4.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/net.java.dev.jna/jna/4.2.2/5012450aee579c3118ff09461d5ce210e0cdc2a9/jna-4.2.2.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.github.jbellis/jamm/0.3.0/a08af6071e57d4eb5d13db780c7810f73b549f1a/jamm-0.3.0.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/io.netty/netty-all/4.0.44.Final/f1408cee8308bebe2c8fe641e3dc268b0297336d/netty-all-4.0.44.Final.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.fusesource/sigar/1.6.4/e91a355d337a0b1991f54181627d63c9973624c3/sigar-1.6.4.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.github.ben-manes.caffeine/caffeine/2.2.6/472291cda8957f45a2e203f15b048cdfd4f261cf/caffeine-2.2.6.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.jctools/jctools-core/1.2.1/ed2af5a88cdfc52df8dcb00f0c03a42d70605930/jctools-core-1.2.1.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/javax.inject/javax.inject/1/6975da39a7040257bd51d21a231b76c915872d38/javax.inject-1.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.lmax/disruptor/3.0.1/d68c363b12827c644bd60469827b862cad7dc0a2/disruptor-3.0.1.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/it.unimi.dsi/fastutil/6.5.7/dec71174d0c8a20f355e5af8b59f25eb424c49d3/fastutil-6.5.7.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.apache.httpcomponents/httpclient/4.2.5/666e26e76f2e87d84e4f16acb546481ae1b8e9a6/httpclient-4.2.5.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.apache.httpcomponents/httpcore/4.2.4/3b7f38df6de5dd8b500e602ae8c2dd5ee446f883/httpcore-4.2.4.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.carrotsearch/hppc/0.5.4/21b0c8e70f4285dd10eaeb517cacb16d5bb2dd63/hppc-0.5.4.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/de.jflex/jflex/1.6.0/9d3b308283617fe047b437839e45dc4fe48a4f2a/jflex-1.6.0.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.github.rholder/snowball-stemmer/1.3.0.581.1/35a89d519949c33c6f28e8f37b3df7893b776ca4/snowball-stemmer-1.3.0.581.1.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/com.googlecode.concurrent-trees/concurrent-trees/2.4.0/2e505b78f9216abebbbdf1c3254bf9f4c565ae43/concurrent-trees-2.4.0.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.jboss.logging/jboss-logging/3.1.0.CR2/28725380c07f917ace4e511db21cc45e9ae5a72b/jboss-logging-3.1.0.CR2.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/commons-logging/commons-logging/1.1.1/5043bfebc3db072ed80fbd362e7caf00e885d8ae/commons-logging-1.1.1.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.apache.ant/ant/1.7.0/9746af1a485e50cf18dcb232489032a847067066/ant-1.7.0.jar:/home/jenkins/.gradle/caches/modules-2/files-2.1/org.apache.ant/ant-launcher/1.7.0/e7e30789211e074aa70ef3eaea59bd5b22a7fa7a/ant-launcher-1.7.0.jar May 17, 2019 11:04:21 PM org.apache.cassandra.service.CassandraDaemon logSystemInfo INFO: JVM Arguments: [-Djava.security.manager=worker.org.gradle.process.internal.worker.child.BootstrapSecurityManager, -Dorg.gradle.native=false, -javaagent:build/tmp/expandedArchives/org.jacoco.agent-0.8.2.jar_2aca8b620b19ecd063f63feff8caaa38/jacocoagent.jar=destfile=build/jacoco/test.exec,append=true,inclnolocationclasses=false,dumponexit=true,output=file,jmx=false, -Xmx2g, -Dfile.encoding=UTF-8, -Duser.country=US, -Duser.language=en, -Duser.variant, -ea] May 17, 2019 11:04:22 PM org.apache.cassandra.utils.NativeLibrary tryMlockall WARNING: Unable to lock JVM memory (ENOMEM). This can result in part of the JVM being swapped out, especially with mmapped I/O enabled. Increase RLIMIT_MEMLOCK or run Cassandra as root. May 17, 2019 11:04:22 PM org.apache.cassandra.service.StartupChecks$1 execute WARNING: jemalloc shared library could not be preloaded to speed up memory allocations May 17, 2019 11:04:22 PM org.apache.cassandra.service.StartupChecks$3 execute WARNING: JMX is not enabled to receive remote connections. Please see cassandra-env.sh for more info. May 17, 2019 11:04:22 PM org.apache.cassandra.service.StartupChecks$5 checkOutOfMemoryHandling WARNING: The JVM is not configured to stop on OutOfMemoryError which can cause data corruption. Use one of the following JVM options to configure the behavior on OutOfMemoryError: -XX:+ExitOnOutOfMemoryError, -XX:+CrashOnOutOfMemoryError, or -XX:OnOutOfMemoryError=""<cmd args>;<cmd args>"" May 17, 2019 11:04:22 PM org.apache.cassandra.utils.SigarLibrary <init> INFO: Initializing SIGAR library no libsigar-amd64-linux.so in java.library.path org.hyperic.sigar.SigarException: no libsigar-amd64-linux.so in java.library.path at org.hyperic.sigar.Sigar.loadLibrary(Sigar.java:172) at org.hyperic.sigar.Sigar.<clinit>(Sigar.java:100) at org.apache.cassandra.utils.SigarLibrary.<init>(SigarLibrary.java:47) at org.apache.cassandra.utils.SigarLibrary.<clinit>(SigarLibrary.java:28) at org.apache.cassandra.service.StartupChecks$7.execute(StartupChecks.java:266) at org.apache.cassandra.service.StartupChecks.verify(StartupChecks.java:125) at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:200) at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:602) at info.archinnov.achilles.embedded.ServerStarter.lambda$start$0(ServerStarter.java:163) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) May 17, 2019 11:04:22 PM org.apache.cassandra.utils.SigarLibrary <init> INFO: Could not initialize SIGAR library org.hyperic.sigar.Sigar.getFileSystemListNative()[Lorg/hyperic/sigar/FileSystem; May 17, 2019 11:04:22 PM org.apache.cassandra.utils.SigarLibrary warnIfRunningInDegradedMode INFO: Sigar could not be initialized, test for checking degraded mode omitted. May 17, 2019 11:04:22 PM org.apache.cassandra.service.StartupChecks$8 execute WARNING: Maximum number of memory map areas per process (vm.max_map_count) 65530 is too low, recommended value: 1048575, you can change it with sysctl. May 17, 2019 11:04:23 PM org.apache.cassandra.cql3.QueryProcessor <clinit> INFO: Initialized prepared statement caches with 10 MB (native) and 10 MB (Thrift) May 17, 2019 11:04:28 PM org.apache.cassandra.db.ColumnFamilyStore <init> INFO: Initializing system.IndexInfo May 17, 2019 11:04:33 PM org.apache.cassandra.db.ColumnFamilyStore <init> INFO: Initializing system.batches May 17, 2019 11:04:33 PM org.apache.cassandra.db.ColumnFamilyStore <init> INFO: Initializing system.paxos May 17, 2019 11:04:33 PM org.apache.cassandra.db.ColumnFamilyStore <init> INFO: Initializing system.local May 17, 2019 11:04:33 PM org.apache.cassandra.db.ColumnFamilyStore <init> INFO: Initializing system.peers May 17, 2019 11:04:33 PM org.apache.cassandra.db.ColumnFamilyStore <init> INFO: Initializing system.peer_events May 17, 2019 11:04:33 PM org.apache.cassandra.db.ColumnFamilyStore <init> INFO: Initializing system.range_xfers May 17, 2019 11:04:33 PM org.apache.cassandra.db.ColumnFamilyStore <init> INFO: Initializing system.compaction_history May 17, 2019 11:04:33 PM org.apache.cassandra.db.ColumnFamilyStore <init> INFO: Initializing system.sstable_activity May 17, 2019 11:04:33 PM org.apache.cassandra.db.ColumnFamilyStore <init> INFO: Initializing system.size_estimates May 17, 2019 11:04:33 PM org.apache.cassandra.db.ColumnFamilyStore <init> INFO: Initializing system.available_ranges May 17, 2019 11:04:33 PM org.apache.cassandra.db.ColumnFamilyStore <init> INFO: Initializing system.transferred_ranges May 17, 2019 11:04:33 PM org.apache.cassandra.db.ColumnFamilyStore <init> INFO: Initializing system.views_builds_in_progress May 17, 2019 11:04:33 PM org.apache.cassandra.db.ColumnFamilyStore <init> INFO: Initializing system.built_views May 17, 2019 11:04:34 PM org.apache.cassandra.db.ColumnFamilyStore <init> INFO: Initializing system.hints May 17, 2019 11:04:34 PM org.apache.cassandra.db.ColumnFamilyStore <init> INFO: Initializing system.batchlog May 17, 2019 11:04:34 PM org.apache.cassandra.db.ColumnFamilyStore <init> INFO: Initializing system.prepared_statements May 17, 2019 11:04:34 PM org.apache.cassandra.db.ColumnFamilyStore <init> INFO: Initializing system.schema_keyspaces May 17, 2019 11:04:34 PM org.apache.cassandra.db.ColumnFamilyStore <init> INFO: Initializing system.schema_columnfamilies May 17, 2019 11:04:34 PM org.apache.cassandra.db.ColumnFamilyStore <init> INFO: Initializing system.schema_columns May 17, 2019 11:04:34 PM org.apache.cassandra.db.ColumnFamilyStore <init> INFO: Initializing system.schema_triggers May 17, 2019 11:04:34 PM org.apache.cassandra.db.ColumnFamilyStore <init> INFO: Initializing system.schema_usertypes May 17, 2019 11:04:34 PM org.apache.cassandra.db.ColumnFamilyStore <init> INFO: Initializing system.schema_functions May 17, 2019 11:04:34 PM org.apache.cassandra.db.ColumnFamilyStore <init> INFO: Initializing system.schema_aggregates May 17, 2019 11:04:34 PM org.apache.cassandra.db.view.ViewManager reload INFO: Not submitting build tasks for views in keyspace system as storage service is not initialized May 17, 2019 11:04:35 PM org.apache.cassandra.db.monitoring.ApproximateTime <clinit> INFO: Scheduling approximate time-check task with a precision of 10 milliseconds May 17, 2019 11:04:36 PM org.apache.cassandra.db.ColumnFamilyStore <init> INFO: Initializing system_schema.keyspaces May 17, 2019 11:04:36 PM org.apache.cassandra.db.ColumnFamilyStore <init> INFO: Initializing system_schema.tables May 17, 2019 11:04:36 PM org.apache.cassandra.db.ColumnFamilyStore <init> INFO: Initializing system_schema.columns May 17, 2019 11:04:36 PM org.apache.cassandra.db.ColumnFamilyStore <init> INFO: Initializing system_schema.triggers May 17, 2019 11:04:36 PM org.apache.cassandra.db.ColumnFamilyStore <init> INFO: Initializing system_schema.dropped_columns May 17, 2019 11:04:36 PM org.apache.cassandra.db.ColumnFamilyStore <init> INFO: Initializing system_schema.views May 17, 2019 11:04:36 PM org.apache.cassandra.db.ColumnFamilyStore <init> INFO: Initializing system_schema.types May 17, 2019 11:04:36 PM org.apache.cassandra.db.ColumnFamilyStore <init> INFO: Initializing system_schema.functions May 17, 2019 11:04:36 PM org.apache.cassandra.db.ColumnFamilyStore <init> INFO: Initializing system_schema.aggregates May 17, 2019 11:04:36 PM org.apache.cassandra.db.ColumnFamilyStore <init> INFO: Initializing system_schema.indexes May 17, 2019 11:04:36 PM org.apache.cassandra.db.view.ViewManager reload INFO: Not submitting build tasks for views in keyspace system_schema as storage service is not initialized May 17, 2019 11:04:43 PM org.apache.cassandra.service.CacheService initKeyCache INFO: Initializing key cache with capacity of 78 MBs. May 17, 2019 11:04:44 PM org.apache.cassandra.service.CacheService initRowCache INFO: Initializing row cache with capacity of 0 MBs May 17, 2019 11:04:44 PM org.apache.cassandra.service.CacheService initCounterCache INFO: Initializing counter cache with capacity of 39 MBs May 17, 2019 11:04:44 PM org.apache.cassandra.service.CacheService initCounterCache INFO: Scheduling counter cache save to every 7200 seconds (going to save all keys). May 17, 2019 11:04:45 PM org.apache.cassandra.utils.memory.BufferPool$GlobalPool <clinit> INFO: Global buffer pool is enabled, when pool is exhausted (max is 455.000MiB) it will allocate on heap May 17, 2019 11:04:45 PM org.apache.cassandra.service.StorageService populateTokenMetadata INFO: Populating token metadata from system tables May 17, 2019 11:04:45 PM org.apache.cassandra.service.StorageService populateTokenMetadata INFO: Token metadata: May 17, 2019 11:04:45 PM org.apache.cassandra.cache.AutoSavingCache$4 run INFO: Completed loading (126 ms; 4 keys) KeyCache cache May 17, 2019 11:04:46 PM org.apache.cassandra.db.commitlog.CommitLog recoverSegmentsOnDisk INFO: No commitlog files found; skipping replay May 17, 2019 11:04:46 PM org.apache.cassandra.service.StorageService populateTokenMetadata INFO: Populating token metadata from system tables May 17, 2019 11:04:46 PM org.apache.cassandra.service.StorageService populateTokenMetadata INFO: Token metadata: May 17, 2019 11:04:46 PM com.datastax.driver.core.Cluster logDriverVersion INFO: DataStax Java driver 3.6.0 for Apache Cassandra May 17, 2019 11:04:46 PM com.datastax.driver.core.GuavaCompatibility selectImplementation INFO: Detected Guava >= 19 in the classpath, using modern compatibility layer May 17, 2019 11:04:46 PM org.apache.cassandra.cql3.QueryProcessor preloadPreparedStatement INFO: Preloaded 0 prepared statements May 17, 2019 11:04:46 PM org.apache.cassandra.service.StorageService initServer INFO: Cassandra version: 3.11.3 May 17, 2019 11:04:46 PM org.apache.cassandra.service.StorageService initServer INFO: Thrift API version: 20.1.0 May 17, 2019 11:04:46 PM org.apache.cassandra.service.StorageService initServer INFO: CQL supported versions: 3.4.4 (default: 3.4.4) May 17, 2019 11:04:46 PM org.apache.cassandra.service.StorageService initServer INFO: Native protocol supported versions: 3/v3, 4/v4, 5/v5-beta (default: 4/v4) May 17, 2019 11:04:47 PM org.apache.cassandra.io.sstable.IndexSummaryManager <init> INFO: Initializing index summary manager with a memory pool size of 78 MB and a resize interval of 60 minutes May 17, 2019 11:04:47 PM org.apache.cassandra.net.MessagingService getServerSockets INFO: Starting Messaging Service on localhost/127.0.0.1:7402 (lo) May 17, 2019 11:04:47 PM org.apache.cassandra.db.SystemKeyspace getLocalHostId WARNING: No host ID found, created 2c4a0fb3-3f99-4dc9-b8ce-696c49397ff1 (Note: This should happen exactly once per node). May 17, 2019 11:04:47 PM org.apache.cassandra.service.StorageService checkForEndpointCollision INFO: Unable to gossip with any peers but continuing anyway since node is in its own seed list May 17, 2019 11:04:47 PM org.apache.cassandra.service.StorageService loadRingState INFO: Loading persisted ring state May 17, 2019 11:04:47 PM org.apache.cassandra.service.StorageService prepareToJoin INFO: Starting up server gossip May 17, 2019 11:04:47 PM org.apache.cassandra.service.StorageService joinTokenRing INFO: This node will not auto bootstrap because it is configured to be a seed node. May 17, 2019 11:04:47 PM org.apache.cassandra.dht.BootStrapper getRandomTokens INFO: Generated random tokens. tokens are [8093352573651155296, 1255299761205442648, 237147709902316497, -6414460801948159706, -692265631546473720, -6349925106465632799, -2290838599616757177, -3899693250198673757, -2772993994204185575, 8089087866004712702, 5068231735858929302, -5167558644545589427, 360993454762086375, 542196909604039136, -6803374460605138731, 196083555209218770, 1200241599712333919, 5654227404071275843, -5045148903690459458, 4233502583565513707, -523445579585129460, 2958269909267187837, 9172778663138516827, -3502069657471259971, 4724375047660570158, -4732389872622011480, -9137803554162665016, -9185131448000476789, 3092260609951491424, -2630316306597263272, -8916593523738972270, -6360769090423175994, -627054276841973363, 6671286854360171008, -3961913191655267891, -7334199941956292508, -6726962132385052978, -5689059185431163971, -8555845346281537704, 258830492211657319, -2012653712297937721, -108744039450423576, -4279678067298205166, 4165697862718949699, 2576622820696309909, -5008119979048704246, -2285465861188851889, 4891213962043930277, 6285391703394904105, 8704893385318118726, -7896826813047434663, -7705819714374060762, 7832406484262255621, -7654457107611978200, 8028994224167636869, 2156943975963993409, -8976281727457930425, 2465790698024335339, -2923514477979153840, -5534775296551680237, 5171266349312716631, -8067020858126907240, -6438955773577643397, -4574461298587954869, -8086007337896874221, -9020656032217757737, 6802061822095527833, 634933143159190575, 8854643586671015212, 1572432267311007224, -5603139559683554056, -7258629247484114560, -7840897423950915644, -1510217434039563032, -2463357311643117503, -4956325040798418202, 6115500955407781739, -1288275576494238535, 4215864935527564765, -1528962903232931779, 4722604763344765571, -8009160379731874423, 110051793379117553, -5528502958182099977, 4198908537864434211, 2641888716575592527, 3231921528527706170, -6938892237196787092, 418041016709170962, -7653994733786236206, 2469917086338061223, 1079278824335470082, 6736784162733336692, -8030387927768979384, 6713594463776934402, 3165444661626966112, 5464395238541882379, 1461419198279863021, -2562809359187005873, 1124425781415188374, -2569808172913592059, -4092265839974429223, -4880153680442413715, -894215290179624781, -9165290107648163562, 6216755920000555762, -1768683908499186420, 6287859442550038405, 7613641812300206749, -8986039305765910186, 1954968829184633473, 120396840549105166, 5968185275773637365, 7817207113775068867, -3765832827667510549, -2500499152800462122, 2420316341545565292, 644084408207246298, -6548768431956015913, 4009978366402998467, -3425410990667570140, -2203588983327017275, 733600460589514769, -7058110119040280524, -1405799009076200914, -1123504967753634583, 9181667504859078977, -8238760864303404599, 4449186665707654636, -2960056327251205986, 9189518171322379319, 6947223910578039083, 7229865026293424791, 2859819456122220525, -1760673821529453936, 6279931866850183612, 3497885641441302767, 8761003819313442817, 2110792100219973664, 7456836699479186347, 8767205395914010629, -8956292176658621817, -1645147337685702874, 651038600548393679, 8165948044651900891, 8275308658171828619, -7992315121699784826, -8354912541684785898, 2180667319387347270, 1025787774451618477, -6693229870234782627, 3697519985506059077, -3078016470170095383, -4867435902462279385, -7759647966862286519, 4858823337556132698, 4634185145226648946, 3006334866876824389, -7019121348909772983, 2087819390509042555, 7579302668431861476, -7764809498129828122, 6598015818679359595, 2643888475606998288, 7657481826623149025, -7159216797148055429, -1610342617087606022, 295338057838655801, -2248334549615903249, 1294398799408522281, -5471839772483676020, 2380862709566656725, -8921246924176401786, -5331505393430668537, -9034389757194010743, 49099361966265077, -1536871342039878583, -6595583135595602607, 437910709730145489, 3719684372144981914, -3363705708844193231, -3679775963116412486, 495029353463672229, 5372366695012296098, -7677546916342470961, -4880078139188303738, 2824219870897099226, 466150252297919736, 6123756633715004105, 8234000794380517516, -639405188749924353, 557391272433690626, 5388374558275047393, -9063025121891929051, -2221197433508671069, -5352076688185095490, -614038822115480191, 6982237274105733687, 1725370053086309085, 2139064782599299159, -2715718715374781476, 6532040807530487427, 1858697887947264613, -6101748565350294708, -4193768923363794451, -7073714388701263724, 2297276102202798791, 3409174302370381067, 7052561865330527385, 7293925472445780344, -8144295387956979749, -8395061389337081336, -931537260467574351, -5744274890418349087, 4386978002114722807, -2230828584397437012, 7401360442485867519, -529141481852289027, 6310248396544777180, 3006748263747810337, 5247660381947867091, -2982267494042199357, 1703623175831153876, 605288953459305900, -6240420935739369793, -4697342188296711353, 7149235565936425070, -5638304968590074846, 4927374814091328393, -2609957475989996229, -942888087290082484, 9027658438748354601, -5378658879746186723, -4934600066053140635, -7145058756992644667, 3207512849369184134, -4417758346814724156, -4181484160616208152, 1807932121829189064, -8943206203830810380, 1904907066863479762, 8408232346186145010, 5644195823981246106, -8633215221899656513, 174144876651303971, 3514379945891942008, 1833465755796893669, 7456052468380217561, -4063129660506161593, -860398053457377468, -988217697559262536, 1625637073381446050, 4465738687272934193, -5081383180402834915, 2880948967491671689, 2477333910322983258] May 17, 2019 11:04:48 PM org.apache.cassandra.service.MigrationManager announceNewKeyspace INFO: Create new Keyspace: KeyspaceMetadata\{name=system_traces, params=KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=2}}, tables=[org.apache.cassandra.config.CFMetaData@16e9f0b9[cfId=c5e99f16-8677-3914-b17e-960613512345,ksName=system_traces,cfName=sessions,flags=[COMPOUND],params=TableParams\{comment=tracing sessions, read_repair_chance=0.0, dclocal_read_repair_chance=0.0, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=0, default_time_to_live=0, memtable_flush_period_in_ms=3600000, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams\{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@7b8bdf9c, extensions={}, cdc=false},comparator=comparator(),partitionColumns=[[] | [client command coordinator duration request started_at parameters]],partitionKeyColumns=[session_id],clusteringColumns=[],keyValidator=org.apache.cassandra.db.marshal.UUIDType,columnMetadata=[client, command, session_id, coordinator, request, started_at, duration, parameters],droppedColumns={},triggers=[],indexes=[]], org.apache.cassandra.config.CFMetaData@6d42f846[cfId=8826e8e9-e16a-3728-8753-3bc1fc713c25,ksName=system_traces,cfName=events,flags=[COMPOUND],params=TableParams\{comment=tracing events, read_repair_chance=0.0, dclocal_read_repair_chance=0.0, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=0, default_time_to_live=0, memtable_flush_period_in_ms=3600000, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams\{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@7b8bdf9c, extensions={}, cdc=false},comparator=comparator(org.apache.cassandra.db.marshal.TimeUUIDType),partitionColumns=[[] | [activity source source_elapsed thread]],partitionKeyColumns=[session_id],clusteringColumns=[event_id],keyValidator=org.apache.cassandra.db.marshal.UUIDType,columnMetadata=[activity, session_id, thread, event_id, source, source_elapsed],droppedColumns={},triggers=[],indexes=[]]], views=[], functions=[], types=[]} May 17, 2019 11:04:48 PM com.datastax.driver.core.ClockFactory newInstance INFO: Using native clock to generate timestamps. May 17, 2019 11:04:49 PM org.apache.cassandra.db.view.ViewManager reload INFO: Not submitting build tasks for views in keyspace system_traces as storage service is not initialized May 17, 2019 11:04:49 PM org.apache.cassandra.db.ColumnFamilyStore <init> INFO: Initializing system_traces.events May 17, 2019 11:04:49 PM org.apache.cassandra.db.ColumnFamilyStore <init> INFO: Initializing system_traces.sessions May 17, 2019 11:04:49 PM org.apache.cassandra.service.MigrationManager announceNewKeyspace INFO: Create new Keyspace: KeyspaceMetadata\{name=system_distributed, params=KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=3}}, tables=[org.apache.cassandra.config.CFMetaData@710f512e[cfId=759fffad-624b-3181-80ee-fa9a52d1f627,ksName=system_distributed,cfName=repair_history,flags=[COMPOUND],params=TableParams\{comment=Repair history, read_repair_chance=0.0, dclocal_read_repair_chance=0.0, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=864000, default_time_to_live=0, memtable_flush_period_in_ms=3600000, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams\{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@7b8bdf9c, extensions={}, cdc=false},comparator=comparator(org.apache.cassandra.db.marshal.TimeUUIDType),partitionColumns=[[] | [coordinator exception_message exception_stacktrace finished_at parent_id range_begin range_end started_at status participants]],partitionKeyColumns=[keyspace_name, columnfamily_name],clusteringColumns=[id],keyValidator=org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type),columnMetadata=[status, id, coordinator, finished_at, participants, exception_stacktrace, parent_id, range_end, range_begin, exception_message, keyspace_name, started_at, columnfamily_name],droppedColumns={},triggers=[],indexes=[]], org.apache.cassandra.config.CFMetaData@31b2c0c7[cfId=deabd734-b99d-3b9c-92e5-fd92eb5abf14,ksName=system_distributed,cfName=parent_repair_history,flags=[COMPOUND],params=TableParams\{comment=Repair history, read_repair_chance=0.0, dclocal_read_repair_chance=0.0, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=864000, default_time_to_live=0, memtable_flush_period_in_ms=3600000, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams\{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@7b8bdf9c, extensions={}, cdc=false},comparator=comparator(),partitionColumns=[[] | [exception_message exception_stacktrace finished_at keyspace_name started_at columnfamily_names options requested_ranges successful_ranges]],partitionKeyColumns=[parent_id],clusteringColumns=[],keyValidator=org.apache.cassandra.db.marshal.TimeUUIDType,columnMetadata=[requested_ranges, exception_message, keyspace_name, successful_ranges, started_at, finished_at, options, exception_stacktrace, parent_id, columnfamily_names],droppedColumns={},triggers=[],indexes=[]], org.apache.cassandra.config.CFMetaData@3ebbc227[cfId=5582b59f-8e4e-35e1-b913-3acada51eb04,ksName=system_distributed,cfName=view_build_status,flags=[COMPOUND],params=TableParams\{comment=Materialized View build status, read_repair_chance=0.0, dclocal_read_repair_chance=0.0, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=864000, default_time_to_live=0, memtable_flush_period_in_ms=3600000, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams\{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@7b8bdf9c, extensions={}, cdc=false},comparator=comparator(org.apache.cassandra.db.marshal.UUIDType),partitionColumns=[[] | [status]],partitionKeyColumns=[keyspace_name, view_name],clusteringColumns=[host_id],keyValidator=org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type),columnMetadata=[view_name, status, keyspace_name, host_id],droppedColumns={},triggers=[],indexes=[]]], views=[], functions=[], types=[]} May 17, 2019 11:04:50 PM org.apache.cassandra.db.view.ViewManager reload INFO: Not submitting build tasks for views in keyspace system_distributed as storage service is not initialized May 17, 2019 11:04:50 PM org.apache.cassandra.db.ColumnFamilyStore <init> INFO: Initializing system_distributed.parent_repair_history May 17, 2019 11:04:50 PM org.apache.cassandra.db.ColumnFamilyStore <init> INFO: Initializing system_distributed.repair_history May 17, 2019 11:04:50 PM org.apache.cassandra.db.ColumnFamilyStore <init> INFO: Initializing system_distributed.view_build_status May 17, 2019 11:04:50 PM org.apache.cassandra.service.StorageService setMode INFO: JOINING: Finish joining ring May 17, 2019 11:04:50 PM com.datastax.driver.core.NettyUtil <clinit> WARNING: Found Netty's native epoll transport in the classpath, but epoll is not available. Using NIO instead. java.lang.UnsatisfiedLinkError: could not load a native library: netty-transport-native-epoll at io.netty.util.internal.NativeLibraryLoader.load(NativeLibraryLoader.java:233) at io.netty.channel.epoll.Native.loadNativeLibrary(Native.java:269) at io.netty.channel.epoll.Native.<clinit>(Native.java:64) at io.netty.channel.epoll.Epoll.<clinit>(Epoll.java:33) at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:264) at com.datastax.driver.core.NettyUtil.<clinit>(NettyUtil.java:57) at com.datastax.driver.core.NettyOptions.eventLoopGroup(NettyOptions.java:95) at com.datastax.driver.core.Connection$Factory.<init>(Connection.java:926) at com.datastax.driver.core.Cluster$Manager.init(Cluster.java:1499) at com.datastax.driver.core.Cluster.init(Cluster.java:208) at com.datastax.driver.core.Cluster.connectAsync(Cluster.java:376) at com.datastax.driver.core.Cluster.connectAsync(Cluster.java:355) at com.datastax.driver.core.Cluster.connect(Cluster.java:305) at info.archinnov.achilles.embedded.AchillesInitializer.initializeFromParameters(AchillesInitializer.java:63) at info.archinnov.achilles.embedded.CassandraEmbeddedServer.<init>(CassandraEmbeddedServer.java:64) at info.archinnov.achilles.embedded.CassandraEmbeddedServerBuilder.buildNativeCluster(CassandraEmbeddedServerBuilder.java:535) at org.apache.beam.sdk.io.cassandra.CassandraIOTest.beforeClass(CassandraIOTest.java:131) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.run(ParentRunner.java:396) at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.runTestClass(JUnitTestClassExecutor.java:110) at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:58) at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:38) at org.gradle.api.internal.tasks.testing.junit.AbstractJUnitTestClassProcessor.processTestClass(AbstractJUnitTestClassProcessor.java:62) at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:51) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24) at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32) at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93) at com.sun.proxy.$Proxy2.processTestClass(Unknown Source) at org.gradle.api.internal.tasks.testing.worker.TestWorker.processTestClass(TestWorker.java:118) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24) at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:175) at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:157) at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:404) at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63) at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:46) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:55) at java.lang.Thread.run(Thread.java:748) Suppressed: java.lang.UnsatisfiedLinkError: no netty-transport-native-epoll in java.library.path at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1867) at java.lang.Runtime.loadLibrary0(Runtime.java:870) at java.lang.System.loadLibrary(System.java:1122) at io.netty.util.internal.NativeLibraryUtil.loadLibrary(NativeLibraryUtil.java:38) at io.netty.util.internal.NativeLibraryLoader.loadLibrary(NativeLibraryLoader.java:316) at io.netty.util.internal.NativeLibraryLoader.load(NativeLibraryLoader.java:136) ... 60 more Suppressed: java.lang.UnsatisfiedLinkError: no netty-transport-native-epoll in java.library.path at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1867) at java.lang.Runtime.loadLibrary0(Runtime.java:870) at java.lang.System.loadLibrary(System.java:1122) at io.netty.util.internal.NativeLibraryUtil.loadLibrary(NativeLibraryUtil.java:38) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at io.netty.util.internal.NativeLibraryLoader$1.run(NativeLibraryLoader.java:336) at java.security.AccessController.doPrivileged(Native Method) at io.netty.util.internal.NativeLibraryLoader.loadLibraryByHelper(NativeLibraryLoader.java:328) at io.netty.util.internal.NativeLibraryLoader.loadLibrary(NativeLibraryLoader.java:306) ... 61 more Caused by: java.lang.RuntimeException: failed to get field ID: DefaultFileRegion.transfered at java.lang.ClassLoader$NativeLibrary.load(Native Method) at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941) at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824) at java.lang.Runtime.load0(Runtime.java:809) at java.lang.System.load(System.java:1086) at io.netty.util.internal.NativeLibraryUtil.loadLibrary(NativeLibraryUtil.java:36) at io.netty.util.internal.NativeLibraryLoader.loadLibrary(NativeLibraryLoader.java:316) at io.netty.util.internal.NativeLibraryLoader.load(NativeLibraryLoader.java:215) ... 60 more May 17, 2019 11:04:50 PM org.apache.cassandra.service.MigrationManager announceNewKeyspace INFO: Create new Keyspace: KeyspaceMetadata\{name=system_auth, params=KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=1}}, tables=[org.apache.cassandra.config.CFMetaData@e261744[cfId=5bc52802-de25-35ed-aeab-188eecebb090,ksName=system_auth,cfName=roles,flags=[COMPOUND],params=TableParams\{comment=role definitions, read_repair_chance=0.0, dclocal_read_repair_chance=0.0, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=7776000, default_time_to_live=0, memtable_flush_period_in_ms=3600000, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams\{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@7b8bdf9c, extensions={}, cdc=false},comparator=comparator(),partitionColumns=[[] | [can_login is_superuser salted_hash member_of]],partitionKeyColumns=[role],clusteringColumns=[],keyValidator=org.apache.cassandra.db.marshal.UTF8Type,columnMetadata=[salted_hash, member_of, role, can_login, is_superuser],droppedColumns={},triggers=[],indexes=[]], org.apache.cassandra.config.CFMetaData@56b519c[cfId=0ecdaa87-f8fb-3e60-88d1-74fb36fe5c0d,ksName=system_auth,cfName=role_members,flags=[COMPOUND],params=TableParams\{comment=role memberships lookup table, read_repair_chance=0.0, dclocal_read_repair_chance=0.0, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=7776000, default_time_to_live=0, memtable_flush_period_in_ms=3600000, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams\{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@7b8bdf9c, extensions={}, cdc=false},comparator=comparator(org.apache.cassandra.db.marshal.UTF8Type),partitionColumns=[[] | []],partitionKeyColumns=[role],clusteringColumns=[member],keyValidator=org.apache.cassandra.db.marshal.UTF8Type,columnMetadata=[role, member],droppedColumns={},triggers=[],indexes=[]], org.apache.cassandra.config.CFMetaData@401bfb07[cfId=3afbe79f-2194-31a7-add7-f5ab90d8ec9c,ksName=system_auth,cfName=role_permissions,flags=[COMPOUND],params=TableParams\{comment=permissions granted to db roles, read_repair_chance=0.0, dclocal_read_repair_chance=0.0, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=7776000, default_time_to_live=0, memtable_flush_period_in_ms=3600000, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams\{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@7b8bdf9c, extensions={}, cdc=false},comparator=comparator(org.apache.cassandra.db.marshal.UTF8Type),partitionColumns=[[] | [permissions]],partitionKeyColumns=[role],clusteringColumns=[resource],keyValidator=org.apache.cassandra.db.marshal.UTF8Type,columnMetadata=[resource, permissions, role],droppedColumns={},triggers=[],indexes=[]], org.apache.cassandra.config.CFMetaData@45f0522b[cfId=5f2fbdad-91f1-3946-bd25-d5da3a5c35ec,ksName=system_auth,cfName=resource_role_permissons_index,flags=[COMPOUND],params=TableParams\{comment=index of db roles with permissions granted on a resource, read_repair_chance=0.0, dclocal_read_repair_chance=0.0, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=7776000, default_time_to_live=0, memtable_flush_period_in_ms=3600000, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams\{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@7b8bdf9c, extensions={}, cdc=false},comparator=comparator(org.apache.cassandra.db.marshal.UTF8Type),partitionColumns=[[] | []],partitionKeyColumns=[resource],clusteringColumns=[role],keyValidator=org.apache.cassandra.db.marshal.UTF8Type,columnMetadata=[resource, role],droppedColumns={},triggers=[],indexes=[]]], views=[], functions=[], types=[]} May 17, 2019 11:04:51 PM org.apache.cassandra.db.view.ViewManager reload INFO: Not submitting build tasks for views in keyspace system_auth as storage service is not initialized May 17, 2019 11:04:51 PM org.apache.cassandra.db.ColumnFamilyStore <init> INFO: Initializing system_auth.resource_role_permissons_index May 17, 2019 11:04:51 PM org.apache.cassandra.db.ColumnFamilyStore <init> INFO: Initializing system_auth.role_members May 17, 2019 11:04:51 PM org.apache.cassandra.db.ColumnFamilyStore <init> INFO: Initializing system_auth.role_permissions May 17, 2019 11:04:51 PM org.apache.cassandra.db.ColumnFamilyStore <init> INFO: Initializing system_auth.roles May 17, 2019 11:04:52 PM org.apache.cassandra.service.NativeTransportService initialize INFO: Netty using Java NIO event loop May 17, 2019 11:04:53 PM org.apache.cassandra.transport.Server start INFO: Using Netty Version: [netty-buffer=netty-buffer-4.0.44.Final.452812a, netty-codec=netty-codec-4.0.44.Final.452812a, netty-codec-haproxy=netty-codec-haproxy-4.0.44.Final.452812a, netty-codec-http=netty-codec-http-4.0.44.Final.452812a, netty-codec-socks=netty-codec-socks-4.0.44.Final.452812a, netty-common=netty-common-4.0.44.Final.452812a, netty-handler=netty-handler-4.0.44.Final.452812a, netty-resolver=netty-resolver-4.1.30.Final.3a9ac82, netty-tcnative=netty-tcnative-1.1.33.Fork26.142ecbb, netty-transport=netty-transport-4.0.44.Final.452812a, netty-transport-native-epoll=netty-transport-native-epoll-4.0.44.Final.452812a, netty-transport-rxtx=netty-transport-rxtx-4.0.44.Final.452812a, netty-transport-sctp=netty-transport-sctp-4.0.44.Final.452812a, netty-transport-udt=netty-transport-udt-4.0.44.Final.452812a] May 17, 2019 11:04:53 PM org.apache.cassandra.transport.Server start INFO: Starting listening for CQL clients on localhost/127.0.0.1:9042 (unencrypted)... May 17, 2019 11:04:53 PM info.archinnov.achilles.embedded.CassandraShutDownHook shutDownNow INFO: Calling stop on Embedded Cassandra server May 17, 2019 11:04:53 PM org.apache.cassandra.service.CassandraDaemon start INFO: Not starting RPC server as requested. Use JMX (StorageService->startRPCServer()) or nodetool (enablethrift) to start it

 

 

 

 

java.lang.NullPointerException at info.archinnov.achilles.embedded.CassandraShutDownHook.shutDownNow(CassandraShutDownHook.java:81) at org.apache.beam.sdk.io.cassandra.CassandraIOTest.afterClass(CassandraIOTest.java:141) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.run(ParentRunner.java:396) at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.runTestClass(JUnitTestClassExecutor.java:110) at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:58) at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:38) at org.gradle.api.internal.tasks.testing.junit.AbstractJUnitTestClassProcessor.processTestClass(AbstractJUnitTestClassProcessor.java:62) at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:51) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24) at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32) at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93) at com.sun.proxy.$Proxy2.processTestClass(Unknown Source) at org.gradle.api.internal.tasks.testing.worker.TestWorker.processTestClass(TestWorker.java:118) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24) at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:175) at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:157) at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:404) at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63) at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:46) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:55) at java.lang.Thread.run(Thread.java:748)",CassandraIOTest failing in presubmit,2,,,ajamato@google.com,True,,ajamato@google.com
beam,BEAM-7342,2019-05-16T22:30:39.000+0000,2019-06-07T16:23:34.000+0000,2019-06-07T16:23:34.000+0000,,Fixed,New Feature,Minor,['2.14.0'],,25200,25200,,60,,100,['testing'],['Testing: general infrastructure'],"Add the ability for map steps to be configured to be splittable. 
Possible configuration options:

 - uneven bundle sizes

 - possible incorrect sizing returned",Extend SyntheticPipeline map steps to be able to be splittable (Beam Python SDK),1,,,laraschmidt,True,laraschmidt,laraschmidt
beam,BEAM-7306,2019-05-14T21:18:12.000+0000,,2019-06-12T08:29:30.000+0000,,,New Feature,Major,,,,,,,,,['dsl-sql'],['DSLs: SQL'],"Currently we reject aggregations with the DISTINCT flag set: [https://github.com/apache/beam/pull/8498]

We should provide support for these aggregations in a scalable way. See the ML discussion on this topic here: [https://lists.apache.org/thread.html/24081b0d0b7f9709a5c0f574149fb6b9e9759cba06734200cf3810bf@%3Cdev.beam.apache.org%3E]",[SQL] Add support for distinct aggregations,1,,,bhulette,True,,bhulette
beam,BEAM-7305,2019-05-14T19:07:32.000+0000,2019-05-15T09:32:00.000+0000,2019-07-04T11:23:04.000+0000,,Fixed,New Feature,Major,['2.14.0'],,33000,33000,,,,100,['runner-jet'],[''],,Add first version of Hazelcast Jet Runner,1,,,mxm,True,jbartok,mxm
beam,BEAM-7304,2019-05-14T17:51:50.000+0000,,2019-05-15T12:05:36.000+0000,,,New Feature,Minor,,,,,,,,,['runner-ideas'],['Runners: ideas for new Beam runners'],"Twister2 is a big data framework which supports both batch and stream processing [1] [2]. The goal is to develop an beam runner for Twister2. 

[1] [https://github.com/DSC-SPIDAL/twister2]

[2] [https://twister2.gitbook.io/twister2/]",Twister2 Beam runner,3,,,pulasthisupun,True,pulasthisupun,pulasthisupun
beam,BEAM-7280,2019-05-13T18:14:25.000+0000,,2019-06-12T08:35:28.000+0000,,,New Feature,Major,,,,,,,,,['io-java-gcp'],"['IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)']",The [BigQueryIO setMaxRetryJobs|https://github.com/apache/beam/blob/c02af609c49f268366cdecf680621e907700e59b/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIO.java#L2171] value is hard coded to 1000. It would be useful to have the 1000 set as default and add a method allowing the user to configure this value as desired. ,Allow for setMaxRetryJobs in BigQueryIO to be configurable,2,,"['bigquery', 'gcp']",Ali T,True,,Ali T
beam,BEAM-7259,2019-05-10T00:03:05.000+0000,2019-06-04T17:22:56.000+0000,2019-06-04T17:22:56.000+0000,,Implemented,New Feature,Minor,['Not applicable'],,,,,,,,"['project-management', 'testing']","['Project Management: monthly reports, administrative tasks', 'Testing: general infrastructure']","Hey Mikhail, could you create a view of the Greenness metric in Grafana ([Stability Critical Jobs Status|http://104.154.241.245/d/McTAiu0ik/stability-critical-jobs-status?orgId=1]) that's aggregated per month? Specifically, as an average greenness for an entire calendar month (ex. April 1 - April 30).

I need to track this metric and seeing the weekly greenness makes it hard to average over a month, especially because different weeks may have different numbers of runs, and they don't line up perfectly over a calendar month. I already tried changing the timespan in the upper right, but when I set it to a month range the graph's points are still one point per week instead of a monthly average.

Also just to be clear, this shouldn't replace the existing graph, just be a new one placed wherever makes the most sense.

 ","Add monthly aggregated view for ""Stability critical jobs status - Greenness per week""",1,,,danoliveira,True,Ardagan,danoliveira
beam,BEAM-7249,2019-05-08T18:46:58.000+0000,2019-06-12T08:57:37.000+0000,2019-06-12T08:57:37.000+0000,,Invalid,New Feature,Trivial,['Not applicable'],,,,,,,,['io-python-gcp'],['IO: Python: Google Cloud Platform'],,Ability to cancel Cloud Bigtable reads,2,,,mf2199,True,,mf2199
beam,BEAM-7236,2019-05-06T23:40:11.000+0000,,2019-05-07T07:46:14.000+0000,,,New Feature,Minor,,,,,,,,,['runner-spark'],['Runners: Spark runner'],"The Beam documentation mentions that Flatten does not require coders of all inputs to be of the same type, as long as the result type is the same. However, the current implementation in the Spark Runner requires all coders to match.",Support transcoding for Flatten in Spark Runner,1,,,ibzib,True,,ibzib
beam,BEAM-7233,2019-05-06T22:22:10.000+0000,,2019-06-07T09:09:35.000+0000,,,New Feature,Major,,,,,,,,,['runner-spark'],['Runners: Spark runner'],,Spark portable runner: support Bundle Finalization,1,,['portability-spark'],ibzib,True,ibzib,ibzib
beam,BEAM-7222,2019-05-03T22:33:47.000+0000,,2019-06-07T09:11:54.000+0000,,,New Feature,Major,,,,,,,,,['runner-spark'],['Runners: Spark runner'],,Spark portable runner: support SDF,1,,['portability-spark'],ibzib,True,ibzib,ibzib
beam,BEAM-7221,2019-05-03T22:30:06.000+0000,2019-06-13T15:24:19.000+0000,2019-06-13T15:24:19.000+0000,,Fixed,New Feature,Major,['2.14.0'],,4200,4200,,,,100,['runner-spark'],['Runners: Spark runner'],,Spark portable runner: support timers,1,,['portability-spark'],ibzib,True,ibzib,ibzib
beam,BEAM-7219,2019-05-03T22:14:02.000+0000,,2019-06-07T09:12:47.000+0000,,,New Feature,Major,,,,,,,,,['runner-spark'],['Runners: Spark runner'],"The Spark portable runner already updates and collects metrics from the SDK harness, but it needs to send the collected metrics on to Spark itself, similar to FlinkMetricContainer ([https://github.com/apache/beam/blob/5c31efbae5fa40c024a47a44af3e6bc8a079cc2a/runners/flink/src/main/java/org/apache/beam/runners/flink/metrics/FlinkMetricContainer.java]).",Spark portable runner: support Spark metrics,1,,['portability-spark'],ibzib,True,ibzib,ibzib
beam,BEAM-7214,2019-05-02T20:46:55.000+0000,2019-05-07T22:09:13.000+0000,2019-06-07T09:13:02.000+0000,,Fixed,New Feature,Major,['2.13.0'],,4200,4200,,,,100,['runner-spark'],['Runners: Spark runner'],"We will need something like FlinkRunnerTest [1] to verify that the Spark runner can run Python pipelines correctly.

[1] [https://github.com/apache/beam/blob/master/sdks/python/apache_beam/runners/portability/flink_runner_test.py]",Run Python validates runner tests on Spark,1,,['portability-spark'],ibzib,True,ibzib,ibzib
beam,BEAM-7212,2019-05-02T16:57:46.000+0000,2019-05-03T13:35:38.000+0000,2019-05-03T13:35:38.000+0000,,Duplicate,New Feature,Major,['Not applicable'],,600,600,,,,100,['io-python-gcp'],['IO: Python: Google Cloud Platform'],,Google Cloud Bigtable IO Connector for Apache Beam,1,,,mf2199,True,,mf2199
beam,BEAM-7191,2019-04-30T17:18:27.000+0000,,2019-06-06T23:40:08.000+0000,,,New Feature,Major,,,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","This would be useful to catch mistakes which should not necessarily fail a pipeline (i.e. a metric missing required metdata should be dropped and quietly logged). Logging this every time the metric is produced would be noisy,

 

an API like log every n occurances or log once ever n seconds would help surface these without filling the logs with spam.

 

See TODO markers for this Jira issue as well to find some places to add this.",Add quieter debug logging API for beam developers.,1,,,ajamato@google.com,True,,ajamato@google.com
beam,BEAM-7165,2019-04-26T20:05:16.000+0000,,2019-04-26T20:05:16.000+0000,,,New Feature,Major,,,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","[https://builds.apache.org/job/beam_PreCommit_Java_Commit/5634/testReport/junit/org.apache.beam.sdk.io/FileIOTest/testMatchWatchForNewFiles/]

Note: This test was flakey and fixed in BEAM-6491, filed this new ticket since I am not sure if its the same issue.
 
h3. Stacktrace
java.lang.AssertionError: FileIO.MatchAll/Reshuffle.ViaRandomKey/Values/Values/Map/ParMultiDo(Anonymous).output: Expected: iterable with items [<Metadata\{resourceId=/tmp/junit7697965342227553672/watch/first, sizeBytes=42, isReadSeekEfficient=true, lastModifiedMillis=0}>, <Metadata\{resourceId=/tmp/junit7697965342227553672/watch/second, sizeBytes=37, isReadSeekEfficient=true, lastModifiedMillis=0}>, <Metadata\{resourceId=/tmp/junit7697965342227553672/watch/third, sizeBytes=99, isReadSeekEfficient=true, lastModifiedMillis=0}>] in any order but: not matched: <Metadata\{resourceId=/tmp/junit7697965342227553672/watch/first, sizeBytes=0, isReadSeekEfficient=true, lastModifiedMillis=0}> at org.apache.beam.sdk.testing.PAssert$PAssertionSite.capture(PAssert.java:169) at org.apache.beam.sdk.testing.PAssert.that(PAssert.java:393) at org.apache.beam.sdk.testing.PAssert.that(PAssert.java:385) at org.apache.beam.sdk.io.FileIOTest.testMatchWatchForNewFiles(FileIOTest.java:262) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.apache.beam.sdk.testing.TestPipeline$1.evaluate(TestPipeline.java:319) at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:265) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54) at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:349) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:314) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:312) at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:292) at org.junit.runners.ParentRunner.run(ParentRunner.java:396) at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.runTestClass(JUnitTestClassExecutor.java:110) at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:58) at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:38) at org.gradle.api.internal.tasks.testing.junit.AbstractJUnitTestClassProcessor.processTestClass(AbstractJUnitTestClassProcessor.java:62) at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:51) at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24) at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32) at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93) at com.sun.proxy.$Proxy2.processTestClass(Unknown Source) at org.gradle.api.internal.tasks.testing.worker.TestWorker.processTestClass(TestWorker.java:118) at sun.reflect.GeneratedMethodAccessor27.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24) at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:175) at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:157) at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:404) at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63) at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:46) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:55) Caused by: java.lang.AssertionError: Expected: iterable with items [<Metadata\{resourceId=/tmp/junit7697965342227553672/watch/first, sizeBytes=42, isReadSeekEfficient=true, lastModifiedMillis=0}>, <Metadata\{resourceId=/tmp/junit7697965342227553672/watch/second, sizeBytes=37, isReadSeekEfficient=true, lastModifiedMillis=0}>, <Metadata\{resourceId=/tmp/junit7697965342227553672/watch/third, sizeBytes=99, isReadSeekEfficient=true, lastModifiedMillis=0}>] in any order but: not matched: <Metadata\{resourceId=/tmp/junit7697965342227553672/watch/first, sizeBytes=0, isReadSeekEfficient=true, lastModifiedMillis=0}> at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:18) at org.junit.Assert.assertThat(Assert.java:966) at org.junit.Assert.assertThat(Assert.java:931) at org.apache.beam.sdk.testing.PAssert$AssertContainsInAnyOrder.apply(PAssert.java:1459) at org.apache.beam.sdk.testing.PAssert$AssertContainsInAnyOrder.apply(PAssert.java:1438) at org.apache.beam.sdk.testing.PAssert$CheckRelationAgainstExpected.apply(PAssert.java:1108) at org.apache.beam.sdk.testing.PAssert$CheckRelationAgainstExpected.apply(PAssert.java:1088) at org.apache.beam.sdk.testing.PAssert.doChecks(PAssert.java:1387) at org.apache.beam.sdk.testing.PAssert$GroupedValuesCheckerDoFn.processElement(PAssert.java:1354) at org.apache.beam.sdk.testing.PAssert$GroupedValuesCheckerDoFn$DoFnInvoker.invokeProcessElement(Unknown Source) at org.apache.beam.repackaged.beam_runners_direct_java.runners.core.SimpleDoFnRunner.invokeProcessElement(SimpleDoFnRunner.java:213) at org.apache.beam.repackaged.beam_runners_direct_java.runners.core.SimpleDoFnRunner.processElement(SimpleDoFnRunner.java:178) at org.apache.beam.repackaged.beam_runners_direct_java.runners.core.SimplePushbackSideInputDoFnRunner.processElementInReadyWindows(SimplePushbackSideInputDoFnRunner.java:78) at org.apache.beam.runners.direct.ParDoEvaluator.processElement(ParDoEvaluator.java:211) at org.apache.beam.runners.direct.DoFnLifecycleManagerRemovingTransformEvaluator.processElement(DoFnLifecycleManagerRemovingTransformEvaluator.java:54) at org.apache.beam.runners.direct.DirectTransformExecutor.processElements(DirectTransformExecutor.java:160) at org.apache.beam.runners.direct.DirectTransformExecutor.run(DirectTransformExecutor.java:124) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)",FileIOTest.testMatchWatchForNewFiles flakey in java presubmit,1,,,ajamato@google.com,True,,ajamato@google.com
beam,BEAM-7164,2019-04-26T16:39:41.000+0000,,2019-04-30T17:44:21.000+0000,,,New Feature,Major,,,,,,,,,['sdk-py-core'],['SDKs: Python'],"*https://builds.apache.org/job/beam_PreCommit_Python_Commit/6035/consoleFull*


*18:05:44* >
 *Task :beam-sdks-python-test-suites-dataflow:setupVirtualenv*
*18:05:44* New python executable in /home/jenkins/jenkins-slave/workspace/beam_PreCommit_Python_Commit/src/build/gradleenv/-410805238/bin/python2.7*18:05:44* Also creating executable in /home/jenkins/jenkins-slave/workspace/beam_PreCommit_Python_Commit/src/build/gradleenv/-410805238/bin/python*18:05:44* Installing setuptools, pkg_resources, pip, wheel...done.*18:05:44* Running virtualenv with interpreter /usr/bin/python2.7*18:05:44* DEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7.*18:05:44* Collecting tox==3.0.0*18:05:44*   Using cached 
[https://files.pythonhosted.org/packages/e6/41/4dcfd713282bf3213b0384320fa8841e4db032ddcb80bc08a540159d42a8/tox-3.0.0-py2.py3-none-any.whl]
*18:05:44* Collecting grpcio-tools==1.3.5*18:05:44*   Using cached 
[https://files.pythonhosted.org/packages/05/f6/0296e29b1bac6f85d2a8556d48adf825307f73109a3c2c17fb734292db0a/grpcio_tools-1.3.5-cp27-cp27mu-manylinux1_x86_64.whl]
*18:05:44* Collecting pluggy<1.0,>=0.3.0 (from tox==3.0.0)*18:05:44*   Using cached 
[https://files.pythonhosted.org/packages/84/e8/4ddac125b5a0e84ea6ffc93cfccf1e7ee1924e88f53c64e98227f0af2a5f/pluggy-0.9.0-py2.py3-none-any.whl]
*18:05:44* Collecting six (from tox==3.0.0)*18:05:44*   Using cached 
[https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl]
*18:05:44* Collecting virtualenv>=1.11.2 (from tox==3.0.0)*18:05:44*   Using cached 
[https://files.pythonhosted.org/packages/4f/ba/6f9315180501d5ac3e707f19fcb1764c26cc6a9a31af05778f7c2383eadb/virtualenv-16.5.0-py2.py3-none-any.whl]
*18:05:44* Collecting py>=1.4.17 (from tox==3.0.0)*18:05:44*   Using cached 
[https://files.pythonhosted.org/packages/76/bc/394ad449851729244a97857ee14d7cba61ddb268dce3db538ba2f2ba1f0f/py-1.8.0-py2.py3-none-any.whl]
*18:05:44* Collecting grpcio>=1.3.5 (from grpcio-tools==1.3.5)*18:05:44*   Using cached 
[https://files.pythonhosted.org/packages/7c/59/4da8df60a74f4af73ede9d92a75ca85c94bc2a109d5f67061496e8d496b2/grpcio-1.20.0-cp27-cp27mu-manylinux1_x86_64.whl]
*18:05:44* Collecting protobuf>=3.2.0 (from grpcio-tools==1.3.5)*18:05:44*   Using cached 
[https://files.pythonhosted.org/packages/ea/72/5eadea03b06ca1320be2433ef2236155da17806b700efc92677ee99ae119/protobuf-3.7.1-cp27-cp27mu-manylinux1_x86_64.whl]
*18:05:44* Collecting futures>=2.2.0; python_version < ""3.2"" (from grpcio>=1.3.5->grpcio-tools==1.3.5)*18:05:44*   ERROR: Could not find a version that satisfies the requirement futures>=2.2.0; python_version < ""3.2"" (from grpcio>=1.3.5->grpcio-tools==1.3.5) (from versions: none)*18:05:44* ERROR: No matching distribution found for futures>=2.2.0; python_version < ""3.2"" (from grpcio>=1.3.5->grpcio-tools==1.3.5)*18:05:46* *18:05:46* >
 *Task :beam-sdks-python-test-suites-dataflow:setupVirtualenv*
 FAILED*18:05:46* 
 ",Python precommit failing on  Java PRs. dataflow:setupVirtualenv,1,,,ajamato@google.com,True,,ajamato@google.com
beam,BEAM-7159,2019-04-26T09:49:09.000+0000,,2019-04-26T09:49:09.000+0000,,,New Feature,Minor,,,,,,,,,['io-ideas'],"['IO: ideas (proposals for new connectors, new designs, etc)']","Add cluster monitoring BI to Apache Beam for monitoring Beam pipelines, monitoring data, and monitoring speed. You can try out various operating platforms.",Add cluster monitoring BI to Apache Beam,1,,,zhanghaitao8,True,,zhanghaitao8
beam,BEAM-7157,2019-04-26T09:03:53.000+0000,2019-04-30T09:19:14.000+0000,2019-04-30T09:19:14.000+0000,,Fixed,New Feature,Major,['2.13.0'],,7800,7800,,,,100,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']",,Allow creation of BinaryCombineFn from lambdas.,1,,,robertwb,True,robertwb,robertwb
beam,BEAM-7146,2019-04-25T17:03:53.000+0000,,2019-04-25T17:03:53.000+0000,,,New Feature,Major,,,,,,,,,['dsl-sql'],['DSLs: SQL'],"Right now code generation is used in Projection (in BeamCalcRel). However, code generation can be used in all places that might need to execute a function. Therefore, this JIRA propose to have a layer (called code generator) that could be used in other rels: join, aggregation, etc.


  ",Code generator is accessible from all RelNode,1,,,amaliujia,True,,amaliujia
beam,BEAM-7145,2019-04-25T13:57:12.000+0000,2019-05-10T19:57:26.000+0000,2019-06-03T20:08:46.000+0000,,Fixed,New Feature,Major,['2.13.0'],,7200,7200,,,,100,['runner-flink'],['Runners: Flink runner'],"Users have asked about Flink 1.8 support. From a quick look,
 * There are changes related to how TypeSerializers are snapshotted. We might have to copy {{CoderTypeSerializer}} to ensure compatibility across the different Flink Runner build targets.
 * StandaloneClusterClient has been removed. We might drop support for legacy deployment and only allow REST.",Make Flink Runner compatible with Flink 1.8,1,,,mxm,True,mxm,mxm
beam,BEAM-7142,2019-04-24T21:42:24.000+0000,,2019-06-18T16:27:52.000+0000,,,New Feature,Major,,,,,,,,,['dsl-sql'],['DSLs: SQL'],"Current way to write BeamSQL test cases is too heavy: developers need to initialize pipeline, deal with PCollection, and use PAssert to verify pipeline results (sometime through INSERT INTO table and read data from table for assertion). 


Data driven testing, instead, should only ask developer to provide SQL query and a expected result in the form of List<Row> (simulate rows from result table). The test execution interface should just be a static function like ""List<Row> run(String query)"", and returned rows can be compared with expected result by checking equality.",Data Driven testing for BeamSQL,1,,,amaliujia,True,,amaliujia
beam,BEAM-7108,2019-04-18T15:42:50.000+0000,,2019-04-18T15:42:50.000+0000,,,New Feature,Major,,,,,,,,,['io-java-gcp'],"['IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)']","According to the BigQuery sink docs:
{code:java}
BigQuery sink currently does not fully support writing to BigQuery time partitioned tables. But writing to a *single* partition may work if that does not involve creating a new table (for example, when writing to an existing table with `create_disposition=CREATE_NEVER` and `write_disposition=WRITE_APPEND`).
{code}
The sink should support partitioning (and clustering).",Support for writing to partitioned and clustered BigQuery tables using Python,1,,,oliviervg1,True,,oliviervg1
beam,BEAM-7093,2019-04-17T09:37:04.000+0000,,2019-04-25T13:40:15.000+0000,,,New Feature,Minor,,,,7800,,,,,['runner-spark'],['Runners: Spark runner'],"Spark 3 is the next release of Spark, we need to fix some issues before we are ready to support it. See subtasks for details.",Support Spark 3 in Spark runner,1,,,iemejia,True,,iemejia
beam,BEAM-7072,2019-04-13T19:39:44.000+0000,,2019-04-29T22:37:00.000+0000,,,New Feature,Major,,,6000,6000,,,,100,['dsl-sql'],['DSLs: SQL'],"Delta JOIN means `JOIN ON a delta b`, where delta could be any condition. Most common case is non equi-join condition.",SQL Delta JOIN,1,,,amaliujia,True,,amaliujia
beam,BEAM-7058,2019-04-11T14:12:51.000+0000,,2019-07-15T21:23:38.000+0000,,,New Feature,Major,,,,,,,,,"['runner-flink', 'sdk-py-harness']","['Runners: Flink runner', 'SDKs: Python: harness for executing UDFs over the Fn API']","With the portable Flink runner, the metric is reported as 0, while the count metric works as expected.

[https://lists.apache.org/thread.html/25eec8104bda6e4c71cc6c5e9864c335833c3ae2afe225d372479f30@%3Cdev.beam.apache.org%3E]

 Edit: metrics are collected properly when using cython, but not without cython. This is because state sampling has yet to be implemented in a way that does not depend on cython [1].

[1] [https://github.com/apache/beam/blob/master/sdks/python/apache_beam/runners/worker/statesampler_slow.py#L62]",Python SDK: Collect metrics in non-cython environment,7,,"['metrics', 'portability-flink', 'portable-metrics-bugs']",thw,True,ajamato@google.com,thw
beam,BEAM-7055,2019-04-11T08:28:07.000+0000,2019-05-22T08:03:24.000+0000,2019-05-22T08:03:24.000+0000,,Duplicate,New Feature,Major,['Not applicable'],,,,,,,,['io-java-jdbc'],['IO: Java: JDBC'],"A new method like  {{withDatasourceFn(SerializableFunction<Void, Datasource> supplierFn)}} would add extra flexibility to users that need to parametrize JdbcIO with more trick configuration parameters by lazingly building it.

We should also refactor existing `DataSourceConfiguration` to comply with this provider style API as its default implementation.",Add to JdbcIO a Datasource provider,1,,,iemejia,True,,iemejia
beam,BEAM-7050,2019-04-10T22:15:36.000+0000,,2019-04-10T22:15:36.000+0000,,,New Feature,Major,,,,,,,,,['runner-dataflow'],['Runners: Google Cloud Dataflow runner'],"There are two maps keyed by the cy_combiner object type in dataflow/internal/apiclient.py
structured_counter_translations and counter_translations 

The only way to make changes today without breaking the legacy python dataflow runner harness.  

(1) Introduce the new cy_combiner and modify the apiclient.py's maps which are keyed by the new types in a first PR
(2) Import and release the dataflow containers
(3) Add your code to use the new cy_combiner in the beam SDK

Prior to step 2, you cannot even run dataflow pipelines using your PR. As you will hit a KeyError when the cy_combiner type is looked up in the api_client maps.

Naturally a non dataflow contributor will just try to use the cy_combiner and unexpectedly break the legacy python dataflow runner harness.  

One solution is to catch the key error and log a warning that the metric is dropped, rather than failing the pipeline.",Antipattern will break legacy dataflow python pipelines if a new cy_combiner is added and used in the python counter_factory,1,,,ajamato@google.com,True,,ajamato@google.com
beam,BEAM-7048,2019-04-10T18:10:59.000+0000,,2019-04-10T18:10:59.000+0000,,,New Feature,Minor,,['2.11.0'],,,,,,,['io-ideas'],"['IO: ideas (proposals for new connectors, new designs, etc)']","I have a dataflow pipeline where first I queried one model, but then for each record that pointed to another model I wanted to be able to fetch that too.

Browsing the {{apache_beam & google.cloud.proto.datastore.v1 }}source I was able to make this happen:

[https://stackoverflow.com/a/55616668/4458510]

I copied the same mechanism from the other Datastore DoFn's and it seems to work fine, and I think something like this would be a good addition to apache_beam.io.gcp.datastore.v1.datastoreio in the sdk

 ",Fetch GCP-Datastore entity by id inside of a Dataflow transform,1,,,alexindaco,True,,alexindaco
beam,BEAM-7045,2019-04-09T21:45:10.000+0000,,2019-04-30T18:32:04.000+0000,,,New Feature,Major,,,,,,,,,"['runner-dataflow', 'sdk-py-core']","['Runners: Google Cloud Dataflow runner', 'SDKs: Python']","Users don't see the element counters in transforms in the Web UI graph representation when running a Python streaming job, which is expected behavior according to [this Beam page|https://beam.apache.org/documentation/sdks/python-streaming/#dataflowrunner-specific-features].

The feature request is to enable the element counters in the Web UI graph representations for transforms for Python streaming jobs in Google Cloud Dataflow.",Element counters in the Web UI graph representations for transforms for Python streaming jobs in Google Cloud Dataflow,1,,"['features', 'usability']",efimmuratov,True,,efimmuratov
beam,BEAM-7044,2019-04-09T17:40:43.000+0000,2019-06-13T15:26:48.000+0000,2019-06-13T15:26:48.000+0000,,Fixed,New Feature,Major,['2.14.0'],,10800,10800,,,,100,['runner-spark'],['Runners: Spark runner'],,Spark portable runner: support user state,3,,['portability-spark'],ibzib,True,ibzib,ibzib
beam,BEAM-7043,2019-04-09T14:54:56.000+0000,2019-06-14T07:45:16.000+0000,2019-06-14T07:45:16.000+0000,,Fixed,New Feature,Minor,['2.14.0'],,57000,57000,,,,100,['io-java-aws'],['IO: Java: Amazon Web Services'],Currently we don't have any feature to write data to AWS DynamoDB. This feature will enable us to send data to DynamoDB,Add DynamoDBIO,2,,,cmach,True,cmach,cmach
beam,BEAM-7029,2019-04-08T14:58:49.000+0000,2019-04-26T10:03:07.000+0000,2019-04-26T10:03:07.000+0000,,Fixed,New Feature,Major,['2.13.0'],,48600,48600,,,,100,"['io-java-kafka', 'runner-flink', 'sdk-py-core']","['IO: Java: Kafka', 'Runners: Flink runner', 'SDKs: Python']","As of BEAM-6730, we can externally configure existing transforms from SDKs. We should add more useful transforms then just {{GenerateSequence}}. 

{{KafkaIO}} is a good candidate.",Support KafkaIO to be configured externally for use with other SDKs,2,,,mxm,True,mxm,mxm
beam,BEAM-7026,2019-04-05T22:52:05.000+0000,,2019-05-30T06:38:33.000+0000,,,New Feature,Major,,,,,,,,,['sdk-py-harness'],['SDKs: Python: harness for executing UDFs over the Fn API'],"I noticed that we are not able to convert the output tag+transform to the pcollection name for metrics (element count/mean byte count), if the Pcollections for the outputed tags are not consumed by a downstream step.

This isn't critical as (1) Arguably there is no pcollection at all. (2) Output but not consumed PCollections are not critical to count metrics on as those can be optomized away entirely (No need to do any work, collect metrics, etc. for an unconsumed pcollection).

However, we are able to count this, but we are unable to assign a pcollection name for it, as in this case there is no information about that output tag defined in the bundle descriptor. The alternative fix is to make sure that its always available, even if not consumed.

Pablo and I looked into this a bit, and he believed it would be possible in pvalue.py's 

DoOutputsTuple class. This fix would require calling __getitem__ on all tags to initialize them properly. However, I had some trouble doing this, as this class is a bit strange since it overrides __getattr__. I found weird behaviors when adding functionality to this code. I don't really get how the code functions today, as its own instance variable usage should trigger the custom __getattr__ code, yet we seem to be using these attrs normally with self.X usages.",Python SDK: Unable to obtain the PCollection for output tags which are not consumed by a downstream step.,2,,,ajamato@google.com,True,,ajamato@google.com
beam,BEAM-7025,2019-04-05T22:39:07.000+0000,,2019-04-05T22:48:11.000+0000,,,New Feature,Major,,,,,,,,,['sdk-py-core'],['SDKs: Python'],"This is an indication of a user misconfiguring a beam pipeline.

This is because its not possible to get a handle to use the produced pcollection for that output tag, if .with_outputs is not used. So this should be disallowed entirely, a run time exception should be thrown.

Note:
The bundle descriptor knows which tags are available for each step. So at runtime it can be detected. But we need to be careful to not test it on every element, for performance purposes
 
i suspect its possible to detect it statically, but may require collecting more information
 
But there should be some code path already collects the elements for the bundle into the different tags when output at that point, at the end of bundle execution we can check for it which would be cheap",Python pipelines should not be able to use output tags that are not defined in with_outputs.,1,,,ajamato@google.com,True,,ajamato@google.com
beam,BEAM-7023,2019-04-05T22:06:02.000+0000,2019-06-03T22:00:43.000+0000,2019-06-03T22:00:43.000+0000,,Fixed,New Feature,Minor,['2.14.0'],,3000,3000,,,,100,['sdk-py-core'],['SDKs: Python'],"WithKeys<K, V> PCollection<V> and either a constant key of type K or a function from V to K, and returns a PCollection<KV<K, V>>, where each of the values in the input PCollection has been paired with either the constant key or a key computed from the value.

It should offer the same API as its Java counterpart: 

[https://github.com/apache/beam/blob/11a977b8b26eff2274d706541127c19dc93131a2/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/WithKeys.java]",WithKeys transform for Python SDK,4,,['starter'],rtnguyen,True,ttanay,rtnguyen
beam,BEAM-7021,2019-04-05T22:02:33.000+0000,2019-05-31T15:52:15.000+0000,2019-05-31T15:52:15.000+0000,,Fixed,New Feature,Minor,['2.14.0'],,9000,9000,,,,100,['sdk-py-core'],['SDKs: Python'],"PTransforms for converting a PCollection or  PCollection
Iterable to a PCollection String

It should offer the same API as its Java counterpart: 

[https://github.com/apache/beam/blob/11a977b8b26eff2274d706541127c19dc93131a2/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/ToString.java]",ToString transform for Python SDK,3,,['starter'],rtnguyen,True,shehzaadn,rtnguyen
beam,BEAM-7019,2019-04-05T21:54:05.000+0000,,2019-06-14T14:40:48.000+0000,,,New Feature,Minor,,,,,,,,,['sdk-py-core'],['SDKs: Python'],"PTransforms for converting between explicit and implicit form of various Beam
values.

It should offer the same API as its Java counterpart: [https://github.com/apache/beam/blob/11a977b8b26eff2274d706541127c19dc93131a2/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/Reify.java]",Reify transform for Python SDK,1,,,rtnguyen,True,shehzaadn,rtnguyen
beam,BEAM-7018,2019-04-05T21:51:41.000+0000,,2019-07-16T10:48:10.000+0000,,,New Feature,Minor,,,18000,18000,,,,100,['sdk-py-core'],['SDKs: Python'],"PTransorms to use Regular Expressions to process elements in a PCollection

It should offer the same API as its Java counterpart: [https://github.com/apache/beam/blob/11a977b8b26eff2274d706541127c19dc93131a2/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/Regex.java]",Regex transform for Python SDK,3,,,rtnguyen,True,shehzaadn,rtnguyen
beam,BEAM-7013,2019-04-05T17:00:55.000+0000,,2019-06-25T23:35:27.000+0000,,,New Feature,Major,,,1200,1200,,,,100,"['extensions-java-sketching', 'sdk-java-core']","['Extensions: Java: Sketching library', 'SDKs: Java: pipeline construction, core transformations']",,A new count distinct transform based on BigQuery compatible HyperLogLog++ implementation,5,,,robinyqiu,True,robinyqiu,robinyqiu
beam,BEAM-7012,2019-04-05T12:17:20.000+0000,2019-04-24T20:45:38.000+0000,2019-05-09T13:54:54.000+0000,,Fixed,New Feature,Major,['2.13.0'],,17400,17400,,,,100,['runner-flink'],['Runners: Flink runner'],TestStream is a primitive transform which is only supported by the DirectRunner. It might be useful to also implement it in the Flink Runner to run similar kind of tests.,Support TestStream in FlinkRunner,1,,,mxm,True,mxm,mxm
beam,BEAM-7006,2019-04-04T16:16:55.000+0000,,2019-04-09T17:06:29.000+0000,,,New Feature,Major,,,,,,,,,['test-failures'],['Automatically filed for test failures'],"[https://builds.apache.org/job/beam_PreCommit_Python_Phrase/331/testReport/junit/apache_beam.runners.portability.fn_api_runner_test/FnApiRunnerSplitTest/test_split_crazy_sdf_2/]

Traceback (most recent call last): File ""/home/jenkins/jenkins-slave/workspace/beam_PreCommit_Python_Phrase/src/sdks/python/test-suites/tox/py35/build/srcs/sdks/python/apache_beam/runners/worker/sdk_worker.py"", line 157, in _execute response = task() File ""/home/jenkins/jenkins-slave/workspace/beam_PreCommit_Python_Phrase/src/sdks/python/test-suites/tox/py35/build/srcs/sdks/python/apache_beam/runners/worker/sdk_worker.py"", line 216, in <lambda> lambda: self.progress_worker.do_instruction(request), request) File ""/home/jenkins/jenkins-slave/workspace/beam_PreCommit_Python_Phrase/src/sdks/python/test-suites/tox/py35/build/srcs/sdks/python/apache_beam/runners/worker/sdk_worker.py"", line 312, in do_instruction request.instruction_id) File ""/home/jenkins/jenkins-slave/workspace/beam_PreCommit_Python_Phrase/src/sdks/python/test-suites/tox/py35/build/srcs/sdks/python/apache_beam/runners/worker/sdk_worker.py"", line 354, in process_bundle_split process_bundle_split=processor.try_split(request)) File ""/home/jenkins/jenkins-slave/workspace/beam_PreCommit_Python_Phrase/src/sdks/python/test-suites/tox/py35/build/srcs/sdks/python/apache_beam/runners/worker/bundle_processor.py"", line 588, in try_split desired_split.estimated_input_elements) File ""/home/jenkins/jenkins-slave/workspace/beam_PreCommit_Python_Phrase/src/sdks/python/test-suites/tox/py35/build/srcs/sdks/python/apache_beam/runners/worker/bundle_processor.py"", line 144, in try_split if total_buffer_size < self.index + 1: AttributeError: 'DataInputOperation' object has no attribute 'index'",test_split_crazy_sdf broken in python presubmit.  'DataInputOperation' object has no attribute 'index',2,,,ajamato@google.com,True,,ajamato@google.com
beam,BEAM-6965,2019-04-01T20:37:42.000+0000,2019-05-06T19:38:12.000+0000,2019-06-07T09:18:58.000+0000,,Fixed,New Feature,Major,['2.13.0'],,,,,,,,['runner-spark'],['Runners: Spark runner'],,Spark portable translator: translate READ,1,,['portability-spark'],ibzib,True,ibzib,ibzib
beam,BEAM-6956,2019-04-01T17:42:57.000+0000,2019-04-01T22:51:04.000+0000,2019-04-01T22:51:05.000+0000,,Fixed,New Feature,Major,['Not applicable'],,,,,,,,['runner-flink'],['Runners: Flink runner'],"I noticed that without this ""–experiments=worker_threads=100"", pipeline will stuck,

The weird thing is, I tried some complex pipeline using the in thread flink method (./gradlew :beam-runners-flink_2.11-job-server:runShadow)

""–experiments=worker_threads=100"" doesn't work, but ""–experiments=worker_threads=1000"" works fine

Then I tried the same pipeline using the separate local flink cluster (./gradlew :beam-runners-flink_2.11-job-server:runShadow -PflinkMasterUrl=localhost:8081), flink version is 1.5.6 (other version doesn't work, see BEAM-6915)

Neither did ""–experiments=worker_threads=1000"" or ""–experiments=worker_threads=10000"" work, pipeline stuck at certain stage (shows running in flink UI but won't finish forever)

any real fix to that? Thanks!",--experiments=worker_threads=100 issue,1,,,1025KB,True,,1025KB
beam,BEAM-6921,2019-03-27T13:46:00.000+0000,,2019-03-27T13:46:00.000+0000,,,New Feature,Minor,,['2.11.0'],,,,,,,['io-java-gcp'],"['IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)']","from a discussion in [https://github.com/apache/beam/pull/8097]
 SpannerIO produces 2 output PCollections:
 * getOutput() -> PCollection<Void>
 ** never has any values
 ** in GlobalWindow
 ** Closed when the input PCollection is closed (ie never in streaming) to indicate when all input has been written
 ** Used in batch pipelines to have 'dependant' bulk imports - where one dataset is not written to Spanner until another has completed writing. (necessary for handling parent/child (1-many) referential integrity)
 * getFailedMutations() -> PCollection<MutationGroup>
 ** only contains values when Mutation[Group]s fail to be written
 ** in GlobalWindow
 ** Not very useful, as the reason for the failure is not given. 

Suggestion: 
 * Deprecate these existing outputs.
 * Make primary output be a PCollection<\{ MutationGroup, CommitTimestamp }> so that the successfully written Mutation[Groups] can be processed further if necessary.
(\{a,b} signifies a container class for these values)
 * Add an additional output of failed mutations PCollection<\{ MutationGroup, FailureMessage}>
 * The existing outputs can be derived from these new outputs

This allows useful error reporting/handling from the failure message, and the ability to continue processing the successful mutations. 

 

(see also BEAM-6887)

 

 ",Improve SpannerIO output,2,,,nielm,True,,nielm
beam,BEAM-6916,2019-03-26T20:26:54.000+0000,2019-05-22T20:04:40.000+0000,2019-05-22T20:04:40.000+0000,,Fixed,New Feature,Minor,['Not applicable'],,15600,15600,589200,604800,589200,2,['website'],['Website: content and bugs'],This page describes the Calcite SQL dialect supported by Beam SQL.,Reorganize Beam SQL docs,1,,,rtnguyen,True,rtnguyen,rtnguyen
beam,BEAM-6891,2019-03-22T17:51:59.000+0000,2019-03-25T17:42:12.000+0000,2019-03-25T17:42:12.000+0000,,Duplicate,New Feature,Major,['Not applicable'],,,,,,,,['runner-flink'],['Runners: Flink runner'],"We want to run beam flink with input/output on local filesystem instead of inside docker, is there a way to let the docker read and write from external path",Support extra mount volumes for PortableRunner's docker container,1,,,1025KB,True,,1025KB
beam,BEAM-6887,2019-03-22T14:16:20.000+0000,,2019-03-22T14:25:14.000+0000,,,New Feature,Minor,,,,,,,,,['io-java-gcp'],"['IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)']","At present, the SpannerIO.Write/WriteGrouped transforms work by collecting an entire bundle of elements, sorts them by table/key, splitting the sorted list into batches (by size and number of cells modified) and then writes each batch to Spanner in a single transaction.

It returns a SpannerWriteResult.java containing :
 # a PCollection<Void> (the main output) - which will have no elements but will be closed to signal when all the input elements have been written (which is never in streaming because input is unbounded)
 # a PCollection<MutationGroup> of elements that failed to write.

This transform is useful as a bulk sink for data because it efficiently writes large amounts of data. 

It is not at all useful as an intermediate step in a streaming pipeline - because it has no useful output in streaming mode. 

I propose that we have a separate Spanner Write transform which simply writes each input Mutation to the database, and then pushes successful Mutations onto its output. 

This would allow use in the middle of a streaming pipeline, where the flow would be
 * Some data streamed in
 * Converted to Spanner Mutations
 * Written to Spanner Database
 * Further processing where the values written to the Spanner Database are used.",Streaming Spanner Writer transform,2,,,nielm,True,nielm,nielm
beam,BEAM-6880,2019-03-21T16:20:36.000+0000,2019-05-21T18:53:16.000+0000,2019-05-23T07:07:40.000+0000,,Fixed,New Feature,Major,['2.14.0'],,28800,28800,,,,100,"['runner-direct', 'test-failures', 'testing']","['Runners: Direct runner and ULR for single machine testing and development', 'Automatically filed for test failures', 'Testing: general infrastructure']","This ticket is about deprecating Java Portable Reference runner.

 

Discussion is happening in [this thread|[https://lists.apache.org/thread.html/0b68efce9b7f2c5297b32d09e5d903e9b354199fe2ce446fbcd240bc@%3Cdev.beam.apache.org%3E]] 

 

Current summary is: disable beam_PostCommit_Java_PVR_Reference job.

Keeping or removing reference runner code is still under discussion. It is suggested to create PR that removes relevant code and start voting there.

 

 ",Deprecate Java Portable Reference Runner,1,,,Ardagan,True,danoliveira,Ardagan
beam,BEAM-6872,2019-03-20T22:55:20.000+0000,2019-06-21T20:37:48.000+0000,2019-06-21T20:37:48.000+0000,,Fixed,New Feature,Minor,['2.14.0'],,28800,28800,,,,100,['runner-dataflow'],['Runners: Google Cloud Dataflow runner'],"Expose an interface for users to run some one-time initialization code when a worker starts up.
This can be useful for things like overriding the Default ZoneRulesProvider, or setting up custom SSL providers.
",Add hook for user-defined JVM initialization in workers,1,,,bhulette,True,bhulette,bhulette
beam,BEAM-6868,2019-03-20T01:35:13.000+0000,,2019-05-06T22:22:10.000+0000,,,New Feature,Major,,,,,,,,,['runner-flink'],['Runners: Flink runner'],,Flink runner supports Bundle Finalization,2,,,boyuanz,True,,boyuanz
beam,BEAM-6841,2019-03-15T03:44:22.000+0000,2019-04-02T20:22:57.000+0000,2019-04-02T20:22:58.000+0000,,Fixed,New Feature,Minor,['2.13.0'],,27000,27000,,,,100,['io-java-gcp'],"['IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)']",,Support reading query results with the BigQuery storage API,2,,,kjung520,True,kjung520,kjung520
beam,BEAM-6834,2019-03-14T18:03:38.000+0000,,2019-05-01T20:11:09.000+0000,,,New Feature,Minor,,,,,,,,,['runner-dataflow'],['Runners: Google Cloud Dataflow runner'],"[https://github.com/apache/beam/blob/master/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/fn/control/RegisterAndProcessBundleOperation.java#L117]

 

These constructor parameters play similar role and it might be worth extracting them to a single class that can be passed around.

 

Map<String, DataflowStepContext> ptransformIdToSystemStepContext,
 Map<String, SideInputReader> ptransformIdToSideInputReader, Map<String, SideInputReader> ptransformIdToSideInputReader,
 Table<String, String, PCollectionView<?>> ptransformIdToSideInputIdToPCollectionView, Table<String, String, PCollectionView<?>> ptransformIdToSideInputIdToPCollectionView,
 Map<String, String> sdkToDfePCollectionMapping,",Extract RegisterAndProcessBundleOperation constructor parameters into a single class.,1,,,Ardagan,True,,Ardagan
beam,BEAM-6833,2019-03-14T18:00:58.000+0000,,2019-03-14T18:01:25.000+0000,,,New Feature,Major,,,,,,,,,['sdk-py-core'],['SDKs: Python'],"dataflow_metrics.py expects a job object to be passed in, it cannot fully query the metrics on its own.

Updating this to use the dataflow_client to also query the job itself (non trivial since the job passed in is a wrapper for the Job proto the dataflow_client obtains, this wrapper is created by dataflow_runner.) Though we may not need any of the extra wrapper features, since we only need access to the graph. But it is unclear if this will work, as the graph DatalowRunner generates is not the same graph as the one queried out.

 

The job graph is use to translate step names",dataflow_metrics.py should be able to fully query the metrics,1,,,ajamato@google.com,True,,ajamato@google.com
beam,BEAM-6832,2019-03-14T16:48:55.000+0000,,2019-04-30T18:30:15.000+0000,,,New Feature,Major,,,,,,,,,['runner-dataflow'],['Runners: Google Cloud Dataflow runner'],"For now, Apache Beam on Google Dataflow doesn't provide functionality to pass custom labels and metadata on VM instances that serve Dataflow job. Only labels on Job is available. 

Actually com.google.api.services.dataflow.model.WorkerPool alredy has the field _metadata_ but metod _setMetadata_ never using.

Need to add functionality to provide custom labels and metadata on VM instances via running Dataflow job on Google cloud.",Populate labels and custom metadata on VMs that serves Dataflow,3,,['google-dataflow'],alex3.14,True,,alex3.14
beam,BEAM-6826,2019-03-13T20:33:09.000+0000,,2019-03-13T20:33:09.000+0000,,,New Feature,Major,,,,,,,,,['sdk-py-harness'],['SDKs: Python: harness for executing UDFs over the Fn API'],"Right now we have Metrics.counter, Metrics.distribution counters which use MetricContainers to store and accumulate metrics.

CounterSet counters which use accumulators to store and accumulate metrics. This code path is optimized using cython. 
See cy_combiners.py

 

Ideally we can keep the user interface (this should not change) for creating metrics with Metrics.counter(), .etc. But use the underlying optimized CounterSets.",All Python counters should use the same code path,1,,,ajamato@google.com,True,,ajamato@google.com
beam,BEAM-6825,2019-03-13T18:29:17.000+0000,2019-07-11T21:20:14.000+0000,2019-07-11T21:20:14.000+0000,,Fixed,New Feature,Minor,['Not applicable'],,5400,44400,,,,100,['sdk-go'],['SDKs: Go'],"Many error messages for common pipeline construction mistakes are unclear and unhelpful. They need to be improved to provide more context, especially for newer users. This bug tracks these error message improvements.",Improve pipeline construction time error messages in Go SDK.,2,,,danoliveira,True,danoliveira,danoliveira
beam,BEAM-6824,2019-03-13T18:21:43.000+0000,2019-04-03T20:21:05.000+0000,2019-04-03T20:21:05.000+0000,,Fixed,New Feature,Major,['2.13.0'],,11400,11400,,,,100,['runner-dataflow'],['Runners: Google Cloud Dataflow runner'],"We want to provide FnApi metrics to Dataflow Users.

This bug covers ElementCount metric.

Current approach utilizes mapping of PCollectionID based on WorkItem and ProcessBundle graphs.",Plumb FnApi ElementCount metrics in Dataflow Runner.,1,,,Ardagan,True,Ardagan,Ardagan
beam,BEAM-6807,2019-03-11T21:54:04.000+0000,,2019-05-23T00:11:17.000+0000,,,New Feature,Major,,,,,,,,,['sdk-py-core'],['SDKs: Python'],"This is similar to BEAM-2572, but for Azure's blobstore.",Implement an Azure blobstore filesystem for Python SDK,2,1,"['GSoC2019', 'azure', 'azureblob', 'gsoc', 'gsoc2019', 'mentor']",pabloem,True,pabloem,pabloem
beam,BEAM-6788,2019-03-08T21:33:25.000+0000,2019-03-12T23:16:58.000+0000,2019-03-12T23:16:58.000+0000,,Implemented,New Feature,Major,['Not applicable'],,2400,2400,,,,100,['sdk-py-harness'],['SDKs: Python: harness for executing UDFs over the Fn API'],"Updated [fn_api_proto](https://github.com/apache/beam/blob/7206b9b758ea6e17d73f203c59634930f57915c9/model/pipeline/src/main/proto/metrics.proto#L72) passes PCollectionID, not PTransfrom+OutputID to identify PCollection for ElementCount metric.",Inject PCollectionID to ElementCount metric in Python SDK,1,,,Ardagan,True,Ardagan,Ardagan
beam,BEAM-6782,2019-03-08T19:39:24.000+0000,,2019-03-08T19:39:24.000+0000,,,New Feature,Major,,,,,,,,,['java-fn-execution'],[''],"The generated tranform names in the SDK are really confusing, they don't really reflect an error the user is familiar with.

Updating this error printing to print the known user step name would improve this significantly
TypeError: 'int' object is not iterable [while running 'generatedPtransform-70']

java.util.concurrent.ExecutionException: java.lang.RuntimeException: Error received from SDK harness for instruction -82: Traceback (most recent call last):
 File ""/usr/local/lib/python2.7/dist-packages/apache_beam/runners/worker/sdk_worker.py"", line 151, in _execute
 response = task()
 File ""/usr/local/lib/python2.7/dist-packages/apache_beam/runners/worker/sdk_worker.py"", line 186, in <lambda>
 self._execute(lambda: worker.do_instruction(work), work)
 File ""/usr/local/lib/python2.7/dist-packages/apache_beam/runners/worker/sdk_worker.py"", line 265, in do_instruction
 request.instruction_id)
 File ""/usr/local/lib/python2.7/dist-packages/apache_beam/runners/worker/sdk_worker.py"", line 281, in process_bundle
 delayed_applications = bundle_processor.process_bundle(instruction_id)
 File ""/usr/local/lib/python2.7/dist-packages/apache_beam/runners/worker/bundle_processor.py"", line 549, in process_bundle
 ].process_encoded(data.data)
 File ""/usr/local/lib/python2.7/dist-packages/apache_beam/runners/worker/bundle_processor.py"", line 140, in process_encoded
 self.output(decoded_value)
 File ""apache_beam/runners/worker/operations.py"", line 222, in apache_beam.runners.worker.operations.Operation.output
 def output(self, windowed_value, output_index=0):
 File ""apache_beam/runners/worker/operations.py"", line 223, in apache_beam.runners.worker.operations.Operation.output
 cython.cast(Receiver, self.receivers[output_index]).receive(windowed_value)
 File ""apache_beam/runners/worker/operations.py"", line 131, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
 self.consumer.process(windowed_value)
 File ""apache_beam/runners/worker/operations.py"", line 537, in apache_beam.runners.worker.operations.DoOperation.process
 with self.scoped_process_state:
 File ""apache_beam/runners/worker/operations.py"", line 538, in apache_beam.runners.worker.operations.DoOperation.process
 delayed_application = self.dofn_receiver.receive(o)
 File ""apache_beam/runners/common.py"", line 723, in apache_beam.runners.common.DoFnRunner.receive
 self.process(windowed_value)
 File ""apache_beam/runners/common.py"", line 729, in apache_beam.runners.common.DoFnRunner.process
 self._reraise_augmented(exn)
 File ""apache_beam/runners/common.py"", line 762, in apache_beam.runners.common.DoFnRunner._reraise_augmented
 raise
 File ""apache_beam/runners/common.py"", line 727, in apache_beam.runners.common.DoFnRunner.process
 return self.do_fn_invoker.invoke_process(windowed_value)
 File ""apache_beam/runners/common.py"", line 418, in apache_beam.runners.common.SimpleInvoker.invoke_process
 output_processor.process_outputs(
 File ""apache_beam/runners/common.py"", line 847, in apache_beam.runners.common._OutputProcessor.process_outputs
 self.main_receivers.receive(windowed_value)
 File ""apache_beam/runners/worker/operations.py"", line 131, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
 self.consumer.process(windowed_value)
 File ""apache_beam/runners/worker/operations.py"", line 537, in apache_beam.runners.worker.operations.DoOperation.process
 with self.scoped_process_state:
 File ""apache_beam/runners/worker/operations.py"", line 538, in apache_beam.runners.worker.operations.DoOperation.process
 delayed_application = self.dofn_receiver.receive(o)
 File ""apache_beam/runners/common.py"", line 723, in apache_beam.runners.common.DoFnRunner.receive
 self.process(windowed_value)
 File ""apache_beam/runners/common.py"", line 729, in apache_beam.runners.common.DoFnRunner.process
 self._reraise_augmented(exn)
 File ""apache_beam/runners/common.py"", line 762, in apache_beam.runners.common.DoFnRunner._reraise_augmented
 raise
 File ""apache_beam/runners/common.py"", line 727, in apache_beam.runners.common.DoFnRunner.process
 return self.do_fn_invoker.invoke_process(windowed_value)
 File ""apache_beam/runners/common.py"", line 418, in apache_beam.runners.common.SimpleInvoker.invoke_process
 output_processor.process_outputs(
 File ""apache_beam/runners/common.py"", line 847, in apache_beam.runners.common._OutputProcessor.process_outputs
 self.main_receivers.receive(windowed_value)
 File ""apache_beam/runners/worker/operations.py"", line 131, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
 self.consumer.process(windowed_value)
 File ""apache_beam/runners/worker/operations.py"", line 537, in apache_beam.runners.worker.operations.DoOperation.process
 with self.scoped_process_state:
 File ""apache_beam/runners/worker/operations.py"", line 538, in apache_beam.runners.worker.operations.DoOperation.process
 delayed_application = self.dofn_receiver.receive(o)
 File ""apache_beam/runners/common.py"", line 723, in apache_beam.runners.common.DoFnRunner.receive
 self.process(windowed_value)
 File ""apache_beam/runners/common.py"", line 729, in apache_beam.runners.common.DoFnRunner.process
 self._reraise_augmented(exn)
 File ""apache_beam/runners/common.py"", line 762, in apache_beam.runners.common.DoFnRunner._reraise_augmented
 raise
 File ""apache_beam/runners/common.py"", line 727, in apache_beam.runners.common.DoFnRunner.process
 return self.do_fn_invoker.invoke_process(windowed_value)
 File ""apache_beam/runners/common.py"", line 418, in apache_beam.runners.common.SimpleInvoker.invoke_process
 output_processor.process_outputs(
 File ""apache_beam/runners/common.py"", line 847, in apache_beam.runners.common._OutputProcessor.process_outputs
 self.main_receivers.receive(windowed_value)
 File ""apache_beam/runners/worker/operations.py"", line 131, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
 self.consumer.process(windowed_value)
 File ""apache_beam/runners/worker/operations.py"", line 537, in apache_beam.runners.worker.operations.DoOperation.process
 with self.scoped_process_state:
 File ""apache_beam/runners/worker/operations.py"", line 538, in apache_beam.runners.worker.operations.DoOperation.process
 delayed_application = self.dofn_receiver.receive(o)
 File ""apache_beam/runners/common.py"", line 723, in apache_beam.runners.common.DoFnRunner.receive
 self.process(windowed_value)
 File ""apache_beam/runners/common.py"", line 729, in apache_beam.runners.common.DoFnRunner.process
 self._reraise_augmented(exn)
 File ""apache_beam/runners/common.py"", line 762, in apache_beam.runners.common.DoFnRunner._reraise_augmented
 raise
 File ""apache_beam/runners/common.py"", line 727, in apache_beam.runners.common.DoFnRunner.process
 return self.do_fn_invoker.invoke_process(windowed_value)
 File ""apache_beam/runners/common.py"", line 418, in apache_beam.runners.common.SimpleInvoker.invoke_process
 output_processor.process_outputs(
 File ""apache_beam/runners/common.py"", line 847, in apache_beam.runners.common._OutputProcessor.process_outputs
 self.main_receivers.receive(windowed_value)
 File ""apache_beam/runners/worker/operations.py"", line 131, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
 self.consumer.process(windowed_value)
 File ""apache_beam/runners/worker/operations.py"", line 537, in apache_beam.runners.worker.operations.DoOperation.process
 with self.scoped_process_state:
 File ""apache_beam/runners/worker/operations.py"", line 538, in apache_beam.runners.worker.operations.DoOperation.process
 delayed_application = self.dofn_receiver.receive(o)
 File ""apache_beam/runners/common.py"", line 723, in apache_beam.runners.common.DoFnRunner.receive
 self.process(windowed_value)
 File ""apache_beam/runners/common.py"", line 729, in apache_beam.runners.common.DoFnRunner.process
 self._reraise_augmented(exn)
 File ""apache_beam/runners/common.py"", line 777, in apache_beam.runners.common.DoFnRunner._reraise_augmented
 raise_with_traceback(new_exn)
 File ""apache_beam/runners/common.py"", line 727, in apache_beam.runners.common.DoFnRunner.process
 return self.do_fn_invoker.invoke_process(windowed_value)
 File ""apache_beam/runners/common.py"", line 418, in apache_beam.runners.common.SimpleInvoker.invoke_process
 output_processor.process_outputs(
 File ""apache_beam/runners/common.py"", line 823, in apache_beam.runners.common._OutputProcessor.process_outputs
 for result in results:
TypeError: 'int' object is not iterable [while running 'generatedPtransform-70']

at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
 at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)
 at org.apache.beam.sdk.util.MoreFutures.get(MoreFutures.java:57)
 at org.apache.beam.runners.dataflow.worker.fn.control.RegisterAndProcessBundleOperation.finish(RegisterAndProcessBundleOperation.java:281)
 at org.apache.beam.runners.dataflow.worker.util.common.worker.MapTaskExecutor.execute(MapTaskExecutor.java:85)
 at org.apache.beam.runners.dataflow.worker.fn.control.BeamFnMapTaskExecutor.execute(BeamFnMapTaskExecutor.java:125)
 at org.apache.beam.runners.dataflow.worker.BatchDataflowWorker.executeWork(BatchDataflowWorker.java:411)
 at org.apache.beam.runners.dataflow.worker.BatchDataflowWorker.doWork(BatchDataflowWorker.java:380)
 at org.apache.beam.runners.dataflow.worker.BatchDataflowWorker.getAndPerformWork(BatchDataflowWorker.java:306)
 at org.apache.beam.runners.dataflow.worker.DataflowRunnerHarness.start(DataflowRunnerHarness.java:195)
 at org.apache.beam.runners.dataflow.worker.DataflowRunnerHarness.main(DataflowRunnerHarness.java:123)
 Suppressed: java.lang.IllegalStateException: Already closed.
 at org.apache.beam.sdk.fn.data.BeamFnDataBufferingOutboundObserver.close(BeamFnDataBufferingOutboundObserver.java:95)
 at org.apache.beam.runners.dataflow.worker.fn.data.RemoteGrpcPortWriteOperation.abort(RemoteGrpcPortWriteOperation.java:215)
 at org.apache.beam.runners.dataflow.worker.util.common.worker.MapTaskExecutor.execute(MapTaskExecutor.java:91)
 ... 6 more
Caused by: java.lang.RuntimeException: Error received from SDK harness for instruction -82: Traceback (most recent call last):
 File ""/usr/local/lib/python2.7/dist-packages/apache_beam/runners/worker/sdk_worker.py"", line 151, in _execute
 response = task()
 File ""/usr/local/lib/python2.7/dist-packages/apache_beam/runners/worker/sdk_worker.py"", line 186, in <lambda>
 self._execute(lambda: worker.do_instruction(work), work)
 File ""/usr/local/lib/python2.7/dist-packages/apache_beam/runners/worker/sdk_worker.py"", line 265, in do_instruction
 request.instruction_id)
 File ""/usr/local/lib/python2.7/dist-packages/apache_beam/runners/worker/sdk_worker.py"", line 281, in process_bundle
 delayed_applications = bundle_processor.process_bundle(instruction_id)
 File ""/usr/local/lib/python2.7/dist-packages/apache_beam/runners/worker/bundle_processor.py"", line 549, in process_bundle
 ].process_encoded(data.data)
 File ""/usr/local/lib/python2.7/dist-packages/apache_beam/runners/worker/bundle_processor.py"", line 140, in process_encoded
 self.output(decoded_value)
 File ""apache_beam/runners/worker/operations.py"", line 222, in apache_beam.runners.worker.operations.Operation.output
 def output(self, windowed_value, output_index=0):
 File ""apache_beam/runners/worker/operations.py"", line 223, in apache_beam.runners.worker.operations.Operation.output
 cython.cast(Receiver, self.receivers[output_index]).receive(windowed_value)
 File ""apache_beam/runners/worker/operations.py"", line 131, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
 self.consumer.process(windowed_value)
 File ""apache_beam/runners/worker/operations.py"", line 537, in apache_beam.runners.worker.operations.DoOperation.process
 with self.scoped_process_state:
 File ""apache_beam/runners/worker/operations.py"", line 538, in apache_beam.runners.worker.operations.DoOperation.process
 delayed_application = self.dofn_receiver.receive(o)
 File ""apache_beam/runners/common.py"", line 723, in apache_beam.runners.common.DoFnRunner.receive
 self.process(windowed_value)
 File ""apache_beam/runners/common.py"", line 729, in apache_beam.runners.common.DoFnRunner.process
 self._reraise_augmented(exn)
 File ""apache_beam/runners/common.py"", line 762, in apache_beam.runners.common.DoFnRunner._reraise_augmented
 raise
 File ""apache_beam/runners/common.py"", line 727, in apache_beam.runners.common.DoFnRunner.process
 return self.do_fn_invoker.invoke_process(windowed_value)
 File ""apache_beam/runners/common.py"", line 418, in apache_beam.runners.common.SimpleInvoker.invoke_process
 output_processor.process_outputs(
 File ""apache_beam/runners/common.py"", line 847, in apache_beam.runners.common._OutputProcessor.process_outputs
 self.main_receivers.receive(windowed_value)
 File ""apache_beam/runners/worker/operations.py"", line 131, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
 self.consumer.process(windowed_value)
 File ""apache_beam/runners/worker/operations.py"", line 537, in apache_beam.runners.worker.operations.DoOperation.process
 with self.scoped_process_state:
 File ""apache_beam/runners/worker/operations.py"", line 538, in apache_beam.runners.worker.operations.DoOperation.process
 delayed_application = self.dofn_receiver.receive(o)
 File ""apache_beam/runners/common.py"", line 723, in apache_beam.runners.common.DoFnRunner.receive
 self.process(windowed_value)
 File ""apache_beam/runners/common.py"", line 729, in apache_beam.runners.common.DoFnRunner.process
 self._reraise_augmented(exn)
 File ""apache_beam/runners/common.py"", line 762, in apache_beam.runners.common.DoFnRunner._reraise_augmented
 raise
 File ""apache_beam/runners/common.py"", line 727, in apache_beam.runners.common.DoFnRunner.process
 return self.do_fn_invoker.invoke_process(windowed_value)
 File ""apache_beam/runners/common.py"", line 418, in apache_beam.runners.common.SimpleInvoker.invoke_process
 output_processor.process_outputs(
 File ""apache_beam/runners/common.py"", line 847, in apache_beam.runners.common._OutputProcessor.process_outputs
 self.main_receivers.receive(windowed_value)
 File ""apache_beam/runners/worker/operations.py"", line 131, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
 self.consumer.process(windowed_value)
 File ""apache_beam/runners/worker/operations.py"", line 537, in apache_beam.runners.worker.operations.DoOperation.process
 with self.scoped_process_state:
 File ""apache_beam/runners/worker/operations.py"", line 538, in apache_beam.runners.worker.operations.DoOperation.process
 delayed_application = self.dofn_receiver.receive(o)
 File ""apache_beam/runners/common.py"", line 723, in apache_beam.runners.common.DoFnRunner.receive
 self.process(windowed_value)
 File ""apache_beam/runners/common.py"", line 729, in apache_beam.runners.common.DoFnRunner.process
 self._reraise_augmented(exn)
 File ""apache_beam/runners/common.py"", line 762, in apache_beam.runners.common.DoFnRunner._reraise_augmented
 raise
 File ""apache_beam/runners/common.py"", line 727, in apache_beam.runners.common.DoFnRunner.process
 return self.do_fn_invoker.invoke_process(windowed_value)
 File ""apache_beam/runners/common.py"", line 418, in apache_beam.runners.common.SimpleInvoker.invoke_process
 output_processor.process_outputs(
 File ""apache_beam/runners/common.py"", line 847, in apache_beam.runners.common._OutputProcessor.process_outputs
 self.main_receivers.receive(windowed_value)
 File ""apache_beam/runners/worker/operations.py"", line 131, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
 self.consumer.process(windowed_value)
 File ""apache_beam/runners/worker/operations.py"", line 537, in apache_beam.runners.worker.operations.DoOperation.process
 with self.scoped_process_state:
 File ""apache_beam/runners/worker/operations.py"", line 538, in apache_beam.runners.worker.operations.DoOperation.process
 delayed_application = self.dofn_receiver.receive(o)
 File ""apache_beam/runners/common.py"", line 723, in apache_beam.runners.common.DoFnRunner.receive
 self.process(windowed_value)
 File ""apache_beam/runners/common.py"", line 729, in apache_beam.runners.common.DoFnRunner.process
 self._reraise_augmented(exn)
 File ""apache_beam/runners/common.py"", line 762, in apache_beam.runners.common.DoFnRunner._reraise_augmented
 raise
 File ""apache_beam/runners/common.py"", line 727, in apache_beam.runners.common.DoFnRunner.process
 return self.do_fn_invoker.invoke_process(windowed_value)
 File ""apache_beam/runners/common.py"", line 418, in apache_beam.runners.common.SimpleInvoker.invoke_process
 output_processor.process_outputs(
 File ""apache_beam/runners/common.py"", line 847, in apache_beam.runners.common._OutputProcessor.process_outputs
 self.main_receivers.receive(windowed_value)
 File ""apache_beam/runners/worker/operations.py"", line 131, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
 self.consumer.process(windowed_value)
 File ""apache_beam/runners/worker/operations.py"", line 537, in apache_beam.runners.worker.operations.DoOperation.process
 with self.scoped_process_state:
 File ""apache_beam/runners/worker/operations.py"", line 538, in apache_beam.runners.worker.operations.DoOperation.process
 delayed_application = self.dofn_receiver.receive(o)
 File ""apache_beam/runners/common.py"", line 723, in apache_beam.runners.common.DoFnRunner.receive
 self.process(windowed_value)
 File ""apache_beam/runners/common.py"", line 729, in apache_beam.runners.common.DoFnRunner.process
 self._reraise_augmented(exn)
 File ""apache_beam/runners/common.py"", line 777, in apache_beam.runners.common.DoFnRunner._reraise_augmented
 raise_with_traceback(new_exn)
 File ""apache_beam/runners/common.py"", line 727, in apache_beam.runners.common.DoFnRunner.process
 return self.do_fn_invoker.invoke_process(windowed_value)
 File ""apache_beam/runners/common.py"", line 418, in apache_beam.runners.common.SimpleInvoker.invoke_process
 output_processor.process_outputs(
 File ""apache_beam/runners/common.py"", line 823, in apache_beam.runners.common._OutputProcessor.process_outputs
 for result in results:
TypeError: 'int' object is not iterable [while running 'generatedPtransform-70']

at org.apache.beam.runners.fnexecution.control.FnApiControlClient$ResponseStreamObserver.onNext(FnApiControlClient.java:157)
 at org.apache.beam.runners.fnexecution.control.FnApiControlClient$ResponseStreamObserver.onNext(FnApiControlClient.java:140)
 at org.apache.beam.vendor.grpc.v1p13p1.io.grpc.stub.ServerCalls$StreamingServerCallHandler$StreamingServerCallListener.onMessage(ServerCalls.java:248)
 at org.apache.beam.vendor.grpc.v1p13p1.io.grpc.ForwardingServerCallListener.onMessage(ForwardingServerCallListener.java:33)
 at org.apache.beam.vendor.grpc.v1p13p1.io.grpc.Contexts$ContextualizedServerCallListener.onMessage(Contexts.java:76)
 at org.apache.beam.vendor.grpc.v1p13p1.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailable(ServerCallImpl.java:263)
 at org.apache.beam.vendor.grpc.v1p13p1.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1MessagesAvailable.runInContext(ServerImpl.java:683)
 at org.apache.beam.vendor.grpc.v1p13p1.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
 at org.apache.beam.vendor.grpc.v1p13p1.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
 at java.lang.Thread.run(Thread.java:745)",Error messages for generatedPtransforms errors confusing,1,,,ajamato@google.com,True,,ajamato@google.com
beam,BEAM-6778,2019-03-07T21:37:13.000+0000,2019-03-21T21:00:47.000+0000,2019-03-27T23:06:38.000+0000,,Implemented,New Feature,Major,['2.12.0'],,34200,34200,,,,100,['sdk-py-harness'],['SDKs: Python: harness for executing UDFs over the Fn API'],,Enable Bundle Finalization in Python SDK,1,,,boyuanz,True,boyuanz,boyuanz
beam,BEAM-6761,2019-03-01T22:19:05.000+0000,,2019-03-01T22:21:16.000+0000,,,New Feature,Major,,,,,,,,,['sdk-py-core'],['SDKs: Python'],"This is blocking my PR at the moment, the output doesn't seem to match the file and I am not sure how to proceed

pydoc Output
 [https://scans.gradle.com/s/im6t66hhy4bdq/console-log?task=:beam-sdks-python:docs#L3|https://www.google.com/url?q=https://scans.gradle.com/s/im6t66hhy4bdq/console-log?task%3D:beam-sdks-python:docs%23L3&sa=D&source=hangouts&ust=1551564233723000&usg=AFQjCNEblSgvJ5E5k_TxqgGujKeUGfuIOw]

Files
 [https://github.com/apache/beam/pull/7936/files|https://www.google.com/url?q=https://github.com/apache/beam/pull/7936/files&sa=D&source=hangouts&ust=1551564233724000&usg=AFQjCNGXSwba2Q4Aod3FAVcYhQXOkQgYvQ]

 

/usr/local/google/home/ajamato/go/src/github.com/apache/beam/sdks/python/apache_beam/testing/metric_result_matchers.py:docstring of apache_beam.testing.metric_result_matchers:13: WARNING: Unexpected indentation.
 /usr/local/google/home/ajamato/go/src/github.com/apache/beam/sdks/python/apache_beam/testing/metric_result_matchers.py:docstring of apache_beam.testing.metric_result_matchers:15: WARNING: Block quote ends without a blank line; unexpected unindent.
 /usr/local/google/home/ajamato/go/src/github.com/apache/beam/sdks/python/apache_beam/testing/metric_result_matchers.py:docstring of apache_beam.testing.metric_result_matchers:18: WARNING: Definition list ends without a blank line; unexpected unindent.
 /usr/local/google/home/ajamato/go/src/github.com/apache/beam/sdks/python/apache_beam/testing/metric_result_matchers.py:docstring of apache_beam.testing.metric_result_matchers:19: WARNING: Definition list ends without a blank line; unexpected unindent.
 /usr/local/google/home/ajamato/go/src/github.com/apache/beam/sdks/python/apache_beam/testing/metric_result_matchers.py:docstring of apache_beam.testing.metric_result_matchers:21: WARNING: Unexpected indentation.
 /usr/local/google/home/ajamato/go/src/github.com/apache/beam/sdks/python/apache_beam/testing/metric_result_matchers.py:docstring of apache_beam.testing.metric_result_matchers:22: WARNING: Block quote ends without a blank line; unexpected unindent.

 

===== copy of the file in its current state (I will probably modify the PR ====

[https://pastebin.com/8bWrPZVJ]

 ",Pydoc is giving criptic error messages,1,,,ajamato@google.com,True,,ajamato@google.com
beam,BEAM-6758,2019-03-01T18:04:31.000+0000,,2019-03-01T18:04:31.000+0000,,,New Feature,Major,,,,,,,,,['java-fn-execution'],[''],"Just wanted to write down this thought before I forget, maybe a second person can think through this and close it out if I am wrong.

I think that there is a bug here, which could leave to data loss when processing the last few elements in a bundle

 

I recently learned that a java CompletableFuture cannot be completedExecptionally (AKA failed) if the future has already completed. So in QueueingBeamFnDataClient if the future is already marked done, we cannot fail it. I noticed this behaviour in a unit test for a bundle receiving data for multiple inputs (where I could not fail one of them execptionally, since it was already marked done). 

 

The potential bug I see would occur if the future is already completed before we fail the element (I think that we rely on this in the allDone method of the QueuingBeamFnDataClient). Imagine processing the last few elements in a bundle, the InBoundDataClient is marked completed because there are no more elements on the GRPC channel coming in, but we fail when processing it.

 

I could be wrong, if somehow the inbound data client futures are guaranteed to not complete until we finish processing the elements themselves. But I don't think this is the case, I think there is some code (GRPCBeamFnDataClient) which will complete the future once it has received all the elements on the channel.

Also we might have other code which mitigates this problem entirely, because the ProcessBundleHandler.processBundle will also throw an exception in this case, which should be enough to fail the bundle and hopefully prevent data loss.

One potential solution is to have two future in the inboundDataClient:
- waitUntilAllElementsReceivedOnGrpc

- waitUntilAllElementsFinishedProcessing (which can be marked in the QueueingBeamFnDataClient).

 

 ","Potential Bug, BeamFnDataClient future finalization",2,,,ajamato@google.com,True,,ajamato@google.com
beam,BEAM-6734,2019-02-22T17:41:18.000+0000,,2019-02-22T17:41:18.000+0000,,,New Feature,Major,,,,,,,,,['java-fn-execution'],['']," 

We have no way to dump and update the metric fake data this test uses. We should consider writing a tool to do this properly. We can change the format as long as its still human readable.

 

I tried and the following failed:

(1) Using json.dumps on the job_metrics

Fails for not being JSON serializable

(2) Using the proto json_format.py tools to dump on the job_metrics

Fails with exception: object has no attribute 'DESCRIPTOR'

[https://github.com/googleapis/google-cloud-python/issues/3485] 

(2) Using the proto text_format.py tools to dump on the job_metrics

Seemed to dump an empty file?

(3) Using this code to turn any object into a simple JSON style dict and print the JSON. This fails because it will print out all the private variables and other things we don't want. We could use this approach and whitelist the relevant fields.
{code:java}
def _create_simple_obj(self, obj):
  # If its a dictionary or defined class type
  if hasattr(obj, '__dict__') or isinstance(obj, dict):
    items = obj.items() if isinstance(obj, dict) else obj.__dict__.items()
    simple_dict = dict()
    for key, value in items:
      simple_dict[key] = self._create_simple_obj(value)
     return simple_dict
  elif isinstance(obj, (tuple, list, set, frozenset)):
    simple_list = []
    for x in obj:
      simple_list.append(self._create_simple_obj(x))
    return simple_list
  else:
    return obj

def _pretty_format_job_metrics(self, job_metrics):
  job_metrics = self._create_simple_obj(obj)
  return json.dumps(obj, indent=4, sort_keys=True)
{code}
 

 

Or we can just give up and not try to dump new ones, just add some basic tests to make sure it can convert the metrics format to MetricResults.

 

Then add more extensive testing on the MetricResults themselves with the matchers we are adding.

 

 

 

 ",dataflow_metrics_test hard to maintain fake data,2,,,ajamato@google.com,True,,ajamato@google.com
beam,BEAM-6730,2019-02-22T14:24:43.000+0000,2019-04-02T15:47:30.000+0000,2019-04-23T10:17:51.000+0000,,Fixed,New Feature,Major,['2.12.0'],,42600,42600,,,,100,"['runner-flink', 'sdk-java-core', 'sdk-py-core']","['Runners: Flink runner', 'SDKs: Java: pipeline construction, core transformations', 'SDKs: Python']","Since https://github.com/apache/beam/pull/7316 we can reference external transforms which are transforms only available in a ""foreign"" SDKs. This allows us to fill the gap in terms of missing transforms in the Python and Go SDK, specifically IO transforms.

We can start collecting/exposing transforms that Beam users need. The following transforms could be interesting:

- KafkaIO / KinesisIO
- CassandraIO / ElasticserchIO / Hbase / Redis
- JDBC
- S3 file system
- GenerateSequence

See also https://s.apache.org/beam-cross-language-io and BEAM-6485.

CC [~robertwb] [~chamikara] [~thw]",Enable configuration of Java transforms (specifically IO) through other SDKs,1,,,mxm,True,mxm,mxm
beam,BEAM-6729,2019-02-22T11:59:33.000+0000,,2019-02-22T11:59:33.000+0000,,,New Feature,Major,,,,,,,,,['sdk-py-core'],['SDKs: Python'],This would allow python functions and DoFn process methods to be annotated in the standard way as per PEP-484 for construction time type checking. Eventually this could be the preferred method for typing pipelines (though the typehints decorators may still be needed for composite transform hinting).,Detect and use PEP-3107 style type annotations for type hints.,3,1,,robertwb,True,,robertwb
beam,BEAM-6728,2019-02-22T10:07:49.000+0000,,2019-02-25T20:40:49.000+0000,,,New Feature,Major,,,,,,,,,['runner-core'],['Runners: shared functionality for all runners'],"GRPC can serve multiple services off of the same address. It would be ideal to allow serving the artifact service off the same port as the job service, especially when firewalls and port forwarding are involved. Of particular note, the artifact staging service address provided in the job preparation response could have a special value indicating ""use the job server address"" as the job server itself may be incapable as providing an address accessible to the client. 

 

[~mxm] [~angoenka]",Allow sharing of the job and artifact service address.,3,,,robertwb,True,,robertwb
beam,BEAM-6727,2019-02-22T08:45:02.000+0000,2019-03-01T15:50:26.000+0000,2019-03-01T15:50:26.000+0000,,Fixed,New Feature,Major,['2.12.0'],,1200,1200,,,,100,['sdk-py-core'],['SDKs: Python'],"I noticed the following lines in the travis logs (used from: https://github.com/apache/beam-wheels)

Running builds with Xcode 6.4 in Travis CI is deprecated and will be removed in January 2019.
If Xcode 6.4 is critical to your builds, please contact our support team at support@travis-ci.com to discuss options.
Services are not supported on osx

We need to change the version we are using to build wheel files.
Lots of newer versions are already supported: https://docs.travis-ci.com/user/reference/osx/#macos-version

[~robertwb][[~boyuanz] is there a reason for us to use xcode 6.4 here?

(Not release blocking because builds are still working even though there is a warning.)",Travis deprecated xcode 6.4 (used for wheel files),2,,,altay,True,robertwb,altay
beam,BEAM-6717,2019-02-20T09:59:12.000+0000,,2019-02-20T09:59:12.000+0000,,,New Feature,Minor,,,,,,,,,['io-ideas'],"['IO: ideas (proposals for new connectors, new designs, etc)']","[Eclipse RDF4J|http://rdf4j.org/] is a set of libraries for processing and handling [RDF|https://en.wikipedia.org/wiki/Resource_Description_Framework] data. The issue suggests to develop the following:

* transforms to read from the filesystem in all supported formats,
* transforms to write back to the filesystem in any supported formats,
* coders to be able to move RDF between transforms.

Partial implementation is available (in Kotlin) at https://github.com/DataFabricRus/scylla-beam-pipelines/blob/master/src/main/kotlin/cc/datafabric/scylladb/pipelines/io/RDF4JIO.kt",Develop RDF4J IO,1,,,kolchinmax,True,,kolchinmax
beam,BEAM-6716,2019-02-20T09:05:42.000+0000,,2019-02-20T09:05:42.000+0000,,,New Feature,Major,,,,,,,,,['website'],['Website: content and bugs'],Elements from successive panes are not necessarily received in order downstream. This should be documented more prominently. ,Update trigger documentation to highlight pitfals.,1,,,robertwb,True,,robertwb
beam,BEAM-6710,2019-02-19T18:15:50.000+0000,,2019-02-19T18:15:50.000+0000,,,New Feature,Major,,,,,,,,,['project-management'],"['Project Management: monthly reports, administrative tasks']","Community metrics dashboard sends user to list of recently opened dashboards, that's empty. This confuses new users. 

We want to add landing page with links to relevant dashboard.

Link: ttp://104.154.241.245/

 ",Add Landing page to  community metrics dashboard,1,,,Ardagan,True,,Ardagan
beam,BEAM-6697,2019-02-16T13:20:26.000+0000,2019-02-26T09:38:04.000+0000,2019-02-26T10:49:54.000+0000,,Fixed,New Feature,Blocker,['2.11.0'],,4200,4200,,,,100,"['io-java-parquet', 'test-failures']","['IO: Java: Parquet', 'Automatically filed for test failures']","Relevant failure logs: 


{code:java}
Caused by: java.lang.RuntimeException: org.apache.beam.sdk.io.parquet.ParquetIO$ReadFiles$BeamParquetInputFile@2de8303e is not a Parquet file (too small length: -1)
    	at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:514)
    	at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:689)
    	at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:595)
    	at org.apache.parquet.hadoop.ParquetReader.initReader(ParquetReader.java:152)
    	at org.apache.parquet.hadoop.ParquetReader.read(ParquetReader.java:135)
    	at org.apache.beam.sdk.io.parquet.ParquetIO$ReadFiles$ReadFn.processElement(ParquetIO.java:221){code}
 

Full logs can be found here: [https://builds.apache.org/view/A-D/view/Beam/view/PerformanceTests/job/beam_PerformanceTests_ParquetIOIT/|https://builds.apache.org/view/A-D/view/Beam/view/PerformanceTests/job/beam_PerformanceTests_ParquetIOIT/1096/console]

 

 

 ",ParquetIO Performance test is failing on (GCS filesystem),4,,,ŁukaszG,True,chamikara,ŁukaszG
beam,BEAM-6696,2019-02-16T01:42:33.000+0000,,2019-06-25T01:51:51.000+0000,,,New Feature,Major,,,,,,,,,['sdk-py-core'],['SDKs: Python'],"Add a PTransform that batches inputs to a desired batch size. Batches will contain only elements of a single key.

It should offer the same API as its Java counterpart: https://github.com/apache/beam/blob/11a977b8b26eff2274d706541127c19dc93131a2/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/GroupIntoBatches.java

Unlike BatchElements transform (https://github.com/apache/beam/blob/11a977b8b26eff2274d706541127c19dc93131a2/sdks/python/apache_beam/transforms/util.py#L461) GroupIntoBatches will use state to batch across bundles as well.",GroupIntoBatches transform for Python SDK,2,,,altay,True,shehzaadn,altay
beam,BEAM-6695,2019-02-16T01:38:11.000+0000,2019-06-04T15:29:09.000+0000,2019-06-04T15:29:09.000+0000,,Fixed,New Feature,Minor,['2.14.0'],,39000,39000,,,,100,['sdk-py-core'],['SDKs: Python'],"Add a PTransform} and Combine.CombineFn for computing the latest element in a PCollection.

It should offer the same API as its Java counterpart: https://github.com/apache/beam/blob/11a977b8b26eff2274d706541127c19dc93131a2/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/Latest.java",Latest transform for Python SDK,3,,,altay,True,ttanay,altay
beam,BEAM-6694,2019-02-16T01:35:12.000+0000,,2019-07-04T10:02:50.000+0000,,,New Feature,Minor,,,,,,,,,['sdk-py-core'],['SDKs: Python'],"Add PTransforms for getting an idea of a PCollection's data distribution using approximate N-tiles (e.g. quartiles, percentiles, etc.), either globally or per-key.

It should offer the same API as its Java counterpart: https://github.com/apache/beam/blob/11a977b8b26eff2274d706541127c19dc93131a2/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/ApproximateQuantiles.java",ApproximateQuantiles transform for Python SDK,3,,,altay,True,,altay
beam,BEAM-6693,2019-02-16T01:33:01.000+0000,2019-05-28T21:31:49.000+0000,2019-06-10T21:38:43.000+0000,,Fixed,New Feature,Minor,['2.14.0'],,45000,45000,,,,100,['sdk-py-core'],['SDKs: Python'],"Add a PTransform for estimating the number of distinct elements in a PCollection and the number of distinct values associated with each key in a PCollection KVs.

it should offer the same API as its Java counterpart: https://github.com/apache/beam/blob/11a977b8b26eff2274d706541127c19dc93131a2/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/ApproximateUnique.java",ApproximateUnique transform for Python SDK,2,,,altay,True,hannahjiang,altay
beam,BEAM-6677,2019-02-15T10:55:51.000+0000,2019-05-20T14:25:58.000+0000,2019-05-20T14:25:58.000+0000,,Done,New Feature,Minor,['Not applicable'],,12000,12000,,,,100,['testing'],['Testing: general infrastructure'],"A Flink cluster that can be set up for testing purposes. Currently, the easiest option seems to be using google cloud Dataproc for this purpose (ready to use init actions, scalable Flink instance, easy to configure). ",Provide Portable Flink cluster setup,2,,,ŁukaszG,True,ŁukaszG,ŁukaszG
beam,BEAM-6671,2019-02-14T17:34:08.000+0000,,2019-02-14T17:34:08.000+0000,,,New Feature,Major,,,,,,,,,['build-system'],"['Build, CI, release systems and processes']","I received a report from a Dataflow user encountering this in Beam 2.9.0 when creating a spanner instance. I wanted to post this here as this is known to be related to dependency conflicts in the past ([https://stackoverflow.com/questions/46684071/error-using-spannerio-in-apache-beam]). 

java.lang.NoSuchFieldError: internal_static_google_rpc_LocalizedMessage_fieldAccessorTable
        at com.google.rpc.LocalizedMessage.internalGetFieldAccessorTable(LocalizedMessage.java:90)
        at com.google.protobuf.GeneratedMessageV3.getDescriptorForType(GeneratedMessageV3.java:121)
        at io.grpc.protobuf.ProtoUtils.keyForProto(ProtoUtils.java:67)
        at com.google.cloud.spanner.spi.v1.SpannerErrorInterceptor.<clinit>(SpannerErrorInterceptor.java:47)
        at com.google.cloud.spanner.spi.v1.GrpcSpannerRpc.<init>(GrpcSpannerRpc.java:136)
        at com.google.cloud.spanner.SpannerOptions$DefaultSpannerRpcFactory.create(SpannerOptions.java:73)",Beam 2.9.0 java.lang.NoSuchFieldError: internal_static_google_rpc_LocalizedMessage_fieldAccessorTable,1,,,ajamato@google.com,True,,ajamato@google.com
beam,BEAM-6655,2019-02-12T09:30:45.000+0000,,2019-03-08T08:54:47.000+0000,,,New Feature,Major,,,12000,12000,,,,100,"['beam-model', 'sdk-java-harness', 'sdk-py-harness']","['Beam Model: general programming model concepts, semantics', 'SDKs: Java: harness for executing UDFs over the Fn API', 'SDKs: Python: harness for executing UDFs over the Fn API']",,Support progress reporting over FnAPI,1,,,robertwb,True,,robertwb
beam,BEAM-6654,2019-02-12T06:28:37.000+0000,,2019-02-27T08:11:43.000+0000,,,New Feature,Minor,,,,,,,,,['io-java-elasticsearch'],['IO: Java: ElasticSearch'],"Similar to HbaseIO.ReadAll, it would be very useful to support a SDF based transform which can compose queries based off of input elements. 

Implementation would split the work similarly to how the current Source API implementation works (leveraging slices within Elasticsearch).

This work can facilitate a transition of ElasticsearchIO to SDF when more runners have implemented support.  Until then, I propose leaving the read() implementation alone and add a readAll method and associated transform to be implemented as an SDF.  Once enough runner support is ready, the existing Read transform would be updated to use the same SDF as the ReadAll transform with a null input element.

A prototype implementation currently exists (albeit developed in a bubble) at [https://github.com/0xdecaf/beam/tree/feature/elasticsearchio.readall] 

 ",Support SDF within ElasticsearchIO,2,,,tmoulton,True,tmoulton,tmoulton
beam,BEAM-6653,2019-02-12T03:33:55.000+0000,,2019-02-12T21:07:22.000+0000,,,New Feature,Major,,,2400,2400,,,,100,['java-fn-execution'],[''],Add basic logging with the stack PTransform name to the Java SDK when a step is stuck.,Implement Lullz logging in the Beam Java SDK,1,,,ajamato@google.com,True,,ajamato@google.com
beam,BEAM-6651,2019-02-11T21:21:35.000+0000,2019-04-23T22:02:39.000+0000,2019-04-23T22:02:39.000+0000,,Invalid,New Feature,Major,['Not applicable'],,,,,,,,['runner-direct'],['Runners: Direct runner and ULR for single machine testing and development'],"Minimal SDF expansion as defined here:

https://docs.google.com/document/d/1cKOB9ToasfYs1kLWQgffzvIbJx2Smy4svlodPRhFrk4/edit#heading=h.eui3hdmmkld4",Implement minimal SDF expansion in Reference Runner,2,,,danoliveira,True,danoliveira,danoliveira
beam,BEAM-6636,2019-02-08T18:50:27.000+0000,2019-02-14T01:37:42.000+0000,2019-02-14T01:37:42.000+0000,,Fixed,New Feature,Major,['Not applicable'],,2400,2400,,,,100,['test-failures'],['Automatically filed for test failures'],"[https://builds.apache.org/job/beam_PostCommit_Java/2563/]

Test failed, no detailed logs available.

Relevant logs:
|org.apache.beam.sdk.io.gcp.storage.GcsKmsKeyIT > testGcsWriteWithKmsKey FAILED|
| java.lang.IllegalArgumentException at GcsKmsKeyIT.java:73|",[beam_PostCommit_Java] testGcsWriteWithKmsKey test fails,2,,['currently-failing'],Ardagan,True,udim,Ardagan
beam,BEAM-6629,2019-02-08T00:52:10.000+0000,,2019-02-08T00:52:10.000+0000,,,New Feature,Major,,,,,,,,,['java-fn-execution'],[''],"I've noticed that the DataflowWorkerLoggingHandler grabs the current execution state and extracts the step name from it.

 

However, in the Java SDK

ExecutionState does not implement step names, I left that logic only in DataflowExecutionState and omitted it from the base class. Thus when the Java SDK uses its SimpleExecutionState, it will not receive these step names and log them properly.

@swegner, can you confirm that this is the behaviour we want, for Java Beam SDKs to also include their step name in the logs, as used by the DatafloWorkerLoggingHandler? Java SDK code will end up using this handler as well, in portability?

[https://github.com/apache/beam/blob/52e73282223980fc0df9fbdbeddb2abb24d6600e/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/logging/DataflowWorkerLoggingHandler.java#L148]

 

Also, it may matter for dataflow the specific type of step name being used. (initial, optomized, user name ,etc.). The full name context will definetly not be available in the beam java SDK,",Make sure Java Apache Beam Logs properly include step names.,1,,,ajamato@google.com,True,,ajamato@google.com
beam,BEAM-6626,2019-02-07T04:08:45.000+0000,,2019-02-07T04:08:57.000+0000,,,New Feature,Major,,,,,,,,,['dsl-sql'],['DSLs: SQL'],Right now it doesn't exist. Very few runners use/support DisplayData. But it seems nice for a UI to display something about the SQL that led to the transform being executed.,SQL Transforms should have meaningful DisplayData with the SQL statement (or sub-expression),1,,,kenn,True,,kenn
beam,BEAM-6622,2019-02-06T22:40:00.000+0000,2019-04-23T23:04:56.000+0000,2019-04-30T18:30:11.000+0000,,Won't Fix,New Feature,Minor,['Not applicable'],,,,,,,,['runner-direct'],['Runners: Direct runner and ULR for single machine testing and development'],"ULR doesn't have large element support (and to be completely honest I don't fully know what large element support encompasses), so someone needs to figure out how to implement this in the ULR.",Add large element support to Java ULR,1,,,danoliveira,True,danoliveira,danoliveira
beam,BEAM-6612,2019-02-06T17:55:06.000+0000,2019-05-03T17:43:12.000+0000,2019-05-03T17:43:12.000+0000,,Fixed,New Feature,Major,['2.7.1'],,6000,6000,,,,100,['java-fn-execution'],[''],"Remove QueueingBeamFnDataClient, which made process() calls all run on the same thread.

[~lcwik] and I came up with this design thinking that it was required to process the bundle in parallel anyways, and we would have good performance. However after speaking to Ken, there is no requirement for a bundle or key to be processed in parallel. Elements are either iterables or single elements which defines the needs for processing a group of elements on the same thread.

Simply performing this change will lead to the following issues:

(1) MetricsContainerImpl and MetricsContainer are not thread safe, so when the process() functions enter the metric container context, they will be accessing an thread-unsafe collection in parallel

(2) An ExecutionStateTracker will be needed in every thread, So we will need to

create an instance and activate it in every GrpC thread which receives a new element.

(Will this get sampled properly, since the trackers will be short lived).

(3) The SimpleExecutionStates being used will need to be thread safe as well? I don't think so, because I don't think that the ExecutionStateSampler invokes them in parallel.

 ",PerformanceRegression in QueueingBeamFnDataClient,1,,,ajamato@google.com,True,ajamato@google.com,ajamato@google.com
beam,BEAM-6610,2019-02-06T15:30:57.000+0000,,2019-05-24T19:59:58.000+0000,,,New Feature,Major,,,,,,,,,['test-failures'],['Automatically filed for test failures'],"Job:

[https://builds.apache.org/job/beam_PostCommit_Python_Verify/7313/console]

[https://builds.apache.org/job/beam_PostCommit_Java/2542/testReport/org.apache.beam.sdk.io.gcp.pubsub/PubsubReadIT/testReadPublicData/]

Gradle target:

*:beam-sdks-python:postCommitIT*

Error:
 *04:44:17* Traceback (most recent call last):*04:44:17* File ""/home/jenkins/jenkins-slave/workspace/beam_PostCommit_Python_Verify/src/sdks/python/apache_beam/io/gcp/pubsub_integration_test.py"", line 108, in setUp*04:44:17* self.pub_client.topic_path(self.project, INPUT_TOPIC + self.uuid))*04:44:17* File ""/home/jenkins/jenkins-slave/workspace/beam_PostCommit_Python_Verify/src/build/gradleenv/1327086738/local/lib/python2.7/site-packages/google/cloud/pubsub_v1/_gapic.py"", line 44, in <lambda>*04:44:17* fx = lambda self, *a, **kw: wrapped_fx(self.api, *a, **kw) # noqa*04:44:17* File ""/home/jenkins/jenkins-slave/workspace/beam_PostCommit_Python_Verify/src/build/gradleenv/1327086738/local/lib/python2.7/site-packages/google/cloud/pubsub_v1/gapic/publisher_client.py"", line 271, in create_topic*04:44:17* request, retry=retry, timeout=timeout, metadata=metadata)*04:44:17* File ""/home/jenkins/jenkins-slave/workspace/beam_PostCommit_Python_Verify/src/build/gradleenv/1327086738/local/lib/python2.7/site-packages/google/api_core/gapic_v1/method.py"", line 143, in __call__*04:44:17* return wrapped_func(*args, **kwargs)*04:44:17* File ""/home/jenkins/jenkins-slave/workspace/beam_PostCommit_Python_Verify/src/build/gradleenv/1327086738/local/lib/python2.7/site-packages/google/api_core/retry.py"", line 270, in retry_wrapped_func*04:44:17* on_error=on_error,*04:44:17* File ""/home/jenkins/jenkins-slave/workspace/beam_PostCommit_Python_Verify/src/build/gradleenv/1327086738/local/lib/python2.7/site-packages/google/api_core/retry.py"", line 179, in retry_target*04:44:17* return target()*04:44:17* File ""/home/jenkins/jenkins-slave/workspace/beam_PostCommit_Python_Verify/src/build/gradleenv/1327086738/local/lib/python2.7/site-packages/google/api_core/timeout.py"", line 214, in func_with_timeout*04:44:17* return func(*args, **kwargs)*04:44:17* File ""/home/jenkins/jenkins-slave/workspace/beam_PostCommit_Python_Verify/src/build/gradleenv/1327086738/local/lib/python2.7/site-packages/google/api_core/grpc_helpers.py"", line 59, in error_remapped_callable*04:44:17* six.raise_from(exceptions.from_grpc_error(exc), exc)*04:44:17* File ""/home/jenkins/jenkins-slave/workspace/beam_PostCommit_Python_Verify/src/build/gradleenv/1327086738/local/lib/python2.7/site-packages/six.py"", line 737, in raise_from*04:44:17* raise value*04:44:17* ResourceExhausted: 429 Your project has exceeded a limit: (type=""topics-per-project"", current=10000, maximum=10000).","[beam_PostCommit_Python_Verify, beam_PostCommit_Java]ResourceExhausted: 429 Your project has exceeded a limit: (type=""topics-per-project"", current=10000, maximum=10000)",2,,"['currently-failing', 'mitigated']",Ardagan,True,kenn,Ardagan
beam,BEAM-6597,2019-02-05T18:54:51.000+0000,,2019-02-05T18:54:51.000+0000,,,New Feature,Major,,,,,,,,,['java-fn-execution'],[''],"I think this is the correct approach, as I don't believe there is any hook in the Java SDK yet for ProcessBundleProgressResponses.


(1) Implement ProcessBundleProgressResponse

See FnHarness.main to add a handle for RequestCase.PROGRESS_BUNDLE.

(2) Refactor ProgressBundleHandler so that the metrics can be extracted from the MetricContainerStep map and SimpleExecutionStates for the instrucitonId when the call comes in. (Right now all these objects only live in the local function, they may need to live in an object instead which can be accessed by both process bundle and progress bundle responses). Be careful to not introduce thread contention. Ideally we need a way to read the values without locking new ones from being written.

(Test 1) Also be sure to simplify RemoteExecutionTest.testMetrics().

By inspecting the metric progress, we can remove the sleeps from this code. Currently there are sleeps in start, process and finish to ensure execution time metrics are added. Instead, once progress bundle responses are introduced, the metrics can be examined here",Put MonitoringInfos/metrics in the Java SDK ProcessBundleProgressResponse,2,,,ajamato@google.com,True,,ajamato@google.com
beam,BEAM-6594,2019-02-05T17:24:58.000+0000,2019-02-21T22:15:17.000+0000,2019-02-21T22:15:17.000+0000,,Duplicate,New Feature,Major,['Not applicable'],,,,,,,,['java-fn-execution'],[''],"org.apache.beam.runners.fnexecution.data.GrpcDataServiceTest.testMessageReceivedBySingleClientWhenThereAreMultipleClients

I keep seeing this test failing in my PRs

[https://builds.apache.org/job/beam_PreCommit_Java_Commit/4018/]

[https://builds.apache.org/job/beam_PreCommit_Java_Commit/4018/testReport/junit/org.apache.beam.runners.fnexecution.data/GrpcDataServiceTest/testMessageReceivedBySingleClientWhenThereAreMultipleClients/]

 

 ",[BEAM-6594] Flakey GrpcDataServiceTest.testMessageReceivedBySingleClientWhenThereAreMultipleClients - failing in precommit,1,,,ajamato@google.com,True,bhulette,ajamato@google.com
beam,BEAM-6588,2019-02-05T13:37:43.000+0000,2019-04-05T11:23:04.000+0000,2019-04-30T18:31:33.000+0000,,Fixed,New Feature,Major,['2.13.0'],,2400,2400,,,,100,['sdk-py-core'],['SDKs: Python'],"If a type hint is specified for an input to beam.Filter, it attempts to infer the output type (as Iterable[input_type], consistent with FlatMap), but that inference appears to have a bug in it.

With the code:


{code:python}
@beam.typehints.with_input_types(int)
def OddFilter(data):
  return data % 2 == 0

def pipeline(root):
  base = root | beam.Create(xrange(100))
  next = base | beam.Filter(OddFilter)
{code}

The following error is returned:

{code:python}
  File ""/google3/experimental/testproj/test_beam.py"", line 26, in pipeline
    next = base | beam.Filter(OddFilter)
  File ""/google3/third_party/py/apache_beam/transforms/core.py"", line 1147, in Filter
    get_type_hints(wrapper).set_output_types(typehints.Iterable[output_hint])
  File ""/google3/third_party/py/apache_beam/typehints/typehints.py"", line 951, in __getitem__
    type_param, error_msg_prefix='Parameter to an Iterable hint'
  File ""/google3/third_party/py/apache_beam/typehints/typehints.py"", line 359, in validate_composite_type_param
    type_param.__class__.__name__))
TypeError: Parameter to an Iterable hint must be a non-sequence, a type, or a TypeConstraint. (<type 'int'>,) is an instance of tuple.
{code}

Explicitly specifying the output type (as beam.typehints.Iterable[int]) works fine.  The code in core.py seems to be correct, but I'm guessing it needs a derefence of the tuple to actually extract the type: http://google3/third_party/py/apache_beam/transforms/core.py?l=1145&rcl=228573657
",Error in inferring output typehints for beam.Filter,2,1,,robertwb,True,altay,robertwb
beam,BEAM-6587,2019-02-05T09:33:38.000+0000,,2019-04-30T18:32:33.000+0000,,,New Feature,Major,,,4800,4800,,,,100,"['sdk-java-harness', 'sdk-py-harness']","['SDKs: Java: harness for executing UDFs over the Fn API', 'SDKs: Python: harness for executing UDFs over the Fn API']","It doesn't have to be understood by the runner, but it would be good if most/all SDKs understood it.",Let StringUtf8 be a well-known coder.,1,,,robertwb,True,lcwik,robertwb
beam,BEAM-6586,2019-02-05T09:28:21.000+0000,,2019-02-05T09:28:21.000+0000,,,New Feature,Major,,,,,,,,,['build-system'],"['Build, CI, release systems and processes']","Related discussion: [https://lists.apache.org/thread.html/770496ee9cf1096d78806fece8dd37716279b51ca5bb600dfa263c55@%3Cdev.beam.apache.org%3E]

cc: [~angoenka]",Design and implement a release process for Beam SDK harness containers.,1,,,tvalentyn,True,,tvalentyn
beam,BEAM-6551,2019-01-30T17:28:56.000+0000,,2019-06-06T23:40:34.000+0000,,,New Feature,Major,['Not applicable'],,,,,,,,['java-fn-execution'],[''],"Noticed this one one of my Java PRs:
 [https://builds.apache.org/job/beam_PreCommit_Python_Commit/3984/console]

[https://builds.apache.org/job/beam_PreCommit_Python_Commit/3984/]

*04:58:37* > *Task :beam-sdks-python:lintPy27* FAILED

 

There does seem to be a failure, but not output as to why and I cannot get it to fail locally to see more details. Not sure how to proceed.",Python Precommit broken beam-sdks-python:lintPy27 FAILED,2,,,ajamato@google.com,True,,ajamato@google.com
beam,BEAM-6549,2019-01-30T11:23:18.000+0000,,2019-02-01T10:31:02.000+0000,,,New Feature,Major,,,,,,,,,['sdk-go'],['SDKs: Go'],"The Python SDK retrieves Runner pipeline options from the JobService: https://github.com/apache/beam/pull/7597 

The Go SDK should do the same.",Make use of DescribePipelineOptionsRequest in Go SDK,2,,,mxm,True,,mxm
beam,BEAM-6546,2019-01-30T01:21:01.000+0000,2019-03-15T12:41:09.000+0000,2019-03-15T12:41:09.000+0000,,Resolved,New Feature,Major,['2.12.0'],,,,,,,,['java-fn-execution'],[''],"This calls equals on the underlying collection of containers it uses, however the containers do not implement equals.

 

 

Open question. It will also be an expensive operation to call equals. Is it necessary to compare MetricContainerStepMaps for equality? Perhaps for unit tests, but is it necessary for production code?",MetricsContainerStepMap equals() method not implemented properly,2,,,ajamato@google.com,True,winkelman.kyle,ajamato@google.com
beam,BEAM-6543,2019-01-29T23:05:14.000+0000,2019-01-30T11:49:38.000+0000,2019-01-30T11:49:38.000+0000,,Fixed,New Feature,Critical,['Not applicable'],,2400,2400,,,,100,['runner-flink'],['Runners: Flink runner'],"For example,

[https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink/2980/]

[https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink/2980/testReport/junit/org.apache.beam.runners.core.metrics/MetricsPusherTest/test/]

java.lang.AssertionError: Expected: is <1000L> but: was <0L>

[https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink/2980/testReport/junit/org.apache.beam.sdk.transforms/ParDoTest$BasicTests/testPipelineOptionsParameter/]

java.lang.AssertionError: ParDo(Anonymous)/ParMultiDo(Anonymous).output: Expected: iterable over [""not fake anymore""] in any order but: Not matched: ""fake option""

 

Seems like the this failure first occurred for PR [https://github.com/apache/beam/pull/7660]

[https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink/2980/]

Max, assigning to you since you authored above PR. But feel free to send this back if you think the failure is unrelated.

 ",Multiple PostCommit_Java_ValidatesRunner_Flink tests are failing due to AssertionError,2,,,chamikara,True,mxm,chamikara
beam,BEAM-6538,2019-01-29T18:48:23.000+0000,,2019-01-29T18:48:23.000+0000,,,New Feature,Major,,,,,,,,,['java-fn-execution'],[''],"I discovered some tests were setting empty step names, when I added  a precondition check to MetricsContainerStep map when adding java SDK metrics. Remove the code which returns the the default non-stepped unbounded MetricsContainer after ensuring step names are never passed in empty or null in tests or from a runner.",Disallow empty step names in ProcesBundleDescriptors,2,,,ajamato@google.com,True,,ajamato@google.com
beam,BEAM-6523,2019-01-28T13:17:44.000+0000,,2019-05-15T19:42:34.000+0000,,,New Feature,Minor,,,4800,4800,,,,100,['runner-flink'],['Runners: Flink runner'],"The Beam documentation mentions that Flatten does not require coders of all inputs to be of the same type, as long as the result type is the same. However, the current implementation in the Flink Runner requires all coders to match.",Support transcoding for Flatten in Flink Runner,1,,,mxm,True,,mxm
beam,BEAM-6509,2019-01-25T20:13:10.000+0000,,2019-02-07T04:47:42.000+0000,,,New Feature,Minor,,,,,,,,,['sdk-ideas'],['SDKs: Coordination on new language-specific SDKs'],I've ran into this issue a few times in dataflow where I am making a network request as part of a staging (IE writing some data to a service as part of a data migration) and I'd like to rate limit the number of network calls that are taking place (globally).,Allow Rate Limiting per Stage,1,,,rockwotj,True,,rockwotj
beam,BEAM-6505,2019-01-25T00:47:00.000+0000,2019-05-03T17:42:20.000+0000,2019-05-03T17:42:21.000+0000,,Fixed,New Feature,Major,['3.0.0'],,,,,,,,['java-fn-execution'],[''],"See the comment added for this issue in ElementCountFnDataReceiver.java

The method used to create these metrics relies on the currently in scope metrics container, though we should use the same metrics container every time this code is invoked instead. There is no need to use the current scoped metric container, which only offers the main benefit to user counters, by attaching the PTransform name to the metrics. In this case the metric does not need the currently scoped PTransform name, since the code is labelling the metrics with the pcollection, and similar cases can manually attach the ptransform name (i.e. for execution time metrics).

We can make the static method LabelledMetrics.counter(metricName) obtain a consistent metric container instead of looking for the currently scoped metric container.",Java SDK - Allow System Counters (which don't need MetricsContainer context),1,,,ajamato@google.com,True,ajamato@google.com,ajamato@google.com
beam,BEAM-6504,2019-01-24T20:10:40.000+0000,2019-03-15T17:31:09.000+0000,2019-04-30T18:28:55.000+0000,,Implemented,New Feature,Major,['2.12.0'],,15600,15600,,,,100,['runner-dataflow'],['Runners: Google Cloud Dataflow runner'],"Underlying fn api support is done in BEAM-2929, this Jira integrates everything into dataflow. 

 

1) introduce a sideInputHandler for dataflow. 

2) wire the handler to dataflow runner (i.e.  ProcessRemoteBundleOperation)",Integration of Portabability sideInput into Dataflow,1,,,ruoyun,True,ruoyun,ruoyun
beam,BEAM-6500,2019-01-24T01:37:23.000+0000,2019-02-14T18:45:51.000+0000,2019-04-30T18:32:52.000+0000,,Fixed,New Feature,Major,['Not applicable'],,2400,2400,,,,100,['java-fn-execution'],[''],"This seems to be broken for all PRs now

 

:beam-runners-direct-java:checkstyleMain FAILED [ant:checkstyle] [WARN] /home/jenkins/jenkins-slave/workspace/beam_PreCommit_Java_Phrase/src/runners/direct-java/src/main/java/org/apache/beam/runners/direct/portable/ReferenceRunner.java:164:3: Missing a Javadoc comment. [JavadocMethod] [ant:checkstyle] [ERROR] /home/jenkins/jenkins-slave/workspace/beam_PreCommit_Java_Phrase/src/runners/direct-java/src/main/java/org/apache/beam/runners/direct/portable/job/ReferenceRunnerJobServer.java:147: Missing a Javadoc comment. [JavadocType]

 

https://scans.gradle.com/s/nwgb7xegklwqo/console-log?task=:beam-runners-direct-java:checkstyleMain",Precomit broken due to style violation,1,,,ajamato@google.com,True,swegner,ajamato@google.com
beam,BEAM-6495,2019-01-23T19:02:50.000+0000,,2019-01-23T19:05:17.000+0000,,,New Feature,Major,,,,,,,,,['java-fn-execution'],[''],"repro, rebase from master and run:

./gradlew compileJava && ./gradlew compileTestJava

https://gradle.com/s/pgdbpg2yfruvm

 

> Task :beam-runners-google-cloud-dataflow-java-fn-api-worker:compileJava
Note: Some input files use or override a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
Note: Some input files use unchecked or unsafe operations.
Note: Recompile with -Xlint:unchecked for details.

FAILURE: Build failed with an exception.

* What went wrong:
Execution failed for task ':beam-sdks-java-extensions-sql:compileJava'.
> Compilation failed with exit code 1; see the compiler error output for details.

* Try:
Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. Run with --scan to get full insights.

* Get more help at https://help.gradle.org

Deprecated Gradle features were used in this build, making it incompatible with Gradle 5.0.
Use '--warning-mode all' to show the individual deprecation warnings.
See https://docs.gradle.org/4.10.3/userguide/command_line_interface.html#sec:command_line_warnings

BUILD FAILED in 3m 51s
123 actionable tasks: 84 executed, 39 up-to-date

Publishing build scan...
https://gradle.com/s/pgdbpg2yfruvm",./gradlew compileJava broken on master branch,2,1,,ajamato@google.com,True,,ajamato@google.com
beam,BEAM-6490,2019-01-22T23:16:22.000+0000,2019-01-22T23:39:51.000+0000,2019-01-22T23:39:51.000+0000,,Duplicate,New Feature,Major,['Not applicable'],,,,,,,,['test-failures'],['Automatically filed for test failures'],"Sample job
[https://builds.apache.org/job/beam_PreCommit_Python_Phrase/152/console]

Error:
*14:18:28* GLOB sdist-make: /home/jenkins/jenkins-slave/workspace/beam_PreCommit_Python_Phrase/src/sdks/python/setup.py*14:18:28* py3 create: /home/jenkins/jenkins-slave/workspace/beam_PreCommit_Python_Phrase/src/sdks/python/target/.tox/py3*14:18:29* py3 installdeps: future==0.16.0*14:18:29* ERROR: invocation failed (exit code 2), logfile: /home/jenkins/jenkins-slave/workspace/beam_PreCommit_Python_Phrase/src/sdks/python/target/.tox/py3/log/py3-1.log*14:18:29* ERROR: actionid: py3*14:18:29* msg: getenv*14:18:29* cmdargs: ['/home/jenkins/jenkins-slave/workspace/beam_PreCommit_Python_Phrase/src/sdks/python/target/.tox/py3/bin/python', '/home/jenkins/jenkins-slave/workspace/beam_PreCommit_Python_Phrase/src/sdks/python/target/.tox/py3/bin/pip', 'install', '--retries', '10', '--process-dependency-links', 'future==0.16.0']*14:18:29* *14:18:29* *14:18:29* Usage:   *14:18:29*   pip install [options] <requirement specifier> [package-index-options] ...*14:18:29*   pip install [options] -r <requirements file> [package-index-options] ...*14:18:29*   pip install [options] [-e] <vcs project url> ...*14:18:29*   pip install [options] [-e] <local project path> ...*14:18:29*   pip install [options] <archive url/path> ...*14:18:29* *14:18:29* no such option: --process-dependency-links*14:18:29* *14:18:29* ERROR: could not install deps [future==0.16.0]; v = InvocationError('/home/jenkins/jenkins-slave/workspace/beam_PreCommit_Python_Phrase/src/sdks/python/target/.tox/py3/bin/python /home/jenkins/jenkins-slave/workspace/beam_PreCommit_Python_Phrase/src/sdks/python/target/.tox/py3/bin/pip install --retries 10 --process-dependency-links future==0.16.0 (see /home/jenkins/jenkins-slave/workspace/beam_PreCommit_Python_Phrase/src/sdks/python/target/.tox/py3/log/py3-1.log)', 2)",Python precommit fail with no such option: --process-dependency-links,1,,['currently-failing'],Ardagan,True,Ardagan,Ardagan
beam,BEAM-6488,2019-01-22T21:29:22.000+0000,2019-02-14T01:43:40.000+0000,2019-04-30T18:29:23.000+0000,,Fixed,New Feature,Major,['2.11.0'],,16200,16200,,,,100,"['beam-model', 'runner-core', 'runner-flink', 'sdk-java-core', 'sdk-py-core']","['Beam Model: general programming model concepts, semantics', 'Runners: shared functionality for all runners', 'Runners: Flink runner', 'SDKs: Java: pipeline construction, core transformations', 'SDKs: Python']","To support running cross-language transforms, Portable Flink runner needs to support executing pipelines with steps defined to be executed for different environments.

I believe this support is already there. If that is the case we should validate that and add any missing tests.

If there are missing pieces, we should figure out details and create more JIRAs as needed. 

CC: [~angoenka]",Portable Flink runner support for running cross-language transforms,3,,,chamikara,True,heejong,chamikara
beam,BEAM-6486,2019-01-22T21:21:50.000+0000,,2019-04-30T18:30:15.000+0000,,,New Feature,Major,,,,,,,,,"['beam-model', 'runner-core', 'sdk-java-core', 'sdk-py-core']","['Beam Model: general programming model concepts, semantics', 'Runners: shared functionality for all runners', 'SDKs: Java: pipeline construction, core transformations', 'SDKs: Python']",We should be able to refer to a transform defined in SDK X from a different SDK Y.,Cross-language transform API,3,,,chamikara,True,chamikara,chamikara
beam,BEAM-6485,2019-01-22T21:18:04.000+0000,,2019-04-22T18:08:07.000+0000,,,New Feature,Major,,,,,,,,,"['beam-model', 'runner-core', 'sdk-java-core', 'sdk-py-core']","['Beam Model: general programming model concepts, semantics', 'Runners: shared functionality for all runners', 'SDKs: Java: pipeline construction, core transformations', 'SDKs: Python']",, Cross-language transform expansion protocol,3,,,chamikara,True,,chamikara
beam,BEAM-6480,2019-01-22T09:55:06.000+0000,2019-07-09T13:01:57.000+0000,2019-07-09T13:01:57.000+0000,,Fixed,New Feature,Major,['2.15.0'],['2.9.0'],12600,12600,,,,100,['io-java-avro'],['IO: Java: Avro'],More generally for sink there is no need to create a mapper API since the previous PTransform can always map in a format the sink support so any sink can assume the format is right.,Add AvroIO.sink for IndexedRecord (FileIO compatible),4,,,romain.manni-bucau,True,ryanskraba,romain.manni-bucau
beam,BEAM-6467,2019-01-18T19:08:02.000+0000,,2019-01-18T19:08:02.000+0000,,,New Feature,Major,,,,,,,,,['java-fn-execution'],[''],"I noticed this in RemoteExecutionTests testMetrics. Investigate and determine the source of this bug, as impulse is supposed to produce only one.",Impulse.create() emitting more than one element,1,,,ajamato@google.com,True,,ajamato@google.com
beam,BEAM-6464,2019-01-18T15:07:24.000+0000,,2019-04-30T18:34:07.000+0000,,,New Feature,Major,,,9000,9000,,,,100,['sdk-py-core'],['SDKs: Python'],These can be controlled by the environment variable. ,PortableRunner should run dockerized SDK workers,2,,,robertwb,True,altay,robertwb
beam,BEAM-6462,2019-01-18T08:38:38.000+0000,,2019-04-30T18:33:32.000+0000,,,New Feature,Minor,,,,,,,,,['testing'],['Testing: general infrastructure'],"To help with the different tests of the portabilty translation of the runners and the Universal Local Ronner. It will be convenient to have a version of SyntheticIO based on SDF, in particular the Unbounded version.",Create an unbounded version of SyntheticIO based on SDF,1,,,iemejia,True,iemejia,iemejia
beam,BEAM-6456,2019-01-16T20:20:45.000+0000,2019-01-16T21:42:33.000+0000,2019-01-16T21:42:33.000+0000,,Fixed,New Feature,Minor,['Not applicable'],,2400,2400,,,,100,['build-system'],"['Build, CI, release systems and processes']","Community metrics located at [http://104.154.241.245|http://104.154.241.245/] have DB utilizing integers for job duration. This causes int overflow on long jobs.

Current mitigation trims duration to int max. We want to update DB schema to utilize bigint for duration. ",[community metrics] Update community metrics DB to utilize BigInt for job duration,1,,,Ardagan,True,Ardagan,Ardagan
beam,BEAM-6455,2019-01-16T18:25:53.000+0000,,2019-02-07T04:58:45.000+0000,,,New Feature,Major,,,,,,,,,['website'],['Website: content and bugs'],"This is breaking the precommit, so I am adding an ignore witha comment linking to this JIRA for in the Rakefile. Please remove it when the precommit dashboard is working again. Or consider making this precommit more intelligent, to only test newly added links. Or have some other intelligent way to not cause time churn on the PR due to a website going down, unrelated to the website change.

/usr/local/google/home/ajamato/go/src/github.com/apache/beam/website/Rakefile",Adding ignore for link validation logic to the precommit dashboard,1,,,ajamato@google.com,True,,ajamato@google.com
beam,BEAM-6439,2019-01-15T16:18:21.000+0000,2019-01-15T23:07:35.000+0000,2019-01-29T10:20:29.000+0000,,Fixed,New Feature,Major,['Not applicable'],,18600,18600,,,,100,"['runner-flink', 'testing']","['Runners: Flink runner', 'Testing: general infrastructure']","Now that they're fast and stable, it would be good to catch changes that break this earlier.",Move Python Flink VR tests to PreCommit,2,,,robertwb,True,robertwb,robertwb
beam,BEAM-6435,2019-01-15T15:57:05.000+0000,2019-04-18T16:16:35.000+0000,2019-04-30T18:33:56.000+0000,,Fixed,New Feature,Major,['Not applicable'],,2400,2400,,,,100,"['runner-flink', 'test-failures']","['Runners: Flink runner', 'Automatically filed for test failures']","Often there are no test failures, but the task fails with 

11:02:59 Execution failed for task ':beam-runners-flink_2.11-job-server:validatesPortableRunnerBatch'.
11:02:59 > Process 'Gradle Test Executor 2' finished with non-zero exit value 137
11:02:59   This problem might be caused by incorrect test process configuration.
11:02:59   Please refer to the test execution section in the user guide at https://docs.gradle.org/4.10.3/userguide/java_plugin.html#sec:test_execution
",beam_PostCommit_Java_PVR_Flink_Batch regularly failing,2,,,robertwb,True,mxm,robertwb
beam,BEAM-6431,2019-01-15T01:11:16.000+0000,2019-05-03T17:41:59.000+0000,2019-05-03T17:41:59.000+0000,,Fixed,New Feature,Major,['3.0.0'],,12000,12000,,,,100,['java-fn-execution'],[''],"This will be done by using the Dataflow worker's StateSampler code. I have put together a refactoring plan
[here|https://docs.google.com/document/d/1OlAJf4T_CTL9WRH8lP8uQOfLjWYfm8IpRXSe38g34k4/edit#]

This will include estimating the processing time for the start, process and finish bundle. The python SDK already has an implementation of this.",Add ExecutionTime metrics to the Beam Java SDK,1,,,ajamato@google.com,True,ajamato@google.com,ajamato@google.com
beam,BEAM-6422,2019-01-14T14:29:19.000+0000,2019-02-08T12:37:38.000+0000,2019-04-30T18:30:32.000+0000,,Won't Fix,New Feature,Minor,['Not applicable'],,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']",Today ‘sdks/java/core’ includes the dependencies and the different implementation of all the compression formats supported by Beam. This implies that compression dependencies tend to leak from the core SDK. This issue tracks the creation of an extensible mechanism to register new compression formats wihout adding more dependencies to core SDK.,Create an extensible mechanism for file (de)compresion,1,,,iemejia,True,iemejia,iemejia
beam,BEAM-6406,2019-01-10T21:50:35.000+0000,2019-03-14T00:16:32.000+0000,2019-04-30T18:32:49.000+0000,,Fixed,New Feature,Major,['Not applicable'],,1200,1200,,,,100,['build-system'],"['Build, CI, release systems and processes']","We are looking for tools to dashboard static analysis reports and set up quality gates for our code.

One of the tools that can handle this and is provided by Apache is [SonarQube|https://cwiki.apache.org/confluence/display/INFRA/SonarQube+Analysis].

We want to add SonarQube analysis tools to our builds to check on feasibility on using it further.",Add SonarQube analysis to jenkins jobs.,1,,,Ardagan,True,Ardagan,Ardagan
beam,BEAM-6404,2019-01-10T09:58:14.000+0000,,2019-04-30T18:33:14.000+0000,,,New Feature,Major,,,7200,7200,,,,100,['sdk-py-core'],['SDKs: Python']," {code:java}
def run(argv=None):
  parser = argparse.ArgumentParser()
  _, pipeline_args = parser.parse_known_args(argv)
  options = pipeline_options.PipelineOptions(pipeline_args)
  numbers = [1, 2]
  with beam.Pipeline(options=options) as p:
    sum_1 = (p
             | 'ReadNumber1' >> transforms.Create(numbers)
             | 'CalculateSum1' >> beam.CombineGlobally(fn_sum))

    sum_2 = (p
             | 'ReadNumber2' >> transforms.Create(numbers)
             | beam.ParDo(_copy_number, pvalue.AsSingleton(sum_1))
             | 'CalculateSum2' >> beam.CombineGlobally(fn_sum))

    _ = ((sum_1, sum_2)
         | beam.Flatten()
         | 'CalculateSum3' >> beam.CombineGlobally(fn_sum)
         | beam.io.WriteToText('out.txt'))

run()
{code}
 
fails with 

KeyError: u'ref_Coder_FastPrimitivesCoder_4_windowed'
",FnAPI translation error,2,,,robertwb,True,robertwb,robertwb
beam,BEAM-6402,2019-01-09T19:54:16.000+0000,,2019-01-09T19:57:13.000+0000,,,New Feature,Minor,,,,,,,,,['testing'],['Testing: general infrastructure'],"Our Jenkins jobs are maintained in source code using the Groovy Job DSL, and kept up-to-date on our Apache Jenkins instance by regularly running the Seed Job.

Updating jobs is pretty easy by modifying the Groovy code, but there's no way to validate changes other than running the Seed Job and affecting the live job definitions. We should have some way to validate job changes outside of running the Seed job on production.",Need ability to test Jenkins job changes without affecting live jobs,1,1,,swegner,True,,swegner
beam,BEAM-6394,2019-01-09T09:43:12.000+0000,,2019-04-30T18:33:45.000+0000,,,New Feature,Major,,,,,,,,,['io-java-parquet'],['IO: Java: Parquet'],Parquet infrastructure does support writing protobuf data to parquet. Beam's ParquetIO could give pipeline developers an option to write protobuf data instead of converting them to avro.,Support for writing protobuf data to parquet,3,,,JozoVilcek,True,ŁukaszG,JozoVilcek
beam,BEAM-6392,2019-01-08T23:06:59.000+0000,,2019-04-30T18:30:04.000+0000,,,New Feature,Major,,,15000,15000,,,,100,['io-java-gcp'],"['IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)']",BigQuery has developed a new streaming egress API which will soon reach public availability. Add support for the new API in BigQueryIO.,Add support for new BigQuery streaming read API to BigQueryIO,3,,,kjung520,True,kjung520,kjung520
beam,BEAM-6390,2019-01-08T20:53:20.000+0000,,2019-02-07T04:30:32.000+0000,,,New Feature,Major,,,,,,,,,['sdk-py-core'],['SDKs: Python'],"Could we please have a version of org.apache.beam.sdk.io.GenerateSequence in the Python SDK?

I notice there is beam.io.utils.CountingSource, which looks similar, but it's not clear whether this is intended to be part of the public API or just for internal testing purposes (it's not exposed via beam.io's __init__).",beam.io.GenerateSequence in the Python SDK,1,,,matthjw,True,,matthjw
beam,BEAM-6379,2019-01-07T18:16:49.000+0000,,2019-01-07T18:24:43.000+0000,,,New Feature,Major,,,,,,,,,['java-fn-execution'],[''],,"Research and introduce thread sanitizer code into the java SDK, and java runner harnesses",1,,,ajamato@google.com,True,,ajamato@google.com
beam,BEAM-6365,2019-01-04T02:55:29.000+0000,2019-02-08T12:47:47.000+0000,2019-04-30T18:33:10.000+0000,,Fixed,New Feature,Minor,['2.11.0'],,9600,9600,,,,100,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","ZStandard (zstd) compression has been gaining popularity since its introduction a few years ago. It is supported in commons-compress, the library we already use for handling file compression and decompression in various formats, so should be straightforward to add support in the Java SDK by adding a ZSTD enum value in Compression.java.",Add ZStandard compression support for Java SDK,1,,['has-pr'],jeff.klukas@gmail.com,True,jeff.klukas@gmail.com,jeff.klukas@gmail.com
beam,BEAM-6327,2018-12-28T19:28:43.000+0000,,2019-06-07T09:23:22.000+0000,,,New Feature,Major,,,8400,8400,,,,100,['runner-direct'],['Runners: Direct runner and ULR for single machine testing and development'],"Currently we must remove all sub-components of any known transform that may have an optional substructure, e.g. [https://github.com/apache/beam/blob/release-2.9.0/sdks/python/apache_beam/runners/portability/portable_runner.py#L126] (for GBK) and [https://github.com/apache/beam/pull/7360] (Reshuffle).",Don't attempt to fuse subtransforms of primitive/known transforms.,4,,['portability'],robertwb,True,ibzib,robertwb
beam,BEAM-6304,2018-12-26T09:00:18.000+0000,,2019-04-30T18:29:33.000+0000,,,New Feature,Major,,['Not applicable'],,,,,,,['io-java-elasticsearch'],['IO: Java: ElasticSearch'],"I use ElasticsearchIO to write my data to elasticSearch. However, the data is from other platform and not easy to check its validity. If we get the invalid data, we can ignore it( even though use batch insert, we can ignore all of them). So, I wish has a registered exception catch function to process it. From now on, I read the source code about write function in ProcessElement, it just throw the exception and cause my job to stop. 

I can catch   pipeline.run().waitUntilFinish() on direct runner and force it run again use while statement ungracefully. However, when it deploy to Flink, it will fail because Flink report exception that it cannot optimize the job.

If there is a method let user to decide how to process exception is required.",can ElasticsearchIO add a ExceptionHandlerFn ,3,1,,ben0123,True,echauchot,ben0123
beam,BEAM-6296,2018-12-21T19:21:52.000+0000,2018-12-27T14:46:55.000+0000,2018-12-27T14:46:55.000+0000,,Duplicate,New Feature,Major,['Not applicable'],['2.9.0'],,,,,,,['runner-spark'],['Runners: Spark runner'],"Hello, everyone,

It would be great to have a Python version of Spark runner available to Python. 

While we are happy of running Apache Beam on Dataflow, there are a few use cases that require different dependencies and OS env which makes it be more appropriate to run on a self-managed Spark cluster. With a spark runner for the python SDK, there will be an option to unify the language to define data pipelines.  

Would like to see the community's feedbacks of this feature.
",Support Python Spark Runner,2,,['python'],eddyxu,True,amitsela,eddyxu
beam,BEAM-6294,2018-12-21T16:42:44.000+0000,2019-01-12T00:56:56.000+0000,2019-02-09T23:43:02.000+0000,,Fixed,New Feature,Major,['2.10.0'],,8400,8400,,,,100,"['runner-flink', 'sdk-py-core']","['Runners: Flink runner', 'SDKs: Python']","Python needs to publish the URN over the FnAPI which is pretty easy, but Flink also needs to ensure that the composite structure does not get fused. Unlike with GBK, we can't assume all runners implement this as a primitive. ",Use Flink's redistribute for reshuffle.,4,,,robertwb,True,robertwb,robertwb
beam,BEAM-6269,2018-12-19T14:07:29.000+0000,2019-01-08T22:59:00.000+0000,2019-02-05T23:29:42.000+0000,,Later,New Feature,Major,['Not applicable'],,25200,25200,,,,100,"['beam-model', 'runner-core', 'sdk-java-core', 'sdk-py-core']","['Beam Model: general programming model concepts, semantics', 'Runners: shared functionality for all runners', 'SDKs: Java: pipeline construction, core transformations', 'SDKs: Python']",,Support mixed-language pipelines.,2,,,robertwb,True,kenn,robertwb
beam,BEAM-6265,2018-12-18T18:54:31.000+0000,2019-05-03T17:43:03.000+0000,2019-05-03T17:43:03.000+0000,,Fixed,New Feature,Major,['2.7.1'],,,,,,,,['java-fn-execution'],[''],"I recommend decoupling the MetricName from the MetricResult object which is used by MetricHttpSink and other MetricResult classes.

 

I was trying to enhance MetricName with a naming style more similar to MonitoringInfo, by allowing a MonitoringInfoMetricName subclass to define a name using a
 * urn - string
 * labels - <string,string> map

I encountered an issue when adding accessor methods to the base MetricName for getUrn and getLabels. The tests in MetricHttpSinkTest fail because serialized the JSON of the object using reflection via a call to ObjectMapper.writeValuesAsString.

To decouple this, a Wire format Pojo could be introduced (that does not use MetricName) to copy the metric onto first before invoking ObjectMapper.writeValuesAsString.

Then if new capabilities are added to internal MetricName, we won't break the MetricHttpSink, we can update it to include new fields as necessary.

 

Additionally, I will propose a new design for MetricResult soon, as it lacks labeling capabilities.

 

The current MetricResult only supports a single label getStep() at the moment.

Additionally, it is focused on a name+namespace style metric.

 ",Decouple Wire of MetricHttpSink from internal java Metric classes. i.e. MetricName,2,,,ajamato@google.com,True,ajamato@google.com,ajamato@google.com
beam,BEAM-6261,2018-12-18T17:14:58.000+0000,,2019-04-30T18:31:55.000+0000,,,New Feature,Major,,,,,,,,,['runner-dataflow'],['Runners: Google Cloud Dataflow runner'],"See [this user@ thread|https://lists.apache.org/thread.html/5eed0fc3beeb9f1c1fe4a623cbcad41cb15d0d80490cafb1f27e4577@%3Cuser.beam.apache.org%3E].

The [Slowly-changing lookup cache|https://cloud.google.com/blog/products/gcp/guide-to-common-cloud-dataflow-use-case-patterns-part-1] pattern described on the GCP blog uses a side input to feed lookup data to join with the main data input. However, the Dataflow runner doesn't update side inputs.

Example pipeline here: https://github.com/lbordwell/sideinput",Dataflow runner does not refresh updated side inputs,2,,,swegner,True,millsd@google.com,swegner
beam,BEAM-6260,2018-12-18T09:51:24.000+0000,,2018-12-19T20:34:43.000+0000,,,New Feature,Major,,,,,,,,,['sdk-go'],['SDKs: Go'],Do we have a plan to implement a KafkaIO for Golang SDK ?,Implement a Kafka IO for Golang SDK,2,,,ox66,True,,ox66
beam,BEAM-6255,2018-12-18T00:00:39.000+0000,,2019-01-05T00:24:42.000+0000,,,New Feature,Major,,,,,,,,,['sdk-go'],['SDKs: Go'],"When taking a sample or other filter, it consider supporting emitting values with an emit func from the accumulator rather than extract a single value. Otherwise general accumulators tend to go through awkward reflective code to construct the slice.

Either the single value return or an emitted return should be supported.",Support emit func output in the Go SDK's  CombineFn ExtractOutput,1,,,lostluck,True,,lostluck
beam,BEAM-6239,2018-12-15T02:08:46.000+0000,2019-01-08T23:01:40.000+0000,2019-01-31T03:41:19.000+0000,,Won't Fix,New Feature,Major,['Not applicable'],,10800,10800,,,,100,['testing-nexmark'],['Testing: Nexmark queries'],"We have BOUNDED_SIDE_INPUT_JOIN that just enriches a stream. Another use case is to sessionize first. I am curious about the different in perf, and how this plays out in SQL.",Nexmark benchmark for raw sessionization then stream enrichment,,,,kenn,True,kenn,kenn
beam,BEAM-6228,2018-12-13T16:57:17.000+0000,,2019-01-25T20:47:49.000+0000,,,New Feature,Minor,,,4200,4200,,,,100,['website'],['Website: content and bugs'],"The website build assumes a git environment, and will fail if built outside of git. As a result, we cannot build the website from the source release.

[~kenn] noticed this during 2.9.0 release validation. See: https://lists.apache.org/thread.html/dc816a5f8c82dd4d19e82666209305ec67d46440fe8edba4534f9b63@%3Cdev.beam.apache.org%3E

{code}
> Configure project :beam-website
No git repository found for :beam-website. Accessing grgit will cause an NPE.

FAILURE: Build failed with an exception.

* Where:
Build file 'website/build.gradle' line: 143

* What went wrong:
A problem occurred evaluating project ':beam-website'.
> Cannot get property 'branch' on null object
{code}",Website build from source release fails due to git dependency,,,,swegner,True,,swegner
beam,BEAM-6226,2018-12-13T00:54:51.000+0000,,2018-12-13T00:54:51.000+0000,,,New Feature,Major,,,,,,,,,['java-fn-execution'],[''],,Add a custom tool to allow custom logging to be added for failing calls made by the RetryHttpRequestInitalizer,1,,,ajamato@google.com,True,,ajamato@google.com
beam,BEAM-6191,2018-12-06T23:17:02.000+0000,2018-12-14T17:44:43.000+0000,2018-12-14T17:44:43.000+0000,,Fixed,New Feature,Minor,['2.10.0'],,3600,3600,,,,100,['runner-dataflow'],['Runners: Google Cloud Dataflow runner'],"The Dataflow runner harness has redundant error logging from a couple different components, which creates log spam and confusion when failures do occur. We should dedupe redundant logs.

From a typical user-code exception, we see at least 3 error logs from the worker:
http://screen/QZxsJOVnvt6

""Aborting operations""

""Uncaught exception occurred during work unit execution. This will be retried.""

""Failure processing work item""
",Redundant error messages for failures in Dataflow runner,1,,,swegner,True,swegner,swegner
beam,BEAM-6163,2018-12-01T02:51:14.000+0000,2018-12-01T15:29:19.000+0000,2018-12-03T10:01:10.000+0000,,Fixed,New Feature,Major,['2.10.0'],,4800,4800,,,,100,['sdk-py-harness'],['SDKs: Python: harness for executing UDFs over the Fn API'],"The current process script used to run process based sdk harness is os dependent and only works on linux.

Make the script work on mac",Support Process environment on Mac,1,,,angoenka,True,angoenka,angoenka
beam,BEAM-6161,2018-11-30T21:53:54.000+0000,2019-05-03T17:42:09.000+0000,2019-05-03T17:42:09.000+0000,,Fixed,New Feature,Major,['3.0.0'],,41400,41400,,,,100,"['java-fn-execution', 'sdk-java-harness']","['', 'SDKs: Java: harness for executing UDFs over the Fn API']",,Add ElementCount MonitoringInfos for the Java SDK,2,,,ajamato@google.com,True,ajamato@google.com,ajamato@google.com
beam,BEAM-6148,2018-11-29T15:46:20.000+0000,,2019-04-30T18:32:02.000+0000,,,New Feature,Major,,,1800,1800,,,,100,['sdk-go'],['SDKs: Go'],"There's no clear path to testing pipelines on runners other than the direct runner. It should be possibly to ""redirect"" tests to use a runner of choice. This would enable more ""testy"" ValidatesRunner tests in Go.

 

In particular, users should need to at least _ import the runner they want, and be able to set a flag.

The tricky bit is ensuring beam.Init is called so that each individual test can convert to WorkerMode when it's spun up as a SDK harness. This can be done by having a TestMain. 

ptest should provide convenience functions to help with this.","Support Go ""Unit"" tests on arbitrary runners",1,,,lostluck,True,lostluck,lostluck
beam,BEAM-6138,2018-11-27T22:17:48.000+0000,2019-05-03T17:41:48.000+0000,2019-05-22T18:44:28.000+0000,,Fixed,New Feature,Major,['3.0.0'],,59400,59400,,,,100,['java-fn-execution'],[''],,Add User Metric Support to Java SDK,2,,,ajamato@google.com,True,ajamato@google.com,ajamato@google.com
beam,BEAM-6133,2018-11-27T13:29:26.000+0000,2019-01-03T15:28:14.000+0000,2019-01-10T23:05:11.000+0000,,Fixed,New Feature,Major,['2.10.0'],,8400,8400,,,,100,['dsl-sql'],['DSLs: SQL'],"Now we support only ScalarFunction UDFs. In Calcite, there are other kinds of UDFs. With TableMacro UDFs users can connect external data sources in a similar way as in TableProvider, but without specifying a schema, or enumerating a list of existing tables in advance. 

An example use case is connecting external metadata service and querying range of partitions.

{code}
SELECT COUNT(*) FROM table(my_udf('dataset', start = '2017-01-01', end = '2018-01-01'))
{code}

Where the implementation of `my_udf` will connect to this service, get file locations for a range of partitions, and translate to PTransform reading it.",[SQL] Add support for TableMacro UDF,1,,,kanterov,True,kanterov,kanterov
beam,BEAM-6128,2018-11-26T21:54:28.000+0000,,2018-11-26T22:02:28.000+0000,,,New Feature,Major,,,,,,,,,['dsl-sql'],['DSLs: SQL'],"As Beam connects different sources, we should allow multi level naming for tables.
e.g. pubsub.topic, bigquery.dataset.name, hive.db.name, etc.",Support multi level in table naming,1,,,amaliujia,True,,amaliujia
beam,BEAM-6119,2018-11-23T06:11:00.000+0000,,2019-02-07T04:30:35.000+0000,,,New Feature,Major,,,,,,,,,['examples-python'],['Examples: Python examples'],,Support for custom Python UnboundedSource ,1,,,hroussel,True,,hroussel
beam,BEAM-6112,2018-11-21T23:09:30.000+0000,,2019-04-25T19:33:06.000+0000,,,New Feature,Major,,,,21600,,,,,['dsl-sql'],['DSLs: SQL'],"Currently a join such as {{CAST(A.x AS BIGINT) = B.y}} will fail, along with other similar simple expressions. We only support joining directly on immediate column references. It can be worked around by inserting a WITH clause for the intermediate value.","SQL could support equijoins on complex expressions, not just column refs",1,1,,kenn,True,,kenn
beam,BEAM-6109,2018-11-21T22:36:40.000+0000,2018-11-27T21:45:21.000+0000,2018-11-27T21:45:21.000+0000,,Fixed,New Feature,Major,['2.10.0'],,,,,,,,['dsl-sql'],['DSLs: SQL'],"Current, verification of supporting triggering and windowing is done before deciding whether to do a side input join. In the case of globally windowed data, this is too strict. We can relax it.","Beam SQL rejects join of unbounded to bounded, though a side input join does succeed",,,,kenn,True,kenn,kenn
beam,BEAM-6097,2018-11-20T05:29:18.000+0000,2019-01-09T15:56:53.000+0000,2019-01-16T09:50:54.000+0000,,Fixed,New Feature,Major,['Not applicable'],,26400,26400,,,,100,"['runner-ideas', 'website']","['Runners: ideas for new Beam runners', 'Website: content and bugs']",Add NemoRunner (http://nemo.apache.org),Add NemoRunner,3,,,wonook,True,wonook,wonook
beam,BEAM-6095,2018-11-19T20:55:37.000+0000,,2019-04-30T18:29:27.000+0000,,,New Feature,Major,,,,,,,,,['io-java-gcp'],"['IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)']","When using PubsubIO, it currently write all PubsubMessage to one single topic.

 

It should be nice if we could use it with some KV<String, PubsubMessage> in order to dispatch PubsubMessage over severals topics.

 

 ",PubsubIO Write to multiple topics,3,,,maxcleme,True,maxcleme,maxcleme
beam,BEAM-6081,2018-11-16T18:21:14.000+0000,,2019-03-05T21:53:50.000+0000,,,New Feature,Minor,,,8400,8400,,,,100,"['build-system', 'testing']","['Build, CI, release systems and processes', 'Testing: general infrastructure']","Our Jenkins infrastructure continuously runs many Dataflow jobs as part of pre- and post-commit tests. These are scheduled against our shared {{apache-beam-testing}} project, which has some amount of GCP quota for these jobs.

Some bugs can cause Dataflow jobs to get stuck and hang indefinitely. This causes many test jobs to stack up, which eats up our GCP quota and then causes all subsequent jobs to fail for quota issues. For an example, see [[BEAM-6080]|https://issues.apache.org/jira/browse/BEAM-6080].

We should harden the Dataflow runner and test framework to prevent Dataflow jobs getting stuck indefinitely, but in reality: bugs happen.

We should add some ""reaper"" process to periodically query for long-running jobs on our Dataflow project and cancel them. This would be fairly straight-forward using the [Dataflow REST API|https://cloud.google.com/dataflow/docs/reference/rest/], and scheduled on Jenkins.

If we build such a mechanism, we should also document the imposed policy (i.e. the threshold for ""long running jobs""), and perhaps some mechanism for opting out. For example, performance benchmarking jobs might be long-running by design.","Create ""Dataflow Reaper"" infrastructure to periodically clean up stuck Dataflow jobs",1,,,swegner,True,alanmyrvold,swegner
beam,BEAM-6071,2018-11-15T14:13:40.000+0000,,2019-02-07T04:46:31.000+0000,,,New Feature,Minor,,,,,,,,,['testing'],['Testing: general infrastructure'],"Now, that BEAM-6011 is resolved we have two types of NEXMark jobs:
 - periodic ones: they publish their results to ""nexmark"" BigQuery dataset
 - jobs triggered from PR comments: they publish their results to ""nexmark_PRs"" dataset

What is more, periodic jobs display plot their results [here|https://apache-beam-testing.appspot.com/explore?dashboard=5647201107705856], [here|https://apache-beam-testing.appspot.com/explore?dashboard=5099379773931520], [here|https://apache-beam-testing.appspot.com/explore?dashboard=5163657986048000] and [here|https://apache-beam-testing.appspot.com/explore?dashboard=5647201107705856].

+The goal:+

We could modify the plots to have datapoints from PR runs. Alternatively, we could create a separate dashboard for pr triggered jobs (which option is better?)

+Extra, optional goal (requires more work):+

Such plots could be embedded to GitHub PR comments so that the feedback is visible on Github directly after job runs.",Show Datapoints of PR triggered NEXMark jobs on NEXMark plots,1,,,ŁukaszG,True,,ŁukaszG
beam,BEAM-6017,2018-11-08T01:02:35.000+0000,2019-04-08T21:42:39.000+0000,2019-04-30T18:29:11.000+0000,,Won't Fix,New Feature,Major,['Not applicable'],,9000,9000,,,,100,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']",,Support nanosecond precision for DATETIME in Schema,1,,,amaliujia,True,amaliujia,amaliujia
beam,BEAM-6000,2018-11-06T20:34:05.000+0000,,2019-06-18T16:27:28.000+0000,,,New Feature,Major,,,,,,,,,['dsl-sql'],['DSLs: SQL'],"
Datetime type
Name	Description	Range
DATETIME	Represents a year, month, day, hour, minute, second, and subsecond.	0001-01-01 00:00:00 to 9999-12-31 23:59:59.999999.
A DATETIME represents a point in time. Each DATETIME contains the following:

year
month
day
hour
minute
second
subsecond
Unlike Timestamps, a DATETIME object does not refer to an absolute instance in time. Instead, it is the civil time, or the time that a user would see on a watch or calendar.

Canonical format
YYYY-[M]M-[D]D[( |T)[H]H:[M]M:[S]S[.DDDDDD]]
YYYY: Four-digit year
[M]M: One or two digit month
[D]D: One or two digit day
( |T): A space or a T separator
[H]H: One or two digit hour (valid values from 00 to 23)
[M]M: One or two digit minutes (valid values from 00 to 59)
[S]S: One or two digit seconds (valid values from 00 to 59)
[.DDDDDD]: Up to six fractional digits (i.e. up to microsecond precision)",Support DATETIME type in BeamSQL,1,,,amaliujia,True,,amaliujia
beam,BEAM-5980,2018-11-06T10:18:29.000+0000,,2019-06-12T08:53:14.000+0000,,,New Feature,Major,,,,353400,,,,,['testing'],['Testing: general infrastructure'],This involves adding a suite of load tests described in this proposal: [https://s.apache.org/load-test-basic-operations],Add load tests for Core Apache Beam opertaions ,1,,,ŁukaszG,True,ŁukaszG,ŁukaszG
beam,BEAM-5972,2018-11-05T17:42:09.000+0000,,2018-11-05T17:42:09.000+0000,,,New Feature,Major,,,,,,,,,['java-fn-execution'],[''],"I noticed that that two separate instances of the ApiServiceDescriptors exist in the instruction request.

This is used as a key to access the BeamFnDataGrpcMultiplexer, and new instances are created if a differing ApiServiceDescriptors is used to access it. 

The InstructionRequest .

I encountered this while trying to add some code in ProcessBundleHandler that needed to access the BeamFnDataGrpcMultiplexer and I used the ProcessBundle root ApiServiceDescriptor.

One usage is in the ProcessBundleDescriptor. The second comes from the RemoteGrpcPort. Which actually comes out of a serialized bytes field. See RemoteGrpcPortWrite::fromPTransform",Multiple ApiServiceDescriptors in InstructionRequest leading to unintentional multiple instances of objects in the Java SDK,1,,,ajamato@google.com,True,,ajamato@google.com
beam,BEAM-5964,2018-11-03T10:39:34.000+0000,2019-05-01T21:25:19.000+0000,2019-05-01T21:25:19.000+0000,,Fixed,New Feature,Major,['2.9.0'],,33600,33600,,,,100,['io-ideas'],"['IO: ideas (proposals for new connectors, new designs, etc)']","h3. Motivation

ClickHouse is open-source columnar DBMS for OLAP. It allows analysis of data that is updated in real time. The project was released as open-source software under the Apache 2 license in June 2016.

h3. Design and implementation
1. Do only writes, reads aren't useful because ClickHouse is designed for OLAP queries
2. For writes, do write in batches and rely on idempotent and atomic inserts supported by replicated tables in ClickHouse
3. Implement ClickHouseIO.Write as PTransform<PCollection<Row>, PDone>
4. Rely on having logic for casting rows between schemas in BEAM-5918, and don't put it in ClickHouseIO.Write

h3. References

[1] http://highscalability.com/blog/2017/9/18/evolution-of-data-structures-in-yandexmetrica.html
[2] https://blog.cloudflare.com/how-cloudflare-analyzes-1m-dns-queries-per-second/
[3] https://blog.cloudflare.com/http-analytics-for-6m-requests-per-second-using-clickhouse/",Add ClickHouseIO.Write,3,,,kanterov,True,kanterov,kanterov
beam,BEAM-5947,2018-11-02T18:05:33.000+0000,,2018-11-02T18:05:33.000+0000,,,New Feature,Major,,,,,,,,,['java-fn-execution'],[''],The BeamFnDataGrpcMultiplexer  close code is never invoked. We should make sure to invoke this on a shutdown code path.,BeamFnDataGrpcMultiplexer is never closed,1,,,ajamato@google.com,True,,ajamato@google.com
beam,BEAM-5852,2018-10-24T18:08:50.000+0000,2019-04-08T21:44:59.000+0000,2019-04-30T18:34:12.000+0000,,Fixed,New Feature,Major,['Not applicable'],,25200,25200,,,,100,['dsl-sql'],['DSLs: SQL'],We could add more functions to BeamSQL (as UDFs) to provide rich functionalities than standard/Calcite functions.,Function extension in BeamSQL,1,,,amaliujia,True,amaliujia,amaliujia
beam,BEAM-5851,2018-10-24T17:28:05.000+0000,2019-05-03T17:43:21.000+0000,2019-05-03T17:43:21.000+0000,,Fixed,New Feature,Major,['Not applicable'],,4800,4800,,,,100,['build-system'],"['Build, CI, release systems and processes']",,Update .gitignore file to remove IntelliJ generated gradle files,1,,,ajamato@google.com,True,ajamato@google.com,ajamato@google.com
beam,BEAM-5850,2018-10-24T17:20:40.000+0000,2019-05-03T17:42:30.000+0000,2019-05-03T17:42:30.000+0000,,Fixed,New Feature,Major,['3.0.0'],,58200,58200,,,,100,['java-fn-execution'],[''],Update BeamFnDataReceiver to place elements into a Queue and consumer then and call the element processing receiver in blockTillReadFinishes,"Make process, finish and start run on the same thread to support metrics.",2,,,ajamato@google.com,True,ajamato@google.com,ajamato@google.com
beam,BEAM-5819,2018-10-23T01:21:36.000+0000,,2019-04-30T18:31:00.000+0000,,,New Feature,Major,,,,114000,,,,,['build-system'],"['Build, CI, release systems and processes']","This is an umbrella ticket for the tasks that will lead to removing the shadow plugin from our main build, speeding our build, simplifying our IDE integration, and improving the readability of our code.",Vendor dependencies that are currently shaded,3,,,kenn,True,kenn,kenn
beam,BEAM-5817,2018-10-22T21:08:36.000+0000,2018-11-27T21:46:00.000+0000,2018-12-06T13:35:31.000+0000,,Fixed,New Feature,Major,['2.10.0'],,30600,41400,,,,100,['testing-nexmark'],['Testing: Nexmark queries'],"Nexmark is a convenient framework for testing the use case of large scale stream enrichment. One way is joining a stream to files, and it can be tested via any source that Nexmark supports.",Nexmark test of joining stream to files,1,,,kenn,True,kenn,kenn
beam,BEAM-5806,2018-10-21T19:20:17.000+0000,,2019-06-20T16:01:33.000+0000,,,New Feature,Minor,,,14400,14400,72000,86400,72000,16,['io-java-gcp'],"['IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)']","By using the PubsubIO to read from or write to Pub/Sub we are obliged to use the PubsubJsonClient to interact with the Pub/Sub API.

This PubsubJsonClient encode the message in base 64 and increase the size of this one by 30% and there is no way to change the PubsubClient used by PubsubIO.

 

What I suggest is to allow developper to change the PubsubClientFactory by specifying it at the definition-time like the following:

{{^PubsubIO.Read<String> read = PubsubIO.readStrings()
          .fromTopic(StaticValueProvider.of(topic))^}}
          .withTimestampAttribute(""myTimestamp"")^}}
          .withIdAttribute(""myId"")^}}
          *.withClientFactory(PubsubGrpcClient.FACTORY)*;^}}

 ",Allow to change the PubsubClientFactory when using PubsubIO,2,,,lhauspie,True,,lhauspie
beam,BEAM-5798,2018-10-19T21:52:40.000+0000,2019-01-08T14:38:54.000+0000,2019-01-16T09:42:05.000+0000,,Fixed,New Feature,Major,['2.10.0'],,20400,20400,,,,100,['io-java-kafka'],['IO: Java: Kafka'],"Add support for writing to Kafka based upon contents of the data. This is similar to the dynamic destination approach for file IO and other sinks.

 

Source of request: https://lists.apache.org/thread.html/a89d1d32ecdb50c42271e805cc01a651ee3623b4df97c39baf4f2053@%3Cuser.beam.apache.org%3E",Add support to write to multiple topics with KafkaIO,4,1,"['newbie', 'starter']",lcwik,True,aromanenko,lcwik
beam,BEAM-5757,2018-10-16T05:53:56.000+0000,,2019-04-30T18:33:38.000+0000,,,New Feature,Major,,['2.7.0'],,,,,,,['io-java-elasticsearch'],['IO: Java: ElasticSearch'],"Hello beam maintainers,

I am trying to delete some index from Elasticsearch using beam pipeline. I couldn't find and delete function in ElasticsearchIO. It would be nice to have and would make sense for cleaning up indexes overtime.

 I also checked some other IO classes and they also don't support delete. Not sure if beam has some policy against supporting delete. 

 Please guide me on this. I am willing to create pull request if this feature makes sense to project contributors.",Elasticsearch IO provide delete function,2,,,praddy,True,echauchot,praddy
beam,BEAM-5654,2018-10-04T19:52:21.000+0000,,2019-02-07T04:30:18.000+0000,,,New Feature,Major,,,,,,,,,['sdk-py-core'],['SDKs: Python'],"We save Dataflow pipelines hitting a key error when hitting this line. This is likely due to a race condition between the key existence check and accessing it.

[https://github.com/apache/beam/blob/v2.6.0/sdks/python/apache_beam/runners/worker/sdk_worker.py#L189]

One fix would be to do a single lookup for the value instead of the double lookup.",Race condition in python sdk_worker causing KeyError,1,,,ajamato@google.com,True,,ajamato@google.com
beam,BEAM-5644,2018-10-04T04:44:51.000+0000,2019-05-13T22:08:06.000+0000,2019-05-13T22:08:06.000+0000,,Fixed,New Feature,Major,['Not applicable'],,43200,43200,,,,100,['dsl-sql'],['DSLs: SQL'],"We can make planner configurable here: [BeamQueryPlanner.java#L145|https://github.com/apache/beam/blob/master/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/BeamQueryPlanner.java#L145]

 

By doing so, we can have different planner implementation to support different SQL dialect.",make Planner configurable ,1,,,amaliujia,True,amaliujia,amaliujia
beam,BEAM-5635,2018-10-03T23:15:48.000+0000,,2019-01-10T16:30:26.000+0000,,,New Feature,Major,,,,,,,,,['io-java-gcp'],"['IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)']",I have seen a user who would like to use Datastore Transactions to rollback a set of records if one of them fails to write. Let's consider this use case for DatastoreIO,FR: Enable Transactional writes with DatastoreIO,1,,,ajamato@google.com,True,,ajamato@google.com
beam,BEAM-5605,2018-10-01T23:02:00.000+0000,,2019-07-08T20:30:41.000+0000,,,New Feature,Major,,,4200,4200,,,,100,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']",Roll-up item tracking work towards supporting portable SplittableDoFn for batch,Support Portable SplittableDoFn for batch,,,['portability'],swegner,True,lcwik,swegner
beam,BEAM-5604,2018-10-01T22:58:22.000+0000,,2019-04-30T18:30:35.000+0000,,,New Feature,Major,,,,,,,,,['runner-dataflow'],['Runners: Google Cloud Dataflow runner'],,Dataflow runner should support splitting bounded bundles for SplittableDoFn ,,,['portability'],swegner,True,,swegner
beam,BEAM-5602,2018-10-01T22:57:39.000+0000,,2019-04-30T18:32:30.000+0000,,,New Feature,Major,,,,,,,,,['runner-dataflow'],['Runners: Google Cloud Dataflow runner'],,Dataflow runner should support reporting progress for bounded SplittableDoFn,,,['portability'],swegner,True,,swegner
beam,BEAM-5601,2018-10-01T22:56:47.000+0000,,2019-04-30T18:33:47.000+0000,,,New Feature,Major,,,,,,,,,['runner-dataflow'],['Runners: Google Cloud Dataflow runner'],,Dataflow runner should support custom windowfn for portability,1,1,['portability'],swegner,True,,swegner
beam,BEAM-5600,2018-10-01T22:55:56.000+0000,,2019-04-30T18:31:33.000+0000,,,New Feature,Major,,,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']",,Splitting for SplittableDoFn should be exposed within runner shared libraries,,,['portability'],swegner,True,,swegner
beam,BEAM-5599,2018-10-01T22:49:14.000+0000,,2019-04-30T18:29:52.000+0000,,,New Feature,Major,,,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']",,Convert all Java BoundedSources into SplittableDoFns,1,,['portability'],swegner,True,,swegner
beam,BEAM-5596,2018-10-01T20:12:57.000+0000,2018-10-02T23:22:29.000+0000,2018-10-04T05:37:45.000+0000,,Not A Problem,New Feature,Major,['Not applicable'],,,,,,,,['dsl-sql'],['DSLs: SQL'],Build BeamSQL as a jar which does not include any extra dependencies. Therefore we can use this jar to use BeamSQL as a lib.,Build BeamSQL as a Library,1,,,amaliujia,True,amaliujia,amaliujia
beam,BEAM-5504,2018-09-25T20:16:37.000+0000,,2018-10-10T20:37:35.000+0000,,,New Feature,Major,,,,,,,,,['dsl-sql'],['DSLs: SQL'],,Pubsub Write by BeamSQL,1,,,amaliujia,True,,amaliujia
beam,BEAM-5492,2018-09-25T01:35:01.000+0000,,2019-02-07T04:46:17.000+0000,,,New Feature,Major,,,,,,,,,['testing'],['Testing: general infrastructure'],"I'm looking at a flake in a postcommit suite that happened last week in Python Dataflow integration test (https://issues.apache.org/jira/browse/BEAM-5415), and the existing logs are not sufficient to debug the problem. The test suite failed, however: 
1. Test Result section in Jenkins postcommit only includes unit tests, and does not include any integration tests.
2. The only available information about the failure is buried in the Full Console Log and is not very informative (copy-pasted below). 
3. There is a line in the console log pointing to: ""XML: /home/jenkins/jenkins-slave/workspace/beam_PostCommit_Python_Verify/src/sdks/python/nosetests.xml"", however as a non-committer I don't have access to Jenkins machines.

This makes rootcausing integration tests failures quite difficult, since we cannot even reliably identify an ID of a failed Dataflow job from the test output.

It would be helpful to capture and persist in Jenkins the invocation of integration test, and all the console output that SDK produces during test execution.

Sample available logs from console output.

13:41:54 test_leader_board_it (apache_beam.examples.complete.game.leader_board_it_test.LeaderBoardIT) ... ERROR
13:41:54 
13:41:54 ======================================================================
13:41:54 ERROR: test_leader_board_it (apache_beam.examples.complete.game.leader_board_it_test.LeaderBoardIT)
13:41:54 ----------------------------------------------------------------------
13:41:54 Traceback (most recent call last):
13:41:54   File ""/home/jenkins/jenkins-slave/workspace/beam_PostCommit_Python_Verify/src/sdks/python/build/gradleenv/local/lib/python2.7/site-packages/nose/plugins/multiprocess.py"", line 812, in run
13:41:54     test(orig)
13:41:54   File ""/home/jenkins/jenkins-slave/workspace/beam_PostCommit_Python_Verify/src/sdks/python/build/gradleenv/local/lib/python2.7/site-packages/nose/case.py"", line 45, in __call__
13:41:54     return self.run(*arg, **kwarg)
13:41:54   File ""/home/jenkins/jenkins-slave/workspace/beam_PostCommit_Python_Verify/src/sdks/python/build/gradleenv/local/lib/python2.7/site-packages/nose/case.py"", line 133, in run
13:41:54     self.runTest(result)
13:41:54   File ""/home/jenkins/jenkins-slave/workspace/beam_PostCommit_Python_Verify/src/sdks/python/build/gradleenv/local/lib/python2.7/site-packages/nose/case.py"", line 151, in runTest
13:41:54     test(result)
13:41:54   File ""/usr/lib/python2.7/unittest/case.py"", line 393, in __call__
13:41:54     return self.run(*args, **kwds)
13:41:54   File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
13:41:54     testMethod()
13:41:54   File ""/home/jenkins/jenkins-slave/workspace/beam_PostCommit_Python_Verify/src/sdks/python/apache_beam/examples/complete/game/leader_board_it_test.py"", line 161, in test_leader_board_it
13:41:54     self.test_pipeline.get_full_options_as_args(**extra_opts))
13:41:54   File ""/home/jenkins/jenkins-slave/workspace/beam_PostCommit_Python_Verify/src/sdks/python/apache_beam/examples/complete/game/leader_board.py"", line 345, in run
13:41:54     'total_score': 'INTEGER',
13:41:54   File ""/home/jenkins/jenkins-slave/workspace/beam_PostCommit_Python_Verify/src/sdks/python/apache_beam/pipeline.py"", line 414, in __exit__
13:41:54     self.run().wait_until_finish()
13:41:54   File ""/home/jenkins/jenkins-slave/workspace/beam_PostCommit_Python_Verify/src/sdks/python/apache_beam/pipeline.py"", line 394, in run
13:41:54     self.to_runner_api(), self.runner, self._options).run(False)
13:41:54   File ""/home/jenkins/jenkins-slave/workspace/beam_PostCommit_Python_Verify/src/sdks/python/apache_beam/pipeline.py"", line 407, in run
13:41:54     return self.runner.run_pipeline(self)
13:41:54   File ""/home/jenkins/jenkins-slave/workspace/beam_PostCommit_Python_Verify/src/sdks/python/apache_beam/runners/dataflow/test_dataflow_runner.py"", line 68, in run_pipeline
13:41:54     self.result.cancel()
13:41:54   File ""/home/jenkins/jenkins-slave/workspace/beam_PostCommit_Python_Verify/src/sdks/python/apache_beam/runners/dataflow/dataflow_runner.py"", line 1167, in cancel
13:41:54     self._update_job()
13:41:54   File ""/home/jenkins/jenkins-slave/workspace/beam_PostCommit_Python_Verify/src/sdks/python/apache_beam/runners/dataflow/dataflow_runner.py"", line 1078, in _update_job
13:41:54     self._job = self._runner.dataflow_client.get_job(self.job_id())
13:41:54   File ""/home/jenkins/jenkins-slave/workspace/beam_PostCommit_Python_Verify/src/sdks/python/apache_beam/utils/retry.py"", line 184, in wrapper
13:41:54     return fun(*args, **kwargs)
13:41:54   File ""/home/jenkins/jenkins-slave/workspace/beam_PostCommit_Python_Verify/src/sdks/python/apache_beam/runners/dataflow/internal/apiclient.py"", line 629, in get_job
13:41:54     response = self._client.projects_locations_jobs.Get(request)
13:41:54   File ""/home/jenkins/jenkins-slave/workspace/beam_PostCommit_Python_Verify/src/sdks/python/apache_beam/runners/dataflow/internal/clients/dataflow/dataflow_v1b3_client.py"", line 659, in Get
13:41:54     config, request, global_params=global_params)
13:41:54   File ""/home/jenkins/jenkins-slave/workspace/beam_PostCommit_Python_Verify/src/sdks/python/build/gradleenv/local/lib/python2.7/site-packages/apitools/base/py/base_api.py"", line 720, in _RunMethod
13:41:54     http, http_request, **opts)
13:41:54   File ""/home/jenkins/jenkins-slave/workspace/beam_PostCommit_Python_Verify/src/sdks/python/build/gradleenv/local/lib/python2.7/site-packages/apitools/base/py/http_wrapper.py"", line 346, in MakeRequest
13:41:54     check_response_func=check_response_func)
13:41:54   File ""/home/jenkins/jenkins-slave/workspace/beam_PostCommit_Python_Verify/src/sdks/python/build/gradleenv/local/lib/python2.7/site-packages/apitools/base/py/http_wrapper.py"", line 396, in _MakeRequestNoRetry
13:41:54     redirections=redirections, connection_type=connection_type)
13:41:54   File ""/home/jenkins/jenkins-slave/workspace/beam_PostCommit_Python_Verify/src/sdks/python/build/gradleenv/local/lib/python2.7/site-packages/oauth2client/transport.py"", line 193, in new_request
13:41:54     redirections, connection_type)
13:41:54   File ""/home/jenkins/jenkins-slave/workspace/beam_PostCommit_Python_Verify/src/sdks/python/build/gradleenv/local/lib/python2.7/site-packages/oauth2client/transport.py"", line 282, in request
13:41:54     connection_type=connection_type)
13:41:54   File ""/home/jenkins/jenkins-slave/workspace/beam_PostCommit_Python_Verify/src/sdks/python/build/gradleenv/local/lib/python2.7/site-packages/httplib2/__init__.py"", line 1694, in request
13:41:54     (response, content) = self._request(conn, authority, uri, request_uri, method, body, headers, redirections, cachekey)
13:41:54   File ""/home/jenkins/jenkins-slave/workspace/beam_PostCommit_Python_Verify/src/sdks/python/build/gradleenv/local/lib/python2.7/site-packages/httplib2/__init__.py"", line 1434, in _request
13:41:54     (response, content) = self._conn_request(conn, request_uri, method, body, headers)
13:41:54   File ""/home/jenkins/jenkins-slave/workspace/beam_PostCommit_Python_Verify/src/sdks/python/build/gradleenv/local/lib/python2.7/site-packages/httplib2/__init__.py"", line 1390, in _conn_request
13:41:54     response = conn.getresponse()
13:41:54   File ""/usr/lib/python2.7/httplib.py"", line 1136, in getresponse
13:41:54     response.begin()
13:41:54   File ""/usr/lib/python2.7/httplib.py"", line 453, in begin
13:41:54     version, status, reason = self._read_status()
13:41:54   File ""/usr/lib/python2.7/httplib.py"", line 409, in _read_status
13:41:54     line = self.fp.readline(_MAXLINE + 1)
13:41:54   File ""/usr/lib/python2.7/socket.py"", line 480, in readline
13:41:54     data = self._sock.recv(self._rbufsize)
13:41:54   File ""/usr/lib/python2.7/ssl.py"", line 756, in recv
13:41:54     return self.read(buflen)
13:41:54   File ""/usr/lib/python2.7/ssl.py"", line 643, in read
13:41:54     v = self._sslobj.read(len)
13:41:54   File ""/home/jenkins/jenkins-slave/workspace/beam_PostCommit_Python_Verify/src/sdks/python/build/gradleenv/local/lib/python2.7/site-packages/nose/plugins/multiprocess.py"", line 276, in signalhandler
13:41:54     raise TimedOutException()
13:41:54 TimedOutException: 'test_leader_board_it (apache_beam.examples.complete.game.leader_board_it_test.LeaderBoardIT)'
13:41:54 
13:41:54 ----------------------------------------------------------------------
13:41:54 XML: /home/jenkins/jenkins-slave/workspace/beam_PostCommit_Python_Verify/src/sdks/python/nosetests.xml
13:41:54 ----------------------------------------------------------------------
13:41:54 Ran 14 tests in 3001.679s
13:41:54 
13:41:54 FAILED (errors=1)
13:41:54 


",Python Dataflow integration tests should export the pipeline console output to Jenkins Test Result section,2,,,tvalentyn,True,,tvalentyn
beam,BEAM-5441,2018-09-20T13:59:56.000+0000,2018-10-02T16:06:15.000+0000,2018-10-02T16:06:15.000+0000,,Fixed,New Feature,Major,['2.8.0'],['2.8.0'],10800,10800,,,,100,"['examples-python', 'sdk-java-core']","['Examples: Python examples', 'SDKs: Java: pipeline construction, core transformations']","The Python SDK wordcount with the PortableRunner throws the following exception:

{noformat}
java.lang.IllegalArgumentException: A PCollectionNode should have exactly one producing PTransformNode, PCollectionNode{id=ref_PCollection_PCollection_26, PCollection=unique_name: ""60write/Write/WriteImpl/DoOnce/Read/Reshuffle/RemoveRandomKeys.None""
coder_id: ""ref_Coder_FastPrimitivesCoder_2""
is_bounded: BOUNDED
windowing_strategy_id: ""ref_Windowing_Windowing_1""
} has [PTransformNode{id=ref_AppliedPTransform_write/Write/WriteImpl/DoOnce/Read/Reshuffle_32, transform=spec {
  urn: ""beam:transform:generic_composite:v1""
  payload: ""<Reshuffle(PTransform) label=[Reshuffle]>""
}
subtransforms: ""ref_AppliedPTransform_write/Write/WriteImpl/DoOnce/Read/Reshuffle/AddRandomKeys_33""
subtransforms: ""ref_AppliedPTransform_write/Write/WriteImpl/DoOnce/Read/Reshuffle/ReshufflePerKey_34""
subtransforms: ""ref_AppliedPTransform_write/Write/WriteImpl/DoOnce/Read/Reshuffle/RemoveRandomKeys_41""
inputs {
  key: ""0""
  value: ""ref_PCollection_PCollection_19""
}
outputs {
  key: ""None""
  value: ""ref_PCollection_PCollection_26""
}
unique_name: ""write/Write/WriteImpl/DoOnce/Read/Reshuffle""
}, PTransformNode{id=ref_AppliedPTransform_write/Write/WriteImpl/DoOnce/Read/Reshuffle/RemoveRandomKeys_41, transform=spec {
  urn: ""urn:beam:transform:pardo:v1""
  payload: ""\n\317\006\n\255\006\n beam:dofn:pickled_python_info:v1\032\210\006eNrFkttP1EAUxtsFZe1yUcEL4v3aRbdFUEFFQEHUEGpSnPhkxml32Gnstv3a6YZNbKIxJf7ZTvdB2ER9NZNMcs6cb3LO73zfaqbPEuYLTj3OupZMWZTtx2k3s/w45cYmC0PmhfxjypKEp1vxdmRAa36HXqBmkrEkjX2eZRjx20EYWrS6DeqnnElO9/PIl0GsFKPm0HsYszaV/YQbOEHqm3Gbf1ABTpYYc1E3d3R1arvTG2Tip6Z91bQfutbRtT2cckoYTaIfoFFinPRtkvE0s7vswN7iPbuaoCV5Ju0ej3p2GHh20pcijhatZTsLJG+pSb+wDs/sYzO3Fq0Va8Fq895CK+mrUot3OscL7CModgXFSvqYIPXVkHW9NlvD5G5jlGiYIrX9CKdLnGlKnHUx7VPq5UEog4hSo8MlkzI1MDNEIugmcSppN27noaJxjsz9Yxs4X+KCi4ukTpXcl5Ri9hCXXMyJSedPC/C5CnBZjJriN9W9z6SukLZ1bXYPV5wd/RBXFVKJayWu/w+kuQzCCukNMbm7XhNTTYXvpotbYkb8HUclwu0Sd1zcFQrCPRemguAUaJLGwFpUBJHMMD9sb/UwyKveFFEm4zQz3r2v3Pe2Shu4r7z9oECrgGWSRhAluRx8l8F2yHicy6PEgpMf4qGXSSy6WCrxyMXjEk8KLJtiXlQfrRR4WuCZKWxHDKqfe6o7lnayhPtUuWVVLOUSL1ysDXe9PpBvFHhZ4NWRfNMjI5VsS6zl3ie8LrDtOBJvrF+Bv0km\022\035ref_Environment_Environment_1""
}
inputs {
  key: ""0""
  value: ""ref_PCollection_PCollection_25""
}
outputs {
  key: ""None""
  value: ""ref_PCollection_PCollection_26""
}
unique_name: ""write/Write/WriteImpl/DoOnce/Read/Reshuffle/RemoveRandomKeys""
}]
        at org.apache.beam.repackaged.beam_runners_core_construction_java.com.google.common.base.Preconditions.checkArgument(Preconditions.java:416)
        at org.apache.beam.runners.core.construction.graph.QueryablePipeline.buildNetwork(QueryablePipeline.java:176)
        at org.apache.beam.runners.core.construction.graph.QueryablePipeline.<init>(QueryablePipeline.java:119)
        at org.apache.beam.runners.core.construction.graph.QueryablePipeline.forPrimitivesIn(QueryablePipeline.java:82)
        at org.apache.beam.runners.core.construction.graph.GreedyPipelineFuser.<init>(GreedyPipelineFuser.java:67)
        at org.apache.beam.runners.core.construction.graph.GreedyPipelineFuser.fuse(GreedyPipelineFuser.java:89)
        at org.apache.beam.runners.flink.FlinkJobInvocation.runPipeline(FlinkJobInvocation.java:96)
        at org.apache.beam.repackaged.beam_runners_flink_2.11.com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:111)
        at org.apache.beam.repackaged.beam_runners_flink_2.11.com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:58)
        at org.apache.beam.repackaged.beam_runners_flink_2.11.com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:75)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
{noformat}

Looks like it was caused by https://github.com/apache/beam/pull/6328",Portable Wordcount fails in GreedyPipelineFuser,3,,"['portability', 'portability-flink']",mxm,True,rdub,mxm
beam,BEAM-5440,2018-09-20T13:30:28.000+0000,,2019-07-13T15:29:21.000+0000,,,New Feature,Major,,,9000,9000,,,,100,"['java-fn-execution', 'sdk-java-core']","['', 'SDKs: Java: pipeline construction, core transformations']","While experimenting with the Python SDK locally, I found it inconvenient that I can't mount a host directory to the Docker containers, i.e. the input must already be in the container and the results of a Write remain inside the container. For local testing, users may want to mount a host directory.

Since BEAM-5288 the {{Environment}} carries explicit environment information, we could a) add volume args to the {{DockerPayload}}, or b) provide a general Docker arguments field.",Add option to mount a directory inside SDK harness containers,7,1,"['portability', 'portability-flink']",mxm,True,,mxm
beam,BEAM-5431,2018-09-20T01:04:45.000+0000,,2019-02-07T04:30:31.000+0000,,,New Feature,Major,,,,,,,,,['sdk-py-core'],['SDKs: Python'],"I'd like to propose a new high-level transform ""StarMap"" for the Python SDK. The transform would be syntactic sugar for ParDo like Map, but would would automatically unpack arguments like [itertools.starmap|https://docs.python.org/3/library/itertools.html#itertools.starmap] from Python's standard library.

The use-case is to handle applying functions to tuples of arguments, which is a common pattern when using Beam's combine and group-by transforms. Right now, it's common to write functions with manual unpacking, e.g., 
{code:java}
def my_func(inputs):
  key, value = inputs
  ...

beam.Map(my_func) {code}
StarMap offers a much more readable alternative: 
{code:java}
def my_func(key, value):
  ...

beam.StarMap(my_func){code}
 

The need for StarMap is especially pressing with the advent of Python 3 support and the eventual wind-down of Python 2. Currently, it's common to achieve this pattern using unpacking in a function definition, e.g., beam.Map(lambda (k, v): my_func(k, v)), but this is invalid syntax in Python 3. My internal search of Google's codebase turns up quite a few matches for ""beam\.Map(lambda\ ("", none of which would work on Python 3.

 ",StarMap transform for Python SDK,4,1,,shoyer,True,,shoyer
beam,BEAM-5419,2018-09-18T11:08:48.000+0000,2018-12-12T20:52:58.000+0000,2018-12-17T18:43:53.000+0000,,Fixed,New Feature,Major,['2.10.0'],,14400,14400,,,,100,"['build-system', 'runner-flink']","['Build, CI, release systems and processes', 'Runners: Flink runner']","Following up on a discussion on the mailing list.

We want to keep the Flink version stable across different versions to avoid upgrade pain for long-term users. At the same time, there are users out there with newer Flink clusters and developers also want to utilize new Flink features.

It would be great to build multiple versions of the Flink Runner against different Flink versions.

When the upgrade is as simple as changing the version property in the build script, this should be pretty straight-forward. If not, having a ""base version"" and applying a patch during the build could be an option. We should avoid duplicating any Runner code.",Build multiple versions of the Flink Runner against different Flink versions,9,2,,mxm,True,mxm,mxm
beam,BEAM-5363,2018-09-11T18:18:58.000+0000,,2019-04-30T18:30:58.000+0000,,,New Feature,Minor,,,,,,,,,['runner-direct'],['Runners: Direct runner and ULR for single machine testing and development'],[https://docs.google.com/document/d/117yRKbbcEdm3eIKB_26BHOJGmHSZl1YNoF0RqWGtqAM],Java DirectRunner should implement retry and support @RequiresStableInput,1,,,robinyqiu,True,danoliveira,robinyqiu
beam,BEAM-5362,2018-09-11T18:08:35.000+0000,,2019-04-30T18:33:17.000+0000,,,New Feature,Minor,,,,,,,,,['runner-samza'],['Runners: Samza'],[https://docs.google.com/document/d/117yRKbbcEdm3eIKB_26BHOJGmHSZl1YNoF0RqWGtqAM],Support @RequiresStableInput on Samza runner,1,,,robinyqiu,True,xinyu,robinyqiu
beam,BEAM-5361,2018-09-11T18:07:36.000+0000,,2019-04-30T18:30:11.000+0000,,,New Feature,Minor,,,,,,,,,['runner-gearpump'],['Runners: Gearpump runner'],[https://docs.google.com/document/d/117yRKbbcEdm3eIKB_26BHOJGmHSZl1YNoF0RqWGtqAM],Support @RequiresStableInput on Gearpump runner,1,,,robinyqiu,True,mauzhang,robinyqiu
beam,BEAM-5360,2018-09-11T18:06:52.000+0000,,2018-09-11T18:10:10.000+0000,,,New Feature,Minor,,,,,,,,,['runner-apex'],['Runners: Apex runner'],[https://docs.google.com/document/d/117yRKbbcEdm3eIKB_26BHOJGmHSZl1YNoF0RqWGtqAM],Support @RequiresStableInput on Apex runner,1,,,robinyqiu,True,,robinyqiu
beam,BEAM-5359,2018-09-11T18:05:13.000+0000,2019-03-18T11:29:02.000+0000,2019-04-30T18:33:15.000+0000,,Fixed,New Feature,Minor,['2.12.0'],,,,,,,,['runner-flink'],['Runners: Flink runner'],[https://docs.google.com/document/d/117yRKbbcEdm3eIKB_26BHOJGmHSZl1YNoF0RqWGtqAM],Support @RequiresStableInput on Flink runner,3,,,robinyqiu,True,aljoscha,robinyqiu
beam,BEAM-5358,2018-09-11T18:04:05.000+0000,,2019-04-30T18:34:16.000+0000,,,New Feature,Minor,,,,,,,,,['runner-spark'],['Runners: Spark runner'],[https://docs.google.com/document/d/117yRKbbcEdm3eIKB_26BHOJGmHSZl1YNoF0RqWGtqAM],Support @RequiresStableInput on Spark runner,1,,,robinyqiu,True,amitsela,robinyqiu
beam,BEAM-5325,2018-09-05T15:49:51.000+0000,,2018-09-05T15:50:34.000+0000,,,New Feature,Minor,,,,,,,,,['sdk-py-core'],['SDKs: Python'],"Filing to follow up on the discussion in: [https://github.com/apache/beam/pull/6266]

We could convert gen_protos to tox task and better control its environment  (grpcio, future dependencies).
  

cc: [~altay]  [~RobbeSneyders]",Convert gen_protos to tox task.,1,,,tvalentyn,True,,tvalentyn
beam,BEAM-5312,2018-09-05T13:57:10.000+0000,,2018-09-05T16:22:32.000+0000,,,New Feature,Minor,,,,,,,,,['sdk-go'],['SDKs: Go'],Please support flag --serviceAccount as documented in https://cloud.google.com/dataflow/security-and-permissions#controller-service-account,Dataflow: specifying a user-managed controller service account,1,,,exm,True,,exm
beam,BEAM-5288,2018-09-03T10:29:39.000+0000,2018-10-03T00:35:07.000+0000,2018-10-24T23:29:06.000+0000,,Fixed,New Feature,Major,['2.8.0'],,61800,61800,,,,100,['beam-model'],"['Beam Model: general programming model concepts, semantics']","As of mailing discussions and BEAM-5187, it has become clear that we need to extend the Environment information. In addition to the Docker environment, the extended environment holds deployment options for 1) a process-based environment, 2) an externally managed environment.

The proto definition, as of now, looks as follows:
{noformat}
 message Environment {

   // (Required) The URN of the payload
   string urn = 1;

   // (Optional) The data specifying any parameters to the URN. If
   // the URN does not require any arguments, this may be omitted.
   bytes payload = 2;
 }

 message StandardEnvironments {
   enum Environments {
     DOCKER = 0 [(beam_urn) = ""beam:env:docker:v1""];

     PROCESS = 1 [(beam_urn) = ""beam:env:process:v1""];

     EXTERNAL = 2 [(beam_urn) = ""beam:env:external:v1""];
   }
 }

 // The payload of a Docker image
 message DockerPayload {
   string container_image = 1;  // implicitly linux_amd64.
 }

 message ProcessPayload {
   string os = 1;  // ""linux"", ""darwin"", ..
   string arch = 2;  // ""amd64"", ..
   string command = 3; // process to execute
   map<string, string> env = 4; // environment variables
 }
{noformat}",Modify Environment to support non-dockerized SDK harness deployments ,5,,,mxm,True,angoenka,mxm
beam,BEAM-5239,2018-08-27T12:56:30.000+0000,2018-09-11T16:18:19.000+0000,2018-09-18T18:55:40.000+0000,,Fixed,New Feature,Major,['2.7.0'],['2.6.0'],13200,13200,,,,100,['runner-flink'],['Runners: Flink runner'],"Because of FLINK-10226, we need to be able to set latencyTrackingConfiguration for flink via FlinkPipelineOptions",Allow configure latencyTrackingInterval,3,,,JozoVilcek,True,JozoVilcek,JozoVilcek
beam,BEAM-5207,2018-08-23T16:31:13.000+0000,2018-08-27T16:11:06.000+0000,2018-08-27T16:11:06.000+0000,,Duplicate,New Feature,Major,['Not applicable'],,,,,,,,['build-system'],"['Build, CI, release systems and processes']","Build failed without much in the way of errors, it appears to be some sort of race condition. https://builds.apache.org/job/beam_PostCommit_Java_GradleBuild/1321

 
All input files are considered out-of-date for incremental task ':beam-runners-apex:compileTestJava'",:beam-runners-apex:compileTestJava dependency issue,2,,,apilloud,True,lcwik,apilloud
beam,BEAM-5187,2018-08-21T09:46:03.000+0000,2018-10-03T10:36:55.000+0000,2018-10-03T17:39:04.000+0000,,Implemented,New Feature,Minor,['2.7.0'],,24000,24000,,,,100,['runner-core'],['Runners: shared functionality for all runners'],"As discussed on the mailing list [1], we want to giver users an option to execute portable pipelines without Docker. Analog to the {{DockerJobBundleFactory}}, a {{ProcessJobBundleFactory}} could be added to directly fork SDK harness processes.

Artifacts will be provided by an artifact directory or could be setup similar to the existing bootstrapping code (""boot.go"") which we use for containers.

The process-based execution can optionally be configured via the pipeline options.

[1] [https://lists.apache.org/thread.html/d8b81e9f74f77d74c8b883cda80fa48efdcaf6ac2ad313c4fe68795a@%3Cdev.beam.apache.org%3E]",Create a ProcessJobBundleFactory for non-dockerized SDK harness,7,1,,mxm,True,mxm,mxm
beam,BEAM-5167,2018-08-18T08:29:33.000+0000,,2019-04-30T18:34:10.000+0000,,,New Feature,Major,,,7200,7200,,,,100,['runner-flink'],['Runners: Flink runner'],"Based on the discussion [https://lists.apache.org/thread.html/0cbf73d696e0d3a5bb8e93618ac9d6bb81daecf2c9c8e11ee220c8ae@%3Cdev.beam.apache.org%3E]

Use SDK Harness concurrency information in Flink runner to schedule bundles.",Use concurrency information from SDK Harness in Flink Portable Runner,5,,,angoenka,True,angoenka,angoenka
beam,BEAM-5166,2018-08-18T08:26:38.000+0000,,2019-04-30T18:32:57.000+0000,,,New Feature,Major,,,,,,,,,"['sdk-go', 'sdk-java-harness', 'sdk-py-harness']","['SDKs: Go', 'SDKs: Java: harness for executing UDFs over the Fn API', 'SDKs: Python: harness for executing UDFs over the Fn API']","Based on the discussion 

[https://lists.apache.org/thread.html/0cbf73d696e0d3a5bb8e93618ac9d6bb81daecf2c9c8e11ee220c8ae@%3Cdev.beam.apache.org%3E] 

Send Concurrency information to Runner from SDKHarness as header of control channel.",Add support for upper bound and optimum concurrency in SDK,3,,,angoenka,True,angoenka,angoenka
beam,BEAM-5148,2018-08-14T15:17:29.000+0000,2019-06-30T01:21:07.000+0000,2019-06-30T01:21:07.000+0000,,Fixed,New Feature,Major,['2.14.0'],['3.0.0'],30000,61800,,,,100,['sdk-py-core'],['SDKs: Python'],"Currently Java SDK has MongoDB support but Python SDK does not. With current portability efforts other runners may soon be able to use Python SDK. Having mongoDB support will allow these runners to execute large scale jobs using it.

Since we need this IO components @ Peat, we started working on a PyPi package available at this repository: [https://github.com/PEAT-AI/beam-extended]",Implement MongoDB IO for Python SDK,4,,,GeoloeG_IsT,True,yichi,GeoloeG_IsT
beam,BEAM-5114,2018-08-08T22:36:41.000+0000,,2019-04-30T18:29:49.000+0000,,,New Feature,Major,,,7800,7800,,,,100,['examples-java'],['Examples: Java examples'],"Producing these artifacts results in several benefits
 * Gives an example of how to package user code for different runners
 * Enables ad-hoc testing of runner changes against real user pipelines easier
 * Enables integration testing end-to-end pipelines against different runner services",Create example uber jars for supported runners,1,,,bsidhom,True,bsidhom,bsidhom
beam,BEAM-5093,2018-08-06T21:35:34.000+0000,2019-04-23T22:22:49.000+0000,2019-04-30T18:33:03.000+0000,,Won't Fix,New Feature,Major,['Not applicable'],,,33000,,,,,['runner-direct'],['Runners: Direct runner and ULR for single machine testing and development'],"The ULR currently cannot be used to run a pipeline directly, does not have adequate testing, and has bugs preventing pipelines from executing properly. This is a general bug for listing issues that must be fixed before the Reference Runner can be considered ready for general use.",Get Universal Local Runner to a state suitable for general use.,2,,,danoliveira,True,danoliveira,danoliveira
beam,BEAM-5075,2018-08-05T04:55:15.000+0000,,2019-04-30T18:32:13.000+0000,,,New Feature,Major,,"['3.0.0', '2.6.0', '2.7.0']",,,3600,3600,3600,,['build-system'],"['Build, CI, release systems and processes']"," Please add OWASP Dependency Check to the build (pom.xml).  OWASP DC makes an outbound REST call to MITRE Common Vulnerabilities & Exposures (CVE) to perform a lookup for each dependant .jar to list any/all known vulnerabilities for each jar.  This step is needed because a manual MITRE CVE lookup/check on the main component does not include checking for vulnerabilities in components or in dependant libraries.

OWASP Dependency check : https://www.owasp.org/index.php/OWASP_Dependency_Check has plug-ins for most Java build/make types (ant, maven, ivy, gradle).   

Also, add the appropriate command to the nightly build to generate a report of all known vulnerabilities in any/all third party libraries/dependencies that get pulled in. example : mvn -Powasp -Dtest=false -DfailIfNoTests=false clean aggregate

Generating this report nightly/weekly will help inform the project's development team if any dependant libraries have a reported known vulnerailities.  Project teams that keep up with removing vulnerabilities on a weekly basis will help protect businesses that rely on these open source componets.",Please add OWASP Dependency Check to the build,1,,"['build', 'easy-fix', 'security']",ABakerIII,True,,ABakerIII
beam,BEAM-5051,2018-07-31T22:09:30.000+0000,,2019-04-30T18:33:12.000+0000,,,New Feature,Major,,,,,,,,,['io-java-gcp'],"['IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)']","[https://github.com/apache/beam/pull/346#discussion_r63624693]

We should cleanup pubsub subscriptions when the pipeline is canceled.",Cleanup pubsub subscriptions,1,,,apilloud,True,apilloud,apilloud
beam,BEAM-4828,2018-07-19T14:22:36.000+0000,2018-08-22T20:20:19.000+0000,2018-08-22T20:20:19.000+0000,,Fixed,New Feature,Major,['2.7.0'],,43800,43800,,,,100,['io-java-aws'],['IO: Java: Amazon Web Services'],"Add an SQS source

 

For people who would like to follow progress or help out: [https://github.com/JohnRudolfLewis/beam/tree/Add-SqsIO]",Add Amazon SqsIO,1,,,JohnRLewis,True,JohnRLewis,JohnRLewis
beam,BEAM-4823,2018-07-19T00:22:37.000+0000,2018-08-23T21:40:29.000+0000,2018-08-23T21:40:29.000+0000,,Fixed,New Feature,Minor,['2.7.0'],['Not applicable'],18000,18000,,,,100,['io-java-aws'],['IO: Java: Amazon Web Services'],,Add Amazon Simple Notification Service (SNS) Sink,2,,,jhalarua,True,jhalarua,jhalarua
beam,BEAM-4822,2018-07-18T22:37:53.000+0000,,2019-04-30T18:30:04.000+0000,,,New Feature,Major,,,,,,,,,['io-java-files'],['IO: Java: Files'],"Some file systems (e.g. GCS) are versioned, and support reading previous generations of files. Since Beam's file support does not currently support this concept, the latest versions of files are always the ones returned. Users should be able to specify that they want to read a previous version of a file in FileIO.",Beam FileIO should support versioned file systems,5,,,reuvenlax,True,chamikara,reuvenlax
beam,BEAM-4807,2018-07-17T22:18:01.000+0000,2018-08-09T21:09:33.000+0000,2018-08-13T18:41:11.000+0000,,Fixed,New Feature,Major,['2.7.0'],,2400,2400,,,,100,['dsl-sql'],['DSLs: SQL'],We should upgrade calcite.,Upgrade calcite to 1.17.0,2,,,apilloud,True,apilloud,apilloud
beam,BEAM-4797,2018-07-16T13:21:42.000+0000,,2019-04-30T18:32:30.000+0000,,,New Feature,Major,,,,,,,,,['io-java-elasticsearch'],['IO: Java: ElasticSearch'],"In some cases, like sampling, the users will be interested in limiting the number of docs a ESIO.read() returns.

It is as simple as allowing the user to pass terminate_after parameter to the IO configuration.",Allow the user to limit the number of result docs in ElasticsearchIO.read(),2,1,,echauchot,True,echauchot,echauchot
beam,BEAM-4792,2018-07-14T16:04:11.000+0000,2018-08-09T22:13:20.000+0000,2018-08-09T22:20:15.000+0000,,Fixed,New Feature,Major,['2.6.0'],,1800,1800,,,,100,['runner-core'],['Runners: shared functionality for all runners'],"Executing bounded-per-element SDFs ""naively"" is pretty easy and doesn't require any special runner features, because it can be expanded into regular transforms. The naive implementation is sufficient for all runners except Dataflow, which has liquid sharding: support for that is a separate JIRA.
",Add support for bounded SDF to all runners,1,,,jkff,True,jkff,jkff
beam,BEAM-4774,2018-07-12T18:57:23.000+0000,2018-08-07T20:38:18.000+0000,2019-06-10T09:46:12.000+0000,,Fixed,New Feature,Major,['2.7.0'],,6600,6600,,,,100,"['dsl-sql', 'testing-nexmark']","['DSLs: SQL', 'Testing: Nexmark queries']",We should publish to a dashboard like pure JAVA does. See BEAM-4283.,Intergrate Nexmark SQL with Perfkit,1,,,apilloud,True,apilloud,apilloud
beam,BEAM-4746,2018-07-09T17:57:41.000+0000,,2019-04-30T18:32:36.000+0000,,,New Feature,Major,,,,,,,,,['dsl-sql'],['DSLs: SQL'],"We should validate the primitive types used in UDFs and UDAFs at creation time. Similar to the verifyPrimitiveType() call in Row. This will ensure the functions don't fail with a cryptic error message.

See: https://stackoverflow.com/questions/51239226/not-able-to-call-udf-in-beamsql",Validate UDF/UDAF types on creation,1,,,apilloud,True,mingmxu,apilloud
beam,BEAM-4721,2018-07-02T22:00:34.000+0000,,2018-07-02T22:00:34.000+0000,,,New Feature,Major,,,,,,,,,['dsl-sql'],['DSLs: SQL'],"Currently, WindowingStrategy is just a property of a PCollection but is unknown during planning. As with boundedness / unboundedness, this should be available statically so it can affect the plan. It may not be literally a WindowingStrategy object, but a SQL-level equivalent.

One windowing-related thing that is interesting is which fields represent the window. If we know this, then we know we can support per-window equijoins as long as a join condition also does an equijoin on the window.","WindowingStrategy, Window fields in the project as a trait of a Rel",1,,,kenn,True,,kenn
beam,BEAM-4720,2018-07-02T21:58:46.000+0000,,2018-07-02T21:58:46.000+0000,,,New Feature,Major,,,,,,,,,['dsl-sql'],['DSLs: SQL'],"Currently we do not know the boundedness or unboundedness of a collection until after planning is complete, so it cannot influence the plan. Instead, BeamJoinRel makes post-planning decisions about what sort of join to implement. We should move this into the planning phase. I think Calcite traits are the intended way to do this.",Boundedness and Unboundedness as a trait of a Rel so we know during planning,1,,,kenn,True,,kenn
beam,BEAM-4719,2018-07-02T21:56:04.000+0000,,2018-07-02T21:56:04.000+0000,,,New Feature,Major,,,,,,,,,['dsl-sql'],['DSLs: SQL'],"Currently, Beam SQL supports LIMIT in two ways:

1. Within a query, the results are subject to LIMIT. This works.
2. The shell knows to cancel a pipeline when the limit is reached, even if there is unfinished unbounded data.

The canceling of a pipeline works via a basic pattern match against the query execution plan, checking a few child nodes of the BeamEnumerableConverter for a BeamSortRel without a collation. If it can figure out what the limit is for the outermost query, then it will cancel the pipeline.

A more robust approach might be to use traits (or some other thorough analysis) to see if there is a known size for the outermost query. This would, for example, be unaffected by any number of layer of non-size-changing transformations.",Enhanced LIMIT support,1,,,kenn,True,,kenn
beam,BEAM-4714,2018-07-02T20:04:44.000+0000,2018-07-03T16:46:04.000+0000,2018-07-03T16:46:04.000+0000,,Fixed,New Feature,Major,['2.6.0'],,3600,3600,,,,100,['dsl-sql'],['DSLs: SQL'],"In this case it was in HOP_END (but not HOP_START, interestingly enough). Unclear why this manifests, but the ""+"" operator should just work for datetime + interval for this.",Some DATETIME PLUS operators end up as ordinary PLUS and crash in accept(),1,,,kenn,True,kenn,kenn
beam,BEAM-4713,2018-07-02T19:39:31.000+0000,2018-07-06T18:50:28.000+0000,2018-07-06T18:50:28.000+0000,,Done,New Feature,Major,['Not applicable'],,3600,3600,,,,100,['dsl-sql'],['DSLs: SQL'],,DECIMAL support in RowJsonDeserializer,1,,,kenn,True,amaliujia,kenn
beam,BEAM-4703,2018-07-01T23:15:12.000+0000,,2018-07-01T23:15:12.000+0000,,,New Feature,Major,,,,,,,,,['dsl-sql'],['DSLs: SQL'],"Currently, operators like {{>, }}{{<=}}, and others dynamically dispatch on the type of their operands. But SQL is statically typed, and we should both validate the types (if Calcite doesn't do it for us) and gain performance by avoiding this dispatch.",SQL operators should have overloading resolved statically,1,,,kenn,True,,kenn
beam,BEAM-4702,2018-07-01T22:03:12.000+0000,,2018-07-03T16:55:35.000+0000,,,New Feature,Major,,,4200,4200,,,,100,['dsl-sql'],['DSLs: SQL'],"Beam SQL runs in two contexts:

1. As a PTransform in a pipeline. A PTransform operates on a PCollection, which is always implicitly windows and a PTransform should operate per-window so it automatically works on bounded and unbounded data. This only works if the query has no windowing operators, in which case the GROUP BY <non-window stuff> should operate per-window.
2. As a top-level shell that starts and ends with SQL. In the relational model there are no implicit windows. Calcite has some extensions for windowing, but they manifest (IMO correctly) as just items in the GROUP BY list. The output of the aggregation is ""just rows"" again. So it should be globally windowed.

The problem is that this semantic fix makes it so we cannot join windowing stream subqueries. Because we don't have retractions, we only support GroupByKey-based equijoins over windowed streams, with the default trigger. _These joins implicitly also join windows_. For example:

{code}
JOIN(left.id = right.id)
  SELECT ... GROUP BY id, TUMBLE(1 hour)
  SELECT ... GROUP BY id, TUMBLE(1 hour)  
{code}

Semantically, there may be a joined row for 1:00pm on the left and 10:00pm on the right. But by the time the right-hand row for 10:00pm shows up, the left one may be GC'd. So this is implicitly, but nondeterministically, joining on the window as well. Before this PR, we left the windowing strategies for left and right in place, and asserted that they matched.

If we re-window into the global window always, there _are no windowed streams_ so you just can't do stream joins. The solution is probably to track which field of a stream is the window and allow joins which also explicitly express the equijoin over the window field.",After SQL GROUP BY <windowing> the result should be globally windowed,1,,,kenn,True,,kenn
beam,BEAM-4701,2018-07-01T14:00:45.000+0000,2018-07-03T16:47:52.000+0000,2018-07-03T16:47:52.000+0000,,Fixed,New Feature,Major,['2.6.0'],,,,,,,,['dsl-sql'],['DSLs: SQL'],"Currently there's a testing gap where tests at the DSL level mostly go through the QueryTransform. So we can hit issues where we have a mismatch in Calcite Avatica, which provides the JDBC layer.",Run SQL DSL-level tests through JDBC driver,,,,kenn,True,kenn,kenn
beam,BEAM-4698,2018-06-30T21:32:24.000+0000,,2019-02-07T04:47:43.000+0000,,,New Feature,Major,,,1200,1200,,,,100,['runner-samza'],['Runners: Samza'],"https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Samza_Gradle/lastCompletedBuild/

This failed the SchemaTest, which we know all runners will fail since it uses a different entry point for {{SimpleDoFnRunner}}. It should be excluded from their test suites.",SchemaTest not sickbayed for Samza runner,1,,,kenn,True,,kenn
beam,BEAM-4695,2018-06-30T04:54:46.000+0000,,2019-04-30T18:30:08.000+0000,,,New Feature,Major,,,,,,,,,['website'],['Website: content and bugs'],,"Add Samza to ""works with"" on the Beam site landing page",,,,kenn,True,xinyu,kenn
beam,BEAM-4689,2018-06-29T12:39:08.000+0000,2018-06-30T00:05:10.000+0000,2018-06-30T21:25:39.000+0000,,Fixed,New Feature,Blocker,['2.6.0'],,8400,8400,,,,100,['runner-dataflow'],['Runners: Google Cloud Dataflow runner'],"The Dataflow postcommit is broken in a way that seems real and user-impacting:

https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow_Gradle/lastCompletedBuild/testReport/junit/org.apache.beam.sdk.transforms/SplittableDoFnTest/testSideInput/

{code}
Caused by: java.lang.IllegalArgumentException: unable to deserialize Serialized DoFnInfo
        ...
Caused by: java.io.InvalidClassException: org.apache.beam.runners.core.construction.SplittableParDo$RandomUniqueKeyFn; local class incompatible: stream classdesc serialVersionUID = 6068396661487412884, local class serialVersionUID = -617521663543732196
{code}

This means that the worker is using a version of the class from its own classpath, not the version from the user's staged pipeline. It implies that the worker is not shading runners-core-construction. Because that is where a ton of utility DoFns live, it is critical that it be shaded.",Dataflow cannot deserialize SplittableParDo DoFns,1,,,kenn,True,jkff,kenn
beam,BEAM-4688,2018-06-29T03:18:23.000+0000,,2018-06-29T03:18:23.000+0000,,,New Feature,Major,,,,,,,,,['dsl-sql'],['DSLs: SQL'],"Currently {{PubsubIO.read().fromTopic(...)}} always creates a subscription that will never be deleted. Beam doesn't have an adequate model for cleanup tasks, but relies on external orchestration to clean up such things. Beam SQL can do this for LIMIT-bounded queries that actually do terminate.",Beam SQL should clean up resources upon termination of the pipeline,,,,kenn,True,,kenn
beam,BEAM-4687,2018-06-29T00:01:26.000+0000,2018-08-03T17:04:22.000+0000,2018-08-03T17:04:22.000+0000,,Fixed,New Feature,Major,['2.7.0'],,6600,6600,,,,100,['testing'],['Testing: general infrastructure'],,Automatically file JIRA for dependency updates,1,,,yifanzou,True,yifanzou,yifanzou
beam,BEAM-4684,2018-06-28T20:02:30.000+0000,,2019-04-30T18:31:52.000+0000,,,New Feature,Major,,,16800,16800,,,,100,['runner-dataflow'],['Runners: Google Cloud Dataflow runner'],https://docs.google.com/document/d/117yRKbbcEdm3eIKB_26BHOJGmHSZl1YNoF0RqWGtqAM,Support @RequiresStableInput on Dataflow runner in Java SDK,,,,robinyqiu,True,robinyqiu,robinyqiu
beam,BEAM-4677,2018-06-28T17:12:47.000+0000,2018-06-29T02:59:21.000+0000,2018-06-29T02:59:21.000+0000,,Fixed,New Feature,Major,['Not applicable'],,,,,,,,['project-management'],"['Project Management: monthly reports, administrative tasks']","Create Jira project that will be assigned to tickets regarding post-commit tests failures.

Suggested name: ""post-commit failures"" or ""post-commits"" for shorter.

 

 ",Create project for tickets for post-commit tests failures,1,,,Ardagan,True,kenn,Ardagan
beam,BEAM-4676,2018-06-28T17:10:57.000+0000,,2019-04-30T18:33:17.000+0000,,,New Feature,Major,,,3600,3600,,,,100,['runner-samza'],['Runners: Samza'],"Add the user guide, examples and capability matrix for Samza runner.",Samza runner documentation,1,,,xinyu,True,xinyu,xinyu
beam,BEAM-4652,2018-06-27T17:25:44.000+0000,2018-06-30T03:46:24.000+0000,2018-06-30T03:46:24.000+0000,,Fixed,New Feature,Critical,['2.6.0'],,20400,20400,,,,100,['io-java-gcp'],"['IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)']","If you try to read a public pubsub topic in the DirectRunner, it will fail with 403 when trying to create a subscription. This is because it tries to create a subscription on the shared public data set.

There is an example used in https://github.com/googlecodelabs/cloud-dataflow-nyc-taxi-tycoon and the dataset is {{projects/pubsub-public-data/topics/taxirides-realtime}}. I discovered that I could not read this in the DirectRunner even though the codelab works. But that 1.x codelab also does not work in the InProcessPipelineRunner, so it has been broken all along.

So you cannot read public data or any other read-only data using PubsubIO.
",PubsubIO: create subscription on different project than the topic,,,,kenn,True,kenn,kenn
beam,BEAM-4651,2018-06-27T16:43:06.000+0000,2018-06-27T19:00:22.000+0000,2018-06-27T19:00:22.000+0000,,Fixed,New Feature,Major,['2.6.0'],,1800,1800,,,,100,['dsl-sql'],['DSLs: SQL'],"We don't have a brilliant way to distribute or launch using just downloaded jars, but it is useful anyhow to be able to quickly build a self-contained SQL shell. The Gradle application plugin does this really well.",Bundled Beam SQL shell build,,,,kenn,True,kenn,kenn
beam,BEAM-4650,2018-06-27T07:34:32.000+0000,,2019-04-30T18:30:20.000+0000,,,New Feature,Major,,,,,,,,,['sdk-py-core'],['SDKs: Python'],"Java supports specifying a retry policy when performing streaming writes to BQ: [https://github.com/apache/beam/blob/master/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/InsertRetryPolicy.java]

 

We should update Python BQ streaming sink to support this as well.

https://github.com/apache/beam/blob/master/sdks/python/apache_beam/io/gcp/bigquery.py#L1430",Add retry policy to Python BQ streaming sink,3,,,chamikara,True,app-tarush,chamikara
beam,BEAM-4641,2018-06-25T21:58:48.000+0000,2018-06-29T22:55:37.000+0000,2018-06-29T22:55:37.000+0000,,Fixed,New Feature,Major,['Not applicable'],,4200,4200,,,,100,['runner-samza'],['Runners: Samza'],"One you have a Jenkins job set up, the latest status should be in the template at https://github.com/apache/beam/blob/master/.github/PULL_REQUEST_TEMPLATE.md",Samza runner postcommit status in PR template,1,,,kenn,True,xinyu,kenn
beam,BEAM-4640,2018-06-25T21:58:11.000+0000,2018-06-29T18:10:35.000+0000,2018-06-29T18:10:35.000+0000,,Fixed,New Feature,Major,['Not applicable'],,4800,4800,,,,100,['runner-samza'],['Runners: Samza'],See https://github.com/apache/beam/blob/master/.test-infra/jenkins/job_PostCommit_Java_ValidatesRunner_Gearpump.groovy for one example.,Samza runner postcommit ValidatesRunner job,,,,kenn,True,xinyu,kenn
beam,BEAM-4626,2018-06-22T21:26:53.000+0000,2018-07-02T19:45:44.000+0000,2018-07-02T19:45:44.000+0000,,Fixed,New Feature,Major,['2.6.0'],,3600,3600,,,,100,['dsl-sql'],['DSLs: SQL'],"Today, SQL can read CSV and allows a {{format}} flag to control what CSV variant is used. But to do easy things and write pure SQL jobs it would be nice to just read the text file as a one-column table and do transformations in SQL.",Support text table format with a single column of the lines of the files,,,,kenn,True,kenn,kenn
beam,BEAM-4615,2018-06-21T21:16:28.000+0000,,2019-04-30T18:32:44.000+0000,,,New Feature,Major,,,21000,21000,,,,100,['runner-flink'],['Runners: Flink runner'],"This includes:
 * A gradle wrapper that can execute the Flink job server driver so that it can be easily run locally for testing/debugging.
 * A shadow (""uber"") target that packages all portable Flink runner dependencies into a runnable jar. This jar can then be submitted to Flink clusters via `flink run`.

 ",Flink job server driver wrapper,2,,,bsidhom,True,bsidhom,bsidhom
beam,BEAM-4601,2018-06-20T17:19:31.000+0000,2018-07-02T18:04:33.000+0000,2018-07-06T21:46:23.000+0000,,Done,New Feature,Major,['Not applicable'],,24000,24000,,,,100,['dsl-sql'],['DSLs: SQL'],"Right now Beam SQL can created a BigQuery table, however, read from BigQuery table is not supported yet. We want to support reading basic data types from BigQuery table (not including ROW type).",BigQuery reads basic types from pure SQL,2,,,amaliujia,True,amaliujia,amaliujia
beam,BEAM-4594,2018-06-20T02:22:04.000+0000,2019-02-15T00:28:09.000+0000,2019-02-15T00:28:10.000+0000,,Implemented,New Feature,Major,['2.9.0'],,26400,26400,,,,100,['sdk-py-core'],['SDKs: Python'],"This issue tracks the implementation of the Beam Python User State and Timer API, described here: [https://s.apache.org/beam-python-user-state-and-timers].",Implement Beam Python User State and Timer API,3,,['portability'],ccy,True,,ccy
beam,BEAM-4575,2018-06-18T17:53:04.000+0000,2018-06-28T16:51:45.000+0000,2018-06-28T16:51:45.000+0000,,Fixed,New Feature,Major,['2.6.0'],,21600,21600,,,,100,['dsl-sql'],['DSLs: SQL'],It would be nice if the Beam graph matched the Calcite graph in structure with each node generating a PTransform that is applied onto the PCollection of it's parent. We should also ensure that each Calcite node only appears in the Beam graph one time.,Beam SQL should cleanly transform graph from Calcite,1,,,apilloud,True,apilloud,apilloud
beam,BEAM-4566,2018-06-15T01:13:59.000+0000,,2018-06-15T01:13:59.000+0000,,,New Feature,Major,,,,,,,,,['sdk-py-core'],['SDKs: Python'],"Currently stage_file is not retried on GCS 503 errors.

https://github.com/apache/beam/blob/ba62679f9ebd9266f6cbea84aa1196cfac30d451/sdks/python/apache_beam/runners/dataflow/internal/apiclient.py#L450",DataflowRunner should retry if it fails to stage artifacts due to transient errors,1,,,altay,True,,altay
beam,BEAM-4550,2018-06-13T05:47:20.000+0000,,2018-06-13T05:47:20.000+0000,,,New Feature,Major,,,,,,,,,['website'],['Website: content and bugs'],"Add information to the web site about API and version support, including the meaning of experimental and deprecated annotations. Currently this information only exists in API docs.",Add information on API support,1,,,altay,True,,altay
beam,BEAM-4546,2018-06-12T21:43:50.000+0000,,2019-02-07T04:53:55.000+0000,,,New Feature,Major,,,5400,5400,,,,100,['sdk-py-core'],['SDKs: Python'],,Implement with hot key fanout for combiners,2,,,altay,True,,altay
beam,BEAM-4532,2018-06-11T18:49:43.000+0000,,2018-06-11T18:49:43.000+0000,,,New Feature,Minor,,,,,,,,,"['beam-model', 'sdk-java-core', 'sdk-py-core']","['Beam Model: general programming model concepts, semantics', 'SDKs: Java: pipeline construction, core transformations', 'SDKs: Python']","Currently, state is per-key-and-window and is freed by window expiration.

In the global window, state never expires. It can be explicitly deleted by the user based on timers or custom criteria, but that might be a pain. [~reuvenlax] suggests a nicer API or model might be helpful.",Consider how to free state with unbounded keys in the global window,,,,kenn,True,,kenn
beam,BEAM-4497,2018-06-05T22:47:58.000+0000,,2019-04-30T18:33:39.000+0000,,,New Feature,Major,,,,,,,,,['website'],['Website: content and bugs'],,Add pages for master Javadocs / Pydocs and incorporate into post-commit job,,,['beam-site-automation-reliability'],swegner,True,,swegner
beam,BEAM-4471,2018-06-04T20:47:52.000+0000,2018-08-22T01:52:52.000+0000,2018-08-22T01:52:52.000+0000,,Fixed,New Feature,Major,['Not applicable'],,3600,3600,,,,100,['website'],['Website: content and bugs'],"This is a request to document how to use Apache Beam on Windows machines.

At minimum, add Windows-specific steps to the Java quickstart guide:

[https://beam.apache.org/get-started/quickstart-java/]

 ",Documentation for Windows SDK users,3,,,udim,True,rfernand,udim
beam,BEAM-4465,2018-06-04T15:30:24.000+0000,2018-12-12T10:27:24.000+0000,2018-12-12T10:29:31.000+0000,,Done,New Feature,Major,['2.9.0'],,,6600,,,,,"['runner-flink', 'testing']","['Runners: Flink runner', 'Testing: general infrastructure']","Flink runner should be able to run IOITs the same way as Dataflow or Direct runner. 

For more details on how the tests are run see the documentation: [https://beam.apache.org/documentation/io/testing/#i-o-transform-integration-tests]",Enable Flink runner to run IOITs,3,,,ŁukaszG,True,,ŁukaszG
beam,BEAM-4464,2018-06-04T15:29:47.000+0000,,2018-06-04T16:01:18.000+0000,,,New Feature,Major,,,,22800,,,,,"['runner-spark', 'testing']","['Runners: Spark runner', 'Testing: general infrastructure']","Spark runner should be able to run IOITs the same way as Dataflow or Direct runner. 

For more details on how the tests are run see the documentation: [https://beam.apache.org/documentation/io/testing/#i-o-transform-integration-tests]",Enable Spark runner to run IOITs,2,,,ŁukaszG,True,,ŁukaszG
beam,BEAM-4462,2018-06-03T23:37:42.000+0000,2018-06-03T23:42:33.000+0000,2018-06-03T23:42:33.000+0000,,Duplicate,New Feature,Major,['Not applicable'],,,,,,,,['io-ideas'],"['IO: ideas (proposals for new connectors, new designs, etc)']",Python SDK could use Parquet I/O,Python Parquet I/O,1,,,brucearctor,True,jkff,brucearctor
beam,BEAM-4444,2018-06-01T16:27:35.000+0000,2018-12-11T22:00:56.000+0000,2018-12-11T22:00:56.000+0000,,Fixed,New Feature,Major,['2.10.0'],,43200,43200,,,,100,['sdk-py-core'],['SDKs: Python'],Add Parquet Support for the Python SDK.,Parquet IO for Python SDK,2,,,brucearctor,True,heejong,brucearctor
beam,BEAM-4431,2018-05-30T18:53:34.000+0000,2018-10-24T20:46:50.000+0000,2018-10-24T20:46:50.000+0000,,Fixed,New Feature,Minor,['2.9.0'],,3600,3600,,,,100,['website'],['Website: content and bugs'],"I found a [thread on the builds@apache.org list|https://lists.apache.org/thread.html/7c84f8bcdd086d119835c8dc0d995b2aaf642f43b70587d0dff3fa04@%3Cbuilds.apache.org%3E] where the Structs project mentions their ""Edit on Github"" website button. I've always thought this would be a nice addition to encourage readers to update documentation whenever they see something out-of-date. It sounds like it would be fairly easy to integrate.","Add ""Edit this Page"" button to website",2,,,swegner,True,alanmyrvold,swegner
beam,BEAM-4415,2018-05-28T15:01:22.000+0000,2018-06-12T09:59:52.000+0000,2018-06-12T09:59:52.000+0000,,Done,New Feature,Major,['2.5.0'],,7800,7800,,,,100,"['io-java-parquet', 'testing']","['IO: Java: Parquet', 'Testing: general infrastructure']","There already is a running job for ParquetIO on Jenkins: [https://builds.apache.org/view/A-D/view/Beam/job/beam_PerformanceTests_ParquetIOIT/] 

There are also Jenkins' Jobs running such tests on an HDFS cluster: [https://builds.apache.org/view/A-D/view/Beam/job/beam_PerformanceTests_AvroIOIT_HDFS/] 

Therefore, we should provide a Performance Test for ParquetIO running on HDFS cluster.",Enable HDFS based Performance Test for ParquetIO,1,,,ŁukaszG,True,ŁukaszG,ŁukaszG
beam,BEAM-4410,2018-05-25T12:19:20.000+0000,2019-07-04T06:05:35.000+0000,2019-07-04T06:05:35.000+0000,,Fixed,New Feature,Major,['Not applicable'],,6600,6600,,,,100,['dsl-euphoria'],['DSLs: Euphoria'],Translation of Broadcast Join. Broadcast join can be very efficient for joins between a large dataset with small dataset that has to fit into memory.  It's implemented with Beam's `sideInput`.,Broadcast Joins,1,,,marek.simunek,True,marek.simunek,marek.simunek
beam,BEAM-4403,2018-05-24T20:24:13.000+0000,,2019-02-07T04:55:53.000+0000,,,New Feature,Major,,,,,,,,,['build-system'],"['Build, CI, release systems and processes']","We should test to ensure all class files are contained within a jar's package path. For example, beam-sdks-java-core-shaded.jar should only have .class files under org/apache/beam/sdk.",Test shaded jars to ensure everything is in the package path,1,,,apilloud,True,,apilloud
beam,BEAM-4402,2018-05-24T20:19:27.000+0000,,2018-05-30T21:03:46.000+0000,,,New Feature,Major,,,10200,10200,,,,100,['build-system'],"['Build, CI, release systems and processes']","We should run our tests against shaded jars.

See https://github.com/apache/beam/pull/5471",Run tests against shaded JAR,2,,,apilloud,True,,apilloud
beam,BEAM-4394,2018-05-23T22:50:11.000+0000,2018-06-28T15:14:41.000+0000,2018-06-28T15:14:41.000+0000,,Fixed,New Feature,Major,['2.6.0'],,9600,9600,,,,100,['build-system'],"['Build, CI, release systems and processes']","""Spotless"" can enforce - and automatically restore - automatic Java formatting. Whenever formatting is off, it tells a user the exact command to fix it.

It isn't (just) about code layout, it is about automation. We have pretty strict style rules enforced by checkstyle. The most efficient way to fix up a file is with autoformat. But if the autoformat hits a bunch of irrelevant lines, that is annoying for a reviewer and obscures git blame.

If we enforce autoformat all the time, then it makes sure that autoformatting a particular PR has minimal effects and is always safe to do.",Consider enabling spotless java format throughout codebase,2,,,kenn,True,kenn,kenn
beam,BEAM-4391,2018-05-23T18:33:59.000+0000,,2019-04-30T18:30:59.000+0000,,,New Feature,Minor,,,10200,10200,,,,100,['examples-python'],['Examples: Python examples'],"Currently, we are writing a blogpost on using the Beam Python SDK for solving distributed optimization tasks. It will include an example of a optimization problem with both discrete and continuous parameters, which is then solved using Apache Beam. ",Example of distributed optimization,3,,,joachimvdh,True,joachimvdh,joachimvdh
beam,BEAM-4389,2018-05-23T09:53:52.000+0000,2018-05-31T14:15:03.000+0000,2018-05-31T14:15:03.000+0000,,Fixed,New Feature,Major,['2.5.0'],['2.4.0'],7200,7200,,,,100,['io-java-elasticsearch'],['IO: Java: ElasticSearch'],"Expose a configuration option on the {{ElasticsearchIO}} to enable partial updates rather than full document inserts. 

Rationale: We have the case where different pipelines process different categories of information of the target entity (e.g. one for taxonomic processing, another for geospatial processing). A read and merge is not possible inside the batch call, meaning the only way to do it is through a join. The join approach is slow, and also stops the ability to run a single process in isolation (e.g. reprocess the geospatial component of all docs).

Use of this configuration parameter has to be used in conjunction with controlling the document ID (possible since BEAM-3201) to make sense.

The client API would include a {{withUseUpdate(...)}} such as:

{code}
source.apply(
  ElasticsearchIO.write()
    .withConnectionConfiguration(connectionConfiguration)
    .withIdFn(new ExtractValueFn(""id""))
    .withUseUpdate(true)
{code}

",Enable partial updates Elasticsearch,3,1,,timrobertson100,True,timrobertson100,timrobertson100
beam,BEAM-4388,2018-05-23T08:26:42.000+0000,,2019-04-30T18:30:34.000+0000,,,New Feature,Major,,,22200,22200,,,,100,['dsl-sql'],['DSLs: SQL'],"Before converting into Beam Pipeline physical plan, logical plan should be optimized and it will be super helpful for efficiently executing Beam PTransforms pipeline. 

Calcite has two ways for optimizing logical plan (HepPlanner and VolcanoPlanner). We can support VolcanoPlanner first and apply calcite builtin optimize rules (like 

FilterJoinRule.FILTER_ON_JOIN) to sql query optimize plans.",Support optimized logical plan,1,,,vectorijk,True,vectorijk,vectorijk
beam,BEAM-4385,2018-05-23T03:16:00.000+0000,2018-06-12T22:22:50.000+0000,2018-06-12T22:22:50.000+0000,,Fixed,New Feature,Major,['2.6.0'],,6000,6000,,,,100,['dsl-sql'],['DSLs: SQL'],Currently the LIKE operator is not supported. It is pretty important.,Support LIKE operator,1,,,kenn,True,amaliujia,kenn
beam,BEAM-4381,2018-05-22T16:53:01.000+0000,,2019-02-07T04:51:49.000+0000,,,New Feature,Major,,,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","This task is about ensuring the generic record of beam can be converted to a more mainstream format (JSON). The usage os javax.json.JsonObject is to ensure it is portable and implementation independent and integrates th emost probably with client libraries (most implementations have a bridge to the jsr anyway)

 

It will likely lead to provide a Dofn Row -> JsonObject and the opposite.

 ",Provide DoFn to convert a Row (record with schema) to a javax.json.JsonObject,1,,,romain.manni-bucau,True,,romain.manni-bucau
beam,BEAM-4374,2018-05-21T18:33:51.000+0000,,2019-06-08T00:03:39.000+0000,,,New Feature,Major,,,60600,60600,,,,100,['beam-model'],"['Beam Model: general programming model concepts, semantics']","Update existing metrics to use the new proto and cataloging schema defined in:

[_https://s.apache.org/beam-fn-api-metrics_]
 * Check in new protos
 * Define catalog file for metrics
 * Port existing metrics to use this new format, based on catalog names+metadata",Update existing metrics in the FN API to use new Metric Schema,2,,,ajamato@google.com,True,,ajamato@google.com
beam,BEAM-4333,2018-05-17T19:20:45.000+0000,2018-06-26T19:43:50.000+0000,2018-06-26T21:00:56.000+0000,,Implemented,New Feature,Minor,['2.6.0'],,22200,22200,,,,100,['sdk-py-core'],['SDKs: Python'],"Add integration tests for the 4 [mobile game examples|https://github.com/apache/beam/tree/master/sdks/python/apache_beam/examples/complete/game].
Setup a Jenkins test to run the above integration tests at a daily frequency.",Add integration tests for mobile game examples,1,,,mariagh,True,mariagh,mariagh
beam,BEAM-4302,2018-05-16T21:28:42.000+0000,2018-06-26T20:18:44.000+0000,2018-06-26T20:18:44.000+0000,,Fixed,New Feature,Major,['2.6.0'],,315600,315600,,,,100,['testing'],['Testing: general infrastructure'],"# For Java, a daily Jenkins test to compare version of all Beam dependencies to the latest version available in Maven Central.
 # For Python, a daily Jenkins test to compare versions of all Beam dependencies to the latest version available in PyPI.",Fix to dependency hell,3,,,yifanzou,True,yifanzou,yifanzou
beam,BEAM-4272,2018-05-10T18:39:37.000+0000,,2018-05-10T18:39:37.000+0000,,,New Feature,Major,,,,,,,,,['sdk-java-harness'],['SDKs: Java: harness for executing UDFs over the Fn API'],"Instead of allocating the assign context for every call, create the context once and update a pointer to the current {{WindowedValue}} whenever we receive a call.",Consider allocating AssignContext only once in the Java SDK Harness,1,,,tgroh,True,,tgroh
beam,BEAM-4270,2018-05-10T18:01:47.000+0000,,2018-06-13T16:53:50.000+0000,,,New Feature,Major,,,,,,,,,['build-system'],"['Build, CI, release systems and processes']",We should build javadoc in precommits so we don't end up with another BEAM-4251,Build javadoc in precommit,2,,,apilloud,True,,apilloud
beam,BEAM-4269,2018-05-10T17:31:23.000+0000,2018-05-10T18:11:27.000+0000,2018-05-15T22:50:50.000+0000,,Duplicate,New Feature,Major,['Not applicable'],,12000,12000,,,,100,['sdk-java-harness'],['SDKs: Java: harness for executing UDFs over the Fn API'],This allows execution of Java WindowFns over the Fn API,Implement Assign Windows in the Java SDK Harness,1,,,tgroh,True,tgroh,tgroh
beam,BEAM-4265,2018-05-09T18:23:26.000+0000,2019-03-11T22:37:22.000+0000,2019-03-11T22:37:22.000+0000,,Fixed,New Feature,Major,['2.12.0'],,,,,,,,['sdk-py-core'],['SDKs: Python'],"When writing to BigQuery using streaming writes, Java SDK supports writing failed records to a dead letter queue: [https://github.com/apache/beam/blob/master/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIO.java#L1375]

 

This is a very useful feature for long running pipelines so we should add this to Python BQ sink: https://github.com/apache/beam/blob/master/sdks/python/apache_beam/io/gcp/bigquery.py#L1279",Add a dead letter queue to Python streaming BigQuery sink,2,,,chamikara,True,,chamikara
beam,BEAM-4259,2018-05-08T18:43:00.000+0000,2018-06-29T18:32:34.000+0000,2018-06-29T18:32:34.000+0000,,Fixed,New Feature,Major,['Not applicable'],,,,,,,,['runner-direct'],['Runners: Direct runner and ULR for single machine testing and development'],,"Instantiate a Pipeline (EvaluationContext, Executor, etc) via Proto format (only) in the DirectRunner",1,,['portability'],tgroh,True,tgroh,tgroh
beam,BEAM-4258,2018-05-08T18:40:10.000+0000,2018-06-29T16:23:37.000+0000,2018-06-29T16:23:37.000+0000,,Fixed,New Feature,Major,['2.6.0'],,5400,5400,,,,100,['runner-direct'],['Runners: Direct runner and ULR for single machine testing and development'],,Integrate Docker Environment Management in the ReferenceRunner,1,,['portability'],tgroh,True,tgroh,tgroh
beam,BEAM-4248,2018-05-07T17:00:03.000+0000,,2018-05-07T18:22:04.000+0000,,,New Feature,Major,,,,,,,,,['io-java-gcp'],"['IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)']",Bigquery is using the really old com.google.api.services client library. We should upgrade to the com.google.cloud version which includes new features and ENUMs for all the constants.,Upgrade Bigquery to com.google.cloud library,1,,,apilloud,True,,apilloud
beam,BEAM-4247,2018-05-07T08:33:52.000+0000,,2019-04-30T18:29:15.000+0000,,,New Feature,Major,['Not applicable'],['2.3.0'],,,,,,,"['runner-dataflow', 'runner-direct']","['Runners: Google Cloud Dataflow runner', 'Runners: Direct runner and ULR for single machine testing and development']","Cannot set region for BigQuery dataset when using direct runner using Apache Beam. I'm trying to get data from Oracle via JdbcIO.read using apache beam to get data and push them to bigquery table. Problem is I have to use direct runner because Oracle DB is local, and the bigquery dataset must located in asia, not the default us-central. so now I cannot push data because the default location dont have my target dataset. I tried extending the GcpOption but no help. Don't suggest me to use the DataflowRunner, because it cannot load data from Oracle local. Thanks.",Set region when BigqueryIO.write using DirectRunner,1,,['newbie'],doduythao,True,,doduythao
beam,BEAM-4244,2018-05-06T23:23:25.000+0000,,2018-05-06T23:23:25.000+0000,,,New Feature,Major,,,,,,,,,"['beam-model', 'runner-core']","['Beam Model: general programming model concepts, semantics', 'Runners: shared functionality for all runners']","Beam runners use coders in various stages of a pipeline to encode/decode data. Coders are executed directly by the runner of a pipeline and user do not have control over exceptions raised during encoding/decoding (could be either due to malformed/corrupted data provided by users or intermediate malformed/corrupted data generated during the system execution).

Currently users can rely on runner-specific worker logging to detect the error and update the pipeline but it would be better if we can provide a way to programmatically handle these errors.",Provide a better way for programmatically handling errors raised while encoding/decoding data,1,,,chamikara,True,,chamikara
beam,BEAM-4241,2018-05-04T23:47:57.000+0000,,2018-05-08T18:41:38.000+0000,,,New Feature,Major,,,,,,,,,['runner-direct'],['Runners: Direct runner and ULR for single machine testing and development'],"This decodes known {{WindowFns}} from the portable representation if there is no associated environment, and runs those {{WindowFns}} on the input stream. As none of those known fns interact with the element directly, this is safe to do outside of an SDK harness.",Implement AssignWindows for known WindowFns in the ReferenceRunner,1,,,tgroh,True,,tgroh
beam,BEAM-4240,2018-05-04T23:46:14.000+0000,2018-06-29T18:32:20.000+0000,2018-06-29T18:32:20.000+0000,,Fixed,New Feature,Major,['Not applicable'],,,,,,,,['runner-direct'],['Runners: Direct runner and ULR for single machine testing and development'],"This is Flatten, Impulse, GroupByKeyOnly, GroupAlsoByWindow (for the implementation of the DirectRunner), and {{ExecutableStage}}

 

{{Flatten}} and {{Impulse}} are straightforwards. {{GroupByKeyOnly}} requires extracting the runner-implementation of the PCollection's {{KeyCoder}}, and {{GroupAlsoByWindow}} and {{ExecutableStage}} both have specific concerns and are tracked independently",Implement the limited set of portable primitives in the Java DirectRunner,1,,,tgroh,True,tgroh,tgroh
beam,BEAM-4239,2018-05-04T23:42:42.000+0000,,2018-05-04T23:43:08.000+0000,,,New Feature,Major,,,,,,,,,['runner-direct'],['Runners: Direct runner and ULR for single machine testing and development'],"This removes the duplication of implementations between java direct runners, while maintaining the ability to debug the code that is invoked.

 

Model enforcement (particularly immutability) is likely to need at least a partial redesign around fusion.",Implement the native Java DirectRunner via the ReferenceRunner and an InProcessServerFactory/Channel,2,,,tgroh,True,,tgroh
beam,BEAM-4238,2018-05-04T23:41:01.000+0000,,2018-05-04T23:43:08.000+0000,,,New Feature,Major,,,,,,,,,['runner-direct'],['Runners: Direct runner and ULR for single machine testing and development'],"Based on current implementation, the 'native' DirectRunner should long-term be replaced with the ReferenceRunner using an in-process channel to communicate with the Java SDK harness. Once that's done, the need for the local-java module to exist vanishes.",Join the local-java and direct-java packages,2,,,tgroh,True,,tgroh
beam,BEAM-4237,2018-05-04T23:39:44.000+0000,,2018-05-04T23:39:44.000+0000,,,New Feature,Minor,,,,,,,,,['runner-direct'],['Runners: Direct runner and ULR for single machine testing and development'],This is utility code that is used between direct-runner versions,Migrate WatermarkManager to the local-java package,1,,,tgroh,True,,tgroh
beam,BEAM-4228,2018-05-02T22:04:01.000+0000,,2018-05-03T00:20:22.000+0000,,,New Feature,Major,,,,,,,,,['runner-flink'],['Runners: Flink runner'],"The use of a reducer that adds all of the elements that it consumes to a list is the primary way in which this occurs - if instead, we produce a filtered iterable, or a collection of filtered iterables, we can lazily iterate over all of the contained elements without having to buffer all of the elements.

 

For an example of where this occurs, see {{Concatenate}} in  {{FlinkBatchPortablePipelineTranslator}}.",The FlinkRunner shouldn't require all of the values for a key to fit in memory,4,,,tgroh,True,,tgroh
beam,BEAM-4217,2018-05-01T00:50:23.000+0000,2018-05-01T23:19:34.000+0000,2018-05-01T23:19:34.000+0000,,Fixed,New Feature,Major,['2.5.0'],,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']",Like BEAM-3883 but for Java.,Java SDK stages artifacts when talking to job server,1,,,jkff,True,,jkff
beam,BEAM-4216,2018-05-01T00:49:29.000+0000,2018-06-15T02:05:41.000+0000,2018-06-15T02:05:41.000+0000,,Fixed,New Feature,Major,['2.6.0'],,,,,,,,['runner-flink'],['Runners: Flink runner'],"This Jira covers the path from the Flink implementation of ArtifactStagingService to the Flink implementation of ArtifactRetrievalService.

 

Sending artifacts to the staging service is tracked per SDK; getting them from the retrieval service is tracked per SDK-harness.",Flink: Staged artifacts are delivered to the SDK container,2,,,jkff,True,,jkff
beam,BEAM-4212,2018-04-30T23:59:26.000+0000,,2018-04-30T23:59:26.000+0000,,,New Feature,Major,,,,,,,,,['testing'],['Testing: general infrastructure'],"Covers both Java and Python. Should be e.g. triggered by the phrase ""Run Flink Portable ValidatesRunner tests"". Eventually will replace the non-portable one, but that's not in scope for the current JIRA.

Depends on  BEAM-4067 and BEAM-4211.",Flink portable ValidatesRunner tests continuously run on Jenkins,3,,,jkff,True,,jkff
beam,BEAM-4211,2018-04-30T23:57:07.000+0000,,2018-04-30T23:57:07.000+0000,,,New Feature,Major,,,,,,,,,"['sdk-py-core', 'testing']","['SDKs: Python', 'Testing: general infrastructure']",Like BEAM-4067 but for Python.,Python: FlinkPortableTestRunner: runs portably via self-started local Flink,2,,,jkff,True,,jkff
beam,BEAM-4210,2018-04-30T23:54:14.000+0000,,2018-04-30T23:54:14.000+0000,,,New Feature,Major,,,,,,,,,['sdk-py-core'],['SDKs: Python'],Like https://issues.apache.org/jira/browse/BEAM-4209 but for the Python portable runner,Python: PortableRunner: p.run() starts a given JobService docker container and runs through it,1,,,jkff,True,,jkff
beam,BEAM-4209,2018-04-30T23:53:22.000+0000,,2018-04-30T23:53:22.000+0000,,,New Feature,Major,,,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","This makes p.run() a self-sufficient entry point for the use case ""I have a cluster of runner Y on-premises and want to run a Beam pipeline against it"".

E.g. BEAM-4170 tracks creation of such a container for Flink.",Java: PortableRunner: p.run() starts a given JobService docker container and runs through it,1,,,jkff,True,,jkff
beam,BEAM-4208,2018-04-30T23:45:59.000+0000,,2018-04-30T23:45:59.000+0000,,,New Feature,Major,,,,,,,,,['testing'],['Testing: general infrastructure'],"This is a helper task so that we can early identify more things that need to be fixed and parallelize that work more effectively.

 

This will be rendered obsolete when we have fully set up ValidatesRunner tests for portable runners on Jenkins (Jira TBD).", Python: Script for running a set of ValidatesRunner tests against given JobService,1,,,jkff,True,,jkff
beam,BEAM-4207,2018-04-30T23:45:29.000+0000,,2018-04-30T23:45:29.000+0000,,,New Feature,Major,,,,,,,,,['testing'],['Testing: general infrastructure'],"This is a helper task so that we can early identify more things that need to be fixed and parallelize that work more effectively.

 

This will be rendered obsolete when we have fully set up ValidatesRunner tests for portable runners on Jenkins (Jira TBD).",Java: script for running set of ValidatesRunner tests against given JobService,1,,,jkff,True,,jkff
beam,BEAM-4206,2018-04-30T23:43:10.000+0000,2018-07-12T21:50:44.000+0000,2018-07-12T21:50:44.000+0000,,Fixed,New Feature,Major,['2.6.0'],,,,,,,,"['runner-flink', 'sdk-py-core']","['Runners: Flink runner', 'SDKs: Python']","A milestone where it is possible to manually start a Flink cluster, a Flink JobService, and get the Java SDK to run a WordCount against that.

 

Corresponds to having carried everything from the hacking branch to master.", Python: WordCount runs against manually started Flink at master,1,,,jkff,True,axelmagn,jkff
beam,BEAM-4205,2018-04-30T23:42:10.000+0000,2018-07-12T21:50:52.000+0000,2018-07-12T21:50:52.000+0000,,Fixed,New Feature,Major,['2.6.0'],,,,,,,,"['runner-flink', 'sdk-java-core']","['Runners: Flink runner', 'SDKs: Java: pipeline construction, core transformations']","A milestone where it is possible to manually start a Flink cluster, a Flink JobService, and get the Java SDK to run a WordCount against that.",Java: WordCount runs against manually started Flink at master,1,,,jkff,True,angoenka,jkff
beam,BEAM-4204,2018-04-30T23:38:41.000+0000,2018-07-02T23:24:47.000+0000,2018-07-02T23:24:47.000+0000,,Fixed,New Feature,Major,['2.5.0'],,9600,9600,,,,100,['sdk-py-core'],['SDKs: Python'],Like BEAM-4071 but for Python. Is this fully encompassed by [https://github.com/apache/beam/blob/master/sdks/python/apache_beam/runners/portability/universal_local_runner.py] ? ,Python: PortableRunner - p.run() via given JobService,2,,,jkff,True,angoenka,jkff
beam,BEAM-4203,2018-04-30T23:21:50.000+0000,,2018-04-30T23:21:58.000+0000,,,New Feature,Major,,,,,,,,,['dsl-sql'],['DSLs: SQL'],We currently only support the collect operation in BeamEnumerableConverter for the direct runner. We should add ways to do this for the other runners too.,Handle collect operation in BeamEnumerableConverter for all runners,2,,,apilloud,True,,apilloud
beam,BEAM-4202,2018-04-30T23:19:46.000+0000,,2018-04-30T23:19:55.000+0000,,,New Feature,Major,,,,,,,,,['dsl-sql'],['DSLs: SQL'],"We are currently using attempted metrics for JDBC counts. This may produce the wrong result.

Per Kenn's Comment:

We probably want to spend some time on the best way to get reliable counts that are meaningful in the way that JDBC expects. I think the SQL <-> IO adapter may have to own it, for those Beam connectors that do things like write ""mutations"" that are not rows at all, etc.",Make JDBC operation counts correct,1,,,apilloud,True,,apilloud
beam,BEAM-4195,2018-04-30T20:05:55.000+0000,,2019-04-30T18:30:34.000+0000,,,New Feature,Major,,,,,,,,,['io-java-gcp'],"['IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)']","Add support for unit testing of Pubsub-related functionality by, for example, wrapping the Pubsub emulator into a Junit test runner or a rule.",Support Pubsub emulator in Junit,1,,,kedin,True,kedin,kedin
beam,BEAM-4194,2018-04-30T18:09:15.000+0000,2018-06-21T23:06:40.000+0000,2018-06-22T21:06:55.000+0000,,Done,New Feature,Major,['2.6.0'],,37200,37200,,,,100,['dsl-sql'],['DSLs: SQL'],"We need to support queries with ""LIMIT xxx"".

Problem is that we don't know when aggregates will trigger, they can potentially accumulate values in global window and never trigger.

If we have some trigger syntax (BEAM-4193), then the use case becomes similar to what we have at the moment, where the user defines the trigger upstream for all inputs. In this case LIMIT probably can be implemented as sample.any(5) with trigger at count.",[SQL] Support LIMIT on Unbounded Data,1,,,kedin,True,amaliujia,kedin
beam,BEAM-4193,2018-04-30T18:02:04.000+0000,,2018-04-30T18:06:36.000+0000,,,New Feature,Major,,,,,,,,,['dsl-sql'],['DSLs: SQL'],"We need to be able to define triggers for the sources in SQL CLI. Otherwise it is impossible to reason about outputs.

One approach is to come up with a simple JSON trigger definition syntax, along the lines of

 
{code:java}
SET OPT( '{ ""trgger"" : { ""DefaultTrigger"" }, ""allowedLateness"" : 0 }' )
{code}
 ",[SQL] Support trigger definition in CLI,1,,,kedin,True,,kedin
beam,BEAM-4190,2018-04-30T16:53:38.000+0000,,2018-05-10T19:48:32.000+0000,,,New Feature,Major,,,,,,,,,['dsl-sql'],['DSLs: SQL'],"Each provider has a dependency on SQL and its respective IO module, so we need to break providers out into their own module by IO module. The SQL CLI also depends on the providers, so we need some way for it to discover providers are actually available.

See: sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider",Don't bake all the table providers into base SQL,1,,,apilloud,True,,apilloud
beam,BEAM-4186,2018-04-29T23:25:37.000+0000,2018-05-05T17:24:20.000+0000,2018-05-05T17:26:23.000+0000,,Won't Fix,New Feature,Minor,['2.4.0'],['2.4.0'],4200,4200,,,,100,['io-java-gcp'],"['IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)']","I want to add a method

      withQuerySplitter(QuerySplitter querySplitter)

to DatastoreV1.Reader.  The implementation is fairly straightforward, except for enforcing the requirement that the query splitter must be Serializable for this to work.

 

 ",Need to be able to set QuerySplitter in DatastoreIO.v1(),1,,,fyellin,True,fyellin,fyellin
beam,BEAM-4183,2018-04-27T19:12:20.000+0000,,2018-04-27T19:12:20.000+0000,,,New Feature,Major,,,,,,,,,['dsl-sql'],['DSLs: SQL'],"Document supported DDL with all supported types, parameters, and properties for each table type",[SQL] Document supported DDL,1,,,kedin,True,,kedin
beam,BEAM-4174,2018-04-25T21:19:50.000+0000,2018-06-18T21:08:40.000+0000,2018-06-18T21:08:40.000+0000,,Fixed,New Feature,Major,['Not applicable'],,,,,,,,['dsl-sql'],['DSLs: SQL'],"We have a MockedBoundedTable which is an in-memory implementation of BeamSqlTable.

We should support it in DDL and use it for testing of DDL/DML/CLI.",[SQL] Add DDL Support for MockedBoundedTable,1,,,kedin,True,kedin,kedin
beam,BEAM-4173,2018-04-25T21:13:06.000+0000,2018-06-18T21:10:03.000+0000,2018-06-18T21:10:03.000+0000,,Fixed,New Feature,Major,['Not applicable'],,,,,,,,['dsl-sql'],['DSLs: SQL'],"BeamSql is a single method which delegates to QueryTransform factory method which creates BeamSqlEnv which creates BeamSqlPlanner which then configures the parser and parses the query.

It looks like we can squash together a lot of it by:

 - replacing BeamSql invocations with direct QueryTransform invocations;

 - combining BeamSqlEnv with BeamSqlPlanner or extracting a higher level configuration object;

 - exposing few more QueryTransform builders to accept either planner or a configuration object;

 - building QueryTransforms in BeamSqlCli;","[SQL] Refactor BeamSql, BeamSqlCli, BeamSqlEnv",1,,,kedin,True,kedin,kedin
beam,BEAM-4168,2018-04-24T20:41:54.000+0000,,2018-04-24T20:41:54.000+0000,,,New Feature,Major,,,,,,,,,['dsl-sql'],['DSLs: SQL'],Create a SQL example which we can execute and verify in runners other than DirectRunner,"[SQL] Create a ""WordCount"" example",1,,,kedin,True,,kedin
beam,BEAM-4167,2018-04-24T20:16:58.000+0000,2018-05-29T22:34:17.000+0000,2018-05-29T22:34:18.000+0000,,Fixed,New Feature,Major,['2.5.0'],,15600,15600,,,,100,['dsl-sql'],['DSLs: SQL'],We need to be able to convert collections to relations in the query to perform any meaningful operations on them. ,Implement UNNEST,1,,,kedin,True,kenn,kedin
beam,BEAM-4162,2018-04-24T00:03:51.000+0000,2018-05-23T03:42:29.000+0000,2018-05-23T03:42:29.000+0000,,Fixed,New Feature,Major,['Not applicable'],,22200,22200,,,,100,['dsl-sql'],['DSLs: SQL'],"Read JSON messages from Pubsub, convert them to Rows (BEAM-4160), wire up to Beam SQL.

 

Use publication time as event timestamp",Wire up PubsubIO+JSON to Beam SQL,1,,,kedin,True,kedin,kedin
beam,BEAM-4160,2018-04-23T23:46:12.000+0000,2018-04-25T19:44:58.000+0000,2018-04-25T19:44:58.000+0000,,Fixed,New Feature,Major,['Not applicable'],,4200,4200,,,,100,"['dsl-sql', 'sdk-java-core']","['DSLs: SQL', 'SDKs: Java: pipeline construction, core transformations']",Automate conversion of JSON objects to Rows to reduce overhead for querying JSON-based sources,Convert JSON objects to Rows,1,,,kedin,True,kedin,kedin
beam,BEAM-4147,2018-04-19T22:23:21.000+0000,2018-08-01T22:30:34.000+0000,2018-08-01T22:30:34.000+0000,,Resolved,New Feature,Major,['2.6.0'],,,85200,,,,,['runner-core'],['Runners: shared functionality for all runners'],We need a way to wire in arbitrary runner artifact storage backends into the job server and through to artifact staging on workers. This requires some new abstractions in front of the job service.,Abstractions for artifact delivery via arbitrary storage backends,3,,,bsidhom,True,axelmagn,bsidhom
beam,BEAM-4145,2018-04-19T21:28:33.000+0000,2018-06-21T19:39:52.000+0000,2018-06-21T19:39:52.000+0000,,Fixed,New Feature,Minor,['2.6.0'],,17400,17400,,,,100,['sdk-java-harness'],['SDKs: Java: harness for executing UDFs over the Fn API'],"Runner code needs to be able to identify incoming harness connections by the worker ids that it assigns to them on creation. This is currently done by the go boot code when the harness runs in a docker container. However, in-process harnesses never specify worker ids. This prevents in-process harnesses from being multiplexed by a runner (most likely the ULR and test code).",Java SDK Harness populates control request headers with worker id,2,,,bsidhom,True,jkff,bsidhom
beam,BEAM-4135,2018-04-19T01:15:48.000+0000,2018-06-01T18:19:29.000+0000,2018-06-01T18:19:29.000+0000,,Fixed,New Feature,Major,['Not applicable'],,9000,9000,,,,100,['runner-direct'],['Runners: Direct runner and ULR for single machine testing and development'],"The ""engine"" consists of the components which determine where to schedule work and route it to the appropriate processors, such as WatermarkManager, DirectBundleProcessor, and associated.

 

These engine components never inspect the actual characteristics of the packaged work (e.g. the PCollection is a token, rather than a rich object), so they should not require use of a PCollection directly - instead, they can be generic.","Remove Use of Java SDK Types in the DirectRunner ""engine""",1,,['portability'],tgroh,True,tgroh,tgroh
beam,BEAM-4131,2018-04-18T23:34:09.000+0000,2018-05-01T05:05:20.000+0000,2018-05-01T05:05:20.000+0000,,Fixed,New Feature,Minor,['2.5.0'],,9600,9600,,,,100,['sdk-py-harness'],['SDKs: Python: harness for executing UDFs over the Fn API'],"The Docker image for the SDK harness should contain SDK code and dependencies so that this does not need to be downloaded from the artifact retrieval service at each boot.

This is required for operation with portable runners right now because the python client does not currently stage the SDK itself (as it does with the Dataflow runner).",Python SDK harness container image contains SDK and dependencies,2,,,bsidhom,True,thw,bsidhom
beam,BEAM-4130,2018-04-18T23:16:02.000+0000,2018-08-24T18:31:26.000+0000,2018-10-17T01:14:43.000+0000,,Fixed,New Feature,Minor,['2.7.0'],,52200,52200,,,,100,['runner-flink'],['Runners: Flink runner'],The portable Flink runner exists as a Job Service that runs somewhere. We need a main entry point that itself spins up the job service (and artifact staging service). The main program itself should be packaged into an uberjar such that it can be run locally or submitted to a Flink deployment via `flink run`.,Portable Flink runner JobService entry point in a Docker container,7,,,bsidhom,True,mxm,bsidhom
beam,BEAM-4128,2018-04-18T22:52:54.000+0000,2018-12-20T11:24:50.000+0000,2018-12-20T11:25:40.000+0000,,Not A Problem,New Feature,Major,['Not applicable'],,,,,,,,['runner-flink'],['Runners: Flink runner'],"This is mostly an optimization, but by having the runner itself handle well-known WindowFns (e.g., global windowing, fixed windowing), we can avoid round trips to the SDK harness. This requires some pipeline surgery pre-translation if we want it to work efficiently in conjunction with fusion.",Handle WindowInto transforms in portable Flink runner directly,2,,,bsidhom,True,aljoscha,bsidhom
beam,BEAM-4127,2018-04-18T22:36:50.000+0000,2018-05-16T02:37:22.000+0000,2018-05-18T03:06:41.000+0000,,Implemented,New Feature,Major,['Not applicable'],,4800,4800,,,,100,['runner-flink'],['Runners: Flink runner'],,Flink portable runner translates streaming pipelines by proto,2,,['portability'],bsidhom,True,thw,bsidhom
beam,BEAM-4125,2018-04-18T22:24:12.000+0000,2018-05-03T21:59:36.000+0000,2018-05-03T21:59:36.000+0000,,Fixed,New Feature,Major,['Not applicable'],,8400,8400,,,,100,['runner-core'],['Runners: shared functionality for all runners'],"This is important for a transform which includes in-environment transforms (such as a lifted Combine), or for runners which use the beam representation as their internal representation (such as the directrunner)",Add a library to manipulate the proto representation of a pipeline,1,,['portability'],tgroh,True,tgroh,tgroh
beam,BEAM-4123,2018-04-18T20:53:01.000+0000,,2018-04-18T20:53:01.000+0000,,,New Feature,Major,,,,,,,,,['runner-core'],['Runners: shared functionality for all runners'],,SDK Harness Environments should be able to report health.,1,,,tgroh,True,,tgroh
beam,BEAM-4093,2018-04-16T21:51:43.000+0000,,2019-04-30T18:30:47.000+0000,,,New Feature,Major,,,6000,6000,,,,100,"['sdk-py-core', 'testing']","['SDKs: Python', 'Testing: general infrastructure']",,Support Python ValidatesRunner test against TestDataflowRunner in streaming,1,,,markflyhigh,True,markflyhigh,markflyhigh
beam,BEAM-4092,2018-04-16T21:49:52.000+0000,2018-04-16T21:54:51.000+0000,2018-04-16T21:54:51.000+0000,,Duplicate,New Feature,Major,['Not applicable'],,,,,,,,"['sdk-py-core', 'testing']","['SDKs: Python', 'Testing: general infrastructure']",,Enable Python ValidatesRunner test against TestDataflowRunner in streaming,1,,,markflyhigh,True,markflyhigh,markflyhigh
beam,BEAM-4089,2018-04-16T16:49:35.000+0000,2018-05-17T14:41:03.000+0000,2018-05-17T14:41:04.000+0000,,Duplicate,New Feature,Major,['Not applicable'],,,,,,,,['testing-nexmark'],['Testing: Nexmark queries'],,Report Nexmark runs to BQ dashboard for anomoly detection,,,,kenn,True,echauchot,kenn
beam,BEAM-4071,2018-04-13T19:31:29.000+0000,2018-04-26T00:03:05.000+0000,2018-05-03T03:06:56.000+0000,,Fixed,New Feature,Minor,['2.5.0'],,69600,69600,,,,100,['runner-core'],['Runners: shared functionality for all runners'],"There needs to be a way to execute Java-SDK pipelines against a portable job server. The job server itself is expected to be started up out-of-band. The ""PortableRunner"" should take an option indicating the Job API endpoint and defer other runner configurations to the backend itself.",Java: PortableRunner - p.run() via given JobService,2,,,bsidhom,True,bsidhom,bsidhom
beam,BEAM-4068,2018-04-13T17:18:37.000+0000,,2018-04-25T02:21:37.000+0000,,,New Feature,Minor,,,,,,,,,['runner-core'],['Runners: shared functionality for all runners'],"Pipeline options are materialized differently by different SDKs. However, in some cases, runners require Java-specific options that are not available elsewhere. We should decide on well-known URNs and use them across SDKs where applicable.",Consistent option specification between SDKs and runners by URN,1,,,bsidhom,True,,bsidhom
beam,BEAM-4067,2018-04-13T17:10:10.000+0000,2018-08-01T22:03:00.000+0000,2018-08-01T22:03:00.000+0000,,Fixed,New Feature,Minor,['2.7.0'],,,,,,,,['runner-flink'],['Runners: Flink runner'],The portable Flink runner cannot be tested through the normal mechanisms used for ValidatesRunner tests because it requires a job server to be constructed out of band and for pipelines to be run through it. We should implement a shim that acts as a standard Java SDK Runner that spins up the necessary server (possibly in-process) and runs against it.,Java: FlinkPortableTestRunner: runs portably via self-started local Flink,5,,,bsidhom,True,angoenka,bsidhom
beam,BEAM-4063,2018-04-13T01:02:25.000+0000,,2018-04-30T23:17:57.000+0000,,,New Feature,Minor,,,,,,,,,['runner-flink'],['Runners: Flink runner'],"As of now, Flink effectively has a dependency on an external storage system for artifact management. This is because the Flink Distributed Cache does not actually distribute and cache blobs itself, but rather expects that each node in a running cluster has access to a well-known artifact resource.

We should get this for free whenever [https://github.com/apache/flink/pull/5580] is merged (likely in 1.5). For now, we will have to defer to external storage systems like GCS or HDFS.",Flink runner supports cluster-wide artifact deployments through the Distributed Cache,3,,,bsidhom,True,,bsidhom
beam,BEAM-4056,2018-04-12T01:47:50.000+0000,2018-04-18T19:26:03.000+0000,2018-04-18T20:03:22.000+0000,,Fixed,New Feature,Minor,['Not applicable'],,10800,10800,,,,100,['runner-core'],['Runners: shared functionality for all runners'],"This is necessary in order to correctly identify side inputs during all phases of portable pipeline execution (fusion, translation, and SDK execution).",Identify Side Inputs by PTransform ID and local name,2,,,bsidhom,True,bsidhom,bsidhom
beam,BEAM-4053,2018-04-11T17:27:47.000+0000,2018-04-24T18:32:49.000+0000,2018-04-24T18:32:49.000+0000,,Fixed,New Feature,Major,['2.5.0'],,6000,6000,,,,100,['build-system'],"['Build, CI, release systems and processes']","To allow assessing if it is reliable, there should be a Go postcommit, initially the same as the precommit.",Go should have a postcommit run on a cron schedule,1,,,alanmyrvold,True,alanmyrvold,alanmyrvold
beam,BEAM-4044,2018-04-10T17:31:27.000+0000,2018-05-10T18:02:21.000+0000,2018-05-10T18:02:21.000+0000,,Fixed,New Feature,Major,['2.5.0'],,58800,58800,,,,100,['dsl-sql'],['DSLs: SQL'],In Calcite 1.15 support for abstract DDL moved into calcite core. We should take advantage of that.,Take advantage of Calcite DDL,1,,,apilloud,True,apilloud,apilloud
beam,BEAM-4038,2018-04-10T01:31:31.000+0000,2018-10-01T21:15:28.000+0000,2018-11-14T14:03:39.000+0000,,Fixed,New Feature,Major,['Not applicable'],,36600,36600,,,,100,['io-java-kafka'],['IO: Java: Kafka'],"Headers have been added to Kafka Consumer/Producer records (KAFKA-4208). The purpose of this JIRA is to support this feature in KafkaIO.  

 ",Support Kafka Headers in KafkaIO,5,,,gkumar7,True,gkumar7,gkumar7
beam,BEAM-4020,2018-04-05T15:40:09.000+0000,2018-07-12T09:29:43.000+0000,2018-07-12T09:29:43.000+0000,,Fixed,New Feature,Minor,['2.6.0'],,16800,16800,,,,100,['io-java-hbase'],['IO: Java: HBase'],"Since the support from runners is still limited, it is probably wise to create a first IO based on the current SDF batch implementation in Java to validate/test it with a real data-store. Since HBase partitioning model is quite straightforward it is a perfect candidate.",Add HBaseIO.readAll() based on SDF,1,,,iemejia,True,iemejia,iemejia
beam,BEAM-4018,2018-04-05T15:37:10.000+0000,2018-04-24T12:04:47.000+0000,2018-09-24T20:26:11.000+0000,,Fixed,New Feature,Minor,['2.5.0'],,26400,26400,,,,100,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","We can have a RestrictionTracker for ByteKey ranges as part of the core sdk so it can be reused by future SDF based IOs like Bigtable, HBase among others.",Add a ByteKeyRangeTracker based on RestrictionTracker for SDF,1,,,iemejia,True,iemejia,iemejia
beam,BEAM-3983,2018-04-02T19:52:21.000+0000,2018-06-18T21:05:06.000+0000,2018-06-18T21:05:06.000+0000,,Fixed,New Feature,Major,['Not applicable'],,120000,120000,,,,100,['dsl-sql'],['DSLs: SQL'],"It would be nice if you could write to BigQuery in SQL without writing any java code. For example:
{code:java}
INSERT INTO bigquery SELECT * FROM PCOLLECTION{code}",BigQuery writes from pure SQL,1,,,apilloud,True,kedin,apilloud
beam,BEAM-3923,2018-03-23T22:00:04.000+0000,,2019-04-30T18:32:25.000+0000,,,New Feature,Major,,,,,,,,,"['sdk-java-core', 'sdk-java-harness', 'sdk-py-core']","['SDKs: Java: pipeline construction, core transformations', 'SDKs: Java: harness for executing UDFs over the Fn API', 'SDKs: Python']","Task here is the do research and perform updates necessary to run a simple Java transform that does not require expansion through Beam portability offering. Initially we'll try to get this working for ULR and will try to expand into other runners later.

This will also including coming up with a reasonable mechanism to define a shim in SDK A for a transform that is fully defined in SDK B.

This should be combined with efforts to introduce mechanisms to expand cross platform transforms (see [1]) to achieve a full cross language transform offering which will, for example, allow IOs defined in SDK B to be used in SDK A.

[1] [https://s.apache.org/beam-mixed-language-pipelines]",Define and add updates needed to run a simple Java transform using Python SDK on top of the reference runner (ULR),1,,,chamikara,True,chamikara,chamikara
beam,BEAM-3921,2018-03-23T10:59:04.000+0000,,2019-04-30T18:32:10.000+0000,,,New Feature,Minor,,['2.5.0'],9000,9000,,,,100,['sdk-ideas'],['SDKs: Coordination on new language-specific SDKs'],An extension with transforms that package the Java Scripting API (JSR-223) [1] to allow users to specialize some transforms via a scripting language. It supports ValueProviders so users can template their scripts also in Dataflow.,Scripting extension based on Java Scripting API (JSR-223),1,,,iemejia,True,iemejia,iemejia
beam,BEAM-3916,2018-03-22T19:32:00.000+0000,,2019-02-07T04:42:37.000+0000,,,New Feature,Major,,,,,,,,,['io-java-gcp'],"['IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)']","BigtableIO currently uses ValueProviders for table, instance, and project IDs.  It should also use ValueProviders for KeyRange and RowFilter to allow templates to be created that can parameterize these values.",BigtableIO should use ValueProviders for KeyRange and RowFilter,1,,,herbertrlee,True,,herbertrlee
beam,BEAM-3910,2018-03-22T01:09:42.000+0000,2018-04-08T16:27:39.000+0000,2018-04-08T16:27:39.000+0000,,Fixed,New Feature,Major,['Not applicable'],,5400,5400,81000,86400,81000,6,['sdk-go'],['SDKs: Go'],"The Go SDK supports all the integer types of the language, but does not support floats.

My plan for coding is to use the same technique the gob package uses, which results in a compact encoding for simple values.

[https://golang.org/src/encoding/gob/encode.go?#L210|https://golang.org/src/encoding/gob/encode.go#L210] with rationale explained in https://golang.org/pkg/encoding/gob/#hdr-Encoding_Details

The resulting uint is then encoded using the existing coders in coderx.",Support floating point values in Go SDK,2,,,wcn3,True,wcn3,wcn3
beam,BEAM-3900,2018-03-21T15:22:19.000+0000,2018-12-14T09:58:06.000+0000,2018-12-14T09:58:06.000+0000,,Fixed,New Feature,Major,['2.9.0'],,13800,66600,,,,100,['dsl-euphoria'],['DSLs: Euphoria'],This is the umbrella issue for integrating [Euphoria API|http://github.com/seznam/euphoria] into Beam.,Introduce Euphoria Java 8 DSL,2,1,,davidmoravek,True,davidmoravek,davidmoravek
beam,BEAM-3867,2018-03-16T22:13:25.000+0000,,2019-04-30T18:33:16.000+0000,,,New Feature,Major,,,2400,2400,,,,100,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","See below for a thread regarding this.

https://lists.apache.org/thread.html/eb006101b9d3cb6bf88ec1f8f29e2aaec96ab5760fb7041336fa541a@%3Cdev.beam.apache.org%3E",Add support for decompressing and reading tar files,2,,,chamikara,True,eachsaj,chamikara
beam,BEAM-3857,2018-03-15T00:01:10.000+0000,,2019-02-07T04:42:47.000+0000,,,New Feature,Major,,,,,,,,,"['io-ideas', 'sdk-py-core']","['IO: ideas (proposals for new connectors, new designs, etc)', 'SDKs: Python']","If possible, I would like to have a dynamic output support for python as available in Java

 ",Dynamic output support for python,1,,,eilalan,True,,eilalan
beam,BEAM-3852,2018-03-14T18:24:48.000+0000,2019-04-05T14:13:23.000+0000,2019-04-30T18:34:18.000+0000,,Fixed,New Feature,Minor,['Not applicable'],,4800,4800,,,,100,['website'],['Website: content and bugs'],"Channel is https://the-asf.slack.com/messages/C9H0YNP3P/

Created short link for #beam channel directly: [https://s.apache.org/beam-slack-channel]

Created short link for self-enrollment: https://s.apache.org/slack-invite",Migrate existing users to new channel ASF slack channel,3,,,lcwik,True,,lcwik
beam,BEAM-3850,2018-03-14T17:51:38.000+0000,,2018-03-23T22:04:41.000+0000,,,New Feature,Major,,,,,,,,,"['io-ideas', 'sdk-py-core']","['IO: ideas (proposals for new connectors, new designs, etc)', 'SDKs: Python']","Following the great summit today, I would like to ask for I/O transform for H5 file in python. If impossible, Java will work as well.

The HDF5 group is very accessible: [https://support.hdfgroup.org/HDF5/]

Example for H5 file can be found @ Mount Sinai Maayan lab: [https://amp.pharm.mssm.edu/archs4/download.html]

I am using it as part of Oriel Research genomic and clinical processing workflow.

I am available for any question.

 ",I/O transform for HDH5 files with extension H5,4,,,eilalan,True,,eilalan
beam,BEAM-3818,2018-03-09T09:01:16.000+0000,2018-04-14T00:27:17.000+0000,2018-04-14T00:27:17.000+0000,,Fixed,New Feature,Minor,['2.5.0'],,40200,40200,,,,100,['sdk-py-core'],['SDKs: Python'],"The streaming DirectRunner should support streaming side input semantics.  Currently, side inputs are only available for globally-windowed side input PCollections.

Also, empty side inputs cause a pipeline stall.",Add support for the streaming side inputs in the Python DirectRunner,1,,,mariagh,True,mariagh,mariagh
beam,BEAM-3788,2018-03-06T22:43:56.000+0000,,2019-06-26T17:48:50.000+0000,,,New Feature,Major,,,,,,,,,['sdk-py-core'],['SDKs: Python'],This will be implemented using the Splittable DoFn framework.,Implement a Kafka IO for Python SDK,14,6,,chamikara,True,,chamikara
beam,BEAM-3784,2018-03-06T04:39:24.000+0000,,2019-06-06T23:25:48.000+0000,,,New Feature,Minor,,,,,,,,,['sdk-ideas'],['SDKs: Coordination on new language-specific SDKs'],"Apache Zeppelin includes an integration with Apache Beam: https://zeppelin.apache.org/docs/0.7.0/interpreter/beam.html

How well does this work for interactive exploration? Can this be enhanced to support Beam SQL? What about unbounded data? Let's find out by exploring the existing interpreter and enhancing it particularly for streaming SQL.

This project will require the ability to read, write, and run Java and SQL. You will come out of it with familiarity with two Apache big data projects and lots of ideas!",Enhance Apache Beam interpreter for Apache Zeppelin,3,,"['SQL', 'bigdata', 'cloud', 'gsoc2018', 'java']",kenn,True,,kenn
beam,BEAM-3783,2018-03-06T04:29:41.000+0000,,2019-04-30T18:29:27.000+0000,,,New Feature,Major,,,,,,,,,['testing-nexmark'],['Testing: Nexmark queries'],"Beam has a number of classic streaming SQL benchmarks known as ""Nexmark"" coded up in both raw Java and also Beam SQL.

So far, expanding functionality has been the focus of Beam SQL so there is little known about performance - we know only that it is a pretty straightforward mapping from SQL to Beam that should work OK a lot of the time. It would be interesting to see where the bottlenecks are when these SQL benchmarks are translated via Beam SQL into a Beam pipeline and then again translated to the native capabilities of e.g. Spark and Flink.

This project will require the ability to read, write, and run Java and SQL.",Streaming Beam SQL benchmarks on all of our runners,4,1,"['SQL', 'bigdata', 'cloud', 'gsoc2018', 'java']",kenn,True,vectorijk,kenn
beam,BEAM-3767,2018-03-01T17:38:55.000+0000,,2018-04-25T07:30:04.000+0000,,,New Feature,Minor,,,,,,,,,['sdk-ideas'],['SDKs: Coordination on new language-specific SDKs'],"Apache Beam [1] is a unified and portable programming model for data processing jobs. The Beam model [2, 3, 4] has rich mechanisms to process endless streams of events.

Complex Event Processing [5] lets you match patterns of events in streams to detect important patterns in data and react to them.

Some examples of uses of CEP are fraud detection for example by detecting unusual behavior (patterns of activity), e.g. network intrusion, suspicious banking transactions, etc. Also trend detection is another interesting use case in the context of sensors and IoT.

The goal of this issue is to implement an efficient pattern matching library inspired by [6] and existing libraries like Apache Flink CEP [7] using the Apache Beam Java SDK and the Beam style guides [8]. Because of the time constraints of GSoC we will probably try to cover first simple patterns of the ‘a followed by b followed by c’ kind, and then if there is still time try to cover more advanced ones e.g. optional, atLeastOne, oneOrMore, etc.

[1] [https://beam.apache.org/]
 [2] [https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101]
 [3] [https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-102]
 [4] [https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43864.pdf]
 [5] [https://en.wikipedia.org/wiki/Complex_event_processing]
 [6] [https://people.cs.umass.edu/~yanlei/publications/sase-sigmod08.pdf]
 [7] [https://ci.apache.org/projects/flink/flink-docs-stable/dev/libs/cep.html]
 [8] [https://beam.apache.org/contribute/ptransform-style-guide/]

 ",A Complex Event Processing (CEP) library/extension for Apache Beam,5,,,iemejia,True,,iemejia
beam,BEAM-3737,2018-02-23T00:08:41.000+0000,,2018-06-09T20:21:39.000+0000,,,New Feature,Major,,,,,,,,,['sdk-py-core'],['SDKs: Python'],"I have a CombineFn for which add_input has very large overhead. I would like to batch the incoming elements into a large batch before each call to add_input to reduce this overhead. In other words, I would like to do something like: 

{{elements | GroupByKey() | BatchElements() | CombineValues(MyCombineFn())}}

Unfortunately, BatchElements is not key-aware, and can't be used after a GroupByKey to batch elements per key. I'm working around this by doing the batching within CombineValues, which makes the CombineFn rather messy. It would be nice if there were a key-aware BatchElements transform which could be used in this context.",Key-aware batching function,6,,,chuanyu,True,,chuanyu
beam,BEAM-3686,2018-02-11T15:11:22.000+0000,,2018-06-21T21:29:51.000+0000,,,New Feature,Major,,,,,,,,,['runner-direct'],['Runners: Direct runner and ULR for single machine testing and development'],,Support stateful processing in merging windows in Java DirectRunner,2,,,kenn,True,,kenn
beam,BEAM-3682,2018-02-09T22:20:15.000+0000,,2019-02-07T04:47:45.000+0000,,,New Feature,Minor,,,,,,,,,['website'],['Website: content and bugs'],"The gray color used for comments in the snippets is hard to read. Gray is OK, just not as light.",Hard to read snippet comments in the programming guide,1,,,mariagh,True,,mariagh
beam,BEAM-3678,2018-02-09T16:36:08.000+0000,,2019-02-07T04:47:46.000+0000,,,New Feature,Minor,,,,,,,,,['website'],['Website: content and bugs'],"The possibility to build pipelines using python data structures instead of PCollections is not contemplated in the programming guide, and it should.","Document ""eager mode"" for the Python SDK DirectRunner in the website",1,,,mariagh,True,,mariagh
beam,BEAM-3671,2018-02-08T23:23:00.000+0000,2018-03-06T00:04:10.000+0000,2018-03-06T18:28:00.000+0000,,Fixed,New Feature,Major,['Not applicable'],,43800,43800,,,,100,['testing'],['Testing: general infrastructure'],"This is required for being able to test with Py3. If possible, it would be best to have 3.5 and 3.6. If not we can settle on a single version.",Need Python 3.x interpreter in Jenkins machines,3,1,,altay,True,alanmyrvold,altay
beam,BEAM-3648,2018-02-08T15:14:54.000+0000,,2019-04-30T18:29:36.000+0000,,,New Feature,Major,,,10800,10800,,,,100,['runner-flink'],['Runners: Flink runner'],,Support Splittable DoFn in Flink Batch Runner,4,,,aljoscha,True,aljoscha,aljoscha
beam,BEAM-3647,2018-02-08T10:01:02.000+0000,,2019-06-06T23:10:53.000+0000,,,New Feature,Major,,['2.2.0'],,,,,,,"['beam-model', 'dsl-sql']","['Beam Model: general programming model concepts, semantics', 'DSLs: SQL']","*Requirement*-: Need to Run Template With Same Logics on Different Tables Data.(Example is Given Below)

 

*Need*: Default Coder is Required So According to Data It Make All Fields as String and Read Data else Thier must be Dynamic Options to Read Coder From GCS as JSON FILE and Parse Data on Basis of That (But We can Pass Location Using ValueProvider) or SomeWhere Else so At Runtime Using ValueProvider.

 

 

*Examples*: I Have Two Tables 1 is Having Column (NAME, CLASS, ROLL, SUB_PRICE)

And 2 Table is (NAME, ROLL, SUB, TEST_MARKS)

 

On Both Tables, I am Just Sorting Table on Basis Of Roll Number so if We can Read Coder at Run Time The Same Template Can Be Used For Different Tables at Run Time.

 

Such Situations Make Our Work Easy and Make Our job Easy.

 ",Default Coder/Reading Coder From File ,3,,,KishanK,True,kedin,KishanK
beam,BEAM-3615,2018-02-03T08:10:53.000+0000,2018-02-03T20:33:56.000+0000,2019-06-10T09:19:45.000+0000,,Not A Problem,New Feature,Critical,['2.1.0'],['2.2.0'],,,,,,,['dsl-sql'],['DSLs: SQL'],"We need to Define Coder Statically in Every Job While Using Beam SQL Like in Previous Version 2.1.0 We Were Reading Avro Coder Files Statically But in 2.2.0 Now We can Read Dynamically Like that only Feature Needed to Introduced So We can Define Dynamically at Run Time Reading From GCS or Table(Table Schema) The Coder So Transformation can Be Performed. Which Makes Dataflow Template More Efficiend For Single Type of Job We can Use it Multiple Times Now We Need to Make Multiple Templates For Different Jobs.

 

 ",Dynamic/Default Coder For Data,2,,"['newbie', 'performance']",KishanK,True,jkff,KishanK
beam,BEAM-3609,2018-02-02T18:26:37.000+0000,,2018-02-02T18:26:37.000+0000,,,New Feature,Major,,,,,,,,,['dsl-sql'],['DSLs: SQL'],"Consider supporting existing UDF libraries, e.g.:

https://github.com/Esri/spatial-framework-for-hadoop/blob/master/hive/src/main/java/com/esri/hadoop/hive/ST_Aggr_Union.java

Will probably need to implement a layer on top of https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/exec/UDF.java",[SQL] Support Hive UDFs,3,,,kedin,True,,kedin
beam,BEAM-3548,2018-01-26T23:57:41.000+0000,,2018-01-26T23:57:41.000+0000,,,New Feature,Major,,,,,,,,,['dsl-sql'],['DSLs: SQL'],"Create a fluid query builder.

 

*Problem*:

SQL queries as java strings are very hard to read and debug. See examples below.

 

*Solution*:

Fluid query builder, along the lines of:
{code:java}
QueryTransform query = 
  QueryBuilder
    .from(pcollection)
    .where(e -> e.id == 3)
    .groupBy(e -> e.name)
    .select();{code}
 

*Examples:*
{code:java|title=BeamSqlAggregationTest.java}
    String sql = ""select f_int2, count(*) as getFieldCount, ""
        + ""sum(f_long) as sum1, avg(f_long) as avg1, max(f_long) as max1, min(f_long) as min1, ""
        + ""sum(f_short) as sum2, avg(f_short) as avg2, max(f_short) as max2, min(f_short) as min2, ""
        + ""sum(f_byte) as sum3, avg(f_byte) as avg3, max(f_byte) as max3, min(f_byte) as min3, ""
        + ""sum(f_float) as sum4, avg(f_float) as avg4, max(f_float) as max4, min(f_float) as min4, ""
        + ""sum(f_double) as sum5, avg(f_double) as avg5, ""
        + ""max(f_double) as max5, min(f_double) as min5, ""
        + ""max(f_timestamp) as max6, min(f_timestamp) as min6, ""
        + ""var_pop(f_double) as varpop1, var_samp(f_double) as varsamp1, ""
        + ""var_pop(f_int) as varpop2, var_samp(f_int) as varsamp2 ""
        + ""FROM TABLE_A group by f_int2"";
{code}
or
{code:java|title=BeamSetOperatorRelBaseTest.java}
    String sql = ""SELECT ""
        + "" order_id, site_id, count(*) as cnt ""
        + ""FROM ORDER_DETAILS GROUP BY order_id, site_id""
        + "", TUMBLE(order_time, INTERVAL '1' HOUR) ""
        + "" UNION SELECT ""
        + "" order_id, site_id, count(*) as cnt ""
        + ""FROM ORDER_DETAILS GROUP BY order_id, site_id""
        + "", TUMBLE(order_time, INTERVAL '2' HOUR) "";
{code}
or
{code:java|title=BeamSqlCliTest.java}
    cli.execute(
        ""create table person (\n""
        + ""id int COMMENT 'id', \n""
        + ""name varchar(31) COMMENT 'name', \n""
        + ""age int COMMENT 'age') \n""
        + ""TYPE 'text' \n""
        + ""COMMENT '' LOCATION 'text://home/admin/orders'""
    );
{code}",[SQL][Java] Fluid query builder,1,,,kedin,True,,kedin
beam,BEAM-3509,2018-01-22T11:55:03.000+0000,,2019-06-12T08:51:36.000+0000,,,New Feature,Major,,['2.2.0'],,,,,,,['dsl-sql'],['DSLs: SQL'],"Partition By Option Will Be Very Help Full for DataFlow Developer To Migrate Query and Do Transformation on That because of Many *Netezza Query and Oracle Query* Consists Of Partition By Which Makes SQL Query More Efficient. *The alternative is Making Joins And Filtering It Can Be Done But It Makes Code Unreadable And Performance Become bad for DataFlow Job.*

Examples: SELECT MIN(COLUMN) OVER (PARTITION BY COLUMN NAME) FROM TABLENAME",PARTITION BY in Beam SQL In Select Command,3,,['performance'],KishanK,True,,KishanK
beam,BEAM-3503,2018-01-19T21:00:41.000+0000,,2019-02-07T04:47:49.000+0000,,,New Feature,Minor,,['2.2.0'],,,,,,,['io-java-gcp'],"['IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)']","PubsubIO does not support the ""DynamicDestinations"" notion that is currently implemented for File-based I/O and BigQueryIO.

It would be nice if PubsubIO could also support this functionality - the ability to write to a Pub/Sub topic dynamically.",PubsubIO - DynamicDestinations,3,,,Nalseez,True,,Nalseez
beam,BEAM-3489,2018-01-17T18:50:54.000+0000,,2019-07-09T18:17:15.000+0000,,,New Feature,Minor,,,10800,10800,,,,100,['io-java-gcp'],"['IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)']","This task is about passing forward the message id from the pubsub proto to the java PubsubMessage.

Add a message id field to PubsubMessage.

Update the coder for PubsubMessage to encode the message id.

Update the translation from the Pubsub proto message to the Dataflow message:

https://github.com/apache/beam/blob/2e275264b21db45787833502e5e42907b05e28b8/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/pubsub/PubsubUnboundedSource.java#L976",Expose the message id of received messages within PubsubMessage,8,2,"['newbie', 'starter']",lcwik,True,thinhha,lcwik
beam,BEAM-3475,2018-01-12T21:46:18.000+0000,,2019-06-12T08:57:49.000+0000,,,New Feature,Major,,,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","A good command line interface uses {{--verbose}} or {{--debug}} to control verbosity for users and contributors to get more info about what is going on in the program. Currently IIUC a user has to tweak JUL which is quite a poor experience and Java-centric rather than a good command line interface. We should add support for this alongside our other tools (like pipeline options parsing) for making it easy to build a pipeline {{main()}}.

Note that this is for the construction & submission side / core SDK. Each runner has its own needs as far as how to control the worker logging, though it is probably still better to have a general way.",Support --verbose and/or --debug options so users don't have to tweak logging backend for basic output,1,,['starter'],kenn,True,,kenn
beam,BEAM-3453,2018-01-11T07:10:06.000+0000,,2019-02-07T05:03:36.000+0000,,,New Feature,Major,,['2.2.0'],,,,,,,['sdk-py-core'],['SDKs: Python'],"Currently, the Beam Python DirectRunner does not allow the usage of data from public Google Cloud PubSub topics.  We should allow this functionality so that users can more easily test Beam Python's streaming functionality.",Allow usage of public Google PubSub topics in Python DirectRunner,4,,,ccy,True,,ccy
beam,BEAM-3446,2018-01-10T07:44:27.000+0000,2018-09-19T15:14:27.000+0000,2018-09-19T15:42:58.000+0000,,Fixed,New Feature,Major,['2.8.0'],,22800,22800,,,,100,['io-java-redis'],['IO: Java: Redis'],"Read operation in RedisIO is for prefix based look ups. While this can be used for exact key matches as well, the number of operations limits the through put of the function.
I suggest exposing current readAll operation as readbyprefix and using more simpler operations for readAll functionality.
ex:
{code:java}
String output = jedis.get(element);
if (output != null) {
    processContext.output(KV.of(element, output));
}
{code}
instead of:
https://github.com/apache/beam/blob/7d240c0bb171af6868f1a6e95196c9dcfc9ac640/sdks/java/io/redis/src/main/java/org/apache/beam/sdk/io/redis/RedisIO.java#L292",RedisIO non-prefix read operations,3,,,vvarma,True,vvarma,vvarma
beam,BEAM-3415,2018-01-05T14:40:56.000+0000,,2019-04-30T18:34:05.000+0000,,,New Feature,Major,,,,,,,,,['testing'],['Testing: general infrastructure'],,JUnit5 support,3,,,romain.manni-bucau,True,romain.manni-bucau,romain.manni-bucau
beam,BEAM-3346,2017-12-13T22:47:03.000+0000,2019-01-09T11:04:45.000+0000,2019-01-09T11:04:45.000+0000,,Duplicate,New Feature,Major,['2.10.0'],['2.3.0'],,,172800,172800,172800,,['io-java-mongodb'],['IO: Java: MongoDB'],"The db.collection.find operation for MongoDb allows for the filter parameter, but not for the projection parameter. This would be extremely helpful for retrieving only a portion of fat documents. Looking over the current java code for MongoDbIO, this looks pretty straightforward.

I think the best approach would be to add a String withProjection method to the builder, similar to the withFilter method, then parse this to Bson and then append it to the find method with .projection(fields(projectionBson)).",Add projections to MongoDb IO,1,,"['I/O', 'Java', 'MongoDB']",makosten@me.com,True,,makosten@me.com
beam,BEAM-3333,2017-12-12T07:37:58.000+0000,2017-12-12T07:41:05.000+0000,2019-06-10T09:43:41.000+0000,,Duplicate,New Feature,Minor,['2.2.0'],,,,,,,,['io-java-elasticsearch'],['IO: Java: ElasticSearch'],"The current Elasticsearch IO is only compatible with Elasticsearch v 2.x and v 5.x. The aim is to have an IO compatible with ES v 6.x. Beyond being able to address v6.x elasticsearch instances, we could also leverage the use of the Elasticsearch pipeline API and also better split the dataset (be as close as possible of desiredBundleSize) thanks to the new ES split API that allows ES shards splitting.",Create Elasticsearch IO compatible with ES 6.x,2,,,f.wal@rechtspraak.nl,True,echauchot,f.wal@rechtspraak.nl
beam,BEAM-3331,2017-12-12T02:38:18.000+0000,2019-06-06T23:31:24.000+0000,2019-06-06T23:31:24.000+0000,,Abandoned,New Feature,Major,['Not applicable'],,,,,,,,['runner-core'],['Runners: shared functionality for all runners'],"This generally splits a Pipeline into multiple distinct executable stages, each of which can be invoked over a Fn API RPC into a single environment.",Construct executable stages for a Pipeline within the Universal Local Runner,2,,['portability'],tgroh,True,,tgroh
beam,BEAM-3330,2017-12-12T02:36:46.000+0000,2019-06-06T23:23:50.000+0000,2019-06-06T23:23:51.000+0000,,Abandoned,New Feature,Major,['Not applicable'],,,,,,,,['runner-core'],['Runners: shared functionality for all runners'],"This needs to be some executable stage that can be converted directly into a ProcessBundleDescriptor and provided as a RegisterBundleRequest. This should be usable as a token within a single execution of the Universal Local Runner to indicate some stage that can be executed within a single environment.

There should initially be a single input PCollection, some number of output PCollections, and associated RemoteGrpcPort read and write nodes associated with those nodes. Determining what is placed within a single executable stage lies elsewhere (see BEAM-3331)",Add a representation for a Fused Stage in the Universal Local Runner,1,,['portability'],tgroh,True,,tgroh
beam,BEAM-3329,2017-12-12T02:32:17.000+0000,2018-06-01T18:21:25.000+0000,2018-06-01T18:21:25.000+0000,,Fixed,New Feature,Minor,['Not applicable'],,,,,,,,['runner-core'],['Runners: shared functionality for all runners'],"For unit testing remote stage execution, a local 'container' would be useful to verify the 

This necessitates adding a test-only dependency on the Java SDK harness from the ULR, and adding an in-process channel.",Add an in-process 'container' manager for testing in the Universal Local Runner,2,,['portability'],tgroh,True,tgroh,tgroh
beam,BEAM-3328,2017-12-12T02:29:24.000+0000,2018-06-01T18:21:42.000+0000,2018-06-01T18:21:42.000+0000,,Fixed,New Feature,Major,['Not applicable'],,,,,,,,['runner-core'],['Runners: shared functionality for all runners'],"Initially this requires creating a Logging Service, Data Service, and Control Service instance, and making them available to any remote container.",Create and relate portability service servers in the Universal Local Runner,1,,['portability'],tgroh,True,tgroh,tgroh
beam,BEAM-3327,2017-12-12T02:28:12.000+0000,2018-05-07T21:31:45.000+0000,2018-05-07T21:32:17.000+0000,,Fixed,New Feature,Major,['Not applicable'],,105000,105000,,,,100,['runner-core'],['Runners: shared functionality for all runners'],This permits remote stage execution for arbitrary environments,Add abstractions to manage Environment Instance lifecycles.,2,,['portability'],tgroh,True,axelmagn,tgroh
beam,BEAM-3326,2017-12-12T02:26:43.000+0000,2018-06-29T16:23:27.000+0000,2018-06-29T16:23:27.000+0000,,Fixed,New Feature,Major,['2.6.0'],,57000,57000,,,,100,['runner-core'],['Runners: shared functionality for all runners'],"This is the supertask for remote execution in the Universal Local Runner (BEAM-2899).

This executes a stage remotely via portability framework APIs",Execute a Stage via the portability framework in the ReferenceRunner,4,,['portability'],tgroh,True,tgroh,tgroh
beam,BEAM-3323,2017-12-11T17:05:38.000+0000,,2019-01-11T08:40:26.000+0000,,,New Feature,Major,,,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","Several IOs have features that exhibit nontrivial behavior when writing unbounded PCollection's - e.g. WriteFiles with windowed writes; BigQueryIO. We need to be able to write integration tests for these features.

Currently we have two ways to generate an unbounded PCollection without reading from a real-world external streaming system such as pubsub or kafka:

1) TestStream, which only works in direct runner - sufficient for some tests but not all: definitely not sufficient for large-scale tests or for tests that need to interact with a real instance of the external system (e.g. BigQueryIO). It is also quite verbose to use.
2) GenerateSequence.from(0) without a .to(), which returns an infinite amount of data.

GenerateSequence.from(a).to(b) returns a finite amount of data, but returns it as a bounded PCollection, and doesn't report the watermark.

I think the right thing to do here, for now, is to make GenerateSequence.from(a).to(b) have an option (e.g. "".asUnbounded()"", where it will return an unbounded PCollection, go through UnboundedSource (or potentially via SDF in runners that support it), and track the watermark properly (or via a configurable watermark fn).",Create a generator of finite-but-unbounded PCollection's for integration testing,5,1,,jkff,True,,jkff
beam,BEAM-3313,2017-12-07T09:28:32.000+0000,,2019-02-07T04:42:41.000+0000,,,New Feature,Major,,,,,,,,,['io-java-gcp'],"['IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)']","Since 2016 it's possible to use DML functionality in BigQuery when using standard SQL. It would be great to also support this in Beam. Analogous to BigQueryIO.Read::fromQuery maybe a BigQueryIO.Write::withQuery method can be added, which is supported when standard SQL is enabled?

The reason why I think this is a major issue: within a few months GDPR goes into effect in the EU. This law enforces companies to support deleting all data from a certain customer. I think this feature would be useful in order to automate the deletion/fuzzing process of personal customer data.",Support BigQuery DML in BigQueryIO.Write,2,1,,wardva,True,,wardva
beam,BEAM-3310,2017-12-06T16:05:19.000+0000,,2019-05-21T22:33:42.000+0000,,,New Feature,Major,,,67800,117000,,,,100,"['runner-extensions-metrics', 'sdk-java-core']","['Runners: Extensions: Metrics', 'SDKs: Java: pipeline construction, core transformations']","The idea is to avoid relying on the runners to provide access to the metrics (either at the end of the pipeline or while it runs) because they don't have all the same capabilities towards metrics (e.g. spark runner configures sinks  like csv, graphite or in memory sinks using the spark engine conf). The target is to push the metrics in the common runner code so that no matter the chosen runner, a user can get his metrics out of beam.
Here is the link to the discussion thread on the dev ML: https://lists.apache.org/thread.html/01a80d62f2df6b84bfa41f05e15fda900178f882877c294fed8be91e@%3Cdev.beam.apache.org%3E
And the design doc:
https://s.apache.org/runner_independent_metrics_extraction",Push metrics to a backend in an runner agnostic way,6,1,,echauchot,True,echauchot,echauchot
beam,BEAM-3236,2017-11-22T14:04:48.000+0000,,2018-02-14T18:22:55.000+0000,,,New Feature,Major,,,,,,,,,['io-ideas'],"['IO: ideas (proposals for new connectors, new designs, etc)']",,Add OrientDbIO,2,,,jbonofre,True,,jbonofre
beam,BEAM-3201,2017-11-16T13:39:15.000+0000,2018-04-11T09:56:37.000+0000,2018-06-12T05:55:19.000+0000,,Fixed,New Feature,Major,['2.5.0'],,,,,,,,['io-java-elasticsearch'],['IO: Java: ElasticSearch'],"*Dynamic documents id*: Today the ESIO only inserts the payload of the ES documents. Elasticsearch generates a document id for each record inserted. So each new insertion is considered as a new document. Users want to be able to update documents using the IO. So, for the write part of the IO, users should be able to provide a document id so that they could update already stored documents. Providing an id for the documents could also help the user on indempotency.
*Dynamic ES type and ES index*: In some cases (streaming pipeline with high throughput) partitioning the PCollection to allow to plug to different ESIO instances (pointing to different index/type) is not very practical, the users would like to be able to set ES index/type per document.


","ElasticsearchIO should allow the user to optionally pass id, type and index per document",9,4,,echauchot,True,timrobertson100,echauchot
beam,BEAM-3194,2017-11-15T13:03:14.000+0000,,2019-04-30T18:32:08.000+0000,,,New Feature,Major,,,11400,11400,,,,100,"['beam-model', 'sdk-java-core']","['Beam Model: general programming model concepts, semantics', 'SDKs: Java: pipeline construction, core transformations']","See the thread: https://lists.apache.org/thread.html/5fd81ce371aeaf642665348f8e6940e308e04275dd7072f380f9f945@%3Cdev.beam.apache.org%3E

We need this in order to have truly cross-runner end-to-end exactly once via replay + idempotence.",Support annotating that a DoFn requires stable / deterministic input for replay/retry,5,,,kenn,True,robinyqiu,kenn
beam,BEAM-3192,2017-11-15T09:48:50.000+0000,,2019-04-30T18:30:29.000+0000,,,New Feature,Major,,,,,,,,,['runner-spark'],['Runners: Spark runner'],"As we did for the StorageLevel, it would be great for an user to be able to provide the Spark partitionner via PipelineOptions.",Be able to specify the Spark Partitioner via the pipeline options,2,1,,jbonofre,True,jbonofre,jbonofre
beam,BEAM-3178,2017-11-13T22:18:52.000+0000,,2019-06-12T08:46:39.000+0000,,,New Feature,Minor,,,,,,,,,"['sdk-java-core', 'sdk-py-core']","['SDKs: Java: pipeline construction, core transformations', 'SDKs: Python']","In some cases BEAM users or runner maintainers may want to increase the logging level produced by the BEAM runners. To make it simpler, SDK should capture a desired logging level in PipelineOptions and pass it on to runners. 

For comparison, we have instrumentation that captures logging for Java SDK harness[1]. It seems that we should have similar option that captures logging granularity on the runner side. 
[1] https://github.com/apache/beam/blob/2b4a6b5d5cb455fe73971fde4caa7d1446cd55aa/sdks/java/core/src/main/java/org/apache/beam/sdk/options/SdkHarnessOptions.java#L62",Add a Pipeline option that sets desired logging level for logs produced by runners.,2,,['portability'],tvalentyn,True,,tvalentyn
beam,BEAM-3171,2017-11-10T18:41:58.000+0000,2018-01-30T20:08:15.000+0000,2018-01-30T20:08:15.000+0000,,Fixed,New Feature,Major,['2.3.0'],,,,,,,,['dsl-sql'],['DSLs: SQL'],"We use BeamSQL to run streaming jobs mostly, and  add a join_as_lookup improvement(internal branch) to cover the streaming-to-batch case(similar as [1]). I could submit a PR as experimental if people are interested. 

The rough solution is, if one source of join node implements {{BeamSeekableTable}} and the other is not, then the join node is converted to a fact-lookup operation.

Ref:
[1] https://docs.google.com/document/d/1B-XnUwXh64lbswRieckU0BxtygSV58hysqZbpZmk03A/edit?usp=sharing 

[~xumingming] [~takidau] for any comments",convert a join into lookup,4,,['experimental'],mingmxu,True,mingmxu,mingmxu
beam,BEAM-3163,2017-11-09T02:10:58.000+0000,2017-11-21T17:35:13.000+0000,2017-11-21T17:36:16.000+0000,,Duplicate,New Feature,Major,['Not applicable'],,,,,,,,['testing'],['Testing: general infrastructure'],,We should run and monitor PostCommit test suites for release branches.,1,,,tvalentyn,True,jaku,tvalentyn
beam,BEAM-3147,2017-11-07T00:43:12.000+0000,2019-03-22T14:14:38.000+0000,2019-06-10T09:45:28.000+0000,,Fixed,New Feature,Major,['Not applicable'],,,36600,,,,,"['dsl-sql', 'testing-nexmark']","['DSLs: SQL', 'Testing: Nexmark queries']","Currently there is a (Nexmark suite)[https://github.com/apache/beam/tree/master/sdks/java/nexmark] running against Java SDK. It has Java object model and runs a set of PTransofrms replicating the queries specified in Nexmark.

The task is to have the same set of queries running on top of Beam SQL.

References:
* (Nexmark Paper)[http://datalab.cs.pdx.edu/niagara/pstream/nexmark.pdf]
* (Nexmark Queries)[http://datalab.cs.pdx.edu/niagara/NEXMark/]
* (Beam Java Nexmark Suite)[https://github.com/apache/beam/tree/master/sdks/java/nexmark] ",Nexmark in SQL,4,,"['newbie,', 'nexmark', 'starter']",kedin,True,kedin,kedin
beam,BEAM-3115,2017-10-27T21:33:00.000+0000,,2017-10-27T21:33:00.000+0000,,,New Feature,Minor,,,,,,,,,['testing'],['Testing: general infrastructure'],Consider splitting PostCommit runs from one shot trigger phrases to have separate build history,Consider splitting PostCommit runs from one shot trigger phrases to have separate build history,1,,,lcwik,True,,lcwik
beam,BEAM-3100,2017-10-26T17:59:28.000+0000,,2017-10-26T17:59:28.000+0000,,,New Feature,Major,,,,,,,,,"['runner-core', 'sdk-java-core']","['Runners: shared functionality for all runners', 'SDKs: Java: pipeline construction, core transformations']","Generally an execution engine should not depend on SDK constructs (e.g. DoFn), as those should be used by the language's SDK harness to execute a UDF. However, there are certain constructs that are used to communicate between the Runner and SDK harness, or are required to fulfill the Runner's responsibilities (e.g. Coder, Window). Those should be accessible without having to depend on the entire SDK",Factor execution time and construction time dependencies into a non-SDK component,1,,,tgroh,True,,tgroh
beam,BEAM-3099,2017-10-26T17:53:37.000+0000,2018-03-06T22:17:52.000+0000,2018-03-06T22:17:52.000+0000,,Implemented,New Feature,Major,['2.5.0'],,6600,6600,,,,100,['sdk-py-core'],['SDKs: Python'],"Currently Java SDK has HDFS support but Python SDK does not. With current portability efforts other runners may soon be able to use Python SDK. Having HDFS support will allow these runners to execute large scale jobs without using GCS. 

Following suggests some libraries that can be used to connect to HDFS from Python.
http://wesmckinney.com/blog/python-hdfs-interfaces/",Implement HDFS FileSystem for Python SDK,4,,,chamikara,True,udim,chamikara
beam,BEAM-3084,2017-10-20T20:15:31.000+0000,,2017-11-08T21:58:23.000+0000,,,New Feature,Major,,,,,,,,,['dsl-sql'],['DSLs: SQL'],"Calcite streaming documentation includes examples for using SQL window
analytic functions
https://calcite.apache.org/docs/stream.html#sliding-windows

From: Kobi Salant <kobi.salant@gmail.com>
dev@beam.apache.org",Support for window analytic functions,1,,,mingmxu,True,,mingmxu
beam,BEAM-3073,2017-10-18T05:40:13.000+0000,,2019-02-07T04:51:46.000+0000,,,New Feature,Minor,,,,,,,,,['io-ideas'],"['IO: ideas (proposals for new connectors, new designs, etc)']","Hi all,
{color:#14892c}I tried to connect Apache Ignite(In-memory) via the beam's sdk:org.apache.beam.sdk.io.jdbc.JdbcIO
Here, i am not sure if the JdbcIO sdk only is provided for some specific Database: MySQL(disk), postgreSQL(disk)?{color}
my java test code is as follows:
import java.sql.PreparedStatement;
import java.sql.SQLException;
import java.util.ArrayList;
import java.util.List;

import org.apache.beam.sdk.Pipeline;
import org.apache.beam.sdk.io.jdbc.JdbcIO;
import org.apache.beam.sdk.options.PipelineOptionsFactory;
import org.apache.beam.sdk.transforms.Create;
import org.apache.beam.sdk.values.KV;
import org.apache.beam.sdk.values.PCollection;

public class BeamtoJDBC {
	public static void main(String[] args) {		
		Integer[] value=new Integer[] {1,2,3,4,5};
		List<KV<Integer, Integer>> dataList = new ArrayList<>();
		int n=value.length;
		int count=0;
		for (int i=0; i<n; i++)
		{
			dataList.add(KV.of(count,value[i]));
			count=count+1;			
		}
		
		Pipeline p = Pipeline.create(PipelineOptionsFactory.fromArgs(args).withValidation().create());
		
		PCollection<KV<Integer, Integer>> data=p.apply(""create data with time"",Create.of(dataList));

		data.apply(JdbcIO.<KV<Integer, Integer>>write()
				.withDataSourceConfiguration(JdbcIO.DataSourceConfiguration
						.create(""org.apache.ignite.IgniteJdbcDriver"", ""jdbc:ignite://localhost:11211/"")
						)		
				.withPreparedStatementSetter(new JdbcIO.PreparedStatementSetter<KV<Integer, Integer>>() {
					public void setParameters(KV<Integer, Integer> element, PreparedStatement query)
						throws SQLException {
						query.setInt(1, element.getKey());
						query.setInt(2, element.getValue());
					}
				})
			);
		p.run();

	}
}

{color:#d04437}my error message is: 
"" InvocationTargetException: org.apache.beam.sdk.util.UserCodeException: java.sql.SQLException: Cannot create PoolableConnectionFactory 
(Failed to establish connection.): Failed to get future result due to waiting timed out. ""{color}

{color:#14892c}I would like to know whether the connection between beam and ignite is feasible or not?{color}

Thanks

Rick
",Add Apache Ignite IO,2,,,Ricklin,True,,Ricklin
beam,BEAM-3062,2017-10-16T17:18:11.000+0000,,2019-02-07T04:46:19.000+0000,,,New Feature,Minor,,,,,,,,,['testing'],['Testing: general infrastructure'],"It appears that only committers can trigger an invocation of a Jenkins test suite on a branch, such as release branch. This is inconvenient. In a situation, when I send a PR to a branch and the precommit tests don't pass, I cannot easily identify whether my PR is at fault, or the branch was already unhealthy.
 
Also, if the release branch for whatever reason becomes unhealthy, we should be able to identify the breakage once it manifests. We have postcommit test runs on master branch. I may be wrong, but I think we don't have them configured for the release branches. ",Provide a signal of a release branch health.,1,,,tvalentyn,True,,tvalentyn
beam,BEAM-3035,2017-10-07T19:43:28.000+0000,2017-11-17T01:29:12.000+0000,2017-11-17T01:29:23.000+0000,,Fixed,New Feature,Minor,['2.3.0'],['2.1.0'],,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","The {{ReifyTimestampsAndWindowsFn}} in {{GatherAllPanes}} looks useful for giving {{MapElements}},  {{Filter}}, etc access to window and pane information.

This JIRA represents the proposal to extract a subset of the {{GatherAllPanes}} functionality into a new class {{ReifyTimestampsAndWindows}}, next to {{ReifyTimestamps}} which does similar functionality but with {{TimestampedValue}}

{{GatherAllPanes}} would then rely on the newly extracted public {{PTransform}} for its previous functionality.",Extract ReifyTimestampsAndWindows from GatherAllPanes,3,,,wtanaka,True,wtanaka,wtanaka
beam,BEAM-3032,2017-10-06T23:37:40.000+0000,,2019-07-04T06:15:02.000+0000,,,New Feature,Minor,,,,,2419200,2419200,2419200,,['io-ideas'],"['IO: ideas (proposals for new connectors, new designs, etc)']","I would like to add a RedshiftIO Java extension to perform bulk read/write to/from AWS Redshift via the UNLOAD and COPY Redshift SQL commands. This requires S3, which is the subject of BEAM-2500.",Add RedshiftIO,5,,,jmarble,True,,jmarble
beam,BEAM-3009,2017-10-02T19:30:01.000+0000,2017-10-14T02:01:28.000+0000,2017-10-14T02:01:28.000+0000,,Fixed,New Feature,Major,['2.3.0'],,,,,,,,"['beam-model', 'sdk-java-core']","['Beam Model: general programming model concepts, semantics', 'SDKs: Java: pipeline construction, core transformations']",See http://s.apache.org/context-fn. This JIRA is about creating the basic machinery needed to support it in the non-portable case and creating a general pattern to follow for transforms that are configured by closures that may need context access.,Implement context access from user code closures,2,,,jkff,True,jkff,jkff
beam,BEAM-3008,2017-10-02T18:45:01.000+0000,2018-01-24T05:53:03.000+0000,2018-01-24T05:53:03.000+0000,,Fixed,New Feature,Major,['2.3.0'],,,,,,,,['io-java-gcp'],"['IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)']",[https://github.com/apache/beam/pull/2057] is an effort towards BigtableIO templatization.  This Issue is a request to get a fully featured template for BigtableIO.,BigtableIO should use ValueProviders ,3,,,sduskis,True,sduskis,sduskis
beam,BEAM-2990,2017-09-26T17:18:28.000+0000,2018-04-16T20:53:41.000+0000,2018-04-16T20:53:41.000+0000,,Fixed,New Feature,Major,['2.5.0'],,22200,22200,,,,100,['dsl-sql'],['DSLs: SQL'],"support Non-scalar types:
MAP 	Collection of keys mapped to values
ARRAY 	Ordered, contiguous collection that may contain duplicates",support data type MAP,3,,,mingmxu,True,mingmxu,mingmxu
beam,BEAM-2986,2017-09-25T15:12:01.000+0000,2017-09-29T17:59:48.000+0000,2017-09-29T18:05:03.000+0000,,Fixed,New Feature,Major,['2.2.0'],,,,,,,,['io-java-gcp'],"['IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)']","It would be nice to expose the avro records from BigQuery exports directly, rather than converting/serializing them to TableRow objects.

The interface could be similar to AvroIO.parseGenericRecords, allowing a user to pass in a custom parseFn to the reader.

There's a thread on the dev@ mailing this w/ discussion:
https://lists.apache.org/thread.html/a045726a0c8d7412f3556ee3aac881920afb040180f42112b393c68e@%3Cdev.beam.apache.org%3E",Support reading avro GenericRecords with BigQueryIO,4,,,SteveNiemitz,True,SteveNiemitz,SteveNiemitz
beam,BEAM-2955,2017-09-13T18:27:39.000+0000,,2019-04-30T18:30:20.000+0000,,,New Feature,Major,,,,,,,,,['io-java-gcp'],"['IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)']","The Cloud Bigtable (CBT) team has had a Dataflow connector maintained in a different repo for awhile. Recently, we did some reworking of the Cloud Bigtable client that would allow it to better coexist in the Beam ecosystem, and we also released a Beam connector in our repository that exposes HBase idioms rather than the Protobuf idioms of BigtableIO.  More information about the customer experience of the HBase connector can be found here: [https://cloud.google.com/bigtable/docs/dataflow-hbase].

The Beam repo is a much better place to house a Cloud Bigtable HBase connector.  There are a couple of ways we can implement this new connector:

# The CBT connector depends on artifacts in the io/hbase maven project.  We can create a new extend HBaseIO for the purposes of CBT.  We would have to add some features to HBaseIO to make that work (dynamic rebalancing, and a way for HBase and CBT's size estimation models to coexist)
# The BigtableIO connector works well, and we can add an adapter layer on top of it.  I have a proof of concept of it here: [https://github.com/sduskis/cloud-bigtable-client/tree/add_beam/bigtable-dataflow-parent/bigtable-hbase-beam].
# We can build a separate CBT HBase connector.

I'm happy to do the work.  I would appreciate some guidance and discussion about the right approach.",Create a Cloud Bigtable HBase connector,3,,,sduskis,True,sduskis,sduskis
beam,BEAM-2952,2017-09-13T02:37:14.000+0000,2017-09-13T03:11:12.000+0000,2017-10-18T03:06:38.000+0000,,Invalid,New Feature,Minor,['Not applicable'],,,,,,,,['examples-java'],['Examples: Java examples'],"Hi all,
I  have a question how to use the beam java sdk: KV.OrderByKey
My java code is as:
int[] key=new int[] {2,1,3,4,5};
double[] value=new double[] {1.0,1.0,1.0,1.0,1.0};
List<KV<Integer, Double>> KVlist = new ArrayList<>();
List<KV<Integer, Double>> KVtest = new ArrayList<>();
int n=value.length;
for (int i=0; i<n; i++){			
　　KVlist.add(KV.of(i, value[i]));
　　System.out.println(KVlist.get(i));
	}
PipelineOptions options = PipelineOptionsFactory.create();		
Pipeline p = Pipeline.create(options);		
PCollection<KV<Integer, Double>> t1=p.apply(""create data"",Create.of(KVlist));
p.run;

Thanks

Rick
",How to use KV.OrderByKey,2,,,Ricklin,True,reuvenlax,Ricklin
beam,BEAM-2950,2017-09-12T23:08:32.000+0000,2018-02-12T19:11:04.000+0000,2018-02-12T19:11:04.000+0000,,Won't Fix,New Feature,Major,['Not applicable'],,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","https://github.com/apache/beam/pull/3814 provides implicit access to side inputs (without a ProcessContext). Luke suggests to have the same for State and, I suppose, timers. We could also have it for PipelineOptions: in any given user code invocation, these are all unambiguous.",Provide implicit access to State,3,,,jkff,True,kenn,jkff
beam,BEAM-2897,2017-09-11T18:23:15.000+0000,2018-04-18T22:58:08.000+0000,2018-04-20T18:30:57.000+0000,,Duplicate,New Feature,Major,['Not applicable'],,,,,,,,['runner-flink'],['Runners: Flink runner'],Filnk should support configurable container/process management as per https://s.apache.org/beam-fn-api-container-contract.,Configurable container/process management,2,,['portability'],herohde,True,,herohde
beam,BEAM-2871,2017-09-09T07:52:15.000+0000,2018-05-07T04:11:30.000+0000,2018-05-07T04:11:30.000+0000,,Fixed,New Feature,Major,['2.5.0'],['2.1.0'],2400,2400,,,,100,['examples-java'],['Examples: Java examples'],"Provide example for how to run external libraries such as c++ code within a DoFn. 

More details of this pattern can be viewed in the blog:
https://cloud.google.com/blog/big-data/2017/07/running-external-libraries-with-cloud-dataflow-for-grid-computing-workloads
",Add examples of running external libraries on workers,2,,,rarokni@gmail.com,True,reuvenlax,rarokni@gmail.com
beam,BEAM-2865,2017-09-08T01:37:28.000+0000,2017-12-19T20:45:11.000+0000,2017-12-19T20:45:11.000+0000,,Fixed,New Feature,Major,['2.3.0'],,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","Design doc: http://s.apache.org/fileio-write

Discussion: https://lists.apache.org/thread.html/cc543556cc709a44ed92262207215eaa0e43a0f573c630b6360d4edc@%3Cdev.beam.apache.org%3E",Implement FileIO.write(),3,,,jkff,True,jkff,jkff
beam,BEAM-2857,2017-09-06T21:30:29.000+0000,,2019-06-05T06:40:56.000+0000,,,New Feature,Major,,,26400,26400,,,,100,['sdk-py-core'],['SDKs: Python'],"Beam Java has a FileIO with operations: match()/matchAll(), readMatches(), which together cover the majority of needs for general-purpose file ingestion. Beam Python should have something similar.

An early design document for this: https://s.apache.org/fileio-beam-python",Create FileIO in Python,8,,"['gsoc', 'gsoc2019', 'mentor']",jkff,True,pabloem,jkff
beam,BEAM-2846,2017-09-06T05:13:47.000+0000,,2017-09-06T05:14:04.000+0000,,,New Feature,Major,,,,,,,,,['runner-mapreduce'],['Runner: MapReduce'],,Support SplittableParDo in MapReduce runner,1,,,peihe0@gmail.com,True,,peihe0@gmail.com
beam,BEAM-2845,2017-09-06T05:12:36.000+0000,,2017-09-06T05:12:36.000+0000,,,New Feature,Major,,,,,,,,,['runner-mapreduce'],['Runner: MapReduce'],,Support TimersInParDo in MapReduce runner,1,,,peihe0@gmail.com,True,,peihe0@gmail.com
beam,BEAM-2835,2017-09-01T09:05:34.000+0000,,2017-09-01T09:05:34.000+0000,,,New Feature,Major,,,,,,,,,['runner-mapreduce'],['Runner: MapReduce'],"Currently, the runner executes MR jobs sequentially with the topological order. This is very inefficient, it will be good to allow jobs to be executed in parallel if they don't depend on either other.",Support executing MapReduce jobs in parallel.,1,,,peihe0@gmail.com,True,,peihe0@gmail.com
beam,BEAM-2830,2017-08-31T03:30:47.000+0000,,2017-08-31T03:30:47.000+0000,,,New Feature,Major,,,,,,,,,['runner-mapreduce'],['Runner: MapReduce'],,Main thread needs to re-throw worker exceptions for ValidatesRunner tests that expects exceptions.,1,,,peihe0@gmail.com,True,,peihe0@gmail.com
beam,BEAM-2829,2017-08-30T20:29:09.000+0000,2017-10-30T21:43:28.000+0000,2017-10-30T21:43:49.000+0000,,Fixed,New Feature,Minor,['2.2.0'],"['2.0.0', '2.1.0']",,,604800,604800,604800,,['runner-dataflow'],['Runners: Google Cloud Dataflow runner'],"Enable setting job labels --labels in DataflowPipelineOptions (earlier Dataflow SDK 1.x supports this)
https://github.com/apache/beam/blob/master/runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/options/DataflowPipelineOptions.java",Add ability to set job labels in DataflowPipelineOptions,3,,,zongweiz,True,altay,zongweiz
beam,BEAM-2828,2017-08-30T18:47:02.000+0000,2017-09-03T23:51:30.000+0000,2017-09-03T23:51:31.000+0000,,Fixed,New Feature,Major,['2.2.0'],,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","Let's have FileIO as a namespace for transforms such as: current Match.filepatterns(); FileIO.read() for reading whole files and FileIO.write() for writing whole files, etc.

Target for 2.2.0 is just creating the namespace and moving Match.filepatterns() into it (https://github.com/apache/beam/pull/3759).

Related JIRAs: https://issues.apache.org/jira/browse/BEAM-2750 and https://issues.apache.org/jira/browse/BEAM-2751",Create FileIO,2,,,jkff,True,jkff,jkff
beam,BEAM-2826,2017-08-30T13:44:32.000+0000,2018-07-02T22:54:39.000+0000,2018-07-02T22:54:39.000+0000,,Fixed,New Feature,Major,['2.2.0'],['2.0.0'],,,,,,,['beam-model'],"['Beam Model: general programming model concepts, semantics']","I'm trying to write an XML file where the source is a text file stored in GCS. The code is running fine but instead of a single XML file, it is generating multiple XML files. (No. of XML files seem to follow total no. of records present in source text file). I have observed this scenario while using 'DataflowRunner'.

When I run the same code in local then two files get generated. First one contains all the records with proper elements and the second one contains only opening and closing root element.

As I learnt,it is expected that it may produce multiple files: e.g. if the runner chooses to process your data parallelizing it into 3 tasks (""bundles""), you'll get 3 files. Some of the parts may turn out empty in some cases, but the total data written will always add up to the expected data.",Need to generate a single XML file when write is performed on small amount of data,4,,,balajee1667,True,jkff,balajee1667
beam,BEAM-2824,2017-08-30T06:49:33.000+0000,,2019-04-30T18:29:10.000+0000,,,New Feature,Major,,,600,600,,,,100,['runner-jstorm'],['Runners: JStorm'],"Here are the work items:
1. supports metrics() in local mode.
2. supports waitUntilFinish() in local mode.
3. uses PipelineResult in TestJStormRunner.
4. supports metrics() in cluster mode.",Support PipelineResult in JStormRunner,2,,,peihe0@gmail.com,True,peihe0@gmail.com,peihe0@gmail.com
beam,BEAM-2822,2017-08-29T19:47:48.000+0000,,2018-01-22T22:25:24.000+0000,,,New Feature,Minor,,,,,,,,,['beam-model'],"['Beam Model: general programming model concepts, semantics']","https://s.apache.org/beam-fn-api-progress-reporting

Note that the ULR reference implementation, when ready, should be useful for every runner.",Add support for progress reporting in fn API,3,,['portability'],vikasrk,True,,vikasrk
beam,BEAM-2819,2017-08-29T05:21:02.000+0000,,2019-04-30T18:30:26.000+0000,,,New Feature,Major,,,,,,,,,['io-java-gcp'],"['IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)']","Currently there is no functionality allowing to read only keys from the Google Datastore through the Datastore IO. In some cases users don't need to read the whole entity, e.g. to filter by certain values in ancestry. This seems to be an important feature as the entity reads are expensive and thus the native Datastore client/API allow to run Key Only queries. ",Key only reads from Google Datastore,1,,['datastore'],ilnar,True,,ilnar
beam,BEAM-2806,2017-08-24T23:30:27.000+0000,2018-02-07T13:50:37.000+0000,2018-02-07T13:50:37.000+0000,,Fixed,New Feature,Major,['2.3.0'],,,,,,,,['runner-flink'],['Runners: Flink runner'],"Beam version: 2.2.0-SNAPSHOT

Here's the code
{code}
PCollectionView<Map<BeamRecord, Iterable<BeamRecord>>> rowsView = rightRows
        .apply(View.<BeamRecord, BeamRecord>asMultimap());
{code}

And exception when running with {{FlinkRunner}}:
{code}
Exception in thread ""main"" java.lang.UnsupportedOperationException: The transform View.CreatePCollectionView is currently not supported.
	at org.apache.beam.runners.flink.FlinkStreamingPipelineTranslator.visitPrimitiveTransform(FlinkStreamingPipelineTranslator.java:113)
	at org.apache.beam.sdk.runners.TransformHierarchy$Node.visit(TransformHierarchy.java:594)
	at org.apache.beam.sdk.runners.TransformHierarchy$Node.visit(TransformHierarchy.java:586)
	at org.apache.beam.sdk.runners.TransformHierarchy$Node.visit(TransformHierarchy.java:586)
	at org.apache.beam.sdk.runners.TransformHierarchy$Node.access$500(TransformHierarchy.java:268)
	at org.apache.beam.sdk.runners.TransformHierarchy.visit(TransformHierarchy.java:202)
	at org.apache.beam.sdk.Pipeline.traverseTopologically(Pipeline.java:440)
	at org.apache.beam.runners.flink.FlinkPipelineTranslator.translate(FlinkPipelineTranslator.java:38)
	at org.apache.beam.runners.flink.FlinkStreamingPipelineTranslator.translate(FlinkStreamingPipelineTranslator.java:69)
	at org.apache.beam.runners.flink.FlinkPipelineExecutionEnvironment.translate(FlinkPipelineExecutionEnvironment.java:104)
	at org.apache.beam.runners.flink.FlinkRunner.run(FlinkRunner.java:113)
	at org.apache.beam.sdk.Pipeline.run(Pipeline.java:297)
	at org.apache.beam.sdk.Pipeline.run(Pipeline.java:283)
{code}
",support View.CreatePCollectionView in FlinkRunner,4,,,mingmxu,True,grzegorz_kolakowski,mingmxu
beam,BEAM-2802,2017-08-24T07:13:28.000+0000,2017-09-01T19:08:51.000+0000,2019-06-10T13:05:00.000+0000,,Fixed,New Feature,Minor,['2.2.0'],,,,,,,,['io-java-text'],['IO: Java: TextIO'],Currently TextIO use {{\r}} {{\n}} or {{\r\n}} or a mix of the two to split a text file into PCollection elements. It might happen that a record is spread across more than one line. In that case we should be able to specify a custom record delimiter to be used in place of the default ones.,TextIO should allow specifying a custom delimiter,5,1,,echauchot,True,echauchot,echauchot
beam,BEAM-2801,2017-08-24T04:07:11.000+0000,2019-03-18T21:04:02.000+0000,2019-07-03T07:02:01.000+0000,,Fixed,New Feature,Major,['2.12.0'],,,,,,,,['sdk-py-core'],['SDKs: Python'],"Currently Python SDK has a native (Dataflow) BigQuery sink. We need to implement a custom BigQuery sink to support following.

* overcome BigQuery per load job quotas by executing multiple load jobs.
* support SDK level features such as data-dependent writes ",Implement a BigQuery custom sink,6,2,,chamikara,True,pabloem,chamikara
beam,BEAM-2783,2017-08-21T11:27:02.000+0000,,2019-04-30T18:33:28.000+0000,,,New Feature,Major,,,,,,,,,['runner-mapreduce'],['Runner: MapReduce'],"It is important to be able to run ValidatesRunner tests.
And, we need wire MapReduce runner with the framework's counters.",Support Counters/Metrics and run ValidatesRunner tests,1,,,peihe0@gmail.com,True,peihe0@gmail.com,peihe0@gmail.com
beam,BEAM-2774,2017-08-17T14:11:09.000+0000,2018-01-04T22:04:19.000+0000,2018-01-04T22:04:19.000+0000,,Fixed,New Feature,Major,['2.3.0'],,,,1209600,1209600,1209600,,['sdk-py-core'],['SDKs: Python'],"A new I/O source for reading (and eventually writing) VCF files [1] for Python. The design doc is available at https://docs.google.com/document/d/1jsdxOPALYYlhnww2NLURS8NKXaFyRSJrcGbEDpY9Lkw/edit

[1] http://samtools.github.io/hts-specs/VCFv4.3.pdf
",Add I/O source for VCF files (python),5,1,,arostami,True,msaul,arostami
beam,BEAM-2771,2017-08-15T22:59:34.000+0000,,2017-09-18T09:17:32.000+0000,,,New Feature,Minor,,,,,,,,,"['sdk-java-core', 'sdk-py-core']","['SDKs: Java: pipeline construction, core transformations', 'SDKs: Python']","Allow dynamically computed information in a pipeline which is materialized as a side input to be used in the same places as ValueProviders.

This could solve the problem mentioned in [https://stackoverflow.com/questions/45649244/google-cloud-dataflow-apache-beam-can-i-use-sideinputs-with-textio-write] if TextSink supported ValueProviders for headers/footers.


",Allow using side inputs as ValueProviders,2,,,lcwik,True,,lcwik
beam,BEAM-2759,2017-08-10T10:10:35.000+0000,,2019-04-30T18:33:42.000+0000,,,New Feature,Major,,,,,,,,,['runner-gearpump'],['Runners: Gearpump runner'],Running {{WindowTest#testMergingCustomWindows()}} and {{WindowTest#testMergingCustomWindowsKeyedCollection()}} in Gearpump runner show that these kind of windows do not get merged.,Support Custom Windows in Gearpump runner,3,,,echauchot,True,mauzhang,echauchot
beam,BEAM-2751,2017-08-07T20:18:27.000+0000,2018-07-02T22:53:44.000+0000,2018-07-02T22:53:44.000+0000,,Fixed,New Feature,Major,['2.2.0'],,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","I'd like to write elements as individual files.

Rather than smashing thousands of outputs into a handful of files as TextIO does (output-00000-of-00005, output-00001-of-00005,...), I want to write each element into unique files.

So if I used WholeFileIO from [BEAM-2750] to read in three files (hi.txt, what.txt, and yes.txt) then I'd like to write the processed files out to individual files with user or data-defined filenames (like hi-modified.txt, what-modified.txt, and yes-modified.txt).

With a WholeFileIO, this would look like:

{code:java}
PCollection<KV<String, Byte[]>> fileNamesAndBytes = p.apply(""Read"", WholeFileIO.read().from(""/path/to/input/dir/*""));
...
// Do stuff that change contents and file names
PCollection<KV<String, Byte[]>> modifedFileNamesAndBytes = ...
...
modifedFileNamesAndBytes.apply(""Write"", WholeFileIO.write().to(""/path/to/output/dir/""));
{code}

This ticket complements [BEAM-2750].
",Write PCollection elements to individual files,2,,,christophhebert,True,jkff,christophhebert
beam,BEAM-2750,2017-08-07T20:14:08.000+0000,2017-09-03T23:51:02.000+0000,2017-12-06T03:48:00.000+0000,,Fixed,New Feature,Major,['2.2.0'],,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","I'd like to read whole files as one element each.

If my input files are hi.txt, what.txt, and yes.txt, then the whole contents of hi.txt are an element of the returned PCollection, the whole contents of what.txt are the next element, etc., giving me a PCollection with three elements.

This contrasts with TextIO which reads a new element for every line of text in the input files.

This read (I'll call it WholeFileIO for now) would work like so:

{code:java}
PCollection<KV<String, Byte[]>> fileNamesAndBytes = p.apply(""Read"", WholeFileIO.read().from(""/path/to/input/dir/*""));
{code}
The above example passes the raw file contents and the filename.

Alternatively, we could pass a PCollection of some sort of FileWrapper around an InputStream to support lazy loading.

This ticket complements [BEAM-2751].
",Read whole files as one PCollection element each,4,,,christophhebert,True,jkff,christophhebert
beam,BEAM-2731,2017-08-04T18:18:08.000+0000,2017-08-09T01:42:59.000+0000,2017-08-09T01:42:59.000+0000,,Duplicate,New Feature,Major,['Not applicable'],,,,,,,,['sdk-py-core'],['SDKs: Python'],"Java SDK has a Reshuffle transform [1] but Python SDK does not have this.

Following factors have to be considered when implementing this transform.

* Transform in Java has been deprecated due to intended side effects not being portable.
* A lot of details related to portability were uncovered/discussed when we tried to add a generic Redistribute transform [2][3].

[1] https://github.com/apache/beam/blob/master/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/Reshuffle.java
[2] https://github.com/apache/beam/pull/1036
[3] https://lists.apache.org/thread.html/ac34c9ac665a8d9f67b0254015e44c59ea65ecc1360d4014b95d3b2e@%3Cdev.beam.apache.org%3E",Add a generic Reshuffle transform to Python SDK,2,,,chamikara,True,,chamikara
beam,BEAM-2728,2017-08-04T09:47:45.000+0000,,2019-05-21T22:37:36.000+0000,,,New Feature,Minor,,,45600,45600,,,,100,['extensions-java-sketching'],['Extensions: Java: Sketching library'],"Goal : Provide an extension library to compute approximate statistics on streams.

Interest : Probabilistic data structures can create an approximation (sketch) of the current state of a stream without storing every element but rather processing each observation quickly to summarize its current state and find useful statistical insights.

Implementation is here : https://github.com/ArnaudFnr/beam/tree/sketching/sdks/java/extensions/sketching

More info : https://docs.google.com/document/d/1Xy6g5RPBYX_HadpIr_2WrUeusiwL0Jo2ACI5PEOP1kc/edit
",Extension for sketch-based statistics,7,,,arnaudfnr,True,arnaudfnr,arnaudfnr
beam,BEAM-2720,2017-08-03T19:01:53.000+0000,2017-10-19T02:56:08.000+0000,2019-06-10T10:10:05.000+0000,,Fixed,New Feature,Major,['2.2.0'],,,,,,,,['io-java-kafka'],['IO: Java: Kafka'],"Kafka 0.11 added support for transactions which allows end-to-end exactly-once semantics. Beam's KafkaIO users can benefit from these while using runners that support exactly-once processing.

I have an implementation of EOS support for Kafka sink : https://github.com/apache/beam/pull/3612
It has two shuffles and builds on Beam state-API and checkpoint barrier between stages (as in Dataflow). Pull request has a longer description.
",Exactly-once Kafka sink,4,,,rangadi,True,rangadi,rangadi
beam,BEAM-2718,2017-08-03T07:26:16.000+0000,2017-11-21T01:12:24.000+0000,2018-02-06T00:07:52.000+0000,,Fixed,New Feature,Minor,['2.3.0'],['Not applicable'],,,,,,,['sdk-py-core'],['SDKs: Python'],"When processing of a bundle fails, the bundle should be retried 3 times (for a total of 4 attempts to process it).",Add bundle retry logic to the DirectRunner,2,,,mariagh,True,mariagh,mariagh
beam,BEAM-2715,2017-08-02T17:08:15.000+0000,,2019-04-30T18:30:41.000+0000,,,New Feature,Major,,,,,,,,,['io-java-gcp'],"['IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)']",We are ingesting from a Pubsub Read and are attempting to mimic the maxNumRecords that was available in 0.6.0. In order to do this we would need to utilize withMaxNumRecords from the BoundedReadFromUnboundedSource class. We would need to utilize the PubsubSource class to create the UnboundedSource from Pubsub. Would it be possible to expose PubsubSource? Currently what is the recommended way to create a bounded read from Pubsub with a set number of records?,Expose PubsubSource to create UnboundedSource and utilize withMaxNumRecords from BoundedReadFromUnboundedSource,6,,['pubsub'],alevy,True,,alevy
beam,BEAM-2709,2017-08-01T20:57:41.000+0000,,2019-04-30T18:33:39.000+0000,,,New Feature,Major,,,,,,,,,"['runner-ideas', 'runner-tez']","['Runners: ideas for new Beam runners', 'Runner: Tez']",Add a TezRunner to Beam,Add TezRunner,4,,,schellbd,True,schellbd,schellbd
beam,BEAM-2698,2017-07-28T23:34:58.000+0000,,2019-02-07T05:08:01.000+0000,,,New Feature,Major,,,,,,,,,['runner-gearpump'],['Runners: Gearpump runner'],,Support Splittable DoFn in GearpumpRunner,1,,['triaged'],mauzhang,True,mauzhang,mauzhang
beam,BEAM-2697,2017-07-28T23:34:02.000+0000,,2019-04-30T18:33:55.000+0000,,,New Feature,Major,,,,,,,,,['runner-gearpump'],['Runners: Gearpump runner'],,Support for new Timer API in GearpumpRunner,1,,,mauzhang,True,mauzhang,mauzhang
beam,BEAM-2696,2017-07-28T23:32:48.000+0000,,2019-04-30T18:32:55.000+0000,,,New Feature,Major,,,,,,,,,['runner-gearpump'],['Runners: Gearpump runner'],,Support new State API in GearpumpRunner ,1,,,mauzhang,True,mauzhang,mauzhang
beam,BEAM-2693,2017-07-28T23:25:40.000+0000,,2019-04-30T18:29:05.000+0000,,,New Feature,Major,,,,,,,,,['runner-gearpump'],['Runners: Gearpump runner'],"This is copied from Kenn's description in other jiras.

To run non-Java SDK code is to put together an operator that manages a Fn API client DoFnRunner and an SDK harness Fn API server.
(filing to organize steps, details of this may evolve as it is implemented).

",GearpumpRunner Fn API based ParDo operator,1,,,mauzhang,True,mauzhang,mauzhang
beam,BEAM-2687,2017-07-27T04:52:42.000+0000,2019-02-15T00:30:12.000+0000,2019-04-08T22:22:17.000+0000,,Implemented,New Feature,Major,['2.9.0'],,30000,30000,,,,100,['sdk-py-core'],['SDKs: Python'],"Python SDK should support stateful processing (https://beam.apache.org/blog/2017/02/13/stateful-processing.html)

In the meantime, runner capability matrix should be updated to show the lack of this feature (https://beam.apache.org/documentation/runners/capability-matrix/)

Use this as an umbrella issue for all related issues.",Python SDK support for Stateful Processing,6,1,,altay,True,,altay
beam,BEAM-2661,2017-07-23T18:42:56.000+0000,2018-07-31T21:55:29.000+0000,2019-03-15T22:14:02.000+0000,,Fixed,New Feature,Major,['2.7.0'],,17400,17400,,,,100,"['io-ideas', 'io-java-kudu']","['IO: ideas (proposals for new connectors, new designs, etc)', 'IO: Java: Kudu']","New IO for Apache Kudu ([https://kudu.apache.org/overview.html]).

This work is in progress [on this branch|https://github.com/timrobertson100/beam/tree/BEAM-2661-KuduIO] with design aspects documented below.
h2. The API

The {{KuduIO}} API requires the user to provide a function to convert objects into operations. This is similar to the {{JdbcIO}} but different to others, such as {{HBaseIO}} which requires a pre-transform stage beforehand to convert into the mutations to apply. It was originally intended to copy the {{HBaseIO}} approach, but this was not possible:
 # The Kudu [Operation|https://kudu.apache.org/apidocs/org/apache/kudu/client/Operation.html] is a fat class, and is a subclass of {{KuduRpc<OperationResponse>}}. It holds RPC logic, callbacks and a Kudu client. Because of this the {{Operation}} does not serialize and furthermore, the logic for encoding the operations (Insert, Upsert etc) in the Kudu Java API are one way only (no decode) because the server is written in C++.
 # An alternative could be to introduce a new object to beam (e.g. {{o.a.b.sdk.io.kudu.KuduOperation}}) to enable {{PCollection<KuduOperation>}}. This was considered but was discounted because:
 ## It is not a familiar API to those already knowing Kudu
 ## It still requires serialization and deserialization of the operations. Using the existing Kudu approach of serializing into compact byte arrays would require a decoder along the lines of [this almost complete example|https://gist.github.com/timrobertson100/df77d1337ba8f5609319751ee7c6e01e]. This is possible but has fragilities given the Kudu code itself continues to evolve. 
 ## It becomes a trivial codebase in Beam to maintain by defer the object to mutation mapping to within the KuduIO transform. {{JdbcIO}} gives us the precedent to do this.

h2. Testing framework

{{Kudu}} is written in C++. While a [TestMiniKuduCluster|https://github.com/cloudera/kudu/blob/master/java/kudu-client/src/test/java/org/apache/kudu/client/TestMiniKuduCluster.java] does exist in Java, it requires binaries to be available for the target environment which is not portable (edit: this is now a [work in progress|https://issues.apache.org/jira/browse/KUDU-2411] in Kudu). Therefore we opt for the following:
 # Unit tests will use a mock Kudu client
 # Integration tests will cover the full aspects of the {{KuduIO}} and use a Docker based Kudu instance",Add KuduIO,4,,,jbonofre,True,timrobertson100,jbonofre
beam,BEAM-2657,2017-07-22T05:51:34.000+0000,2017-08-09T19:41:05.000+0000,2019-06-10T12:55:56.000+0000,,Fixed,New Feature,Major,['2.2.0'],,,,,,,,"['io-ideas', 'io-java-solr']","['IO: ideas (proposals for new connectors, new designs, etc)', 'IO: Java: Solr']",I'm working on a new SolrIO ( this components borrow som design's idea from ElasticsearchIO ) providing both bounded source and sink.,Add Solr IO,3,,,caomanhdat,True,caomanhdat,caomanhdat
beam,BEAM-2653,2017-07-21T07:18:53.000+0000,2017-07-21T17:00:58.000+0000,2017-07-21T17:00:58.000+0000,,Not A Problem,New Feature,Minor,['Not applicable'],,,,,,,,"['beam-model', 'sdk-py-core']","['Beam Model: general programming model concepts, semantics', 'SDKs: Python']","Trying to sort a collection by key.

The Java SDK has this implementation: https://beam.apache.org/documentation/sdks/javadoc/2.0.0/org/apache/beam/sdk/values/KV.OrderByKey.html

But it seems to be missing in the Python SDK.


",Python SDK missing OrderByKey,2,,,srfrnk,True,,srfrnk
beam,BEAM-2645,2017-07-20T02:33:36.000+0000,,2017-07-20T02:33:36.000+0000,,,New Feature,Major,,,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']",,Implement DisplayData translation to/from protos,1,,,kenn,True,,kenn
beam,BEAM-2644,2017-07-19T18:55:37.000+0000,2017-08-30T20:18:34.000+0000,2017-08-30T20:18:35.000+0000,,Fixed,New Feature,Major,['2.2.0'],,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","Many transforms that take ValueProvider's have different codepaths for when the provider is accessible or not. However, as far as I can tell, there is no good way to construct a transform with an inaccessible ValueProvider, and then test how it runs with an actual value supplied.

The only way I could come up with is mimicking https://github.com/apache/beam/blob/master/sdks/java/core/src/test/java/org/apache/beam/sdk/options/ValueProviderTest.java#L202 , which is very ugly.",Make it easier to test runtime-accessible ValueProvider's,2,,,jkff,True,jkff,jkff
beam,BEAM-2643,2017-07-19T18:42:10.000+0000,2017-08-10T20:40:42.000+0000,2017-08-10T20:40:42.000+0000,,Fixed,New Feature,Major,['2.2.0'],,,,,,,,['sdk-py-core'],['SDKs: Python'],"Java SDK now has TextIO.read_all() API that allows reading a massive number of files by moving from using the BoundedSource API (which may perform expensive source operations on the control plane) to using ParDo operations.

https://github.com/apache/beam/blob/master/sdks/java/core/src/main/java/org/apache/beam/sdk/io/TextIO.java#L170

This API should be added for Python SDK as well.

This form of reading files does not support dynamic work rebalancing for now. But this should not matter much when reading a massive number of relatively small files. In the future this API can support dynamic work rebalancing through Splittable DoFn.

cc: [~jkff]",Add TextIO and AvroIO read transforms that can read a PCollection of files,3,,,chamikara,True,chamikara,chamikara
beam,BEAM-2640,2017-07-19T17:51:35.000+0000,2017-07-27T22:25:18.000+0000,2017-07-27T22:25:18.000+0000,,Fixed,New Feature,Major,['2.2.0'],,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","When you have a ValueProvider<T> that may or may not be accessible at construction time, a common task is to wrap it into a single-element PCollection<T>. This is especially common when migrating an IO connector that used something like Create.of(query) followed by a ParDo, to having query be a ValueProvider.

Currently this is done in an icky way (e.g. https://github.com/apache/beam/blob/master/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/datastore/DatastoreV1.java#L615)

We should have a convenience helper for that.",Introduce Create.ofProvider(ValueProvider),2,,,jkff,True,jkff,jkff
beam,BEAM-2596,2017-07-12T00:33:11.000+0000,,2019-04-30T18:30:23.000+0000,,,New Feature,Major,,,,,,,,,"['build-system', 'testing']","['Build, CI, release systems and processes', 'Testing: general infrastructure']",,Break up Jenkins PreCommit into individual steps.,2,,,jaku,True,jaku,jaku
beam,BEAM-2588,2017-07-11T22:03:10.000+0000,2018-06-15T02:05:18.000+0000,2018-06-15T02:05:18.000+0000,,Fixed,New Feature,Major,['2.6.0'],,21600,21600,,,,100,['runner-flink'],['Runners: Flink runner'],The portable Flink runner needs to be wired into a job server so that it can accept jobs the job api (https://s.apache.org/beam-job-api).,Portable Flink Runner Job API,6,,['portability'],kenn,True,robertwb,kenn
beam,BEAM-2586,2017-07-11T18:30:47.000+0000,2017-08-07T19:54:01.000+0000,2017-08-07T19:54:01.000+0000,,Won't Fix,New Feature,Minor,['Not applicable'],,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","We frequently process text files delimited by something other than newlines, including delimited only by end of file.

First option:
When we want to delimit by commas (or something else), we could use TextIO to read in line by line and apply a transform to split each line on commas. When we want to delimit by whole file, we could combine the elements of the PCollection output from TextIO that come from the same file into one element.

Second option:
Alternatively to complicating (and slowing) our pipelines with the methods above, we could write custom FileBasedSources for each use case.

Third option:
Preferably, we'd like to generalize TextIO to accept delimiters other than the default: \n, \r, \r\n.

I'll attach a pull request for how we envision this generalization of TextIO to look.

If this is not the direction Beam would like to go with TextIO, then we'll stick to maintaining our own TextIO or our own FileBasedSources to achieve this functionality.",Accommodate custom delimiters in TextIO,2,,,christophhebert,True,davor,christophhebert
beam,BEAM-2584,2017-07-11T16:49:21.000+0000,,2018-02-14T18:14:31.000+0000,,,New Feature,Minor,,['Not applicable'],,,,,,,['io-ideas'],"['IO: ideas (proposals for new connectors, new designs, etc)']","Given the current work to support IOs for in-memory data stores like Redis and Memcached it makes probably sense to have a connector for the common API that Java has for this type of systems, with this we can support other in-memory grids like Apache Ignite, Hazelcast, etc.
Note: I am not 100% sure if the API has a generic way to discover at least per server splits but this is worth exploring.",Create a JCache based IO,2,,,iemejia,True,,iemejia
beam,BEAM-2558,2017-07-06T21:17:44.000+0000,2017-07-28T22:24:30.000+0000,2017-07-28T22:24:30.000+0000,,Fixed,New Feature,Major,['2.2.0'],,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","This should be in the style of {{CoderProperties}} or {{EqualsTester}}: the user should provide some inputs, and it should exercise potential paths by which those elements may be accumulated and ensure that they all produce the same results.",Add a CombineFnTester,3,,,tgroh,True,tgroh,tgroh
beam,BEAM-2548,2017-06-30T18:03:04.000+0000,,2017-06-30T18:03:04.000+0000,,,New Feature,Major,,,,,,,,,['sdk-py-core'],['SDKs: Python'],"A proper batching DoFn should remember the window associated with each element, hold the timestamp back, and re-emit the results with the windows/timestamps of the inputs

cc: [~robertwb]",Batching DoFn,2,,,altay,True,,altay
beam,BEAM-2547,2017-06-30T17:59:50.000+0000,,2018-02-14T18:22:55.000+0000,,,New Feature,Major,,,,,,,,,['io-ideas'],"['IO: ideas (proposals for new connectors, new designs, etc)']","Implement IO Source for Java and Python SDKs

Work on this had previously been done for the Google Cloud DataFlow Java SDK but was later removed. [old code|https://github.com/GoogleCloudPlatform/DataflowJavaSDK/tree/e0e56e0911e18ad08c5f9ed245c76849503ab7c7/contrib/firebaseio/src/main/java/com/google/cloud/dataflow/contrib/firebase], related [pull request|https://github.com/GoogleCloudPlatform/DataflowJavaSDK/pull/69]

CC: [~altay] , [~chamikara]",FireBase IO,2,,,patrick.reames,True,,patrick.reames
beam,BEAM-2546,2017-06-30T08:37:54.000+0000,,2019-04-30T18:29:28.000+0000,,,New Feature,Major,,,,,,,,,['io-ideas'],"['IO: ideas (proposals for new connectors, new designs, etc)']",,Create InfluxDbIO,2,1,,jbonofre,True,jbonofre,jbonofre
beam,BEAM-2535,2017-06-28T19:56:04.000+0000,,2019-05-06T21:27:05.000+0000,,,New Feature,Major,,,13800,13800,,,,100,"['beam-model', 'sdk-java-core']","['Beam Model: general programming model concepts, semantics', 'SDKs: Java: pipeline construction, core transformations']","Today, we have insufficient control over the event time timestamp of elements output from a timer callback.

1. For an event time timer, it is the timestamp of the timer itself.
 2. For a processing time timer, it is the current input watermark at the time of processing.

But for both of these, we may want to reserve the right to output a particular time, aka set a ""watermark hold"".

A naive implementation of a {{TimerWithWatermarkHold}} would work for making sure output is not droppable, but does not fully explain window expiration and late data/timer dropping.

In the natural interpretation of a timer as a feedback loop on a transform, timers should be viewed as another channel of input, with a watermark, and items on that channel _all need event time timestamps even if they are delivered according to a different time domain_.

I propose that the specification for when a timer should fire should be separated (with nice defaults) from the specification of the event time of resulting outputs. These timestamps will determine a side channel with a new ""timer watermark"" that constrains the output watermark.
 - We still need to fire event time timers according to the input watermark, so that event time timers fire.
 - Late data dropping and window expiration will be in terms of the minimum of the input watermark and the timer watermark. In this way, whenever a timer is set, the window is not going to be garbage collected.
 - We will need to make sure we have a way to ""wake up"" a window once it is expired; this may be as simple as exhausting the timer channel as soon as the input watermark indicates expiration of a window

This is mostly aimed at end-user timers in a stateful+timely {{DoFn}}. It seems reasonable to use timers as an implementation detail (e.g. in runners-core utilities) without wanting any of this additional machinery. For example, if there is no possibility of output from the timer callback.",Allow explicit output time independent of firing specification for all timers,6,,,kenn,True,shehzaadn,kenn
beam,BEAM-2513,2017-06-24T17:56:37.000+0000,2019-02-01T15:17:59.000+0000,2019-02-01T15:17:59.000+0000,,Duplicate,New Feature,Major,['Not applicable'],,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']",Motivation and proposed implementation in https://s.apache.org/textio-sdf,TextIO should support watching files for new entries,2,,,jkff,True,jkff,jkff
beam,BEAM-2512,2017-06-24T17:56:15.000+0000,2017-08-05T00:52:48.000+0000,2019-02-01T15:17:59.000+0000,,Fixed,New Feature,Major,['2.2.0'],,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']",Motivation and proposed implementation in https://s.apache.org/textio-sdf,TextIO should support watching for new files,3,,,jkff,True,jkff,jkff
beam,BEAM-2511,2017-06-24T17:51:32.000+0000,2017-07-11T23:27:10.000+0000,2017-07-11T23:27:10.000+0000,,Fixed,New Feature,Major,['2.2.0'],,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']",Motivation and proposed implementation in https://s.apache.org/textio-sdf,TextIO should support reading a PCollection of filenames,3,,,jkff,True,jkff,jkff
beam,BEAM-2507,2017-06-23T02:47:30.000+0000,,2018-06-21T21:30:04.000+0000,,,New Feature,Major,,,,,,,,,['runner-dataflow'],['Runners: Google Cloud Dataflow runner'],,Support stateful processing in merging windows in DataflowRunner,2,,,kenn,True,,kenn
beam,BEAM-2500,2017-06-22T17:11:53.000+0000,2018-01-27T20:37:38.000+0000,2019-06-10T10:13:32.000+0000,,Fixed,New Feature,Minor,['2.3.0'],,,,,,,,['io-java-aws'],['IO: Java: Amazon Web Services'],"Note that this is for providing direct integration with S3 as an Apache Beam FileSystem.

There is already support for using the Hadoop S3 connector by depending on the Hadoop File System module[1], configuring HadoopFileSystemOptions[2] with a S3 configuration[3].

1: https://github.com/apache/beam/tree/master/sdks/java/io/hadoop-file-system
2: https://github.com/apache/beam/blob/master/sdks/java/io/hadoop-file-system/src/main/java/org/apache/beam/sdk/io/hdfs/HadoopFileSystemOptions.java#L53
3: https://wiki.apache.org/hadoop/AmazonS3
",Add support for S3 as a Apache Beam FileSystem,13,3,,lcwik,True,jmarble,lcwik
beam,BEAM-2478,2017-06-20T01:55:15.000+0000,2019-03-11T21:57:32.000+0000,2019-04-30T18:31:33.000+0000,,Won't Do,New Feature,Major,['Not applicable'],,,,,,,,['dsl-sql'],['DSLs: SQL'],eg: COUNT(DISTINCT empno),Distinct Aggregates,4,,,lzljs3620320,True,mingmxu,lzljs3620320
beam,BEAM-2456,2017-06-16T07:05:00.000+0000,2019-01-09T12:40:53.000+0000,2019-01-09T12:40:53.000+0000,,Fixed,New Feature,Major,['Not applicable'],,,,,,,,['sdk-ideas'],['SDKs: Coordination on new language-specific SDKs'],"The Spark runner provides a convenient feature which is the metric sink.

By configuration, it allows us to configure a metric sink using {{metrics.properties}} configuration containing:

{code}
driver.sink.graphite.class=org.apache.beam.runners.spark.metrics.sink.GraphiteSink
driver.sink.graphite.host=localhost
driver.sink.graphite.port=2003
driver.sink.graphite.prefix=spark
driver.sink.graphite.period=1
driver.sink.graphite.unit=SECONDS 
{code}

This approach is very convenient to send the metric to any sink. I think we can apply this logic in generic way working with any runner.

The idea is to use a metric poll thread in the pipeline (that we can enable via {{PipelineOptions}}) and send to a sink.

I started a PoC about that.",Introduce generic metric poll thread interceptor & generic sink,1,,,jbonofre,True,jbonofre,jbonofre
beam,BEAM-2431,2017-06-09T18:36:44.000+0000,2017-08-18T17:59:57.000+0000,2017-09-20T18:37:58.000+0000,,Fixed,New Feature,Major,['2.2.0'],,,,,,,,['beam-model'],"['Beam Model: general programming model concepts, semantics']","The ""Runner API"" today is actually just a definition of what constitutes a Beam pipeline. It needs to actually be a (very small) API.

This would allow e.g. a Java-based job launcher to respond to launch requests and state queries from a Python-based adapter.

The expected API would be something like a distillation of the APIs for PipelineRunner and PipelineResult (which is really ""Job"") via analyzing how these both look in Java and Python.",Model Runner interactions in RPC layer for Runner API,3,,['portability'],kenn,True,sb2nov,kenn
beam,BEAM-2430,2017-06-09T18:32:55.000+0000,2018-02-05T23:49:01.000+0000,2018-02-05T23:49:01.000+0000,,Done,New Feature,Major,['2.3.0'],,,,,,,,['runner-core'],['Runners: shared functionality for all runners'],"As the portability framework comes into focus, let's fill out the support code for making it easy to onboard a new runner.

There is some amount of using the Fn API that has to do only with the fact that a runner is implemented in Java, and is not specific to that runner. This should be part of the runners-core library, and designed so that a runner can set it up however it likes, and just pass elements without having to explicitly manage things like requests, responses, protos, and coders.
",Java FnApiDoFnRunner to share across runners,4,,['portability'],kenn,True,tgroh,kenn
beam,BEAM-2417,2017-06-06T11:39:40.000+0000,,2019-02-07T05:07:58.000+0000,,,New Feature,Major,,,,,,,,,['io-ideas'],"['IO: ideas (proposals for new connectors, new designs, etc)']",,Add ZeroMQIO,1,,['triaged'],jbonofre,True,jbonofre,jbonofre
beam,BEAM-2416,2017-06-06T11:39:23.000+0000,,2019-04-30T18:30:44.000+0000,,,New Feature,Major,,,,,,,,,['io-ideas'],"['IO: ideas (proposals for new connectors, new designs, etc)']",,Add AkkaIO,2,,,jbonofre,True,jbonofre,jbonofre
beam,BEAM-2415,2017-06-06T11:38:11.000+0000,,2019-04-30T18:31:50.000+0000,,,New Feature,Major,,,,,,,,,['io-ideas'],"['IO: ideas (proposals for new connectors, new designs, etc)']",,Add FacebookIO,2,,,jbonofre,True,jbonofre,jbonofre
beam,BEAM-2414,2017-06-06T11:37:52.000+0000,,2019-04-30T18:31:49.000+0000,,,New Feature,Major,,,,,,,,,['io-ideas'],"['IO: ideas (proposals for new connectors, new designs, etc)']",,Add TwitterIO,4,,,jbonofre,True,jbonofre,jbonofre
beam,BEAM-2403,2017-06-02T08:07:25.000+0000,2018-04-10T00:09:26.000+0000,2018-04-10T00:09:26.000+0000,,Duplicate,New Feature,Minor,['2.5.0'],['2.0.0'],,,,,,,['io-java-gcp'],"['IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)']","Currently every BigQuery job runs in BATCH mode when issued from Beam which could delay the execution of a scheduled job unnecessarily. By allowing to provide the priority INTERACTIVE with BigQueryIO this could be resolved.

See https://stackoverflow.com/questions/44198891/change-google-cloud-dataflow-bigquery-priority/44223123",Provide BigQueryIO with method to indicate Priority (INTERACTIVE/BATCH),3,,,jroxtheworld,True,chamikara,jroxtheworld
beam,BEAM-2402,2017-06-02T02:54:34.000+0000,,2019-04-30T18:29:19.000+0000,,,New Feature,Major,,,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","We need a timestamp-driven trigger to use as a cheaper (or more efficient) version of the ProcessingTime trigger.

The problem of using ProcessingTime trigger is that current runners' supports are not very efficient, and couldn't work for pipelines that have lots of keys (for example, flink runner will scan timers for all keys when watermark advance).

We have used AfterPane.elementGapAtMost() trigger in our production, and want to merge it back. And, we believe it could be the solution for people who have the similar issue.

Implementation for reference:
https://github.com/apache/beam/compare/master...peihe:custom-after-pane?expand=1",Support AfterPane.elementGapAtMost() trigger and its combination with elementCountAtLeast(),4,1,,peihe0@gmail.com,True,peihe0@gmail.com,peihe0@gmail.com
beam,BEAM-2399,2017-06-01T18:07:10.000+0000,,2017-07-18T21:20:10.000+0000,,,New Feature,Major,,,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']",,Add a registration mechanism for WindowFns and their URNs.,1,,,robertwb,True,,robertwb
beam,BEAM-2395,2017-06-01T09:15:02.000+0000,2018-09-12T21:47:02.000+0000,2018-09-12T21:47:02.000+0000,,Duplicate,New Feature,Major,['Not applicable'],,,,,,,,['sdk-py-core'],['SDKs: Python'],"Developing a read and write IO for BigTable for the Python SDK. 

Working / design document can be found here: https://docs.google.com/document/d/1iXeQvIAsGjp9orleDy0o5ExU-eMqWesgvtt231UoaPg/edit?usp=sharing",BigtableIO for Python SDK,4,1,['features'],matthiasa4,True,matthiasa4,matthiasa4
beam,BEAM-2363,2017-05-25T17:06:16.000+0000,,2019-02-07T05:07:54.000+0000,,,New Feature,Minor,,['2.0.0'],1200,1200,,,,100,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']",https://lists.apache.org/thread.html/0adff14ac37b798726ad2dd4b107ed4095a5204800d501cb88057820@%3Cdev.beam.apache.org%3E,Function composition for SerializableFunction,2,,['triaged'],wtanaka,True,wtanaka,wtanaka
beam,BEAM-2357,2017-05-24T09:20:13.000+0000,2017-07-20T08:17:16.000+0000,2019-06-10T10:17:25.000+0000,,Fixed,New Feature,Minor,['2.1.0'],,,,,,,,['io-java-hcatalog'],['IO: Java: HCatalog'],"Support for reading and writing from Hive's catalog HCatalog. HCatalogIO will allow access to the Hive records so they can be processed with Beam.

This JIRA is to create a native implementation using the HCatalog ReaderWriter API.",Add HCatalogIO (Hive),2,,,iemejia,True,sesh.cr@gmail.com,iemejia
beam,BEAM-2355,2017-05-24T02:19:25.000+0000,2017-07-19T14:37:33.000+0000,2017-09-07T17:11:33.000+0000,,Fixed,New Feature,Major,['Not applicable'],,,,,,,,['dsl-sql'],['DSLs: SQL'],https://calcite.apache.org/docs/reference.html#comparison-operators,support comparison operator,,,,xumingming,True,xumingming,xumingming
beam,BEAM-2333,2017-05-19T20:02:02.000+0000,2017-08-28T21:43:13.000+0000,2017-09-11T16:06:34.000+0000,,Fixed,New Feature,Major,['2.1.0'],,,,,,,,['runner-core'],['Runners: shared functionality for all runners'],,Rehydrate Pipeline from Runner API proto,2,,['portability'],kenn,True,kenn,kenn
beam,BEAM-2328,2017-05-19T16:36:42.000+0000,2017-09-11T06:03:36.000+0000,2019-06-10T10:18:04.000+0000,,Fixed,New Feature,Major,['2.2.0'],,,,,,,,['io-java-tika'],['IO: Java: Tika'],"Apache Tika is a popular project that offers an extensive support for parsing the variety of file formats. It is used in many projects including Lucene and Elastic Search. 
Supporting a Tika Input (Read) at the Beam level would be of major interest to many users.

PR is to follow",Add TikaIO,6,1,,sergey_beryozkin,True,sergey_beryozkin,sergey_beryozkin
beam,BEAM-2292,2017-05-14T18:20:01.000+0000,2017-05-27T21:43:21.000+0000,2017-05-27T21:43:26.000+0000,,Fixed,New Feature,Major,['Not applicable'],,,,,,,,['dsl-sql'],['DSLs: SQL'],"To support method {{registerPCollectionAsTable}} in BeamSQL DSL.

This feature makes it possible to break down a complex query into several sub-queries, and assemble it by developers. ",PCollection as a Table,2,,,mingmxu,True,mingmxu,mingmxu
beam,BEAM-2276,2017-05-12T08:45:12.000+0000,2017-05-26T05:58:22.000+0000,2017-06-06T12:08:52.000+0000,,Fixed,New Feature,Major,['2.1.0'],,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","It should be easier for Beam users to specify filename policy which understands windowing concepts.

Discussion here 

https://lists.apache.org/thread.html/b53e437894eb511c9161d34fc77c657300b77a7be75f0fab6566b3d6@%3Cdev.beam.apache.org%3E",make it easier to specify windowed filename policy,2,,,zborisha,True,zborisha,zborisha
beam,BEAM-2255,2017-05-11T07:47:59.000+0000,2017-05-17T02:38:19.000+0000,2017-09-07T17:12:14.000+0000,,Fixed,New Feature,Major,['Not applicable'],,,,,,,,['dsl-sql'],['DSLs: SQL'],"Currently Beam does not have `global sort` support[1]. So we will implement a limited version of `ORDER BY` using `org.apache.beam.sdk.transforms.Top` for now. So it will support sql like:

{code:sql}
select * from t order by id desc limit 10;
{code}

but not support `ORDER BY` without a `limit`:

{code:sql}
select * from t order by id desc
{code}

and also not support `ORDER BY` with a `offset`:

{code:sql}
select * from t order by id desc limit 10, 10;
{code}

[1]. https://lists.apache.org/thread.html/bc0e65a3bb653b8fd0db96bcd4c9da5af71a71af5a5639a472167808@1464278191@%3Cdev.beam.apache.org%3E
",support ORDER BY clause,1,,,xumingming,True,xumingming,xumingming
beam,BEAM-2254,2017-05-11T06:19:20.000+0000,2017-05-14T05:06:44.000+0000,2017-05-14T18:12:28.000+0000,,Duplicate,New Feature,Major,['Not applicable'],,,,,,,,['dsl-sql'],['DSLs: SQL'],,support UDF,1,,,mingmxu,True,mingmxu,mingmxu
beam,BEAM-2248,2017-05-10T11:44:55.000+0000,2017-05-31T01:54:29.000+0000,2019-06-10T10:19:58.000+0000,,Fixed,New Feature,Major,['2.1.0'],,,,,,,,['io-java-kafka'],['IO: Java: Kafka'],"This Kafka 0.10.x adds support for a searchable index for each topic based off of message timestamps. It enables consumer support for offset lookup by timestamp.
So we can add a start read time to set start offset.",KafkaIO support to use start read time to set start offset,3,,,lzljs3620320,True,lzljs3620320,lzljs3620320
beam,BEAM-2247,2017-05-10T10:07:38.000+0000,2017-06-21T02:06:57.000+0000,2017-06-21T02:06:57.000+0000,,Fixed,New Feature,Major,['Not applicable'],,,,,,,,['dsl-sql'],['DSLs: SQL'],https://calcite.apache.org/docs/reference.html#datetime-functions,support date/time functions,2,,['dsl_sql_merge'],xumingming,True,xumingming,xumingming
beam,BEAM-2228,2017-05-09T12:26:15.000+0000,,2019-04-30T18:33:54.000+0000,,,New Feature,Major,,,,,,,,,['io-ideas'],"['IO: ideas (proposals for new connectors, new designs, etc)']","Currently, {{XmlIO}} can ""only"" read from a XML file (using {{from()}}).

I could be interesting to have XmlIO ""generic"" {{PTransform}} to convert each element in an incoming {{PCollection}} as POJO.

For instance, doing something like:

{code}
pipeline.apply(""Receive XML from JMS"", JmsIO.read().withConnectionFactory(xx).withQueue(""MyQueue""))
            .apply(""Convert as POJO"", XmlIO.read()...)
            .apply(""Convert POJO to something else"", ....)
            .apply(""Store somewhere"", ...))
{code}",XmlIO could work with a PCollection,1,,,jbonofre,True,jbonofre,jbonofre
beam,BEAM-2203,2017-05-07T08:07:17.000+0000,,2019-04-30T18:32:52.000+0000,,,New Feature,Major,,,,,,,,,['dsl-sql'],['DSLs: SQL'],,Arithmetic operators: support DATETIME & DATETIME_INTERVAL,5,,,xumingming,True,kedin,xumingming
beam,BEAM-2200,2017-05-06T12:35:41.000+0000,,2017-05-06T12:35:41.000+0000,,,New Feature,Major,,,,,,,,,['dsl-sql'],['DSLs: SQL'],,Support describe statement,,,,xumingming,True,,xumingming
beam,BEAM-2199,2017-05-06T12:34:43.000+0000,2017-05-20T09:01:44.000+0000,2017-05-20T09:01:44.000+0000,,Duplicate,New Feature,Major,['Not applicable'],,,,,,,,['dsl-sql'],['DSLs: SQL'],,Support explain statement,,,,xumingming,True,,xumingming
beam,BEAM-2198,2017-05-06T12:33:55.000+0000,,2017-05-06T12:33:55.000+0000,,,New Feature,Major,,,,,,,,,['dsl-sql'],['DSLs: SQL'],,Support merge statement,,,,xumingming,True,,xumingming
beam,BEAM-2197,2017-05-06T12:31:05.000+0000,,2017-05-06T12:31:05.000+0000,,,New Feature,Major,,,,,,,,,['dsl-sql'],['DSLs: SQL'],,Support explain statement,,,,xumingming,True,,xumingming
beam,BEAM-2196,2017-05-06T12:22:57.000+0000,2017-07-18T22:53:12.000+0000,2017-07-18T22:53:12.000+0000,,Fixed,New Feature,Major,['Not applicable'],,,,,,,,['dsl-sql'],['DSLs: SQL'],,support UDF,2,,,xumingming,True,mingmxu,xumingming
beam,BEAM-2195,2017-05-06T12:17:46.000+0000,2017-05-13T06:10:57.000+0000,2017-09-07T17:12:42.000+0000,,Fixed,New Feature,Major,['Not applicable'],,,,,,,,['dsl-sql'],['DSLs: SQL'],https://calcite.apache.org/docs/reference.html#conditional-functions-and-operators,support conditional functions & operators,1,,,xumingming,True,xumingming,xumingming
beam,BEAM-2194,2017-05-06T12:11:33.000+0000,,2018-08-08T20:26:43.000+0000,,,New Feature,Major,,,,,,,,,['dsl-sql'],['DSLs: SQL'],,JOIN: cross join,3,,,xumingming,True,,xumingming
beam,BEAM-2193,2017-05-06T12:10:46.000+0000,2017-07-02T13:53:13.000+0000,2017-07-02T13:53:13.000+0000,,Fixed,New Feature,Major,['Not applicable'],,,,,,,,['dsl-sql'],['DSLs: SQL'],,"JOIN: inner join, left join, right join, full outer join",2,,['dsl_sql_merge'],xumingming,True,xumingming,xumingming
beam,BEAM-2192,2017-05-06T12:02:25.000+0000,2017-06-27T09:09:57.000+0000,2017-06-27T09:09:57.000+0000,,Fixed,New Feature,Major,['Not applicable'],,,,,,,,['dsl-sql'],['DSLs: SQL'],,support DISTINCT select,1,,['dsl_sql_merge'],xumingming,True,mingmxu,xumingming
beam,BEAM-2185,2017-05-05T21:08:22.000+0000,,2019-02-13T17:54:07.000+0000,,,New Feature,Major,,,,,,,,,['io-java-kafka'],['IO: Java: Kafka'],"KafkaIO could be a useful source for batch applications as well. It could implement a bounded source. The primary question is how the bounds are specified.

One option : Source specifies a time period (say 9am-10am), and KafkaIO fetches appropriate start and end offsets based on time-index in Kafka. This would suite many batch applications that are launched on a scheduled.

Another option is to always read till the end and commit the offsets to Kafka. Handling failures and multiple runs of a task might be complicated.

",KafkaIO bounded source,8,3,,rangadi,True,,rangadi
beam,BEAM-2150,2017-05-03T14:10:55.000+0000,2017-05-11T02:00:35.000+0000,2017-05-11T02:00:35.000+0000,,Fixed,New Feature,Minor,['2.1.0'],,,,,,,,"['io-java-gcp', 'sdk-java-core']","['IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)', 'SDKs: Java: pipeline construction, core transformations']","When working with heavily nested folder structures in Google Cloud Storage, it's great to make use of recursive wildcards, which the current API explicitly does not support.

This code hasn't been touched in 2 years so it's likely that simply no one's gotten around to it yet.",Support for recursive wildcards in GcsPath,4,,,meunierd,True,meunierd,meunierd
beam,BEAM-2147,2017-05-03T10:29:12.000+0000,2017-05-15T18:33:05.000+0000,2017-05-15T18:33:05.000+0000,,Fixed,New Feature,Major,['2.0.0'],,,,,,,,['runner-dataflow'],['Runners: Google Cloud Dataflow runner'],These are disabled currently because of bugs in the {{TestDataflowRunner}}'s diagnosis of whether something is an {{AssertionError}} or some other {{RuntimeException}}.,Re-enable UsesTimersInParDo tests for DataflowRunner,2,,,kenn,True,kenn,kenn
beam,BEAM-2146,2017-05-03T10:26:43.000+0000,,2019-04-30T18:33:57.000+0000,,,New Feature,Major,,,,,,,,,['runner-dataflow'],['Runners: Google Cloud Dataflow runner'],"The {{ValidatesRunner}} test suite works on Dataflow with streaming mode forced, and just needs CI setup.",Continuously run ValidatesRunner suite on Dataflow forcing streaming mode,1,,,kenn,True,alanmyrvold,kenn
beam,BEAM-2112,2017-04-28T13:34:31.000+0000,,2019-04-30T18:31:19.000+0000,,,New Feature,Major,,,,,,,,,['runner-spark'],['Runners: Spark runner'],"As a test, Nexmark query7 can be used
run Nexmark query7 (https://github.com/iemejia/beam/tree/BEAM-160-nexmark) in streaming mode using Spark.
Run main in
{code}org.apache.beam.integration.nexmark.drivers.NexmarkSparkDriver{code}
with VMOptions:
{code} -Dspark.ui.enabled=false -DSPARK_LOCAL_IP=localhost -Dsun.io.serialization.extendedDebugInfo=true {code}
with Program arguments:
{code}--query=7  --streaming=true --numEventGenerators=4 --manageResources=false --monitorJobs=true --enforceEncodability=false --enforceImmutability=false{code}
StackTrace is 
{code}
Exception in thread ""main"" java.lang.IllegalStateException: No TransformEvaluator registered for UNBOUNDED transform class org.apache.beam.sdk.transforms.View$CreatePCollectionView
	at com.google.common.base.Preconditions.checkState(Preconditions.java:518)
	at org.apache.beam.runners.spark.translation.streaming.StreamingTransformTranslator$Translator.translateUnbounded(StreamingTransformTranslator.java:529)
	at org.apache.beam.runners.spark.SparkRunner$Evaluator.translate(SparkRunner.java:435)
	at org.apache.beam.runners.spark.SparkRunner$Evaluator.doVisitTransform(SparkRunner.java:405)
	at org.apache.beam.runners.spark.SparkRunner$Evaluator.visitPrimitiveTransform(SparkRunner.java:395)
	at org.apache.beam.sdk.runners.TransformHierarchy$Node.visit(TransformHierarchy.java:488)
	at org.apache.beam.sdk.runners.TransformHierarchy$Node.visit(TransformHierarchy.java:483)
	at org.apache.beam.sdk.runners.TransformHierarchy$Node.visit(TransformHierarchy.java:483)
	at org.apache.beam.sdk.runners.TransformHierarchy$Node.visit(TransformHierarchy.java:483)
	at org.apache.beam.sdk.runners.TransformHierarchy$Node.access$400(TransformHierarchy.java:232)
	at org.apache.beam.sdk.runners.TransformHierarchy.visit(TransformHierarchy.java:207)
	at org.apache.beam.sdk.Pipeline.traverseTopologically(Pipeline.java:384)
	at org.apache.beam.runners.spark.translation.streaming.SparkRunnerStreamingContextFactory.call(SparkRunnerStreamingContextFactory.java:88)
	at org.apache.beam.runners.spark.translation.streaming.SparkRunnerStreamingContextFactory.call(SparkRunnerStreamingContextFactory.java:47)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$$anonfun$10.apply(JavaStreamingContext.scala:776)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$$anonfun$10.apply(JavaStreamingContext.scala:775)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:864)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:775)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at org.apache.beam.runners.spark.SparkRunner.run(SparkRunner.java:155)
	at org.apache.beam.runners.spark.SparkRunner.run(SparkRunner.java:85)
	at org.apache.beam.sdk.Pipeline.run(Pipeline.java:277)
	at org.apache.beam.integration.nexmark.NexmarkRunner.run(NexmarkRunner.java:1281)
	at org.apache.beam.integration.nexmark.NexmarkDriver.runAll(NexmarkDriver.java:69)
	at org.apache.beam.integration.nexmark.drivers.NexmarkSparkDriver.main(NexmarkSparkDriver.java:46)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
{code}",Add support for PCollectionView in spark runner in streaming mode,2,,,echauchot,True,amitsela,echauchot
beam,BEAM-2097,2017-04-27T14:21:34.000+0000,,2017-04-27T22:08:07.000+0000,,,New Feature,Major,,,,,,,,,['sdk-py-core'],['SDKs: Python'],"It would be great if SQL support could be added to Python SDK similar to JDBC IO in Java SDK.

May be SQLAlchemy Core could be used to support multiple databases and dialects to provide unified API analogous to JDBC?",Python SDK should support SQL connector like JDBC IO,2,,,rajas.abhyankar,True,,rajas.abhyankar
beam,BEAM-2089,2017-04-27T08:31:40.000+0000,,2019-04-30T18:31:47.000+0000,,,New Feature,Major,,,,,,,,,['runner-gearpump'],['Runners: Gearpump runner'],Allow user to use Maven to submit pipelines to Gearpump cluster without building a shaded jar,Enable programmatic execution of Gearpump pipelines.,1,,,HuafengWang,True,HuafengWang,HuafengWang
beam,BEAM-2088,2017-04-27T06:53:18.000+0000,2019-01-09T13:18:56.000+0000,2019-01-09T13:18:56.000+0000,,Duplicate,New Feature,Major,['Not applicable'],,,,,,,,['io-ideas'],"['IO: ideas (proposals for new connectors, new designs, etc)']",I started to work on a {{SocketIO}} (unbounded) dealing with simple data (text) received over the network.,Add SocketIO,1,,,jbonofre,True,,jbonofre
beam,BEAM-2083,2017-04-26T20:38:40.000+0000,2018-06-07T21:49:15.000+0000,2018-06-07T21:49:15.000+0000,,Fixed,New Feature,Major,['2.5.0'],,,,,,,,['sdk-go'],['SDKs: Go'],"Allow users of the Go programming language (https://golang.org/) to write Beam pipelines in this language. The effort is focusing on full-fledged SDK that leverages the Beam Fn API to bootstrap a native Go experience.

Initial design:

        https://s.apache.org/beam-go-sdk-design-rfc

Development in the master branch. Work in progress. YMMV.",Develop a Go SDK for Beam,12,5,,wcn3,True,herohde,wcn3
beam,BEAM-2063,2017-04-24T15:56:50.000+0000,2017-05-02T23:50:09.000+0000,2017-05-14T18:16:20.000+0000,,Duplicate,New Feature,Blocker,['Not applicable'],,,,,,,,['dsl-sql'],['DSLs: SQL'],"Support data type DATE/TIME/TIMESTAMP, which could be used in aggregation.

New data type support include changes:
1. type mapping in {{BeamSQLRow}}, and coder support;
2. {{BeamSqlPrimitive}} for static values;",type DATE/TIME/TIMESTAMP support,1,,,mingmxu,True,xumingming,mingmxu
beam,BEAM-2059,2017-04-24T13:08:37.000+0000,2017-07-12T17:10:38.000+0000,2017-07-12T17:10:38.000+0000,,Fixed,New Feature,Minor,['2.1.0'],,,,,,,,['runner-dataflow'],['Runners: Google Cloud Dataflow runner'],Metrics are currently only available in batch mode.,Implement Metrics support for streaming Dataflow runner,2,,,meunierd,True,bchambers,meunierd
beam,BEAM-2054,2017-04-22T18:23:50.000+0000,2017-04-24T15:55:45.000+0000,2017-04-24T16:49:30.000+0000,,Fixed,New Feature,Major,['2.0.0'],,,,,,,,['build-system'],"['Build, CI, release systems and processes']",,Upgrade dataflow.version to v1b3-rev196-1.22.0,1,,,drieber,True,drieber,drieber
beam,BEAM-2026,2017-04-20T11:38:30.000+0000,,2019-04-30T18:33:00.000+0000,,,New Feature,Major,,,,,,,,,['runner-direct'],['Runners: Direct runner and ULR for single machine testing and development'],"In documentation (https://beam.apache.org/documentation/runners/direct/) it is written that direct runner does not try to run efficiently, but it serves mostly for development and debugging.

I would suggest that there should be also an efficient direct runner. If Beam tries to be an unified programming model, for some smaller tasks I would love to implement them in Beam, just to keep the code in the same model, but it would be OK to run it as a normal smaller program (maybe inside one Docker container), without any distribution across multiple machines. In the future, if usage grows, I could then replace underlying runner with something distributed.",High performance direct runner,5,,,mitar,True,mitar,mitar
beam,BEAM-2010,2017-04-18T23:12:02.000+0000,2017-06-07T16:03:03.000+0000,2017-06-07T16:03:08.000+0000,,Fixed,New Feature,Major,['Not applicable'],,,,,,,,['dsl-sql'],['DSLs: SQL'],"Expose programming interfaces to use BeamSQL. With these APIs, developers can inject SQL in their code, to help create a pipeline from SQL statements. A proposed use case looks like:
{code}
BeamSQLEnvironment sqlEnv = BeamSQLEnvironment.create(options);

sqlEnv.addAsSource(BeamSqlTable table);
//...
sqlEnv.addAsSink(BeamSqlTable table);
//...

Pipeline pipeline = sqlEnv.explainPipeline(String sqlStatement);

pipeline.run().waitUntilFinish();
{code}",expose programming interface,2,,,mingmxu,True,mingmxu,mingmxu
beam,BEAM-2009,2017-04-18T23:00:30.000+0000,,2018-04-13T16:38:20.000+0000,,,New Feature,Major,,,,,,,,,['dsl-sql'],['DSLs: SQL'],"support JdbcIO in both source/sink part:
1. as source, JdbcIO read data from databases that supports JDBC such as Oracle/MySQL/Cassandra/...;
It leads to a bounded pipeline;
2. as sink, JdbcIO can persistent data from both unbounded and bounded pipeline;",support JdbcIO as source/sink,2,,,mingmxu,True,,mingmxu
beam,BEAM-2008,2017-04-18T22:53:38.000+0000,2017-05-13T03:08:45.000+0000,2017-07-13T08:20:07.000+0000,,Fixed,New Feature,Major,['Not applicable'],,,,,,,,['dsl-sql'],['DSLs: SQL'],"Support common-used aggregation functions in SQL, including:
COUNT
SUM
MAX
MIN
",aggregation functions support,2,,,mingmxu,True,mingmxu,mingmxu
beam,BEAM-2006,2017-04-18T22:50:24.000+0000,2017-05-10T17:13:01.000+0000,2017-05-14T18:14:26.000+0000,,Fixed,New Feature,Blocker,['Not applicable'],,,,,,,,['dsl-sql'],['DSLs: SQL'],"There're several methods, TUMBLE/HOP/SESSION, introduced in Calcite 1.12, which represent a window-aggregation operation. 

In BeamSQL, it's expected to leverage these methods to determine window function, set trigger strategy, also handle event_time/watermark properly. 
",window support ,2,,,mingmxu,True,mingmxu,mingmxu
beam,BEAM-2005,2017-04-18T21:10:08.000+0000,2017-05-02T04:38:35.000+0000,2019-06-10T10:42:38.000+0000,,Fixed,New Feature,Major,['2.0.0'],,,,,,,,['io-java-hadoop-file-system'],['IO: Java: Hadoop File System'],"Beam's FileSystem creates an abstraction for reading from files in many different places. 

We should add a Hadoop FileSystem implementation (https://hadoop.apache.org/docs/r2.8.0/api/org/apache/hadoop/fs/FileSystem.html) - that would enable us to read from any file system that implements FileSystem (including HDFS, azure, s3, etc..)

I'm investigating this now.",Add a Hadoop FileSystem implementation of Beam's FileSystem,7,1,,sisk,True,lcwik,sisk
beam,BEAM-1960,2017-04-13T06:51:08.000+0000,2017-04-21T05:41:31.000+0000,2019-06-10T10:44:08.000+0000,,Fixed,New Feature,Major,['2.0.0'],,,,,,,,['io-java-hadoop-format'],['IO: Java: Hadoop Format'],"Add Kubernetes cluster scripts for Elasticsearch and Cassandra for both large and small clusters to run Elasticsearch and Cassandra ITs and Performance tests.

Create separate clusters for small - integration test and large- performance test.",Hadoop InputFormat - Add Kubernetes large and small cluster Scripts for Cassandra and Elasticsearch tests,2,,,diptik,True,diptik,diptik
beam,BEAM-1946,2017-04-12T05:36:43.000+0000,,2019-05-13T15:41:55.000+0000,,,New Feature,Major,,,,,,,,,['io-ideas'],"['IO: ideas (proposals for new connectors, new designs, etc)']",Create a RestIO for read or write data by resful api.,Add IO module for restful api,18,6,"['IO', 'features', 'restful']",r7raul,True,jbonofre,r7raul
beam,BEAM-1941,2017-04-12T02:45:22.000+0000,,2019-04-30T18:29:59.000+0000,,,New Feature,Major,,,,,,,,,['runner-ideas'],['Runners: ideas for new Beam runners'],"The source watermark metrics show the consumer latency of Source. 
It allows the user to know the health of the job, or it can be used to monitor and alarm.
Since each runner is likely already tracking a watermark, another option here is to just have the runner report it appropriately, rather than having the source report it using metrics. This also addresses the fact that even if the source has advanced to 8:00, the runner may still know about buffered elements at 7:00, and so not advance the watermark all the way to 8:00. [~bchambers]
Includes:
1.Source watermark (`min` amongst all splits):
   type = Gauge, namespace = io, name = source_watermark
2.Source watermark per split:
   type = Gauge, namespace = io.splits, name = <split_id>.source_watermark",Add Watermark Metrics in Runners,6,1,,lzljs3620320,True,lzljs3620320,lzljs3620320
beam,BEAM-1936,2017-04-11T21:28:01.000+0000,,2017-10-20T19:42:50.000+0000,,,New Feature,Major,,,,,,,,,['io-java-gcp'],"['IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)']","Currently the PubsubIO runner only allows the caller to set a custom timestamp if the timestamp is defined in the attributes of the message.  This can be problematic when the user does not control the publisher.  In such a case, proper windowing of data requires the timestamp to be pulled out of the message payload.  

Since a payload could have arbitrary data, the user would have to provide a Function<T, String>() that would extract the timestamp from the payload:

PubsubIo.Read.timestampLabel(Function<T, String> extractor);",Allow user provided function to extract custom timestamp from payload in pubsubIO,3,,,berkoben,True,,berkoben
beam,BEAM-1928,2017-04-10T22:13:23.000+0000,2017-07-24T17:56:45.000+0000,2017-08-21T22:17:43.000+0000,,Fixed,New Feature,Major,['2.2.0'],,,,,,,,"['beam-model', 'runner-core']","['Beam Model: general programming model concepts, semantics', 'Runners: shared functionality for all runners']","Permit conversion of Java SDK components to runner API protocol buffers, and the extraction of those SDK components from the protocol buffers.",Populate Runner API Components from the Java SDK,2,,,tgroh,True,tgroh,tgroh
beam,BEAM-1919,2017-04-08T14:41:10.000+0000,,2017-05-11T06:02:22.000+0000,,,New Feature,Major,,,,,,,,,['sdk-ideas'],['SDKs: Coordination on new language-specific SDKs'],"A foundation for standard IO metrics, which could be used by all IOs.
A standard IO metric is a namespace + name combination which all IOs which report a metric in the same vein will adhere to.
Also, supply factories and members for these metrics, accessible to IOs via the SDK of the language they were written in.

[Proposal document|https://s.apache.org/standard-io-metrics]",Standard IO Metrics,3,,,aviemzur,True,,aviemzur
beam,BEAM-1906,2017-04-07T16:25:45.000+0000,2018-01-21T23:33:17.000+0000,2018-01-21T23:33:17.000+0000,,Resolved,New Feature,Major,['Not applicable'],,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']",,Serialize/Deserialize ParDos in the Java SDK via ParDoPayload,2,,,tgroh,True,kenn,tgroh
beam,BEAM-1899,2017-04-06T06:49:23.000+0000,,2019-04-30T18:34:04.000+0000,,,New Feature,Major,,,,,,,,,['runner-ideas'],['Runners: ideas for new Beam runners'],Implement a runner for JStorm https://github.com/alibaba/jstorm,JStorm runner,6,,,peihe0@gmail.com,True,basti.lj,peihe0@gmail.com
beam,BEAM-1893,2017-04-05T23:33:22.000+0000,,2019-06-04T12:11:48.000+0000,,,New Feature,Major,,,14400,14400,,,,100,['io-ideas'],"['IO: ideas (proposals for new connectors, new designs, etc)']",Create a {{CouchbaseIO}} for Couchbase database.,Add CouchbaseIO,5,,"['Couchbase', 'IO', 'features']",mingmxu,True,Guobao,mingmxu
beam,BEAM-1888,2017-04-05T17:01:20.000+0000,,2019-04-30T18:33:24.000+0000,,,New Feature,Major,,,,,,,,,['runner-apex'],['Runners: Apex runner'],,Support Splittable DoFn in Apex Runner,2,,,thw,True,thw,thw
beam,BEAM-1866,2017-04-03T19:43:11.000+0000,2019-05-22T09:52:26.000+0000,2019-05-22T09:52:26.000+0000,,Fixed,New Feature,Major,['Not applicable'],,12000,61200,,,,100,['beam-model'],"['Beam Model: general programming model concepts, semantics']","As part of the Fn API work, we need to define a Metrics interface between the Runner and the SDK. Right now, Metrics are simply lost.",Fn API support for Metrics,4,,['portability'],dhalperi@google.com,True,,dhalperi@google.com
beam,BEAM-1857,2017-04-01T15:07:50.000+0000,,2019-02-10T09:31:35.000+0000,,,New Feature,Major,,,,,,,,,['io-ideas'],"['IO: ideas (proposals for new connectors, new designs, etc)']",,Add Neo4jIO,7,6,,kassemshehady,True,,kassemshehady
beam,BEAM-1855,2017-04-01T08:10:08.000+0000,2017-04-01T08:14:02.000+0000,2018-07-07T06:33:27.000+0000,,Fixed,New Feature,Major,['2.0.0'],,,,,,,,['runner-flink'],['Runners: Flink runner'],,Support Splittable DoFn in Flink Streaming runner,3,,,jkff,True,aljoscha,jkff
beam,BEAM-1824,2017-03-28T23:42:46.000+0000,2018-07-02T22:51:59.000+0000,2018-07-02T22:51:59.000+0000,,Won't Fix,New Feature,Major,['Not applicable'],,,,,,,,"['runner-dataflow', 'sdk-java-core']","['Runners: Google Cloud Dataflow runner', 'SDKs: Java: pipeline construction, core transformations']","[~bchambers] suggested the following idea: while the runner implementation of SDF [BEAM-65] is not yet complete enough to support dynamic rebalancing (especially over the Fn API), we can special-case the case of Create.of(single input) + ParDo(SDF) by running it via BoundedSource.

This will allow us to start transitioning bounded IO connectors to SDF API while preserving the dynamic rebalancing feature in the common case when the source is known at pipeline submission time.

And then, when SDF runner support catches up, we'll simply add APIs to the IO connectors for reading from a PCollection of inputs, and those will enjoy the same benefits. Actually we can add such APIs earlier, with the caveat that they won't support dynamic rebalancing, but in this case it's ok because there'll be no performance regression because these APIs didn't exist before.

Proposal document: http://s.apache.org/sdf-via-source",Adapter for running SDF on a statically known input as a Source,3,,,jkff,True,jkff,jkff
beam,BEAM-1814,2017-03-27T10:33:50.000+0000,2017-04-29T10:13:45.000+0000,2017-05-04T14:29:14.000+0000,,Duplicate,New Feature,Major,['Not applicable'],,,,,,,,['runner-spark'],['Runners: Spark runner'],,Support for new State API in Spark runner,3,,,echauchot,True,amitsela,echauchot
beam,BEAM-1758,2017-03-20T13:37:48.000+0000,2017-07-06T13:16:13.000+0000,2017-07-06T13:16:13.000+0000,,Invalid,New Feature,Minor,['Not applicable'],,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","It could be useful to disable metrics reporting to reduce overhead in running pipelines.
",Option to disable metrics reporting (Metrics API),4,1,,iemejia,True,,iemejia
beam,BEAM-1754,2017-03-19T23:40:41.000+0000,,2019-04-30T18:29:09.000+0000,,,New Feature,Major,,,,,,,,,['sdk-ideas'],['SDKs: Coordination on new language-specific SDKs'],"I like the philosophy behind DataFlow and found the Java and Python samples highly comprehensible. However, I have to admit that for most Node.js developers who have little background on typed languages and are used to get up to speed with frameworks incredibly fast, learning Dataflow might take some learning curve that they/we're not used to. So, I wonder if at any point in time Dataflow will provide a Node.js SDK. Maybe this is out of the question, but I wanted to run it by the team as it would be awesome to have something along these lines!

Thanks,
Diego

Question originaly posted in SO:
http://stackoverflow.com/questions/42893436/will-dataflow-ever-support-node-js-with-and-sdk-similar-to-java-or-python
",Will Dataflow ever support Node.js with an SDK similar to Java or Python?,6,1,['node.js'],dzuluaga,True,,dzuluaga
beam,BEAM-1705,2017-03-13T20:26:42.000+0000,2017-10-23T23:39:42.000+0000,2017-10-23T23:39:42.000+0000,,Won't Fix,New Feature,Minor,['Not applicable'],,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","After pipeline is compiled and run in different runners, such as Spark, Flink, it shows the data flow as a graph. There's limitation here:
1. the job must be run first;
2. The pipeline in a runner is not exact same as original designed, some steps are merged for example.

What to do:
a method or tool to show the DAG of pipeline in a graph, to display the native logic components, flows.",visualize the DAG of Pipeline,1,,,mingmxu,True,mingmxu,mingmxu
beam,BEAM-1703,2017-03-13T15:07:22.000+0000,2017-05-11T15:14:58.000+0000,2017-05-11T15:14:58.000+0000,,Fixed,New Feature,Minor,['Not applicable'],,,,,,,,['io-java-gcp'],"['IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)']","Following the ongoing work to support this on DatastoreIO (BEAM-923), it makes sense to support this too.",Add support for local execution to PubsubIO using the google cloud emulator,4,,"['newbie', 'starter']",iemejia,True,zborisha,iemejia
beam,BEAM-1702,2017-03-13T15:06:42.000+0000,2017-05-18T20:09:02.000+0000,2017-05-18T20:09:02.000+0000,,Fixed,New Feature,Minor,['2.1.0'],,,,,,,,['io-java-gcp'],"['IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)']","Following the ongoing work to support this on DatastoreIO (BEAM-923), it makes sense to support this too.",Add support for local execution to BigtableIO using the google cloud emulator,4,,"['newbie', 'starter']",iemejia,True,zborisha,iemejia
beam,BEAM-1699,2017-03-13T04:45:12.000+0000,,2019-04-30T18:30:05.000+0000,,,New Feature,Major,,,,,,,,,"['beam-model', 'sdk-java-core', 'sdk-py-core']","['Beam Model: general programming model concepts, semantics', 'SDKs: Java: pipeline construction, core transformations', 'SDKs: Python']","Add support for Timer metric type to the SDK.
Interface should be along the lines of:
{code}
void update(Duration duration);
<T> T time(Callable<T>  event);
Closeable time();
{code}
Compare to http://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/Timer.html

Could reuse code for this and https://issues.apache.org/jira/browse/BEAM-1613",Timer metric type,,,,aviemzur,True,bchambers,aviemzur
beam,BEAM-1697,2017-03-13T00:35:23.000+0000,2017-06-28T19:57:07.000+0000,2017-06-28T19:57:07.000+0000,,Fixed,New Feature,Major,['Not applicable'],,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","Today, when an event time timer fires, the outputs are associated with its firing timestamp. But it might be useful to be able to have the output timestamp be at some offset or explicit time instead, so the output watermark could be held further from the input watermark.",Allow output time that is some offset from target time of event time timer,2,,,kenn,True,kenn,kenn
beam,BEAM-1696,2017-03-13T00:33:39.000+0000,2017-06-28T19:56:53.000+0000,2017-06-28T19:56:53.000+0000,,Duplicate,New Feature,Major,['Not applicable'],,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","Today, a processing time timer is not associated with a particular output timestamp; instead the user must explicitly output to the desired timestamp.

This has a few drawbacks:

 - Probably need to maintain state that indicates what the timestamp should be
 - The output watermark is not held to that timestamp

Something like {{processingTimer.set(...).withOutputTimestamp(...)}} (or perhaps some more involved API with an {{OutputTimeFn}}-like policy attached) would work nicely, so when {{@OnTimer}} is called the right timestamp is automatic.",Allow explicit output time for processing time timers,3,,,kenn,True,kenn,kenn
beam,BEAM-1679,2017-03-10T10:42:46.000+0000,,2019-02-07T04:51:47.000+0000,,,New Feature,Major,,,,,,,,,['io-ideas'],"['IO: ideas (proposals for new connectors, new designs, etc)']",,create gRPC IO,3,,,zborisha,True,,zborisha
beam,BEAM-1675,2017-03-09T17:54:15.000+0000,,2019-02-07T04:51:44.000+0000,,,New Feature,Major,,,,,,,,,['io-java-jdbc'],['IO: Java: JDBC'],"In JdbcIO.read(), a Coder is required to handle the serialize/deserialize of input record similar as below example. It's better to leverage Coders defined in CoderRegistry.  

{code}
.withCoder(KvCoder.of(BigEndianIntegerCoder.of(), StringUtf8Coder.of()))
{code}",deprecate withCoder() in JdbcIO,2,,,mingmxu,True,,mingmxu
beam,BEAM-1650,2017-03-08T11:56:48.000+0000,2017-03-14T21:02:45.000+0000,2017-03-14T21:02:45.000+0000,,Fixed,New Feature,Major,['Not applicable'],,,,,,,,['website'],['Website: content and bugs'],"Contributors who develop using IntelliJ IDEa or Eclipse could benefit from having an Eclipse code formatter xml which matches the checkstyle enforcements of Beam.

Add such a code style xml to the repository and link to it from the website under the IDE setup sections: https://beam.apache.org/contribute/contribution-guide/#intellij
https://beam.apache.org/contribute/contribution-guide/#eclipse",Add code style xml to the project repo and website ,2,,,aviemzur,True,aviemzur,aviemzur
beam,BEAM-1641,2017-03-07T19:41:41.000+0000,,2017-05-03T02:17:24.000+0000,,,New Feature,Minor,,,,,,,,,['runner-flink'],['Runners: Flink runner'],"The ""continuation trigger"" for a processing time trigger is a synchronized processing time trigger. Today, this throws an exception in the FlinkRunner.

The supports the following:

 - GBK1
 - GBK2

When GBK1 fires due to processing time past the first element in the pane and that element arrives at GBK2, it will wait until all the other upstream keys have also processed and emitted corresponding data.

Sorry for the terseness of explanation - writing quickly so I don't forget.",Implement real synchronized processing time in Flink runner,5,,,kenn,True,,kenn
beam,BEAM-1637,2017-03-07T14:35:23.000+0000,2017-09-15T09:47:49.000+0000,2019-06-10T12:03:29.000+0000,,Fixed,New Feature,Minor,['2.2.0'],,,,,,,,['io-java-elasticsearch'],['IO: Java: ElasticSearch'],"The current Elasticsearch IO (see https://issues.apache.org/jira/browse/BEAM-425) is only compatible with Elasticsearch v 2.x. The aim is to have an IO compatible with ES v 5.x. Beyond being able to address v5.x elasticsearch instances, we could also leverage the use of the Elasticsearch pipeline API and also better split the dataset (be as close as possible of desiredBundleSize) thanks to the new ES split API that allows ES shards splitting.",Create Elasticsearch IO compatible with ES 5.x,7,3,,echauchot,True,echauchot,echauchot
beam,BEAM-1631,2017-03-06T21:46:03.000+0000,,2019-04-30T18:29:33.000+0000,,,New Feature,Major,,,,,,,,,['runner-flink'],['Runners: Flink runner'],"As far as I understand, running Beam pipelines on a Flink cluster can be done in two ways:
* Run directly with a Flink runner, and specifying {{--flinkMaster}} pipeline option via, say, {{mvn exec}}.
* Produce a bundled JAR, and use {{bin/flink}} to submit the same pipeline.

These two ways are equivalent, and work well on a standalone Flink cluster.

Submitting to a Flink-on-YARN is more complicated. You can still produce a bundled JAR, and use {{bin/flink -yid <applicationid>}} to submit such a job. However, that seems impossible with a Flink runner directly.

If so, we should add the ability to the Flink runner to submit a job to a Flink-on-YARN cluster directly.",Flink runner: submit job to a Flink-on-YARN cluster,5,,,davor,True,aljoscha,davor
beam,BEAM-1616,2017-03-03T10:29:50.000+0000,,2017-04-03T16:52:48.000+0000,,,New Feature,Major,,,,14400,,,,,"['beam-model', 'sdk-java-core', 'sdk-py-core']","['Beam Model: general programming model concepts, semantics', 'SDKs: Java: pipeline construction, core transformations', 'SDKs: Python']","Add support for Gauge metric type to the SDK.
This will serve to get the last value reported.

Interface should be along the lines of:
{code}
void set(long value);
{code}

Compare to http://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/Gauge.html",Gauge Metric type,2,,,aviemzur,True,,aviemzur
beam,BEAM-1613,2017-03-03T10:21:41.000+0000,,2017-04-03T16:55:58.000+0000,,,New Feature,Major,,,,,,,,,"['beam-model', 'sdk-java-core', 'sdk-py-core']","['Beam Model: general programming model concepts, semantics', 'SDKs: Java: pipeline construction, core transformations', 'SDKs: Python']","Add support for Meter metric type to the SDK.

Interface should be along the lines of:
{code}
void mark();
void mark(int n);
{code}

Compare to http://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/Meter.html",Meter metric type,,,,aviemzur,True,,aviemzur
beam,BEAM-1589,2017-03-02T03:38:05.000+0000,,2019-06-13T15:44:24.000+0000,,,New Feature,Major,,,4800,4800,,,,100,"['runner-core', 'sdk-java-core']","['Runners: shared functionality for all runners', 'SDKs: Java: pipeline construction, core transformations']","See BEAM-1517
This allows the user to do some work before the state's garbage collection.
It seems kind of annoying, but on the other hand forgetting to set a final timer to flush state is probably data loss most of the time.
FlinkRunner does this work very simply, but other runners, such as DirectRunner, need to traverse all the states to do this, and maybe it's a little hard.",Add OnWindowExpiration method to Stateful DoFn,6,,,lzljs3620320,True,,lzljs3620320
beam,BEAM-1585,2017-03-02T00:49:03.000+0000,2017-06-15T20:49:48.000+0000,2017-11-05T22:22:56.000+0000,,Fixed,New Feature,Major,['Not applicable'],,,,,,,,['sdk-py-core'],['SDKs: Python'],"BEAM-1441 implements the new BeamFileSystem in the python SDK but currently lacks the ability to add user implemented file systems.

This needs to be executed in the worker so should be packaged correctly with the pipeline code. ",Ability to add new file systems to beamFS in the python sdk,2,,,sb2nov,True,sb2nov,sb2nov
beam,BEAM-1581,2017-03-01T10:00:04.000+0000,2018-02-26T20:32:42.000+0000,2018-02-26T20:32:42.000+0000,,Won't Fix,New Feature,Major,['Not applicable'],,,,,,,,['io-ideas'],"['IO: ideas (proposals for new connectors, new designs, etc)']","JSON source and sink to read/write JSON files.
Similarly to {{XmlSource}}/{{XmlSink}}, these be a {{JsonSource}}/{{JonSink}} which are a {{FileBaseSource}}/{{FileBasedSink}}.
Consider using methods/code (or refactor these) found in {{AsJsons}} and {{ParseJsons}}

The {{PCollection}} of objects the user passes to the transform should be embedded in a valid JSON file
The most common pattern for this is a large object with an array member which holds all the data objects and other members for metadata.
Examples of public JSON APIs: https://www.sitepoint.com/10-example-json-files/
Another pattern used is a file which is simply a JSON array of objects.",JSON source and sink,7,2,,aviemzur,True,,aviemzur
beam,BEAM-1542,2017-02-23T17:22:41.000+0000,2018-02-26T20:15:02.000+0000,2019-04-29T23:18:35.000+0000,,Fixed,New Feature,Major,['2.1.0'],,4200,4200,,,,100,['io-java-gcp'],"['IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)']",Is there a source/sink for Spanner in the works?   If not I would gladly give this a shot.,Need Source/Sink for Spanner,6,,,guymolinari,True,mkhadikov,guymolinari
beam,BEAM-1536,2017-02-22T15:37:17.000+0000,,2017-04-10T16:50:39.000+0000,,,New Feature,Major,,,,,,,,,['examples-java'],['Examples: Java examples'],"This will help getting examples setup and deployed quickly in a multi-node Spark standalone cluster.

This tutorial can be used as a guideline:
https://tensorflow.github.io/serving/serving_inception

Some work going on in k8s sig-big-data that could be built-on:
https://docs.google.com/document/d/1pnF38NF6N5eM8DlK088XUW85Vms4V2uTsGZvSp8MNIA/edit#heading=h.i8o3am52bhuk",Build a kubernetes environment for running examples on Spark runner,2,,,nlamba,True,,nlamba
beam,BEAM-1535,2017-02-22T15:32:50.000+0000,,2019-04-30T18:29:36.000+0000,,,New Feature,Major,,,,,,,,,['examples-java'],['Examples: Java examples'],"This will help getting examples setup and deployed quickly in a multi-node cluster.

This tutorial can be used as a guideline:
https://tensorflow.github.io/serving/serving_inception",Build a kubernetes environment for running examples on (direct) Java runner ,3,,,nlamba,True,nlamba,nlamba
beam,BEAM-1534,2017-02-22T15:28:05.000+0000,,2019-04-30T18:33:46.000+0000,,,New Feature,Major,,,,,,,,,['examples-java'],['Examples: Java examples'],"This will help create a repeatable developer environment setup.

Other Apache projects can be used as a reference:
https://github.com/apache/ambari/tree/trunk/dev-support/docker
https://github.com/apache/geode/blob/develop/docker/Dockerfile
",Create a dockerized developer environment for Beam,5,,,nlamba,True,ekremaksoy,nlamba
beam,BEAM-1521,2017-02-21T21:56:16.000+0000,2017-04-24T17:00:31.000+0000,2017-04-24T17:00:32.000+0000,,Not A Problem,New Feature,Minor,['Not applicable'],,,,,,,,['io-java-gcp'],"['IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)']",It would be easy and helpful to also pass the element when choosing a table for a BigQuery write. And perhaps future-proof this by a Context parameter.,Pass element (and/or generic context) for choosing a table in BigQueryIO,2,,['backward-incompatible'],kenn,True,,kenn
beam,BEAM-1505,2017-02-17T07:47:52.000+0000,2017-04-27T06:51:09.000+0000,2019-06-10T12:06:58.000+0000,,Implemented,New Feature,Major,['Not applicable'],,,,,,,,['io-java-xml'],['IO: Java: Xml'],"Similar to what Aviem did for JSON (BEAM-1466), we can provide utils for XML and JAXB.",Provide XML format utils extension,1,,,jbonofre,True,jbonofre,jbonofre
beam,BEAM-1488,2017-02-14T22:02:32.000+0000,,2019-02-07T04:51:51.000+0000,,,New Feature,Major,,,,,,,,,['io-ideas'],"['IO: ideas (proposals for new connectors, new designs, etc)']","Add a ConsoleIO, which can read from console as input, and output to console as output. I use this a lot for local debugging/testing.",Add ConsoleIO ,3,,,mingmxu,True,,mingmxu
beam,BEAM-1483,2017-02-14T06:22:08.000+0000,2017-06-13T09:45:33.000+0000,2017-06-13T09:45:33.000+0000,,Fixed,New Feature,Major,['2.1.0'],,,,,,,,['runner-flink'],['Runners: Flink runner'],,Support SetState in Flink runner,3,,,kenn,True,lzljs3620320,kenn
beam,BEAM-1482,2017-02-14T06:21:04.000+0000,2017-05-10T17:12:03.000+0000,2017-05-10T17:12:03.000+0000,,Duplicate,New Feature,Major,['Not applicable'],,,,,,,,['runner-spark'],['Runners: Spark runner'],,Support SetState in Spark runner,2,,,kenn,True,,kenn
beam,BEAM-1481,2017-02-14T06:20:43.000+0000,,2017-02-14T06:20:43.000+0000,,,New Feature,Major,,,,,,,,,['runner-gearpump'],['Runners: Gearpump runner'],,Support SetState in Gearpump runner,1,,,kenn,True,,kenn
beam,BEAM-1480,2017-02-14T06:20:21.000+0000,,2017-02-14T06:20:21.000+0000,,,New Feature,Major,,,,,,,,,['runner-apex'],['Runners: Apex runner'],,Support SetState in Apex runner,1,,,kenn,True,,kenn
beam,BEAM-1479,2017-02-14T06:20:05.000+0000,,2017-02-14T06:20:05.000+0000,,,New Feature,Major,,,,,,,,,['runner-dataflow'],['Runners: Google Cloud Dataflow runner'],,Support SetState in Dataflow runner,2,1,,kenn,True,,kenn
beam,BEAM-1478,2017-02-14T06:19:44.000+0000,,2017-02-14T06:19:44.000+0000,,,New Feature,Major,,,,,,,,,['runner-gearpump'],['Runners: Gearpump runner'],,Support MapState in Gearpump runner,1,,,kenn,True,,kenn
beam,BEAM-1477,2017-02-14T06:19:16.000+0000,2017-05-10T17:10:54.000+0000,2017-05-10T17:11:17.000+0000,,Duplicate,New Feature,Major,['Not applicable'],,,,,,,,['runner-spark'],['Runners: Spark runner'],,Support MapState in Spark runner,2,,,kenn,True,,kenn
beam,BEAM-1476,2017-02-14T06:18:59.000+0000,2017-06-07T02:40:32.000+0000,2017-06-07T02:40:59.000+0000,,Fixed,New Feature,Major,['2.1.0'],,,,,,,,['runner-flink'],['Runners: Flink runner'],,Support MapState in Flink runner,2,,,kenn,True,peihe0@gmail.com,kenn
beam,BEAM-1475,2017-02-14T06:18:41.000+0000,,2017-02-14T06:18:41.000+0000,,,New Feature,Major,,,,,,,,,['runner-apex'],['Runners: Apex runner'],,Support MapState in Apex runner,1,,,kenn,True,,kenn
beam,BEAM-1474,2017-02-14T06:18:18.000+0000,,2017-03-30T18:24:16.000+0000,,,New Feature,Major,,,,,,,,,['runner-dataflow'],['Runners: Google Cloud Dataflow runner'],,Support MapState in DataflowRunner,6,5,,kenn,True,,kenn
beam,BEAM-1472,2017-02-13T23:57:53.000+0000,2017-12-15T19:00:41.000+0000,2017-12-15T19:00:41.000+0000,,Fixed,New Feature,Major,['2.1.0'],,,,,,,,['sdk-py-core'],['SDKs: Python'],"Basically, we need to do something like https://github.com/apache/beam/pull/1988/files for Python.

This is key for non-Python runners being able to understand trigger specs from Python.",Use cross-language serialization schema for triggers in Python SDK,2,,,kenn,True,robertwb,kenn
beam,BEAM-1466,2017-02-11T09:03:07.000+0000,2017-02-27T21:08:29.000+0000,2019-06-10T12:07:56.000+0000,,Fixed,New Feature,Major,['0.6.0'],,,,,,,,['extensions-java-json'],['Extensions: Java: Json'],"Create a JSON extension module which will contain transforms to aid with handling JSONs.

Suggested transforms:
* Parse JSON strings to type OutputT.
* Parse InputT to JSON strings.",JSON utils extension,2,,,aviemzur,True,aviemzur,aviemzur
beam,BEAM-1449,2017-02-09T20:00:12.000+0000,,2019-02-07T04:30:11.000+0000,,,New Feature,Major,,['Not applicable'],,,,,,,['build-system'],"['Build, CI, release systems and processes']","We are working on splitting python sdk into multiple packages and it'll be a good time to invest in getting a central build tool to work across languages so that we can share common test specs, common proto etc.

PS: Please revert the spec duplication https://github.com/apache/beam/pull/1964 once it can be shared.",Maven should be able to build and run tests across multiple languages,2,,,sb2nov,True,,sb2nov
beam,BEAM-1441,2017-02-09T00:07:28.000+0000,2017-04-04T20:49:22.000+0000,2017-05-02T00:00:21.000+0000,,Fixed,New Feature,Major,['2.0.0'],,,,,,,,['sdk-py-core'],['SDKs: Python'],"Based on proposal [1], an IOChannelFactory interface was added to Java SDK  [2].

We should add a similar interface to Python SDK and provide proper implementations for native files, GCS, and other useful formats.

Python SDK currently has a basic ChannelFactory interface [3] which is used by FileBasedSource [4].

[1] https://docs.google.com/document/d/11TdPyZ9_zmjokhNWM3Id-XJsVG3qel2lhdKTknmZ_7M/edit#heading=h.kpqagzh8i11w
[2] https://github.com/apache/beam/blob/master/sdks/java/core/src/main/java/org/apache/beam/sdk/util/IOChannelFactory.java
[3] https://github.com/apache/beam/blob/master/sdks/python/apache_beam/io/fileio.py#L107
[4] https://github.com/apache/beam/blob/master/sdks/python/apache_beam/io/filebasedsource.py

 ",Add FileSystem support to Python SDK,5,,,chamikara,True,sb2nov,chamikara
beam,BEAM-1440,2017-02-08T23:52:20.000+0000,,2019-03-25T17:30:57.000+0000,,,New Feature,Major,,,,,,,,,['sdk-py-core'],['SDKs: Python'],"Currently we have a BigQuery native source for Python SDK [1].
This can only be used by Dataflow runner.

We should  implement a Beam BigQuery source that implements iobase.BoundedSource [2] interface so that other runners that try to use Python SDK can read from BigQuery as well. Java SDK already has a Beam BigQuery source [3].

[1] https://github.com/apache/beam/blob/master/sdks/python/apache_beam/io/gcp/bigquery.py
[2] https://github.com/apache/beam/blob/master/sdks/python/apache_beam/io/iobase.py#L70
[3] https://github.com/apache/beam/blob/master/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIO.java#L1189",Create a BigQuery source (that implements iobase.BoundedSource) for Python SDK,13,6,,chamikara,True,,chamikara
beam,BEAM-1436,2017-02-08T17:58:15.000+0000,,2017-02-08T17:58:15.000+0000,,,New Feature,Major,,,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","Right now modules have no great way to register their relevant coders. One option is to use `AutoService`. We should explore this and other ways, so that coders can be adjacent to the library for which they are relevant.","Consider automatic coder registration mechanism, such as AutoService",3,,,kenn,True,,kenn
beam,BEAM-1398,2017-02-05T19:27:11.000+0000,2017-04-29T15:19:32.000+0000,2019-06-10T12:09:31.000+0000,,Implemented,New Feature,Major,['2.0.0'],,,,,,,,['io-java-kafka'],['IO: Java: Kafka'],"Add metrics to {{KafkaIO}} using the metrics API.

Metrics (Feel free to add more ideas here) per split (Where applicable):
* Backlog in bytes.
* Backlog in number of messages.
* Messages consumed.
* Bytes consumed.
* Messages produced.

Add {{NeedsRunner}} test which creates a pipeline and asserts metrics values.",KafkaIO metrics,3,,,aviemzur,True,aviemzur,aviemzur
beam,BEAM-1397,2017-02-05T19:21:02.000+0000,2017-03-28T04:49:58.000+0000,2017-03-28T04:49:58.000+0000,,Done,New Feature,Major,['2.0.0'],,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","Introduce the usage of metrics API in IOs.

POC using {{CountingInput}}:
* Add metrics to {{CountingInput}}
* {{RunnableOnService}} test which creates a pipeline which asserts these metrics.
* Close any gaps in Direct runner and Spark runner to support these metrics.",Introduce IO metrics,3,,,aviemzur,True,aviemzur,aviemzur
beam,BEAM-1377,2017-02-02T01:28:05.000+0000,2017-06-20T23:48:49.000+0000,2017-06-23T01:49:17.000+0000,,Fixed,New Feature,Major,['2.1.0'],,,,,,,,['runner-dataflow'],['Runners: Google Cloud Dataflow runner'],"Dataflow runner should support splittable DoFn.
However, Dataflow batch and streaming runners will support it quite differently, streaming being the somewhat easier one. The current issue is about the streaming runner.",Support Splittable DoFn in Dataflow streaming runner,4,,,jkff,True,jkff,jkff
beam,BEAM-1328,2017-01-27T03:54:52.000+0000,2017-04-19T18:47:27.000+0000,2017-04-19T18:47:27.000+0000,,Fixed,New Feature,Minor,['2.0.0'],,,,,,,,"['sdk-java-core', 'sdk-py-core']","['SDKs: Java: pipeline construction, core transformations', 'SDKs: Python']","This is an upcoming blocker for Python, as the Python SDK needs to be able to ship the pieces of the windowing strategy in a way a runner can grok. Only the WindowFn should remain language-specific.",Serialize/deserialize WindowingStrategy in a language-agnostic manner,3,,,kenn,True,kenn,kenn
beam,BEAM-1327,2017-01-27T03:52:02.000+0000,2017-05-02T20:32:03.000+0000,2017-05-05T17:18:44.000+0000,,Fixed,New Feature,Minor,['2.0.0'],,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","The class {{OutputTimeFn}} is overkill for a Fn API crossing. There are only three sensible values known: MIN, MAX, EOW. The interface is right for implementing these, but the full class is left over from the days when there was little cost to shipping new kinds of fns. An enum is concise.

This can be done ""mostly"" backwards compatibly with legacy adapters in place, but might be less confusing without them.",Replace OutputTimeFn with enum,5,,['backward-incompatible'],kenn,True,kenn,kenn
beam,BEAM-1325,2017-01-26T14:41:02.000+0000,,2017-03-30T16:49:56.000+0000,,,New Feature,Major,,,,,,,,,['runner-dataflow'],['Runners: Google Cloud Dataflow runner'],"See https://github.com/GoogleCloudPlatform/DataflowJavaSDK/issues/538

The code that infers {{--filesToStage}} from the classpath does not seem to support a classpath jar. which uses a {{Class-Path}} manifest to indicate which other jars are needed at runtime. This is apparently a standard, approved way to pass the classpath.

We need to add this support to {{DataflowRunner#detectClasspathResourcesToStage}}, but correctly representing the user's classpath continues to be challenging.

(As {{FlinkRunner}} has copied this code, we should probably either pull the common utility somewhere public or update the FlinkRunner too?)",DataflowRunner support for Class-Path jars,2,,,dhalperi@google.com,True,,dhalperi@google.com
beam,BEAM-1318,2017-01-26T01:18:22.000+0000,2019-06-16T12:24:21.000+0000,2019-06-16T12:24:29.000+0000,,Done,New Feature,Minor,['2.11.0'],,,,,,,,['sdk-py-core'],['SDKs: Python'],"Since PipelineOptions uses argparse, it is possible that some options are actually consumed by the program. In that case a better usage pattern would be to pass only unconsumed options to PipelineOptions but we cannot enforce this.

This cannot be an error because of the above reason, but we can show a warning.",PipelineOptions should warn if there are unused options,2,,"['newbie', 'starter']",altay,True,,altay
beam,BEAM-1290,2017-01-20T19:03:42.000+0000,2017-04-06T17:11:29.000+0000,2017-04-06T17:11:29.000+0000,,Incomplete,New Feature,Minor,['Not applicable'],,,,,,,,"['sdk-java-core', 'sdk-py-core']","['SDKs: Java: pipeline construction, core transformations', 'SDKs: Python']","Failing to check status is error prone, and for the currently supported sdk languages it is normal to raise an exception to signal abnormal behavior.

Could we consider changing waitUntilFinish to rethrow the original exception when the execution fails during the wait.",Consider rehtrowing the original exception from the waitUntilFinish,1,,,altay,True,,altay
beam,BEAM-1287,2017-01-20T05:34:59.000+0000,,2019-02-07T04:47:47.000+0000,,,New Feature,Major,,,,,,,,,"['beam-model', 'sdk-java-core', 'sdk-py-core']","['Beam Model: general programming model concepts, semantics', 'SDKs: Java: pipeline construction, core transformations', 'SDKs: Python']","The new {{DoFn}} design allows us to have specialized output receivers, such as a key-preserving output (the default is non-key-preserving) or non-window-preserving (the default is window-preserving) output. This JIRA is for the latter, with an emphasis on making the two as analogous as we can.

{code}
new DoFn<A, B>() {
  @ProcessElement
  public void processElement(ProcessContext c, OutputToWindow receiver) {
    receiver.outputWithTimestamp(value, timestamp, window);
  }
}
{code}

After this change, window assignment need not be a primitive.

Why is this OK? The primary motivation for keeping windows strongly separated is because they yield parallelism if we don't impose any requirement that multiple windows for a single key be co-located or linearized. We should be able to process a single key with millions of non-merging windows in parallel without having to reify the windows (though this isn't _that_ bad). That is a major change/improvement over the vague assumption that keys are the atom of parallelism.

This change will not remove this property, as it pertains to input and state. The analogy with keys:

 - Stateful DoFn requires the ability to access key-and-window state. For some runners, perhaps this does not require colocation. For runners that want to do this efficiently/locally, it means some key-and-window colocation operation followed by only key-and-window preserving transforms. So outputting to a new window breaks the invariant, just as a non-key-preserving transform would. Until we had the new {{DoFn}} we couldn't know if non-window-preserving output was used.

 - Non-key-preserving output also breaks any idea that combined aggregates are actually one per key, etc. So windows can work the same way.

 - Timestamps are interesting. By analogy with keys, timestamps would be just part of the value and able to change freely. This doesn't work so well because of lateness. To avoid digging deeper into changing anything, this proposal just suggests that a timestamp is provided, and whether it is allowed to be late is governed by the same rules as {{outputWithTimestamp}}.

 - Not clear if this has uses for merging windows.

This change is entirely backwards compatible, but given that it removes a primitive and is rather little effort, it might bear earlier consideration. No work will begin until it is brought to the dev list.",Give new DoFn the ability to output to a particular window,5,,,kenn,True,,kenn
beam,BEAM-1271,2017-01-13T19:34:49.000+0000,,2019-04-30T18:32:24.000+0000,,,New Feature,Minor,,['Not applicable'],,,2419200,2419200,2419200,,['io-ideas'],"['IO: ideas (proposals for new connectors, new designs, etc)']",Develop the Apache Accumulo IO for write and read operations.,Develop Apache Accumulo IO,3,2,,wyatt.frelot,True,wyatt.frelot,wyatt.frelot
beam,BEAM-1267,2017-01-11T22:40:25.000+0000,2019-03-04T19:33:40.000+0000,2019-03-04T19:33:41.000+0000,,Duplicate,New Feature,Major,['Not applicable'],,,,,,,,['io-java-gcp'],"['IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)']",Because it helps make schema evolution easier.,BigQueryIO.Write should support the ignoreUnknownValues option,5,1,"['newbie', 'starter']",g-eorge,True,,g-eorge
beam,BEAM-1265,2017-01-11T01:59:08.000+0000,,2019-02-07T05:03:46.000+0000,,,New Feature,Major,,,,,,,,,['sdk-py-core'],['SDKs: Python'],Continue the work started in https://issues.apache.org/jira/browse/BEAM-428,Add streaming support to Python DirectRunner,9,6,,altay,True,,altay
beam,BEAM-1261,2017-01-10T23:13:51.000+0000,,2017-09-11T20:46:29.000+0000,,,New Feature,Major,,,,,,,,,['beam-model'],"['Beam Model: general programming model concepts, semantics']","For example, even if the elements are being processed in fixed windows of an hour, it may be desirable for the state to ""roll over"" between windows (or be available to all windows).

It will also be necessary to figure out when this state should be deleted (TTL? maximum retention?)

Another problem is how to deal with out of order data. If data comes in from the 10:00 AM window, should its state changes be visible to the data in the 9:00 AM window? ",State API should allow state to be managed in different windows,3,,,bchambers,True,,bchambers
beam,BEAM-1240,2017-01-03T17:07:21.000+0000,2018-10-26T07:42:32.000+0000,2019-03-15T11:46:06.000+0000,,Fixed,New Feature,Major,['2.9.0'],,54000,54000,,,,100,['io-ideas'],"['IO: ideas (proposals for new connectors, new designs, etc)']",,Create RabbitMqIO,7,,,jbonofre,True,jbonofre,jbonofre
beam,BEAM-1237,2017-01-02T10:43:36.000+0000,2017-06-27T13:34:19.000+0000,2019-06-10T12:13:56.000+0000,,Fixed,New Feature,Major,['2.1.0'],,,,,,,,['io-java-amqp'],['IO: Java: AMQP'],,Create AmqpIO,2,,,jbonofre,True,jbonofre,jbonofre
beam,BEAM-1233,2016-12-30T09:34:05.000+0000,2017-01-09T21:25:08.000+0000,2017-01-09T21:27:56.000+0000,,Fixed,New Feature,Major,['Not applicable'],,,,,,,,['sdk-py-core'],['SDKs: Python'],"Tensorflow is an open source Machine Learning project, which is getting lots of attention these days. Apache Beam can be used as a good preprocessing tool for this Machine Learning tool, however Tensorflow supports limited number of input file formats -- only csv and its own record format (so called TFRecord).

On the other hand, Apache Beam doesn't support reading/writing in TFRecord format. This would be useful once it supports TFRecordIO natively.",Implement TFRecordIO (Reading/writing Tensorflow Standard format),2,,,younghee,True,altay,younghee
beam,BEAM-1198,2016-12-21T21:06:52.000+0000,2017-03-23T04:57:08.000+0000,2017-08-21T22:18:03.000+0000,,Fixed,New Feature,Major,"['Not applicable', '2.0.0']",,,,,,,,['beam-model'],"['Beam Model: general programming model concepts, semantics']","For side inputs, the field {{PCollectionView.fromIterableInternal}} implies an ""iterable"" materialization of the contents of a PCollection, which is adapted to the desired user-facing type within a UDF (today the PCollectionView ""is"" the UDF)

In practice, runners get adequate performance by special casing just a couple of types of PCollectionView in a rather cumbersome manner.

The design to improve this is https://s.apache.org/beam-side-inputs-1-pager and this makes the de facto standard approach the actual model.",ViewFn: explicitly decouple runner materialization of side inputs from SDK-specific mapping,3,,,kenn,True,kenn,kenn
beam,BEAM-1196,2016-12-21T19:31:26.000+0000,2017-09-11T21:13:02.000+0000,2017-09-11T21:13:02.000+0000,,Fixed,New Feature,Major,['2.2.0'],,,,,,,,['beam-model'],"['Beam Model: general programming model concepts, semantics']","The beam_runner_api.proto definition is a language-independent representation of the Pipeline that SDKs are expected to be able to serialize a Pipeline in.

These should be pushed all the way through and added to the Java SDK.",Serialize/deserialize Pipeline/TransformHierarchy to Runner API Protos,4,,['portability'],kenn,True,kenn,kenn
beam,BEAM-1195,2016-12-21T19:27:18.000+0000,2017-03-23T04:48:37.000+0000,2017-08-21T22:17:47.000+0000,,Fixed,New Feature,Major,['2.0.0'],,,,,,,,['beam-model'],"['Beam Model: general programming model concepts, semantics']","We have recently gotten to the point where triggers are just syntax, but it is still shipped via Java serialization. To make it language-independent, we need a concrete syntax.

Something like the following is fairly concise, tag adjacent to payload. I haven't bothered making up fully verbose/namespaced URNs here.

{code}
{
    ""$urn"": ""OrFinally"",
    ""main"": {
      ""$urn"": ""EndOfWindow"",
      ""early"": <foo>
    },
    ""finally"": {
      ""$urn"": ""AfterCount"",
      ""count"": 45
    }
}
{code}",Give triggers a cross-language serialization schema,3,,,kenn,True,kenn,kenn
beam,BEAM-1193,2016-12-21T19:15:57.000+0000,2017-04-10T16:51:40.000+0000,2017-08-21T22:17:47.000+0000,,Fixed,New Feature,Major,['Not applicable'],,,,,,,,['beam-model'],"['Beam Model: general programming model concepts, semantics']",,Give Coders URNs and document their binary formats outside the Java code base,2,,,kenn,True,robertwb,kenn
beam,BEAM-1192,2016-12-21T18:35:39.000+0000,2017-09-11T21:13:18.000+0000,2017-09-11T21:13:18.000+0000,,Fixed,New Feature,Major,['2.2.0'],,,,,,,,['beam-model'],"['Beam Model: general programming model concepts, semantics']","In the [Beam Runner AP|https://s.apache.org/beam-runner-api], transforms of interest to runners are to be identified by URN.

Currently, Java-based runners use `instanceof` checks to both override transforms and to implement primitive transforms. This language and SDK-specific behavior should be replaced by adding these URNs, and checking them.","Give transforms URNs, use them instead of instanceof checks",1,,['portability'],kenn,True,kenn,kenn
beam,BEAM-1164,2016-12-15T19:02:01.000+0000,,2017-03-30T17:21:48.000+0000,,,New Feature,Minor,,,,,,,,,['beam-model'],"['Beam Model: general programming model concepts, semantics']","Runners generally can't tell if a DoFn is mutating inputs, but assuming so by default leads to significant performance implications from unnecessary copying (around sibling fusion, etc). So instead the model prevents mutating inputs, and the Direct Runner validates this behavior. (See: http://beam.incubator.apache.org/contribute/design-principles/#make-efficient-things-easy-rather-than-make-easy-things-efficient) 

However, if users are processing a small number of large records by making incremental changes (for example, genomics use cases), the cost of immutability requirement can be very large. As a workaround, users sometimes do suboptimal things (fusing ParDos by hand) or undefined things when they expect the immutability requirement is unnecessarily strict (adding no-op coders in places they hope the runner won't be materializing things, mutating things anyway when they don't expect sibling fusion to happen, etc).

We should consider adding a signal (MutatingDoFn?) that users explicitly opt in to to say their code may mutate inputs. The runner can then use this assumption to either prevent optimizations that would break in the face of this or insert additional copies as needed to allow optimizations to preserve semantics.

See this related user@ discussion:
https://lists.apache.org/thread.html/f39689f54147117f3fc54c498eff1a20fa73f1be5b5cad5b6f816fd3@%3Cuser.beam.apache.org%3E",Allow a DoFn to opt in to mutating it's input,8,,,frances,True,,frances
beam,BEAM-1157,2016-12-14T19:40:21.000+0000,2017-03-20T21:52:20.000+0000,2019-06-10T12:55:27.000+0000,,Fixed,New Feature,Major,['0.6.0'],['Not applicable'],,,,,,,"['io-ideas', 'io-java-hbase']","['IO: ideas (proposals for new connectors, new designs, etc)', 'IO: Java: HBase']",Support for reading and writing to HBase. An initial plan is to keep a similar API to that of BigTableIO so users can switch between both systems (when possible).,Create HBaseIO,2,,,iemejia,True,iemejia,iemejia
beam,BEAM-1132,2016-12-12T04:42:57.000+0000,2019-02-10T00:00:36.000+0000,2019-04-30T18:33:16.000+0000,,Fixed,New Feature,Minor,['Not applicable'],,7200,7200,,,,100,['testing'],['Testing: general infrastructure'],"Jenkins has a JaCoCo plugin and other Apache projects use it. We should try it too (and might as well disable coveralls, as I don't know anyone who looks at it).

If this takes more than 10 minutes to set up, then another option is to just archive JaCoCo's HTML reports so we can browse them.

Either of these should take just minutes and yield huge benefits in visibility of where we have really bad coverage.",Jenkins JaCoCo plugin for Beam,2,,,kenn,True,michel,kenn
beam,BEAM-1131,2016-12-12T04:39:00.000+0000,,2019-02-07T04:46:17.000+0000,,,New Feature,Major,,,,,,,,,['testing'],['Testing: general infrastructure'],"japicmp is what Flink uses to test backwards-compatibility, configuration here: https://github.com/apache/flink/blob/41d5875bfc272f2cd5c7e8c8523036684865c1ce/pom.xml#L1184

At our first stable release, we should activate this plugin too! (configuration to be determined by community discussion). We should have it installed and configured beforehand, informational-only, so that we have a sense of it.",Consider japicmp for testing backwards-compatibility,4,,,kenn,True,,kenn
beam,BEAM-1122,2016-12-09T19:07:00.000+0000,2016-12-09T20:41:48.000+0000,2016-12-09T20:41:49.000+0000,,Fixed,New Feature,Minor,['0.4.0'],['0.4.0'],,,,,,,['runner-dataflow'],['Runners: Google Cloud Dataflow runner'],Remove the `--enableProfilingAgent` flag and add a `--saveProfilesToGcs` flag to the `DataflowProfilingOptions`.,Change Dataflow profiling options to support saving to GCS,2,,['backward-incompatible'],bchambers,True,bchambers,bchambers
beam,BEAM-1118,2016-12-09T04:15:21.000+0000,,2016-12-09T04:15:21.000+0000,,,New Feature,Major,,,,,,,,,['runner-gearpump'],['Runners: Gearpump runner'],,Support for new Timer API in Gearpump runner,1,,,kenn,True,,kenn
beam,BEAM-1117,2016-12-09T04:14:59.000+0000,2017-01-06T03:22:59.000+0000,2017-01-06T03:22:59.000+0000,,Fixed,New Feature,Major,['0.5.0'],,,,,,,,['runner-direct'],['Runners: Direct runner and ULR for single machine testing and development'],,Support for new Timer API in Direct runner,2,,,kenn,True,kenn,kenn
beam,BEAM-1116,2016-12-09T04:14:43.000+0000,2017-02-28T14:30:55.000+0000,2017-02-28T14:30:55.000+0000,,Fixed,New Feature,Major,['0.6.0'],,,,,,,,['runner-flink'],['Runners: Flink runner'],,Support for new Timer API in Flink runner,4,,,kenn,True,lzljs3620320,kenn
beam,BEAM-1115,2016-12-09T04:14:27.000+0000,,2019-04-30T18:28:59.000+0000,,,New Feature,Major,,,,,,,,,['runner-spark'],['Runners: Spark runner'],,Support for new Timer API in Spark runner,2,,,kenn,True,amitsela,kenn
beam,BEAM-1114,2016-12-09T04:14:12.000+0000,,2017-04-29T20:00:11.000+0000,,,New Feature,Major,,,,,,,,,['runner-apex'],['Runners: Apex runner'],,Support for new Timer API in Apex runner,3,,,kenn,True,,kenn
beam,BEAM-1113,2016-12-09T04:13:19.000+0000,2017-03-23T04:49:56.000+0000,2017-05-07T23:25:18.000+0000,,Fixed,New Feature,Major,['0.6.0'],,,,,,,,['runner-dataflow'],['Runners: Google Cloud Dataflow runner'],,Support for new Timer API in Dataflow runner,2,,,kenn,True,kenn,kenn
beam,BEAM-1080,2016-12-02T22:10:40.000+0000,,2019-04-30T18:29:58.000+0000,,,New Feature,Major,,,,,,,,,['sdk-py-core'],['SDKs: Python'],"There is only one unit test right now that tries to fetch actual gcp credentials instead of mocking. This test fails when the credentials are not available on the machine in which it is running. 
",python sdk apiclient needs proper unit tests,1,,"['newbie', 'starter']",vikasrk,True,,vikasrk
beam,BEAM-1076,2016-12-02T19:07:09.000+0000,2017-03-15T18:36:46.000+0000,2017-03-22T21:32:11.000+0000,,Fixed,New Feature,Major,['2.0.0'],,,,,,,,['io-java-gcp'],"['IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)']",,DatastoreIO template Options,2,,,vikasrk,True,vikasrk,vikasrk
beam,BEAM-1059,2016-11-29T13:02:17.000+0000,2017-02-27T18:03:54.000+0000,2019-06-10T12:15:53.000+0000,,Won't Fix,New Feature,Major,['Not applicable'],,,,,,,,['io-ideas'],"['IO: ideas (proposals for new connectors, new designs, etc)']",,Create ExecIO,2,,,jbonofre,True,jbonofre,jbonofre
beam,BEAM-1047,2016-11-23T23:57:38.000+0000,2017-03-24T20:41:46.000+0000,2017-03-24T20:41:46.000+0000,,Fixed,New Feature,Major,['2.0.0'],,,,,,,,['runner-dataflow'],['Runners: Google Cloud Dataflow runner'],Tracking bug.,DataflowRunner: support regionalization.,2,,,peihe0@gmail.com,True,dhalperi@google.com,peihe0@gmail.com
beam,BEAM-1038,2016-11-22T18:50:11.000+0000,2017-01-24T05:15:59.000+0000,2017-01-24T05:15:59.000+0000,,Fixed,New Feature,Major,['0.5.0'],,,,,,,,['runner-dataflow'],['Runners: Google Cloud Dataflow runner'],,Support for new State API in DataflowRunner,2,,,kenn,True,kenn,kenn
beam,BEAM-1037,2016-11-22T18:48:57.000+0000,2017-09-30T22:24:08.000+0000,2017-09-30T22:24:08.000+0000,,Implemented,New Feature,Major,['2.2.0'],,,,,,,,['runner-apex'],['Runners: Apex runner'],,Support for new State API in ApexRunner,3,,,kenn,True,thw,kenn
beam,BEAM-1036,2016-11-22T18:47:26.000+0000,2017-02-28T10:11:52.000+0000,2017-04-02T09:34:11.000+0000,,Fixed,New Feature,Major,['0.6.0'],,,,,,,,['runner-flink'],['Runners: Flink runner'],,Support for new State API in FlinkRunner,4,,,kenn,True,lzljs3620320,kenn
beam,BEAM-1035,2016-11-22T18:46:34.000+0000,,2017-05-10T17:11:39.000+0000,,,New Feature,Major,,,,,,,,,['runner-spark'],['Runners: Spark runner'],,Support for new State API in SparkRunner,6,,,kenn,True,,kenn
beam,BEAM-1021,2016-11-21T19:58:20.000+0000,2017-03-15T18:38:54.000+0000,2017-03-15T18:38:54.000+0000,,Fixed,New Feature,Major,"['Not applicable', '0.6.0']",,,,,,,,['sdk-py-core'],['SDKs: Python'],,DatastoreIO for python,2,,,vikasrk,True,vikasrk,vikasrk
beam,BEAM-1017,2016-11-21T09:33:36.000+0000,2017-10-17T06:00:58.000+0000,2019-06-10T12:17:28.000+0000,,Fixed,New Feature,Major,['2.2.0'],,,,,,,,['io-java-redis'],['IO: Java: Redis'],,Add RedisIO,3,1,,jbonofre,True,jbonofre,jbonofre
beam,BEAM-1016,2016-11-19T20:16:52.000+0000,2017-05-18T20:02:03.000+0000,2017-05-18T20:02:03.000+0000,,Won't Fix,New Feature,Minor,['Not applicable'],,,,,,,,['io-java-gcp'],"['IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)']","Currently the Content-Type for compressed files overrides the original Content-Type.


So
Content-Type: text/plain

becomes
Content-Type: application/gzip


We should instead consider keeping

Content-Type: text/plain
and adding
Content-Encoding: gzip

This will paly nice with Cloud Storage automatic transcoding:
https://cloud.google.com/storage/docs/transcoding",Use Content-Type and Content-Encoding (as opposed to overriding Content-Type) for compressed files,3,,,katsiapis@google.com,True,,katsiapis@google.com
beam,BEAM-1003,2016-11-17T18:08:59.000+0000,2016-12-21T21:00:54.000+0000,2016-12-21T21:00:54.000+0000,,Duplicate,New Feature,Major,['Not applicable'],,,,,,,,['beam-model'],"['Beam Model: general programming model concepts, semantics']","Sometimes the kind of computations one wants to perform in startBundle depend on side inputs (and, implicitly, the window). For example, one might want to initialize a (non-serializable) stateful object. In particular, this leads to users incorrectly (in the case of triggered or non-globally-windowed side inputs) memoizing this computation in the first processElement call. 

One option would be to fold this into a customizable ViewFn. ",Enable caching of side-input dependent computations,3,,,robertwb,True,frances,robertwb
beam,BEAM-1002,2016-11-17T18:08:52.000+0000,,2017-04-19T18:49:04.000+0000,,,New Feature,Major,,,,,,,,,['beam-model'],"['Beam Model: general programming model concepts, semantics']","Sometimes the kind of computations one wants to perform in startBundle depend on side inputs (and, implicitly, the window). For example, one might want to initialize a (non-serializable) stateful object. In particular, this leads to users incorrectly (in the case of triggered or non-globally-windowed side inputs) memoizing this computation in the first processElement call. 

One option would be to fold this into a customizable ViewFn. ",Enable caching of side-input dependent computations,3,,,robertwb,True,,robertwb
beam,BEAM-994,2016-11-16T21:44:21.000+0000,2017-06-22T17:13:02.000+0000,2019-06-10T12:29:26.000+0000,,Won't Fix,New Feature,Minor,['Not applicable'],,,,,,,,['io-java-aws'],['IO: Java: Amazon Web Services'],http://stackoverflow.com/questions/40624544/what-is-best-practice-of-the-the-case-of-writing-text-output-into-s3-bucket is one of the examples of the need for such a feature. ,Support for S3 file source and sink,3,,,sb2nov,True,lcwik,sb2nov
beam,BEAM-978,2016-11-15T02:25:42.000+0000,2017-05-18T19:05:11.000+0000,2017-05-18T19:05:11.000+0000,,Fixed,New Feature,Major,['0.4.0'],,,,,,,,['io-java-gcp'],"['IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)']",Bulk get-file-sizes could be useful in FileBasedSource (It currently does sampling and total size estimation with one RPC request per file.),Support bulk get file sizes in GcsUtil,2,,,peihe0@gmail.com,True,peihe0@gmail.com,peihe0@gmail.com
beam,BEAM-961,2016-11-10T21:22:23.000+0000,2016-12-06T10:17:39.000+0000,2016-12-06T10:17:39.000+0000,,Fixed,New Feature,Trivial,['0.4.0'],,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","TL;DR: Add {{startingAt}} to {{CountingInput}}.

Right now you can have {{CountingInput.upTo(someNumber)}} but it came up in a test that if you want to have, say, one PCollection that is 1 through 10 and another that is 11 through 20 - so you know they are disjoint - then it requires some boilerplate to add 10 to every element. That boilerplate should be part of the {{CountingInput}}",CountingInput could have starting number,3,,"['easy', 'newbie', 'starter']",kenn,True,vladisav,kenn
beam,BEAM-943,2016-11-08T07:19:46.000+0000,2016-11-21T19:56:49.000+0000,2016-11-21T19:56:49.000+0000,,Fixed,New Feature,Major,['Not applicable'],,,,,,,,['sdk-py-core'],['SDKs: Python'],,Implement Datastore query splitter for python,2,,,vikasrk,True,vikasrk,vikasrk
beam,BEAM-936,2016-11-07T20:26:11.000+0000,,2016-11-07T22:35:34.000+0000,,,New Feature,Minor,,,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']",CC [~kenn],@DefaultCoder(MyCoder.class) should work if MyCoder has .of(),1,,,dhalperi@google.com,True,,dhalperi@google.com
beam,BEAM-923,2016-11-07T17:00:23.000+0000,,2019-04-30T18:30:06.000+0000,,,New Feature,Minor,,,,,,,,,['io-java-gcp'],"['IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)']",Adding locahost option enables to connect to a Datastore emulator that is running locally,Add support for local execution to DatastoreIO using the google cloud emulator,4,,,vikasrk,True,vikasrk,vikasrk
beam,BEAM-920,2016-11-06T11:33:05.000+0000,2017-02-28T22:39:13.000+0000,2017-02-28T22:39:13.000+0000,,Fixed,New Feature,Major,['0.6.0'],,,,,,,,['runner-spark'],['Runners: Spark runner'],"Implement event-time based aggregation using triggers, panes and watermarks.","Support triggers, panes and watermarks.",2,,,amitsela,True,amitsela,amitsela
beam,BEAM-912,2016-11-04T15:35:46.000+0000,,2017-03-30T17:18:12.000+0000,,,New Feature,Major,,,,,,,,,['sdk-ideas'],['SDKs: Coordination on new language-specific SDKs'],"1.We can support some data-driven trigger, so we need expose data in OnElementContext of onElement method. 
2.We can support more flexible join, so we need expose buffer tag in TriggerContext, now this buffer tag is in SystemReduceFn.

for example: SELECT STREAM * FROM Orders AS o JOIN Shipments AS s
ON o.orderId = s.orderId AND s.rowtime BETWEEN o.rowtime AND o.rowtime + INTERVAL '1' HOUR;

link: https://issues.apache.org/jira/browse/BEAM-101",Range join in Beam,5,,,lzljs3620320,True,,lzljs3620320
beam,BEAM-894,2016-11-03T19:55:59.000+0000,2016-12-01T22:11:28.000+0000,2016-12-01T22:14:29.000+0000,,Won't Fix,New Feature,Major,['Not applicable'],,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","FileBasedSink lefts temp files behind for failed bundles, and it forces finalize() to depend on pattern match.

However, pattern matching is not always reliable for eventual consistency file system, such as GCS.

Given we now have DoFn.TearDown, we can improve FileBasedSink (and in general Write transform) to remove temp files/resources early when DoFn bundles fail.",Using @Teardown to remove temp files from failed bundles in Write.WriteBundles,2,,,peihe0@gmail.com,True,peihe0@gmail.com,peihe0@gmail.com
beam,BEAM-885,2016-11-02T21:28:44.000+0000,2017-05-04T20:30:53.000+0000,2017-08-21T22:17:47.000+0000,,Fixed,New Feature,Major,['2.0.0'],,,,,,,,"['beam-model', 'sdk-java-core']","['Beam Model: general programming model concepts, semantics', 'SDKs: Java: pipeline construction, core transformations']","The specification of a Pipeline should be independent of its PipelineOptions. This delays specification of the options, including choices like Pipeline Runner.",Move PipelineOptions from Pipeline.create() to Pipeline.run(),4,,['backward-incompatible'],tgroh,True,kenn,tgroh
beam,BEAM-881,2016-11-02T15:41:00.000+0000,,2019-07-04T09:53:49.000+0000,,,New Feature,Major,,,,,,,,,['io-ideas'],"['IO: ideas (proposals for new connectors, new designs, etc)']","Now, each IO is using a different data format. For instance, the {{JmsIO.Read}} provides a {{PCollection}} of {{JmsRecord}} (and {{JmsIO.Write}} expects also a {{JmsRecord}}), {{KafkaIO.Read}} provides a {{PCollection}} of {{KafkaRecord}}.

It could appear a bit ""complex"" for users to manipulate such kind of data format: some users may expect kind of standard format.

Without modifying the existing IO, we could add a {{PTransform}} (as part of the IO) that an user can optionally use. This transform will convert the IO data format (let say {{JmsRecord}} for instance) to a standard Avro {{IndexedRecord}}.","Provide a PTransform in IOs providing a ""standard"" Avro IndexedRecord",5,1,,jbonofre,True,jbonofre,jbonofre
beam,BEAM-873,2016-11-01T00:53:55.000+0000,2016-11-11T19:24:40.000+0000,2016-11-17T20:32:50.000+0000,,Fixed,New Feature,Minor,['0.4.0'],,,,,,,,['sdk-py-core'],['SDKs: Python'],,Support BigQuery 2.0 input format,2,,,sb2nov,True,sb2nov,sb2nov
beam,BEAM-869,2016-10-31T04:20:38.000+0000,,2019-02-07T05:07:54.000+0000,,,New Feature,Major,,,,,,,,,['runner-gearpump'],['Runners: Gearpump runner'],"This is a prerequisite for full support of BEAM-27, and related to BEAM-35.

I haven't looked into what kind of work this might take.

(Since I am filing individual tickets for each runner, generalized discussion should probably be on the dev list directly on or BEAM-35)
",Support set/delete of timer by ID in Gearpump runner,1,,['triaged'],kenn,True,mauzhang,kenn
beam,BEAM-868,2016-10-31T04:19:37.000+0000,,2019-04-30T18:34:05.000+0000,,,New Feature,Major,,,,,,,,,['runner-apex'],['Runners: Apex runner'],"This is a prerequisite for full support of BEAM-27, and related to BEAM-35.

I haven't looked into what kind of work this would take.

(Since I am filing individual tickets for each runner, generalized discussion should probably be on the dev list directly on or BEAM-35)",Support set/delete of timers by ID in Apex runner,1,,,kenn,True,thw,kenn
beam,BEAM-867,2016-10-31T04:18:14.000+0000,2018-11-19T16:11:50.000+0000,2019-01-15T18:30:03.000+0000,,Fixed,New Feature,Major,['2.9.0'],,1800,1800,,,,100,['runner-flink'],['Runners: Flink runner'],"This is a prerequisite for full support of BEAM-27, and related to BEAM-35.

My current understanding is that this might be actually removed from Flink itself, and needs implementation in terms of state. It might be reasonable to just remove deletion from Beam; it is just a nice-to-have for user experience IMO, and perhaps a performance help for certain runners.

(Since I am filing individual tickets for each runner, generalized discussion should probably be on the dev list directly on or BEAM-35)",Support set/delete of timers by ID in Flink runner,4,,,kenn,True,mxm,kenn
beam,BEAM-866,2016-10-31T04:16:18.000+0000,,2019-04-30T18:30:57.000+0000,,,New Feature,Major,,,,,,,,,['runner-spark'],['Runners: Spark runner'],"This is a prerequisite for full support of BEAM-27, and related to BEAM-35.

If I understand the code and your comments directly, this may not require any action on your part, but only updates to common SDK and runners-core code. Filing the ticket to be sure.

(Since I am filing individual tickets for each runner, generalized discussion should probably be on the dev list directly on or BEAM-35)",Support set/delete of timer by ID in Spark runner,1,,,kenn,True,amitsela,kenn
beam,BEAM-865,2016-10-31T04:14:50.000+0000,2017-03-23T04:48:13.000+0000,2017-03-23T04:48:13.000+0000,,Fixed,New Feature,Major,['0.6.0'],,,,,,,,['runner-direct'],['Runners: Direct runner and ULR for single machine testing and development'],"This is a prerequisite for full support of BEAM-27, and related to BEAM-35.

(Since I am filing individual tickets for each runner, generalized discussion should probably be on the dev list directly on or BEAM-35)",Support set/delete of timer by ID in DirectRunner,1,,,kenn,True,kenn,kenn
beam,BEAM-846,2016-10-27T16:15:54.000+0000,2017-04-26T18:36:22.000+0000,2017-08-21T22:17:45.000+0000,,Fixed,New Feature,Major,['2.0.0'],,,,,,,,"['beam-model', 'sdk-java-core']","['Beam Model: general programming model concepts, semantics', 'SDKs: Java: pipeline construction, core transformations']","Currently the main WindowFn provides as getSideInputWindow method. Instead, this mapping should be specified per-side-input (thought the default mapping would remain the same). ",Decouple side input window mapping from WindowFn,6,,['backward-incompatible'],robertwb,True,tgroh,robertwb
beam,BEAM-840,2016-10-26T22:46:24.000+0000,2016-12-13T21:09:37.000+0000,2019-06-10T12:39:50.000+0000,,Fixed,New Feature,Minor,['0.4.0'],,,,,,,,['extensions-java-sorter'],['Extensions: Java: Sorter'],"Add an extension that provides a PTransform which performs local(non-distributed) sorting. It will sort in memory until the buffer is full, then flush to disk and use external sorting.
    
Consumes a PCollection of KVs from primary key to iterable of secondary key and value KVs and sorts the iterables. Would probably be called after a GroupByKey. Uses coders to convert secondary keys and values into byte arrays and does a lexicographical comparison on the secondary keys.

Uses Hadoop as an external sorting library.",Add Java SDK extension to support non-distributed sorting,3,,,mshanklin,True,mshanklin,mshanklin
beam,BEAM-834,2016-10-26T02:32:10.000+0000,,2017-03-09T18:10:53.000+0000,,,New Feature,Major,,,,,,,,,['website'],['Website: content and bugs'],"We have a number of technical docs that might be useful as a deep dive - mostly aimed at contributors but potentially important for power users. These are not the 1-pagers and design docs, but the descriptions of existing features or the project. Candidates include:

 - Some parts of [Triggers in Apache Beam (incubating)|https://s.apache.org/beam-triggers]
 - Probably much of [Lateness in Apache Beam (incubating)|https://s.apache.org/beam-lateness]
 - Some bits of [Static Display Data|https://docs.google.com/document/d/11enEB9JwVp6vO0uOYYTMYTGkr3TdNfELwWqoiUg5ZxM/edit?usp=sharing]
 - Some pieces of the [Technical Vision|https://docs.google.com/document/d/1UyAeugHxZmVlQ5cEWo_eOPgXNQA1oD-rGooWOSwAqh8/edit?usp=sharing]?
",Add HTML/md versions of technical reference docs,1,,,kenn,True,,kenn
beam,BEAM-830,2016-10-25T21:32:29.000+0000,2016-12-10T19:03:02.000+0000,2016-12-17T23:10:43.000+0000,,Fixed,New Feature,Major,['0.4.0'],,,,,,,,['runner-apex'],['Runners: Apex runner'],Currently the ApexRunner only support execution in embedded mode. Add the support to package the dependencies and run the Apex app on a YARN cluster.,Launcher for ApexRunner execution on YARN cluster ,3,,,thw,True,thw,thw
beam,BEAM-802,2016-10-24T04:15:26.000+0000,,2019-04-30T18:30:06.000+0000,,,New Feature,Minor,,,,,6048000,6048000,6048000,,['sdk-py-core'],['SDKs: Python'],"Goal:  Enable users to run pipelines from templates filled via CL (pipeline options)
Background: Currently, the Runner creates the JSON pipeline description which can be sent to the worker as is, since everything is already defined there (with links to gs:// for input and binaries). With the parametrized approach, those descriptions are empty and filled by the user or defaulted, so the pipeline needs to be stored somewhere first until the values become available.
Tasks:
1- Create template-style pipeline description (TemplateRunner)
The graph description is now a template (some parts are not filled) that needs to be saved.
2- Define values to inject to the template (ValueProviders API)
The placeholders can be filled with default values (static) or with dynamic key/value pairs provided at runtime (dynamic)",Support Dynamic PipelineOptions for python,5,2,,mariagh,True,mariagh,mariagh
beam,BEAM-782,2016-10-19T22:27:30.000+0000,2017-03-17T21:30:57.000+0000,2017-03-17T21:30:57.000+0000,,Fixed,New Feature,Major,['Not applicable'],,,,,,,,['sdk-py-core'],['SDKs: Python'],"See:
https://github.com/apache/incubator-beam/pull/1087
https://issues.apache.org/jira/browse/BEAM-770

e.g. the DirectRunner can be specified with (among others) any of
""--runner=direct"", ""--runner=directrunner"", ""--runner=DirectRunner"",
""--runner=Direct"", or ""--runner=directRunner""", Resolve runners in a case-insensitive manner.,3,,['sdk-consistency'],altay,True,sb2nov,altay
beam,BEAM-780,2016-10-19T22:21:33.000+0000,2017-03-17T21:19:31.000+0000,2017-03-17T21:19:31.000+0000,,Fixed,New Feature,Major,['Not applicable'],,,,,,,,['sdk-py-core'],['SDKs: Python'],"Remove aggregators and replace them with the metrics API.

See: https://issues.apache.org/jira/browse/BEAM-147 for the Java SDK.",Add support for pipeline metrics,2,,['sdk-consistency'],altay,True,pabloem,altay
beam,BEAM-722,2016-10-06T01:18:34.000+0000,2017-02-07T02:20:47.000+0000,2017-02-07T02:20:47.000+0000,,Fixed,New Feature,Major,['Not applicable'],,,,,,,,['sdk-py-core'],['SDKs: Python'],The DisplayData feature has been added to the Java SDK (see blog post announcing it: https://cloud.google.com/blog/big-data/2016/06/dataflow-updates-see-more-details-about-your-pipelines). We need now to add it to the Python SDK.,Add Display Data to the Python SDK,3,,,pabloem,True,frances,pabloem
beam,BEAM-701,2016-10-03T18:43:33.000+0000,2016-10-12T17:40:19.000+0000,2016-10-17T21:38:57.000+0000,,Fixed,New Feature,Major,['0.3.0-incubating'],,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']",Support compression on Avro Write.,Support Avro Compression for writes,3,,['features'],ravwojdyla,True,ravwojdyla,ravwojdyla
beam,BEAM-699,2016-10-03T08:18:00.000+0000,,2019-04-30T18:32:33.000+0000,,,New Feature,Major,,,,,,,,,['io-ideas'],"['IO: ideas (proposals for new connectors, new designs, etc)']","Now that we have {{JdbcIO}}, another way to deal with database is to use JPA. Thanks to JPA, we would be able to deal with {{PCollection}} of Entities (POJO containing the JPA annotation).
The think I would like to test the JPA engine (providing the {{EntityManager}}) in the executors.",Add JpaIO,2,,,jbonofre,True,jbonofre,jbonofre
beam,BEAM-654,2016-09-20T22:49:14.000+0000,,2017-03-23T06:19:19.000+0000,,,New Feature,Major,,,,,,,,,['beam-model'],"['Beam Model: general programming model concepts, semantics']","The primary example of a merging window today is {{Sessions}} by gap duration, in which the merged window is the interval enclosure / span of the windows being merged.

However, another reasonable abstract use case is a session identified by id with an explicit end event. We might consider that the session ends with no gap duration after the end event. In this case, the merged window may be smaller than the enclosure of the sub-windows. Sometimes this has been referred to as ""merging shrinks the window"".

Perhaps the only requirement is that the merged window contains the timestamps of the data therein, but we should document this clearly. The current spec is [""Does whatever merging is necessary""|https://github.com/apache/incubator-beam/blob/master/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/windowing/WindowFn.java#L106]

There are repercussions for triggers, some documented in the [trigger design doc|https://s.apache.org/beam-triggers]: With nonzero allowed lateness, {{Sessions}} by gap duration can switch a trigger from ON_TIME or LATE behavior back to speculative behavior, or cause another ON_TIME firing. Conversely, sessions with abrupt termination/shrinking may have that behavior _as well as_ an ON_TIME and subsequent LATE firings due only to the merging (this already works properly).","When and how can merging windows ""shrink"" or ""grow""?",4,1,,kenn,True,,kenn
beam,BEAM-653,2016-09-20T20:52:38.000+0000,,2017-08-21T22:15:49.000+0000,,,New Feature,Major,,,,,,,,,['beam-model'],"['Beam Model: general programming model concepts, semantics']","{{WindowFn#isCompatible}} doesn't really have a spec. In practice, it is used primarily when flattening together multiple PCollections. All of the WindowFns must be compatible, and then just a single WindowFn is selected arbitrarily for the output PCollection.

In consequence, downstream of the Flatten, the merging behavior will be taken from this WindowFn.

Currently, there are some mismatches:

 - Sessions with different gap durations _are_ compatible today, but probably shouldn't be since merging makes little sense. (The use of tiny proto-windows is an implementation detail anyhow)
 - SlidingWindows and FixedWindows _could_ reasonably be compatible if they had the same duration, though it might be odd.

Either way, we should just nail down what we actually mean so we can arrive at a verdict in these cases.",Refine specification for WindowFn.isCompatible() ,4,,,kenn,True,,kenn
beam,BEAM-651,2016-09-20T20:25:32.000+0000,2016-12-06T16:35:51.000+0000,2017-02-27T17:32:01.000+0000,,Fixed,New Feature,Minor,['0.4.0'],,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","This would give fairly pithy answers to StackOverflow questions sometimes.

When choosing between .getOutputCoder() and .getOutputTypeDescriptor() for a transform/DoFn we often choose the type, so the coder registry can do its thing.

This would also give a similar choice between .setCoder(...) and .setTypeDescriptor(...).

And anyhow we have the intention of removing our practice of the ""*Internal"" suffix, so this one might be most easily solved by making it public.",Add public TypedPValue.setTypeDescriptor,3,,"['easy', 'easyfix', 'starter']",kenn,True,nssalian,kenn
beam,BEAM-644,2016-09-20T18:19:11.000+0000,,2019-04-23T09:44:23.000+0000,,,New Feature,Major,,,,,,,,,['beam-model'],"['Beam Model: general programming model concepts, semantics']","There is a general need, especially important in the presence of SplittableDoFn, to be able to assign new timestamps to elements without making them late or droppable.

 - DoFn.withAllowedTimestampSkew is inadequate, because it simply allows one to produce late data, but does not allow one to shift the watermark so the new data is on-time.
 - For a SplittableDoFn, one may receive an element such as the name of a log file that contains elements for the day preceding the log file. The timestamp on the filename must currently be the beginning of the log. If such elements are constantly flowing, it may be OK, but since we don't know that element is coming, in that absence of data, the watermark may advance. We need a way to keep it far enough back even in the absence of data holding it back.

One idea is a new primitive ShiftWatermark / AdjustTimestamps with the following pieces:

 - A constant duration (positive or negative) D by which to shift the watermark.
 - A function from TimestampedElement<T> to new timestamp that is >= t + D

So, for example, AdjustTimestamps(<-60 minutes>, f) would allow f to make timestamps up to 60 minutes earlier.

With this primitive added, outputWithTimestamp and withAllowedTimestampSkew could be removed, simplifying DoFn.

Alternatively, all of this functionality could be bolted on to DoFn.

This ticket is not a proposal, but a record of the issue and ideas that were mentioned.",Primitive to shift the watermark while assigning timestamps,21,2,,kenn,True,,kenn
beam,BEAM-643,2016-09-19T22:31:40.000+0000,2017-01-09T20:40:35.000+0000,2017-01-09T20:40:35.000+0000,,Fixed,New Feature,Major,['Not applicable'],,,,,,,,['sdk-py-core'],['SDKs: Python'],Users should be able to specify a custom service account which can be used when creating VMs. This feature is specify to DataflowRunner and corresponding user option will be added to GoogleCloudOptions.,Allow users to specify a custom service account,2,,,chamikara,True,chamikara,chamikara
beam,BEAM-638,2016-09-19T15:33:14.000+0000,2017-05-07T15:31:28.000+0000,2017-05-09T09:03:14.000+0000,,Fixed,New Feature,Major,['2.0.0'],,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","Today, if the pipeline source is unbounded, and the sink expects a bounded collection, there's no way to use a single pipeline. Even a window creates a chunk on the unbounded PCollection, but the ""sub"" PCollection is still unbounded.
It would be helpful for users to have a Window function that create a bounded PCollection (on the window) from an unbounded PCollection coming from the source.","Add sink transform to write bounded data per window, pane, [and key] even when PCollection is unbounded",8,1,,jbonofre,True,davor,jbonofre
beam,BEAM-636,2016-09-16T18:24:16.000+0000,2016-12-01T21:58:20.000+0000,2016-12-28T22:15:01.000+0000,,Fixed,New Feature,Major,['0.3.0-incubating'],,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']",The TypeDescriptors class is missing the Char and Byte TypeDescriptor methods.,"Add Char, Byte to TypeDescriptors",2,,,eljefe6aa,True,eljefe6aa,eljefe6aa
beam,BEAM-635,2016-09-15T22:27:00.000+0000,2016-12-02T19:47:10.000+0000,2016-12-02T19:47:11.000+0000,,Not A Problem,New Feature,Major,['Not applicable'],,,,,,,,['runner-flink'],['Runners: Flink runner'],"Support Latest Flink Release of version of 1.1.0 in BEAM 0.2.0-incubating release

0.2.0-incubating has just been released.  It will be great if we can add support for latest Flink 1.1.0 release.  
",Release 0.2.0-incubating - Support Flink Release Version 1.1.2,6,,,sumitkchawla,True,,sumitkchawla
beam,BEAM-621,2016-09-09T09:05:24.000+0000,,2019-04-30T18:30:30.000+0000,,,New Feature,Major,,,,,,,,,"['sdk-java-core', 'sdk-py-core']","['SDKs: Java: pipeline construction, core transformations', 'SDKs: Python']","Currently, we have the {{MapElements}} {{PTransform}} that ""convert"" a {{PCollection}} of {{KV}} to another {{PCollection}} (for instance {{String}}).

A very classic mapping function is to just have the keys or values of {{KV}}.

To do it currently, we can use {{MapElements}} or a generic {{ParDo}} (with {{DoFn}}).

It would be helpful and reduce the user code to have {{MapValues}} and {{MapKeys}}. It would take a {{PCollection}} of {{KV}}: {{MapKeys}} will map the input {{PCollection}} to a {{PCollection}} of {{K}} and {{MapValues}} to a {{PCollection}} of {{V}}.

",Add MapValues and MapKeys functions,4,,"['newbie', 'starter']",jbonofre,True,jbonofre,jbonofre
beam,BEAM-612,2016-09-01T04:26:01.000+0000,,2016-09-19T02:49:14.000+0000,,,New Feature,Major,,,,,,,,,['runner-ideas'],['Runners: ideas for new Beam runners'],"We thinking about contributing the BSP computing engine to the Beam w/ few examples e.g., wordcount and streaming similarity-join computation HAMA-983.

It would be really helpful if someone can guide us on this idea.",Add BSP runner,5,,,udanax,True,,udanax
beam,BEAM-607,2016-08-31T08:11:44.000+0000,,2018-02-14T18:14:33.000+0000,,,New Feature,Major,,,,,,,,,['io-ideas'],"['IO: ideas (proposals for new connectors, new designs, etc)']","I'd like to add an IO for the new DistributedLog streams - http://distributedlog.io

- bounded source and sink (sealed streams)
- unbounded source and sink (unsealed streams)

",Add DistributedLog IO,4,,,khurrumnasimm,True,,khurrumnasimm
beam,BEAM-606,2016-08-30T11:52:53.000+0000,2016-12-23T07:03:21.000+0000,2019-06-10T12:43:06.000+0000,,Fixed,New Feature,Major,['0.5.0'],,,,,,,,['io-java-mqtt'],['IO: Java: Mqtt'],"As we now have JmsIO, it would make sense to provide a MQTT IO (unbounded).",Create MqttIO,2,,,jbonofre,True,jbonofre,jbonofre
beam,BEAM-596,2016-08-26T18:40:50.000+0000,2017-03-22T21:13:54.000+0000,2017-03-22T21:13:54.000+0000,,Fixed,New Feature,Major,['2.0.0'],,,,,,,,['runner-direct'],['Runners: Direct runner and ULR for single machine testing and development'],,Support cancel() and waitUntilFinish() in DirectRunner,3,,,peihe0@gmail.com,True,tgroh,peihe0@gmail.com
beam,BEAM-595,2016-08-26T18:40:01.000+0000,2016-12-05T11:28:35.000+0000,2016-12-05T11:28:35.000+0000,,Fixed,New Feature,Major,['0.4.0'],,,,,,,,['runner-spark'],['Runners: Spark runner'],"We introduced both functions to PipelineResult.
Currently, both of them throw UnsupportedOperationExcedption in Spark runner.",Support non-blocking run() in SparkRunner and cancel() and waitUntilFinish() in Spark EvaluationContext,3,,,peihe0@gmail.com,True,,peihe0@gmail.com
beam,BEAM-594,2016-08-26T18:33:49.000+0000,2016-08-29T17:51:00.000+0000,2016-08-29T17:51:46.000+0000,,Duplicate,New Feature,Major,['Not applicable'],,,,,,,,['runner-flink'],['Runners: Flink runner'],"We introduced both functions to PipelineResult.
Currently, both of them throw UnsupportedOperationException in Flink runner.",Support cancel() and waitUntilFinish() in FlinkRunnerResult,2,,,peihe0@gmail.com,True,,peihe0@gmail.com
beam,BEAM-593,2016-08-26T18:33:39.000+0000,,2019-04-30T18:30:25.000+0000,,,New Feature,Major,,,3600,3600,,,,100,['runner-flink'],['Runners: Flink runner'],"We introduced both functions to PipelineResult.
Currently, both of them throw UnsupportedOperationException in Flink runner.",Support unblocking run() in FlinkRunner and cancel() and waitUntilFinish() in FlinkRunnerResult,10,,,peihe0@gmail.com,True,aljoscha,peihe0@gmail.com
beam,BEAM-590,2016-08-25T21:05:25.000+0000,2016-08-25T21:43:11.000+0000,2016-08-25T21:43:11.000+0000,,Duplicate,New Feature,Minor,['Not applicable'],,,,,,,,['examples-java'],['Examples: Java examples'],"I am removing references to dataflow website in examples, such as:
https://cloud.google.com/dataflow/java-sdk/wordcount-example

Creating this issue to track web docs that we might want to port to Beam.",Port examples web docs from Dataflow to Beam website.,2,,,peihe0@gmail.com,True,,peihe0@gmail.com
beam,BEAM-575,2016-08-22T18:04:20.000+0000,2016-09-29T21:52:32.000+0000,2016-09-29T21:52:32.000+0000,,Fixed,New Feature,Major,['0.3.0-incubating'],,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","Beam SDK recently gained support for ""display data"", which is construction-time metadata about a pipeline. Display data is limited in that it doesn't provide a way to publish runtime metadata.

Aggregators are the current method for pipelines to publish runtime metadata, but currently focuses on counter-type metadata. It would be valuable to expand the support for dynamically-computed properties like ""pub-sub subscription id"" or ""sink export job status"". This would fit nicely into the aggregators API by providing a ""Latest"" combiner with String-typed data.","""Latest String"" Aggregator",1,,,swegner,True,swegner,swegner
beam,BEAM-563,2016-08-17T23:32:56.000+0000,2019-05-16T19:03:47.000+0000,2019-05-16T22:06:00.000+0000,,Done,New Feature,Major,['2.14.0'],,,,,,,,['sdk-py-core'],['SDKs: Python'],https://issues.apache.org/jira/browse/BEAM-562 will add setup and teardown methods to DoFns. Update DirectRunner to add support for these new methods.,DoFn Reuse: Update DirectRunner,2,,,altay,True,,altay
beam,BEAM-562,2016-08-17T23:30:14.000+0000,2019-05-16T17:16:34.000+0000,2019-05-16T22:06:33.000+0000,,Done,New Feature,Major,['2.14.0'],,39000,39000,,,,100,['sdk-py-core'],['SDKs: Python'],"Java SDK added setup and teardown methods to the DoFns. This makes DoFns reusable and provide performance improvements. Python SDK should add support for these new DoFn methods:

Proposal doc: https://docs.google.com/document/d/1LLQqggSePURt3XavKBGV7SZJYQ4NW8yCu63lBchzMRk/edit?ts=5771458f#
",DoFn Reuse: Add new methods to DoFn,5,2,['sdk-consistency'],altay,True,myffical@gmail.com,altay
beam,BEAM-558,2016-08-15T23:47:59.000+0000,2016-08-29T22:56:32.000+0000,2016-08-29T22:56:57.000+0000,,Fixed,New Feature,Critical,['0.3.0-incubating'],,,,,,,,['runner-dataflow'],['Runners: Google Cloud Dataflow runner'],Tracking bug for tests we disable for Dataflow that depend on this behavior :),DataflowRunner should support setup/teardown,2,,,dhalperi@google.com,True,tgroh,dhalperi@google.com
beam,BEAM-553,2016-08-15T19:16:37.000+0000,2017-01-09T19:22:41.000+0000,2017-01-09T19:22:42.000+0000,,Fixed,New Feature,Major,['Not applicable'],,,,,,,,['sdk-py-core'],['SDKs: Python'],"Currently, the text source implementation available for Python SDK [1] is a Dataflow native source which only works efficiently for Dataflow runner. We should add a custom text source on top of custom file-based source framework [2] so that other runner implementations can potentially use the same text source implementation.

Custom text source implementation for Java SDK is at [3].

[1] https://github.com/apache/incubator-beam/blob/python-sdk/sdks/python/apache_beam/io/fileio.py#L70
[2] https://github.com/apache/incubator-beam/blob/python-sdk/sdks/python/apache_beam/io/filebasedsource.py
[3] https://github.com/apache/incubator-beam/blob/master/sdks/java/core/src/main/java/org/apache/beam/sdk/io/TextIO.java#L745",Add a custom text source,2,,,chamikara,True,chamikara,chamikara
beam,BEAM-551,2016-08-15T14:30:51.000+0000,2017-03-30T17:27:31.000+0000,2017-03-30T17:27:31.000+0000,,Fixed,New Feature,Minor,['Not applicable'],,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","During the graph construction phase, the given SDK generates an initial
execution graph for the program.  At execution time, this graph is
executed, either locally or by a service.  Currently, Beam only supports
parameterization at graph construction time.  Both Flink and Spark supply
functionality that allows a pre-compiled job to be run without SDK
interaction with updated runtime parameters.

In its current incarnation, Dataflow can read values of PipelineOptions at
job submission time, but this requires the presence of an SDK to properly
encode these values into the job.  We would like to build a common layer
into the Beam model so that these dynamic options can be properly provided
to jobs.

Please see
https://docs.google.com/document/d/1I-iIgWDYasb7ZmXbGBHdok_IK1r1YAJ90JG5Fz0_28o/edit
for the high-level model, and
https://docs.google.com/document/d/17I7HeNQmiIfOJi0aI70tgGMMkOSgGi8ZUH-MOnFatZ8/edit
for
the specific API proposal.
",Support Dynamic PipelineOptions,3,,,sam.mcveety@gmail.com,True,sgmc@google.com,sam.mcveety@gmail.com
beam,BEAM-547,2016-08-12T05:48:04.000+0000,2017-04-10T16:50:07.000+0000,2017-04-10T16:50:07.000+0000,,Fixed,New Feature,Minor,['Not applicable'],['0.3.0-incubating'],,,86400,86400,86400,,['sdk-py-core'],['SDKs: Python'],"In BEAM-378 we've integrated the Python SDK in the main Maven build. 

Initially I wanted to also align versions, but after discussing it with [~silviuc@google.com] we kept that aside for the moment. 

Closing [PR #537|https://github.com/apache/incubator-beam/pull/537] [~altay] brings the issue back. So it may make sense to revisit that idea.",Align Python SDK version with Maven,4,,,wikier,True,frances,wikier
beam,BEAM-544,2016-08-10T20:14:46.000+0000,2016-09-07T22:40:59.000+0000,2019-06-10T13:06:03.000+0000,,Fixed,New Feature,Minor,['0.3.0-incubating'],,,,,,,,['io-java-text'],['IO: Java: TextIO'],"Being able to add a header/footer to each file that is written via TextIO would cover several simple text file format issues.

Original ask:
https://github.com/GoogleCloudPlatform/DataflowJavaSDK/issues/360",Add header/footer support to TextIO.Write,2,,,lcwik,True,staslev,lcwik
beam,BEAM-540,2016-08-09T21:16:09.000+0000,2016-10-05T22:06:04.000+0000,2016-10-05T22:06:04.000+0000,,Fixed,New Feature,Major,['0.3.0-incubating'],,,,,,,,['runner-dataflow'],['Runners: Google Cloud Dataflow runner'],Dataflow streaming jobs running on windmill do not need data disks.,Dataflow streaming jobs running on windmill do not need data disks,3,,,drieber,True,drieber,drieber
beam,BEAM-529,2016-08-04T01:36:24.000+0000,,2019-06-05T15:06:18.000+0000,,,New Feature,Minor,,,,,,,,,['sdk-py-core'],['SDKs: Python'],Users are going to mutate inputs and outputs of DoFn inappropriately. We should help their tests fail to catch such mistakes. (Similar to the DirectPipelineRunner in Java SDK),Check immutability violations in DirectPipelineRunner,4,,"['newbie', 'starter']",altay,True,,altay
beam,BEAM-528,2016-08-04T01:32:49.000+0000,2016-10-05T19:51:34.000+0000,2017-04-27T18:25:48.000+0000,,Fixed,New Feature,Minor,['Not applicable'],,,,,,,,['sdk-py-core'],['SDKs: Python'],"Experimental/deprecation warnings: use the warnings standard module in conjunction with decorators as described here:

https://docs.python.org/2/library/warnings.html

Some code sample for a deprecated decorator that is kinda/sorta similar.",Add @experimental annotations ,3,,['starter'],altay,True,mariagh,altay
beam,BEAM-518,2016-08-03T01:15:39.000+0000,,2019-06-12T08:46:51.000+0000,,,New Feature,Minor,,,,,,,,,['sdk-py-core'],['SDKs: Python'],"Expand the list of matchers for assert_that,

Example of work from Java: https://docs.google.com/document/d/1fZUUbG2LxBtqCVabQshldXIhkMcXepsbv2vuuny8Ix4/edit?pref=2&pli=1#heading=h.lt80jryok8cs", More sophisticated assert matchers,3,,"['newbie', 'starter']",altay,True,,altay
beam,BEAM-498,2016-07-29T02:56:20.000+0000,2016-12-21T06:34:33.000+0000,2017-01-20T22:38:49.000+0000,,Fixed,New Feature,Major,['0.3.0-incubating'],,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']",,Make DoFnWithContext the new DoFn,5,,['backward-incompatible'],kenn,True,kenn,kenn
beam,BEAM-476,2016-07-20T16:11:13.000+0000,,2016-07-20T16:11:13.000+0000,,,New Feature,Minor,,,,,,,,,['io-java-gcp'],"['IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)']","{{BigQueryIO.Write}} right now requires that the table schema be known at pipeline construction time. There are two feature requests:

1. It should be possible to operate in some modes (where table is not created, but only appended-to or truncated) where the schema need not be known at construction time.

2. It might be possible to operate in such a way that tables are created at runtime using a schema that is determined dynamically at runtime. We would possibly need the schema sent as a side input.",BigQueryIO.Write: schema not known at construction time,,,,dhalperi@google.com,True,,dhalperi@google.com
beam,BEAM-458,2016-07-15T18:34:25.000+0000,2016-10-18T22:59:58.000+0000,2016-10-18T22:59:58.000+0000,,Duplicate,New Feature,Major,['Not applicable'],,,,,,,,['beam-model'],"['Beam Model: general programming model concepts, semantics']","Flink has added support for CodeHale Metrics (https://ci.apache.org/projects/flink/flink-docs-master/monitoring/metrics.html)

These metrics are more advanced then the current Accumulators. 

Adding support for these to Beam level should be a good addition.

https://github.com/apache/flink/pull/1947#issuecomment-233029166",Support for Flink Metrics ,3,,,sumitkchawla,True,,sumitkchawla
beam,BEAM-456,2016-07-15T09:06:58.000+0000,2016-09-04T19:58:21.000+0000,2019-06-10T12:44:43.000+0000,,Fixed,New Feature,Major,['0.3.0-incubating'],,,,,,,,['io-java-mongodb'],['IO: Java: MongoDB'],IO (bounded source and sink) to deal with MongoDB.,Add MongoDBIO,2,,,jbonofre,True,jbonofre,jbonofre
beam,BEAM-455,2016-07-15T08:12:38.000+0000,2016-07-20T17:23:46.000+0000,2016-07-20T17:23:46.000+0000,,Fixed,New Feature,Major,['Not applicable'],,,,,,,,['sdk-py-core'],['SDKs: Python'],"Verifying correctness of custom sources can be tricky, for example, due to  complex interactions patterns that can occur when performing dynamic work rebalancing.

Having a test harness with utilities for easily testing sources will be extremely useful for users.

The proposed test harness will be similar to the one that is already available for Java SDK [1] but will be adjusted to match the Python custom source API.

[1] https://github.com/apache/incubator-beam/blob/master/sdks/java/core/src/main/java/org/apache/beam/sdk/testing/SourceTestUtils.java",Implement a test harness for verifying correctness of custom sources,2,,,chamikara,True,chamikara,chamikara
beam,BEAM-444,2016-07-12T22:05:34.000+0000,2016-12-01T22:04:42.000+0000,2016-12-01T22:04:44.000+0000,,Won't Fix,New Feature,Major,['Not applicable'],,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","Currently, blockOnRun is implemented in different ways by runners.
DirectRunner did blockOnRun based on DirectOptions.isBlockOnRun.
Dataflow have a separate BlockingDataflowRunner.
Flink and Spark runners might or might not block depends on their implementation on run().

I think DirectRunner's approach is the right way to go, and isBlockOnRun options need to be promoted to the general PipelineOptions.",Promote isBlockOnRun() to PipelineOptions.,2,,,peihe0@gmail.com,True,peihe0@gmail.com,peihe0@gmail.com
beam,BEAM-443,2016-07-12T04:48:42.000+0000,2016-12-01T22:06:17.000+0000,2016-12-01T22:06:17.000+0000,,Fixed,New Feature,Major,['0.2.0-incubating'],,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","waitToFinish() and cancel() are two most common operations for users to interact with a started pipeline.

Right now, they are only available in DataflowPipelineJob. But, it is better to move them to the common interface, so people can start implement them in other runners, and runner agnostic code can interact with PipelineResult better.",PipelineResult needs waitUntilFinish() and cancel(),2,,,peihe0@gmail.com,True,peihe0@gmail.com,peihe0@gmail.com
beam,BEAM-428,2016-07-06T22:50:39.000+0000,2017-01-11T01:59:45.000+0000,2017-01-11T01:59:46.000+0000,,Fixed,New Feature,Major,['Not applicable'],,,,,,,,['sdk-py-core'],['SDKs: Python'],"InProcessRunner is a bundle based drop in replacement for DirectRunner.

Similar to its Java equivalent it improves DirectRunner by executing transforms in parallel using bundles similar to a service based implementations. It offers better performance and more validation options.

Initially it will be a runner for executing batch jobs only. The target of this phase is to develop a drop in replacement for DirectRunner. Later it will be improved by adding streaming execution.",InProcessRunner - Bundle based local runner,2,,,altay,True,altay,altay
beam,BEAM-427,2016-07-06T08:53:45.000+0000,2019-01-10T11:10:45.000+0000,2019-01-10T11:10:45.000+0000,,Fixed,New Feature,Major,['Not applicable'],,,,,,,,['runner-ideas'],['Runners: ideas for new Beam runners'],"It could be interesting to have a Kafka Connect runner (http://docs.confluent.io/2.0.0/connect/).

Even if the programming model is not exactly the same, most of fn and pipelines could run on Kafka Connect.",Add Kafka Connect runner,4,1,,jbonofre,True,jamesmalone,jbonofre
beam,BEAM-425,2016-07-05T10:22:32.000+0000,2017-01-04T12:54:46.000+0000,2019-06-10T12:45:23.000+0000,,Fixed,New Feature,Major,['0.5.0'],,,,,,,,"['io-ideas', 'io-java-elasticsearch']","['IO: ideas (proposals for new connectors, new designs, etc)', 'IO: Java: ElasticSearch']",I'm working on a new ElasticsearchIO providing both bounded source and sink.,Create Elasticsearch IO,6,,,jbonofre,True,echauchot,jbonofre
beam,BEAM-424,2016-07-05T10:21:05.000+0000,,2019-04-30T18:31:35.000+0000,,,New Feature,Major,,,,,,,,,['io-ideas'],"['IO: ideas (proposals for new connectors, new designs, etc)']","Even if it's not really for production (mostly for demo purpose), it would be great to have a SocketIO (kind of similar to what we have in Spark).",Create UDP/TCP SocketIO,3,,,jbonofre,True,jbonofre,jbonofre
beam,BEAM-404,2016-06-30T20:44:22.000+0000,2017-05-18T18:34:21.000+0000,2017-05-18T18:34:21.000+0000,,Fixed,New Feature,Major,['2.0.0'],,,,,,,,['io-java-gcp'],"['IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)']","Right now, PubsubIO only lets uses access the message payload, decoded with the user-provided coder.

We should add a mode in which the source can return a message with the metadata (attributes) as well.",PubsubIO should have a mode that supports maintaining message attributes.,7,4,,dhalperi@google.com,True,reuvenlax,dhalperi@google.com
beam,BEAM-386,2016-06-29T18:15:31.000+0000,2016-12-28T22:29:34.000+0000,2018-05-25T14:49:54.000+0000,,Fixed,New Feature,Major,['0.2.0-incubating'],,2400,2400,,,,100,['runner-dataflow'],['Runners: Google Cloud Dataflow runner'],"UnboundedReadFromBoundedSource is done.
Make Dataflow runner use it.",Dataflow runner to support Read.Bounded in streaming mode.,3,,,peihe0@gmail.com,True,peihe0@gmail.com,peihe0@gmail.com
beam,BEAM-360,2016-06-20T18:38:42.000+0000,2016-07-20T17:22:55.000+0000,2016-07-20T17:22:55.000+0000,,Fixed,New Feature,Major,['Not applicable'],,,,,,,,['sdk-py-core'],['SDKs: Python'],"We already have a framework for creating new sources for Beam Python SDK - https://github.com/apache/incubator-beam/blob/python-sdk/sdks/python/apache_beam/io/iobase.py#L326

It would be great if we can add a framework on top of this that encapsulates logic common to sources that are based on files. This framework can include following features that are common to sources based on files.
(1) glob expansion
(2) support for new file-systems
(3) dynamic work rebalancing based on byte offsets
(4) support for reading compressed files.

Java SDK has a similar framework and it's available at - https://github.com/apache/incubator-beam/blob/master/sdks/java/core/src/main/java/org/apache/beam/sdk/io/FileBasedSource.java",Add a framework for creating Python-SDK sources for new file types,2,,,chamikara,True,chamikara,chamikara
beam,BEAM-332,2016-06-09T20:34:21.000+0000,,2019-04-30T18:30:34.000+0000,,,New Feature,Minor,,,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","{{TableRow}} has poorly behaved equality, and a certain amount of automatic coercion, so matchers based equals() are not applicable in tests. It would be handy to have matchers such as ""isTableRowEqualTo(otherRow)"".",Matcher(s) for TableRow,3,,['starter'],kenn,True,,kenn
beam,BEAM-313,2016-05-31T10:03:46.000+0000,2016-08-29T12:06:19.000+0000,2018-08-31T11:12:10.000+0000,,Fixed,New Feature,Major,['0.3.0-incubating'],,6600,6600,,,,100,['runner-spark'],['Runners: Spark runner'],"The general use case is that the SparkPipelineRunner creates its own Spark context and uses it for the pipeline execution.
Another alternative is to provide the SparkPipelineRunner with an existing spark context. This can be interesting for a lot of use cases where the Spark context is managed outside of beam (context reuse, advanced context management, spark job server, ...).

code sample : https://github.com/amarouni/incubator-beam/commit/fe0bb517bf0ccde07ef5a61f3e44df695b75f076",Enable the use of an existing spark context with the SparkPipelineRunner,4,,,amarouni,True,jbonofre,amarouni
beam,BEAM-301,2016-05-23T07:37:07.000+0000,2017-09-11T20:49:14.000+0000,2017-09-11T20:49:14.000+0000,,Fixed,New Feature,Major,['2.2.0'],,,,,,,,"['dsl-sql', 'sdk-ideas']","['DSLs: SQL', 'SDKs: Coordination on new language-specific SDKs']","The SQL DSL helps developers to build a Beam pipeline from SQL statement in String directly. 
In Phase I, it starts to support INSERT/SELECT queries with FILTERs, one example SQL as below:
{code}
INSERT INTO `SUB_USEREVENT` (`SITEID`, `PAGEID`, `PAGENAME`, `EVENTTIMESTAMP`)
(SELECT STREAM `USEREVENT`.`SITEID`, `USEREVENT`.`PAGEID`, `USEREVENT`.`PAGENAME`, `USEREVENT`.`EVENTTIMESTAMP`
FROM `USEREVENT` AS `USEREVENT`
WHERE `USEREVENT`.`SITEID` > 10)
{code}

A design doc is available at https://docs.google.com/document/d/1Uc5xYTpO9qsLXtT38OfuoqSLimH_0a1Bz5BsCROMzCU/edit?usp=sharing.",Add a Beam SQL DSL,24,3,,jbonofre,True,mingmxu,jbonofre
beam,BEAM-269,2016-05-06T22:11:26.000+0000,2016-06-16T18:55:53.000+0000,2016-10-20T21:46:53.000+0000,,Fixed,New Feature,Major,['0.2.0-incubating'],,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']",There isn't a coder for BigDecimal. This class is especially important for financial companies to represent money.,Create BigDecimal Coder,2,,,eljefe6aa,True,eljefe6aa,eljefe6aa
beam,BEAM-266,2016-05-06T10:48:46.000+0000,,2016-05-10T04:50:44.000+0000,,,New Feature,Major,,,,,,,,,['sdk-ideas'],['SDKs: Coordination on new language-specific SDKs'],"This concept can be used by business people with little or no programming ability if you have a Visual Editors. If the editor is an components, the editor can be reused in integration with other products.",Visual Pipeline Designers / Editors,7,4,,sirinath,True,,sirinath
beam,BEAM-261,2016-05-06T07:16:45.000+0000,2016-10-26T16:27:41.000+0000,2016-12-17T23:08:33.000+0000,,Fixed,New Feature,Major,['0.4.0'],,,,,,,,['runner-apex'],['Runners: Apex runner'],"Like Spark, Flink and GearPump, Apache Apex also does have advantages. Is it possible to have a runner for Apache Apex?",Apache Apex Runner,8,,,sirinath,True,thw,sirinath
beam,BEAM-256,2016-05-04T02:28:01.000+0000,2016-07-28T17:48:28.000+0000,2016-10-20T20:56:37.000+0000,,Fixed,New Feature,Major,['0.1.0-incubating'],,,,,,,,['testing'],['Testing: general infrastructure'],,Add lifecycle event verifiers for Beam pipelines.,2,,['backward-incompatible'],jaku,True,jaku,jaku
beam,BEAM-252,2016-05-02T18:38:04.000+0000,2016-12-02T19:55:23.000+0000,2016-12-17T23:10:35.000+0000,,Fixed,New Feature,Major,['0.4.0'],,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","There needs to be an easier way to run Regular Expressions as part of a transform. This will make string-based ETL much easier.

The transform should support using the matches and find methods. The transform should allow you to choose a group in the regex to output. The transform should allow single strings to be output or KV's of strings.",Make Regex Transform,4,,,eljefe6aa,True,eljefe6aa,eljefe6aa
beam,BEAM-245,2016-05-02T11:35:25.000+0000,2017-06-07T06:05:54.000+0000,2019-06-10T12:48:58.000+0000,,Fixed,New Feature,Major,['2.1.0'],,,,,,,,"['io-ideas', 'io-java-cassandra']","['IO: ideas (proposals for new connectors, new designs, etc)', 'IO: Java: Cassandra']",I started an IO providing source and sink for Apache Cassandra.,Create Cassandra IO,6,,,jbonofre,True,jbonofre,jbonofre
beam,BEAM-244,2016-05-02T11:33:58.000+0000,2016-10-02T09:02:04.000+0000,2019-06-10T12:49:18.000+0000,,Fixed,New Feature,Major,['0.3.0-incubating'],,,,,,,,"['io-ideas', 'io-java-jdbc']","['IO: ideas (proposals for new connectors, new designs, etc)', 'IO: Java: JDBC']",,Create JDBC IO,6,1,,jbonofre,True,jbonofre,jbonofre
beam,BEAM-233,2016-04-27T21:09:47.000+0000,2019-05-21T22:53:05.000+0000,2019-05-21T22:53:05.000+0000,,Fixed,New Feature,Major,['Not applicable'],,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","There should be a helper method to make it easier to register Avro specific record classes. This will be the most common type that needs to be registered. The code would look something like:
{code:java}
public class AvroHelper {
    public static void registerAvro(Pipeline p, Class<? extends SpecificRecordBase> clazz) {
        p.getCoderRegistry().registerCoder(clazz, new CoderFactory() {
            @Override
            public Coder<?> create(List<? extends Coder<?>> componentCoders) {
                return AvroCoder.of(clazz);
            }

            @Override
            public List<Object> getInstanceComponents(Object value) {
                return null;
            }
        });
    }
}
{code}

With usage:
{code:java}
        Pipeline p = Pipeline.create(options);
        
        AvroHelper.registerAvro(p, LogEntry.class);
{code}",Make Registering Avro Specific Records Easier,5,1,['starter'],eljefe6aa,True,,eljefe6aa
beam,BEAM-228,2016-04-25T23:18:16.000+0000,2016-07-07T19:26:55.000+0000,2016-07-07T19:26:55.000+0000,,Won't Fix,New Feature,Major,['Not applicable'],,,,,,,,['project-management'],"['Project Management: monthly reports, administrative tasks']",This issue tracks the creation of a merge bot for Beam. This merge bot should watch the Beam github repository and queue and merge pull requests which are marked LGTM and good for merge by an approved Beam committer.,Create a merge bot for Beam,3,,,jaku,True,jaku,jaku
beam,BEAM-227,2016-04-25T20:46:37.000+0000,2016-04-26T05:44:20.000+0000,2016-06-25T17:05:23.000+0000,,Invalid,New Feature,Major,['Not applicable'],,,,,,,,['runner-core'],['Runners: shared functionality for all runners'],"As a failsafe against bizzare jar versioning problems it would be great if the worker could simply print its build version (as a hard-coded, impossible to mess up string)

I just saw a customer who's jars were all tagged 1.5.1 but their contents were 1.5.0. I am not able rightly to apprehend the kind of confusion of implementation that could provoke such a scenario.",Log sdk version in worker-startup logs,2,,,mshields822,True,davor,mshields822
beam,BEAM-221,2016-04-22T21:50:20.000+0000,,2019-06-11T23:00:06.000+0000,,,New Feature,Minor,,,,,,,,,['io-java-gcp'],"['IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)']","Make it easy to read and write binary files of Protobuf objects. If there is a standard open source format for this, use it.

If not, roll our own and implement it?",ProtoIO,2,,"['newbie', 'starter']",dhalperi@google.com,True,anyyw,dhalperi@google.com
beam,BEAM-216,2016-04-20T18:32:49.000+0000,2016-05-07T06:01:01.000+0000,2016-06-25T17:04:24.000+0000,,Duplicate,New Feature,Major,['Not applicable'],,,,,,,,['runner-ideas'],['Runners: ideas for new Beam runners'],,Create Storm Runner ,6,,,sriharsha,True,jbonofre,sriharsha
beam,BEAM-214,2016-04-19T22:23:11.000+0000,2018-06-11T21:52:20.000+0000,2018-06-11T21:52:20.000+0000,,Fixed,New Feature,Minor,['2.5.0'],,60600,60600,,,,100,['io-ideas'],"['IO: ideas (proposals for new connectors, new designs, etc)']",Would be nice to support Parquet files with projection and predicates.,Create Parquet IO,18,8,,sinisa_lyh,True,jbonofre,sinisa_lyh
beam,BEAM-210,2016-04-19T18:15:00.000+0000,2017-06-29T06:12:00.000+0000,2017-08-21T22:17:46.000+0000,,Fixed,New Feature,Major,['2.1.0'],,,,,,,,"['beam-model', 'sdk-java-core']","['Beam Model: general programming model concepts, semantics', 'SDKs: Java: pipeline construction, core transformations']","Today, ON_TIME panes are emitted whether or not they are empty. We had decided that for final panes the user would want to control this behavior, to control data volume. But for ON_TIME panes no such control exists. The rationale is perhaps that the ON_TIME pane is a fundamental result that should not be elided. To be considered: whether this is what we want.",Allow control of empty ON_TIME panes analogous to final panes,7,,,mshields822,True,peihe0@gmail.com,mshields822
beam,BEAM-209,2016-04-19T16:00:28.000+0000,2017-05-18T19:54:38.000+0000,2017-05-18T19:54:38.000+0000,,Duplicate,New Feature,Major,['Not applicable'],,,,,,,,['io-java-gcp'],"['IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)']","The pattern in Nexmark Q10 (which needs to be made more robust against key range moves) has now been used by at least two customers.

Consider packaging it as a standard PTransform. ",PCollection-to-batched-gcs-files PTransform,4,1,,mshields822,True,,mshields822
beam,BEAM-205,2016-04-18T16:54:59.000+0000,2016-04-18T23:47:54.000+0000,2016-06-25T17:06:14.000+0000,,Duplicate,New Feature,Major,['Not applicable'],,,,,,,,['runner-direct'],['Runners: Direct runner and ULR for single machine testing and development'],"IoT applications often want to distribute processing between the device and the cloud. Device will often massage data, filter, window or batch for communication efficiency, etc.

During development one could imagine wanting to refactor processing between the device and the cloud. So could imagine having both sides in BEAM would be compelling.

This bug is to explore an 'in process' runner lite. Ie retain all the streaming support, but disable anything which burns cpu/mem only for the purposes of acid testing pipeline wrt serialiazability, concurrency safety, order assumptions, etc.",An 'in process' runner optimized for efficiency rather than debugging,5,,,mshields822,True,davor,mshields822
beam,BEAM-198,2016-04-15T08:09:06.000+0000,,2019-04-30T18:32:00.000+0000,,,New Feature,Major,,,,,,,,,['runner-spark'],['Runners: Spark runner'],"Currently, the Spark runner translates batch pipelines into RDD code, meaning it doesn't benefit from the optimizations DataFrames (which isn't type-safe) enjoys.

With Datasets, batch pipelines will benefit the optimizations, adding to that that Datasets are type-safe and encoder-based they seem like a much better fit for the Beam model.

Looking ahead, Datasets is a good choice since it's the basis for the future of Spark streaming as well  (Structured Streaming) so this will hopefully lay a solid foundation for a native integration between Spark 2.0 and Beam.",Spark runner batch translator to work with Datasets instead of RDDs,2,,,amitsela,True,jbonofre,amitsela
beam,BEAM-165,2016-04-02T17:35:54.000+0000,,2019-04-30T18:33:24.000+0000,,,New Feature,Major,,,,,,,,,"['runner-ideas', 'runner-mapreduce']","['Runners: ideas for new Beam runners', 'Runner: MapReduce']","I think a MapReduce runner could be a good addition to Beam. It would allow users to smoothly ""migrate"" from MapReduce to Spark or Flink.

Of course, the MapReduce runner will run in batch mode (not stream).",Add Hadoop MapReduce runner,10,6,,jbonofre,True,peihe0@gmail.com,jbonofre
beam,BEAM-150,2016-03-24T22:23:33.000+0000,2016-06-21T06:58:07.000+0000,2016-06-25T16:47:50.000+0000,2016-03-31,Fixed,New Feature,Major,['Not applicable'],,,,,,,,['website'],['Website: content and bugs'],Validate links on https://github.com/apache/incubator-beam-site when static content is generated.,Link validation on static content generation,1,,,jamesmalone,True,jamesmalone,jamesmalone
beam,BEAM-147,2016-03-23T22:21:44.000+0000,2018-03-26T16:52:50.000+0000,2018-03-26T16:52:50.000+0000,,Fixed,New Feature,Major,['2.0.0'],,,3600,,,,,"['beam-model', 'sdk-java-core', 'sdk-py-core']","['Beam Model: general programming model concepts, semantics', 'SDKs: Java: pipeline construction, core transformations', 'SDKs: Python']","The existing Aggregators are confusing both because of their name and because they serve multiple purposes.

Previous discussions around Aggregators/metrics/etc:
[Discussion on user list|http://mail-archives.apache.org/mod_mbox/incubator-beam-user/201603.mbox/browser] 
[Discussion on dev list|http://mail-archives.apache.org/mod_mbox/incubator-beam-dev/201603.mbox/browser] 
Exact name still being bikeshedded.

[Design document|http://s.apache.org/beam-metrics-api]",Introduce an easy API for pipeline metrics,9,2,,robertwb,True,bchambers,robertwb
beam,BEAM-137,2016-03-21T09:28:13.000+0000,,2017-11-08T23:54:41.000+0000,,,New Feature,Major,,,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","Right now, most of users provide the pipeline options via the main arguments.
For instance, it's the classic way to provide pipeline input, etc.

For convenience, it would be great that the pipeline looks for options in conf/[pipeline_name]-default.conf by default, and override the options using the main arguments.

Thoughts ?",Add implicit conf/pipeline-default.conf options file,4,,,jbonofre,True,,jbonofre
beam,BEAM-135,2016-03-18T18:02:58.000+0000,2017-04-05T17:42:42.000+0000,2017-04-05T17:42:42.000+0000,,Fixed,New Feature,Major,['2.0.0'],,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","We regularly receive questions about how to write a {{DoFn}} that operates on batches of elements. Example answers include:

http://stackoverflow.com/questions/35065109/can-datastore-input-in-google-dataflow-pipeline-be-processed-in-a-batch-of-n-ent/35068341#35068341

http://stackoverflow.com/questions/30177812/partition-data-coming-from-csv-so-i-can-process-larger-patches-rather-then-indiv/30178170#30178170

Possible APIs could be to wrap a {{DoFn}} and include a batch size, or to create a utility like {{Filter}}, {{Partition}}, etc. that takes a {{SerializableFunction}} or a {{SimpleFunction}}.","Utilities for ""batching"" elements in a DoFn",7,,,bchambers,True,echauchot,bchambers
beam,BEAM-132,2016-03-18T17:06:31.000+0000,2016-04-05T05:43:18.000+0000,2016-10-20T21:46:33.000+0000,,Fixed,New Feature,Major,['0.1.0-incubating'],,,,,,,,['project-management'],"['Project Management: monthly reports, administrative tasks']",,Release process for Beam artifacts to Maven,5,,,kkl0u,True,jbonofre,kkl0u
beam,BEAM-131,2016-03-18T17:04:15.000+0000,2016-05-17T15:15:24.000+0000,2016-10-20T21:46:54.000+0000,,Fixed,New Feature,Major,['0.1.0-incubating'],,,,,,,,['runner-flink'],['Runners: Flink runner'],,Write to jdbc/database ,6,,,kkl0u,True,kkl0u,kkl0u
beam,BEAM-130,2016-03-18T17:03:21.000+0000,2016-05-10T20:32:34.000+0000,2016-10-20T21:46:31.000+0000,,Fixed,New Feature,Major,['0.1.0-incubating'],,,,,,,,['runner-flink'],['Runners: Flink runner'],,Checkpointing of custom sources and sinks,3,,,kkl0u,True,aljoscha,kkl0u
beam,BEAM-129,2016-03-18T17:02:38.000+0000,2016-05-03T13:38:22.000+0000,2016-06-25T17:04:52.000+0000,,Invalid,New Feature,Major,['Not applicable'],,,,,,,,['runner-flink'],['Runners: Flink runner'],Support pubsub IO,Support pubsub IO,4,,,kkl0u,True,,kkl0u
beam,BEAM-128,2016-03-18T16:59:38.000+0000,2016-06-25T17:09:13.000+0000,2016-06-25T17:09:13.000+0000,,Fixed,New Feature,Major,['Not applicable'],,,,,,,,['runner-flink'],['Runners: Flink runner'],,The transform BigQueryIO.Read is currently not supported.,3,,,kkl0u,True,,kkl0u
beam,BEAM-127,2016-03-18T16:34:24.000+0000,2016-05-11T09:10:04.000+0000,2016-10-20T21:46:42.000+0000,,Fixed,New Feature,Major,['0.1.0-incubating'],,,,,,,,['runner-flink'],['Runners: Flink runner'],Currently the Flink runner for streaming does not support any sinks apart from writing on the local filesystems of the task managers. This has to change. A possible way is to add support for UnboundedFlinkSinks that wrap around the currently supported sinks in Flink.,Support for Unbounded sinks in Streaming,3,,,kkl0u,True,mxm,kkl0u
beam,BEAM-124,2016-03-17T14:07:37.000+0000,2016-10-20T18:19:54.000+0000,2016-10-20T18:19:55.000+0000,,Fixed,New Feature,Major,['Not applicable'],,,,,,,,['testing'],['Testing: general infrastructure'],Set up testing infrastructure so that an end to end test for WordCount (both batch and streaming) will be run periodically. ,Testing -- End to End WordCount Batch and Streaming Tests,5,,,stevewheeler,True,markflyhigh,stevewheeler
beam,BEAM-123,2016-03-16T23:48:43.000+0000,,2019-05-21T22:54:28.000+0000,,,New Feature,Minor,,,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","Add functionality to skip header rows when reading from a csv file.

http://stackoverflow.com/questions/28450554/skipping-header-rows-is-it-possible-with-cloud-dataflow",Skip header row from csv ,8,,"['newbie', 'starter']",davin.pidoto,True,,davin.pidoto
beam,BEAM-117,2016-03-16T17:50:38.000+0000,2016-05-25T23:08:22.000+0000,2016-10-20T21:46:36.000+0000,,Fixed,New Feature,Major,['0.1.0-incubating'],,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","As described in the following doc, we would like the SDK to allow associating display metadata with PTransforms.

https://docs.google.com/document/d/11enEB9JwVp6vO0uOYYTMYTGkr3TdNfELwWqoiUg5ZxM/edit?usp=sharing",Implement the API for Static Display Metadata,3,,,bchambers,True,swegner,bchambers
beam,BEAM-106,2016-03-10T17:27:09.000+0000,,2019-02-01T16:08:35.000+0000,,,New Feature,Major,,,,,,,,,['sdk-ideas'],['SDKs: Coordination on new language-specific SDKs'],"Ported from: https://github.com/GoogleCloudPlatform/DataflowJavaSDK/issues/50

There are a variety of use cases which would benefit from native support for conditional iteration.

For instance, http://stackoverflow.com/questions/31654421/conditional-iterations-in-google-cloud-dataflow/31659923?noredirect=1#comment51264604_31659923 asks about being able to write a loop like the following:

{code}
PCollection data  = ...
while(needsMoreWork(data)) {
  data = doAStep(data)
}
{code}
If there are specific use cases please let us know the details. In the future we will use this issue to post progress updates.",Native support for conditional iteration,14,7,,lcwik,True,,lcwik
beam,BEAM-104,2016-03-09T22:02:25.000+0000,2016-03-17T23:47:10.000+0000,2016-10-20T21:46:54.000+0000,,Fixed,New Feature,Major,['0.1.0-incubating'],,,,259200,259200,259200,,['website'],['Website: content and bugs'],"Given the variety of runners we're going to support, we'd like to have a clear and relatively comprehensive capability matrix published on the website and maintained over time. We're currently planning on having multiple levels of granularity, from simple yes/no to detailed paragraph-level explanations, to allow for various display options.

Our proposal is:

1. Have runner implementors fill out an initial set of capabilities in this spreadsheet Frances cooked up: https://docs.google.com/spreadsheets/d/1OM077lZBARrtUi6g0X0O0PHaIbFKCD6v0djRefQRE1I/edit#gid=939018601
If there are additional capabilities you'd like to propose, please do so.

2. Once filled out, I will move those data into YAML form in the website source and provide a basic rendering of it on the website. We'll also have an initial blog post announcing it. The YAML will become the source of truth that gets maintained over time, with version control thanks to being in the repo.
",Capability matrix for Beam website,1,,,takidau,True,frances,takidau
beam,BEAM-101,2016-03-09T08:53:34.000+0000,,2017-06-02T03:42:27.000+0000,,,New Feature,Major,,,,,,,,,['beam-model'],"['Beam Model: general programming model concepts, semantics']","For some applications, it's useful to declare a pane/window to be emitted (or finished) based on its contents. The simplest of these is the AfterCount trigger, but more sophisticated predicates could be constructed.

The requirements for consistent trigger firing are essentially that the state of the trigger form a lattice and that the ""should fire?"" question is a monotonic predicate on the lattice. Basically it asks ""are we high enough up the lattice?""

Because the element types may change between the application of Windowing and the actuation of the trigger, one idea is to extract the relevant data from the element at Windowing and pass it along implicitly where it can be combined and inspected in a type safe way later (similar to how timestamps and windows are implicitly passed with elements).

",Data-driven triggers,9,1,,robertwb,True,,robertwb
beam,BEAM-96,2016-03-04T22:01:34.000+0000,2016-03-21T22:35:19.000+0000,2016-06-25T17:15:45.000+0000,,Fixed,New Feature,Major,['0.1.0-incubating'],,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","The proposal of composed combine functions is following:
pc.apply(
    Combine.perKey(
         CombineFns.composeKeyed()
            .with(identityFn, new MaxIntegerFn(), maxLatencyTag)
            .with(identityFn, new MeanFn<Integer>(), meanLatencyTag)));

Example code:
   * PCollection<KV<K, Integer>> latencies = ...;
   *
   * TupleTag<Integer> maxLatencyTag = new TupleTag<Integer>();
   * TupleTag<Double> meanLatencyTag = new TupleTag<Double>();
   *
   * SimpleFunction<Integer, Integer> identityFn =
   *     new SimpleFunction<Integer, Integer>() {
   *       @Override
   *       public Integer apply(Integer input) {
   *           return input;
   *       }};
   * PCollection<KV<K, CoCombineResult>> maxAndMean = latencies.apply(
   *     Combine.perKey(
   *         CombineFns.composeKeyed()
   *            .with(identityFn, new MaxIntegerFn(), maxLatencyTag)
   *            .with(identityFn, new MeanFn<Integer>(), meanLatencyTag)));
   *
   * PCollection<T> finalResultCollection = maxAndMean
   *     .apply(ParDo.of(
   *         new DoFn<KV<K, CoCombineResult>, T>() {
   *           @Override
   *           public void processElement(ProcessContext c) throws Exception {
   *             KV<K, CoCombineResult> e = c.element();
   *             Integer maxLatency = e.getValue().get(maxLatencyTag);
   *             Double meanLatency = e.getValue().get(meanLatencyTag);
   *             .... Do Something ....
   *             c.output(...some T...);
   *           }
   *         }));",Support composing combine functions,3,,,peihe0@gmail.com,True,peihe0@gmail.com,peihe0@gmail.com
beam,BEAM-93,2016-03-03T18:59:25.000+0000,2016-10-20T18:21:28.000+0000,2017-05-16T04:33:23.000+0000,,Fixed,New Feature,Minor,['0.1.0-incubating'],,,,,,,,['runner-dataflow'],['Runners: Google Cloud Dataflow runner'],The Dataflow runner for Beam should support Compute Engine subnetworks for the workers: https://cloud.google.com/compute/docs/subnetworks,Support Compute Engine Subnetworks,3,,,sgmc@google.com,True,sgmc@google.com,sgmc@google.com
beam,BEAM-92,2016-03-03T17:08:11.000+0000,,2019-02-07T04:47:47.000+0000,,,New Feature,Major,,,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","Current sink API writes all data to a single destination, but there are many use cases where different pieces of data need to be routed to different destinations where the set of destinations is data-dependent (so can't be implemented with a Partition transform).

One internally discussed proposal was an API of the form:
{code}
PCollection<Void> PCollection<T>.apply(
    Write.using(DoFn<T, SinkT> where,
                MapFn<SinkT, WriteOperation<WriteResultT, T>> how)
{code}

so an item T gets written to a destination (or multiple destinations) determined by ""where""; and the writing strategy is determined by ""how"" that produces a WriteOperation (current API - global init/write/global finalize hooks) for any given destination.

This API also has other benefits:
* allows the SinkT to be computed dynamically (in ""where""), rather than specified at pipeline construction time
* removes the necessity for a Sink class entirely
* is sequenceable w.r.t. downstream transforms (you can stick transforms onto the returned PCollection<Void>, while the current Write.to() returns a PDone)",Data-dependent sinks,11,4,,jkff,True,,jkff
beam,BEAM-91,2016-03-03T05:12:17.000+0000,,2017-08-11T20:34:51.000+0000,,,New Feature,Major,,,,,2419200,2419200,2419200,,['beam-model'],"['Beam Model: general programming model concepts, semantics']","We still haven't added retractions to Beam, even though they're a core part of the model. We should document all the necessary aspects (uncombine, reverting DoFn output with DoOvers, sink integration, source-level retractions, etc), and then implement them.",Retractions,26,12,,takidau,True,,takidau
beam,BEAM-85,2016-03-02T01:00:16.000+0000,2016-12-17T22:26:28.000+0000,2016-12-17T22:26:29.000+0000,,Fixed,New Feature,Major,['0.5.0'],,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","We should validate two things:

# DataflowAssert is not added to a pipeline that has already been run.
# The pipeline is run after the DataflowAssert is added.

If either of these are not validated, then it is possible that the test doesn't actually verify anything.


This code should throw an assertion error or fail in some other way.
{code}
Pipeline p = TestPipeline.create();
PCollection<Boolean> value = p.apply(Create.of(Boolean.FALSE));
p.run();

DataflowAssert.thatSingleton(value).isEqualTo(true);
{code}

but it would pass silently.


similarly, this code wills pass silently:
{code}
Pipeline p = TestPipeline.create();
PCollection<Boolean> value = p.apply(Create.of(Boolean.FALSE));
DataflowAssert.thatSingleton(value).isEqualTo(true);
{code}",PAssert needs sanity check that it's used correctly,3,,,dhalperi@google.com,True,staslev,dhalperi@google.com
beam,BEAM-84,2016-03-01T11:45:33.000+0000,2016-11-23T09:34:45.000+0000,2016-11-23T09:34:45.000+0000,,Duplicate,New Feature,Major,['Not applicable'],,,,,,,,['runner-spark'],['Runners: Spark runner'],"Implement Beam Sessions, or Session Windows, by maintaining the session state. Spark 1.6 presents mapWithState as an improvement to updateStateByKey so It'd be better to use it.

See ""Session Windows"" in Dataflow documentation here: https://cloud.google.com/dataflow/model/windowing#Functions

Also, this blog post from Databricks would be a good place to start: https://databricks.com/blog/2016/02/01/faster-stateful-stream-processing-in-spark-streaming.html",Add support for Session Windows - Beam Sessions,1,,,amitsela,True,,amitsela
beam,BEAM-80,2016-02-29T23:22:36.000+0000,2016-03-21T22:41:25.000+0000,2016-06-25T17:15:44.000+0000,,Fixed,New Feature,Major,['0.1.0-incubating'],,,,,,,,['runner-dataflow'],['Runners: Google Cloud Dataflow runner'],"This is a missing feature of combine with context.

Combiner lifting is currently disabled for (Keyed)CombineWithContext with a passing through ParDo.",Support combiner lifting for (Keyed)CombineWithContext,3,,,peihe0@gmail.com,True,peihe0@gmail.com,peihe0@gmail.com
beam,BEAM-79,2016-02-27T04:46:07.000+0000,2019-06-03T16:01:14.000+0000,2019-06-03T16:01:14.000+0000,,Fixed,New Feature,Major,['2.4.0'],,,,,,,,['runner-gearpump'],['Runners: Gearpump runner'],"Intel is submitting Gearpump (http://www.gearpump.io) to ASF (https://wiki.apache.org/incubator/GearpumpProposal). Appears to be a mix of low-level primitives a la MillWheel, with some higher level primitives like non-merging windowing mixed in. Seems like it would make a nice Beam runner.",Gearpump runner,10,1,,takidau,True,mauzhang,takidau
beam,BEAM-76,2016-02-25T19:12:02.000+0000,2019-02-01T15:34:05.000+0000,2019-02-01T15:34:05.000+0000,,Fixed,New Feature,Major,['Not applicable'],,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']",FileBasedSource takes a [single glob|https://github.com/GoogleCloudPlatform/DataflowJavaSDK/blob/master/sdk/src/main/java/com/google/cloud/dataflow/sdk/io/FileBasedSource.java#L96]. If the user wants to read from an explicit file list or multiple globs they have to use multiple sources and Flatten. This seems like an oversight.,FileBasedSource should take a list of files/globs,5,,,dhalperi@google.com,True,,dhalperi@google.com
beam,BEAM-75,2016-02-25T17:49:29.000+0000,,2016-02-25T17:49:29.000+0000,,,New Feature,Major,,,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","SourceTestUtils has an exhaustive test for splitAtFraction. However, we should also add a runner-independent test that, for example, can verify that dynamic work rebalancing is actually invoked for a new BoundedSource.

An example of this test was proposed in https://github.com/GoogleCloudPlatform/DataflowJavaSDK/blob/master/sdk/src/test/java/com/google/cloud/dataflow/sdk/transforms/GroupByKeyTest.java#L435 , but was specific to the Google Cloud Dataflow service.",Running tests of dynamic work rebalancing,,,,dhalperi@google.com,True,,dhalperi@google.com
beam,BEAM-74,2016-02-25T17:47:17.000+0000,,2016-02-25T17:47:17.000+0000,,,New Feature,Major,,,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","SourceTestUtils has a fair bit of support for testing things like dynamic work rebalancing and initial splitting.

We should add variants that test UnboundedSource checkpoint-and-resume, etc.",Testing harness for Unbounded Sources,,,,dhalperi@google.com,True,,dhalperi@google.com
beam,BEAM-73,2016-02-25T17:33:36.000+0000,2017-03-29T18:30:39.000+0000,2017-03-29T18:30:39.000+0000,,Duplicate,New Feature,Minor,['2.0.0'],,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","Many Sources can be thought of as providing a byte[] payload -- e.g. TextIO bytes between newlines, or PubSubIO messages. Therefore, we originally suggested a Coder as the thing to use to decode these byte[] into T (what I'll call Parsing).

Consider the case of a text file of integers.

123\n
456\n
...

We want a PCollection<Integer> out, so we can use TextualIntegerCoder with TextIO.Read. However, that Coder will get propagated as the default coder for that PCollection (and may be used in downstream DoFns). This seem bad as, once the data is parsed, we probably want to use VarIntCoder or another Coder that is more CPU- and Space-efficient.

Another design pattern is
    TextIO.Read() -> MapElements<String, Integer> (lambda s : Integer.parseInt(s))

This has better behavior, but now we go from byte[] to String to Integer rather than directly from byte[] to Integer.

The solution seems to be to explicitly add Parser and Coder abstractions.",IO design pattern: Decouple Parsers and Coders,2,,['backward-incompatible'],dhalperi@google.com,True,,dhalperi@google.com
beam,BEAM-72,2016-02-25T17:28:59.000+0000,,2016-02-25T17:28:59.000+0000,,,New Feature,Minor,,,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","* FileBasedSource: user may want to know original path, directory, line number, byte offset, etc.
* PubSubIO: user may want to know things about the originating message (such as metadata attributes), not just they payload. BEAM-53
* etc.

We should have a standard pattern for this that provides a good guideline for implementing sources.",Source design pattern: option to retain metadata,,,,dhalperi@google.com,True,,dhalperi@google.com
beam,BEAM-71,2016-02-25T17:08:21.000+0000,,2019-04-30T18:31:11.000+0000,,,New Feature,Major,,,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","See also: BEAM-70

We should have a library of watermark implementations:

* now: for when items use arrival timestamps
* latest element: for when events arrive in timestamp order
* moving average over last K time intervals
* bucketed moving average, ...
",Watermark library,6,,['Watermark'],dhalperi@google.com,True,app-tarush,dhalperi@google.com
beam,BEAM-70,2016-02-25T17:06:59.000+0000,2019-06-01T03:51:27.000+0000,2019-06-01T03:51:27.000+0000,,Won't Do,New Feature,Major,['Not applicable'],,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","Right now, the UnboundedSource type hierarchy is 1 deep -- UnboundedSource and then things like CountingSource or KafkaSource directly on top.

To ease implementors, we should have more abstract classes in the ""common source types"". Some examples may be:

- KeyRangeUnboundedSource -- for things like PubSub, Bigtable, Kafka, Kinesis
- StreamSource -- for reading from a socket or other stream.",More expressive UnboundedSource hierarchy and utilities,2,,,dhalperi@google.com,True,,dhalperi@google.com
beam,BEAM-69,2016-02-25T16:53:11.000+0000,2016-07-12T05:26:54.000+0000,2016-07-12T05:26:54.000+0000,,Fixed,New Feature,Major,['Not applicable'],,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","See also: https://issues.apache.org/jira/browse/BEAM-68

Because the Beam Model does not support limiting parallelism of a specific step, individual runners needs to add runner-specific overrides of TextIO.Write and AvroIO.Write to enforce the `numShards` parameter. 

DataflowPipelineRunner and DirectPipelineRunner do that now; other runners will need to do so as well, or ignore/reject the writes, until BEAM-68 is resolved.",TextIO/AvroIO numShards may not work in different runners,,,,dhalperi@google.com,True,dhalperi@google.com,dhalperi@google.com
beam,BEAM-68,2016-02-25T16:50:46.000+0000,,2019-01-10T15:55:01.000+0000,,,New Feature,Major,,,,,,,,,['beam-model'],"['Beam Model: general programming model concepts, semantics']","Users may want to limit the parallelism of a step. Two classic uses cases are:

- User wants to produce at most k files, so sets TextIO.Write.withNumShards(k).
- External API only supports k QPS, so user sets a limit of k/(expected QPS/step) on the ParDo that makes the API call.

Unfortunately, there is no way to do this effectively within the Beam model. A GroupByKey with exactly k keys will guarantee that only k elements are produced, but runners are free to break fusion in ways that each element may be processed in parallel later.

To implement this functionaltiy, I believe we need to add this support to the Beam Model.",Support for limiting parallelism of a step,10,2,,dhalperi@google.com,True,,dhalperi@google.com
beam,BEAM-67,2016-02-25T16:32:25.000+0000,,2018-05-24T14:08:51.000+0000,,,New Feature,Minor,,,,,,,,,['io-ideas'],"['IO: ideas (proposals for new connectors, new designs, etc)']",Bounded source has been requested in the past.,Apache Sqoop connector,1,,,dhalperi@google.com,True,,dhalperi@google.com
beam,BEAM-66,2016-02-25T16:29:53.000+0000,2019-02-01T15:48:35.000+0000,2019-02-01T15:48:35.000+0000,,Fixed,New Feature,Minor,['Not applicable'],,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","(Blocked on SplittableDoFn, https://issues.apache.org/jira/browse/BEAM-65.)

This is a streaming glob watcher, for instance a directory tailer -- poll a directory read the data from new files as they show up. To get this right in a streaming pipeline requires careful management of timestamps and watermarks -- e.g., a filename or other metadata should reflect a timestamp earlier than any records in it, so that we are able to provide accurate lower bounds on output timestamp.",Unbounded source for directory/file scanning,5,2,,dhalperi@google.com,True,,dhalperi@google.com
beam,BEAM-65,2016-02-25T16:26:09.000+0000,2018-02-13T19:55:51.000+0000,2018-02-13T19:55:51.000+0000,,Fixed,New Feature,Minor,['2.2.0'],,,,,,,,['beam-model'],"['Beam Model: general programming model concepts, semantics']","SplittableDoFn is a proposed enhancement for ""dynamically splittable work"" to the Beam model.

Among other things, it would allow a unified implementation of bounded/unbounded sources with dynamic work rebalancing and the ability to express multiple scalable steps (e.g., global expansion -> file sizing & parsing -> splitting files into independently-processable blocks) via composition rather than inheritance.

This would make it much easier to implement many types of sources, to modify and reuse existing sources. Also, it would improve scalability of the Beam model by moving things like splitting a source from the control plane (where it is today -- glob -> List<FileBasedSource> sent over service APIs) into the data plane (PCollection<Glob> -> PCollection<FileName> -> ...).",SplittableDoFn,10,3,,dhalperi@google.com,True,jkff,dhalperi@google.com
beam,BEAM-64,2016-02-25T16:18:43.000+0000,,2016-02-25T16:18:43.000+0000,,,New Feature,Major,,,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","See also https://issues.apache.org/jira/browse/BEAM-56

Right now the supported compression types are listed in an enum. We should have a more generic way of registering compression methods and the needed streamfactories, etc.",General decompression registry,,,,dhalperi@google.com,True,,dhalperi@google.com
beam,BEAM-63,2016-02-25T16:12:07.000+0000,2016-10-20T18:25:22.000+0000,2016-10-20T18:25:23.000+0000,,Fixed,New Feature,Minor,['0.3.0-incubating'],,,,,,,,['io-java-gcp'],"['IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)']","1. Change the API to DatastoreIO.Read so that it exposes a `PTransform<PBegin,Entity>` rather than a `BoundedSource<Entity>`

2. Under the hood, rewrite the Source to implement dynamic work rebalancing. Datastore team's suggestion is to simply request a large number of splits initially and then use `Create.<Query>of(splits)` for dynamic assigning of Query splits to different workers.",DatastoreIO.Read: dynamic work rebalancing,,,,dhalperi@google.com,True,vikasrk,dhalperi@google.com
beam,BEAM-62,2016-02-25T16:09:49.000+0000,,2016-02-25T16:09:49.000+0000,,,New Feature,Minor,,,,,,,,,['io-java-gcp'],"['IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)']","Link to Datastore recommendations: https://cloud.google.com/datastore/docs/concepts/errors

Sounds like 403 and 409 should be retried. Other stuff may be fine.",DatastoreIO: upgrade error handling,,,,dhalperi@google.com,True,,dhalperi@google.com
beam,BEAM-61,2016-02-25T16:08:58.000+0000,2016-10-20T18:25:40.000+0000,2016-10-20T18:25:41.000+0000,,Fixed,New Feature,Minor,['0.3.0-incubating'],,,,,,,,['io-java-gcp'],"['IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)']",,DatastoreIO: unbounded sink,,,,dhalperi@google.com,True,vikasrk,dhalperi@google.com
beam,BEAM-60,2016-02-25T16:07:56.000+0000,,2017-03-28T23:20:26.000+0000,,,New Feature,Major,,,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","Many cloud and distributed filesystems are eventually consistent, for instance Amazon s3 and Google Cloud Storage.

To work around this, many systems that produce files such as Beam's FileBasedSinks, or Google BigQuery will provide methods to determine the number and set of files produced. E.g.,

* Beam FileBasedSink uses -00000-of-NNNNN
* BigQuery export jobs uses -000000 -000001 -000002 ... until an empty file is produced
* Another system may produce a .filelist suffix that contains a list of all files.

Users should be able to supply a glob to FileBasedSource but additionally supply a ""glob expander"" that can provide a custom implementation for file expansion. That way, e.g., Beam pipelines can be run back-to-back-to-back where each consumes the output of the previous, on an inconsistent filesystem, without data loss.",FileBasedSource/IOChannelFactory: Custom glob expansion,2,,,dhalperi@google.com,True,,dhalperi@google.com
beam,BEAM-59,2016-02-25T16:00:40.000+0000,2017-05-05T18:53:44.000+0000,2017-05-05T18:53:44.000+0000,,Fixed,New Feature,Major,['2.0.0'],,,,,,,,"['io-java-gcp', 'sdk-java-core']","['IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)', 'SDKs: Java: pipeline construction, core transformations']","Right now, FileBasedSource and FileBasedSink communication is mediated by IOChannelFactory. There are a number of issues:

* Global configuration -- e.g., all 'gs://' URIs use the same credentials. This should be per-source/per-sink/etc.
* Supported APIs -- currently IOChannelFactory is in the ""non-public API"" util package and subject to change. We need users to be able to add new backends ('s3://', 'hdfs://', etc.) directly, without fear that they will be broken.
* Per-backend features: e.g., creating buckets in GCS/s3, setting expiration time, etc.

Updates:
Design docs posted on dev@ list:
Part 1: IOChannelFactory Redesign: 
https://docs.google.com/document/d/11TdPyZ9_zmjokhNWM3Id-XJsVG3qel2lhdKTknmZ_7M/edit#

Part 2: Configurable BeamFileSystem:
https://docs.google.com/document/d/1-7vo9nLRsEEzDGnb562PuL4q9mUiq_ZVpCAiyyJw8p8/edit#heading=h.p3gc3colc2cs",Switch from IOChannelFactory to FileSystems,10,,,dhalperi@google.com,True,dhalperi@google.com,dhalperi@google.com
beam,BEAM-58,2016-02-25T15:52:08.000+0000,,2016-02-25T15:52:08.000+0000,,,New Feature,Minor,,,,,,,,,['io-java-gcp'],"['IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)']",Customer-supplied encryption keys are now in Beta. https://cloud.google.com/compute/docs/disks/customer-supplied-encryption,Support Google Cloud Storage encryption keys,,,,dhalperi@google.com,True,,dhalperi@google.com
beam,BEAM-57,2016-02-25T15:45:15.000+0000,2016-11-03T19:53:29.000+0000,2016-11-03T19:53:30.000+0000,,Fixed,New Feature,Minor,['0.4.0'],,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","Currently, FileBasedSink writes intermediate files and final output files to the same folder.  Since deletion of intermediate files is prone to eventual consistency issues with GCS,S3,other systems and users may use globs in subsequent pipelines, intermediate files should be written to a different folder.

This may be a subdirectory or a pipeline-defined temp directory (with probably some additional subdirectories for this source).",FileBasedSink should write intermediate output to a different folder,2,,,dhalperi@google.com,True,jkff,dhalperi@google.com
beam,BEAM-56,2016-02-25T15:43:15.000+0000,2016-08-15T03:28:48.000+0000,2016-08-15T03:28:56.000+0000,,Fixed,New Feature,Minor,['0.2.0-incubating'],,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']",,Support ZIP compression in CompressedSource,1,,,dhalperi@google.com,True,jbonofre,dhalperi@google.com
beam,BEAM-55,2016-02-25T15:42:12.000+0000,2016-10-14T17:37:21.000+0000,2016-10-14T17:37:22.000+0000,,Fixed,New Feature,Minor,['0.3.0-incubating'],,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","FileBasedSink (also TextIO.Write, AvroIO.Write, etc). does not have an option for compressing its output.

In general, we discourage compression because it limits or blocks scalably reading from a file in parallel. However, users may want it -- so we should support the option (with appropriate warnings).",Allow users to compress FileBasedSink output files,3,1,,dhalperi@google.com,True,jeffkpayne@gmail.com,dhalperi@google.com
beam,BEAM-54,2016-02-25T15:38:09.000+0000,,2017-05-16T08:55:55.000+0000,,,New Feature,Minor,,,,,,,,,['io-java-gcp'],"['IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)']","Currently in Java code we validate only topic's/subscription's format. We should also optionally validate its existence, like we do for other sources, like BigQueryIO.",PubSubIO: add validation of topic/subscription,3,,,dhalperi@google.com,True,,dhalperi@google.com
beam,BEAM-53,2016-02-25T15:35:03.000+0000,2016-05-20T16:00:02.000+0000,2016-10-20T21:46:38.000+0000,,Fixed,New Feature,Major,['0.1.0-incubating'],,,,,,,,['runner-core'],['Runners: shared functionality for all runners'],"PubSubIO is currently only partially implemented in Java: the DirectPipelineRunner uses a non-scalable API in a single-threaded manner.

In contrast, the DataflowPipelineRunner uses an entirely different code path implemented in the Google Cloud Dataflow service.

We need to reimplement PubSubIO in Java in order to support other runners in a scalable way.

Additionally, we can take this opportunity to add new features:

* getting timestamp from an arbitrary lambda in arbitrary formats rather than from a message attribute in only 2 formats.
* exposing metadata and attributes in the elements produced by PubSubIO.Read
* setting metadata and attributes in the messages written by PubSubIO.Write
",PubSubIO: reimplement in Java,4,1,,dhalperi@google.com,True,mshields822,dhalperi@google.com
beam,BEAM-52,2016-02-25T15:25:00.000+0000,2016-06-06T16:31:15.000+0000,2019-06-10T12:50:05.000+0000,,Fixed,New Feature,Major,['0.1.0-incubating'],,,,,,,,['io-java-kafka'],['IO: Java: Kafka'],"We should support Apache Kafka. The priority list is probably:

* UnboundedSource
* unbounded Sink
* BoundedSource
* bounded Sink

The connector should be well-tested, especially around UnboundedSource checkpointing and resuming, and data duplication.",KafkaIO - unbounded source & sink,9,1,,dhalperi@google.com,True,rangadi,dhalperi@google.com
beam,BEAM-51,2016-02-25T15:20:14.000+0000,,2018-12-05T00:14:43.000+0000,,,New Feature,Minor,,,,,,,,,['io-ideas'],"['IO: ideas (proposals for new connectors, new designs, etc)']","We should implement a CSV-based source.

One possibility would be to support the same options as BigQuery. https://cloud.google.com/bigquery/preparing-data-for-bigquery#dataformats These options are:

fieldDelimiter: allowing a custom delimiter... csv vs tsv, etc. My guess is this is critical. One common delimiter that people use is 'thorn' (þ).

quote: Custom quote char. By default, this is '""', but this allows users to set it to something else, or, perhaps more commonly, remove it entirely (by setting it to the empty string). For example, tab-separated files generally don't need quotes.

allowQuotedNewlines: whether you can quote newlines. In the official CSV RFC, newlines can be quoted.. that is, you can have ""a"", ""b\n"", ""c"" in a single line. This makes splitting of large csv files impossible, so we should disallow quoted newlines by default unless the user really wants them (in which case, they'll get worse performance).

allowJaggedRows: This allows inferring null if not enough columns are specified. Otherwise we give an error for the row.

ignoreUnknownValues: The opposite of allowJaggedRows, this means that if a user has _too_ many values for the schema, we will ignore the ones we don't recognize, rather than reporting an error for the row.

skipHeaderRows: How many header lines are in the file.

encoding: UTF8-vs latin1, etc.
compression: gzip, bzip, etc.",Implement a CSV file reader,6,3,,dhalperi@google.com,True,,dhalperi@google.com
beam,BEAM-50,2016-02-25T15:15:26.000+0000,2016-05-03T20:01:09.000+0000,2016-10-20T18:26:36.000+0000,,Fixed,New Feature,Minor,['0.1.0-incubating'],,,,,,,,['io-java-gcp'],"['IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)']","BigQueryIO.Write is currently implemented in a somewhat hacky way.

Unbounded sink:
* The DirectPipelineRunner and the DataflowPipelineRunner use StreamingWriteFn and BigQueryTableInserter to insert rows using BigQuery's streaming writes API.

Bounded sink:
* The DirectPipelineRunner still uses streaming writes.
* The DataflowPipelineRunner uses a different code path in the Google Cloud Dataflow service that writes to GCS and the initiates a BigQuery load job.
* Per-window table destinations do not work scalably. (See Beam-XXX).

We need to reimplement BigQueryIO.Write fully in Java code in order to support other runners in a scalable way.

I additionally suggest that we revisit the design of the BigQueryIO sink in the process. A short list:

* Do not use TableRow as the default value for rows. It could be Map<String, Object> with well-defined types, for example, or an Avro GenericRecord. Dropping TableRow will get around a variety of issues with types, fields named 'f', etc., and it will also reduce confusion as we use TableRow objects differently than usual (for good reason).

* Possibly support not-knowing the schema until pipeline execution time.

* Our builders for BigQueryIO.Write are useful and we should keep them. Where possible we should also allow users to provide the JSON objects that configure the underlying table creation, write disposition, etc. This would let users directly control things like table expiration time, table location, etc., Would also optimistically let users take advantage of some new BigQuery features without code changes.

* We could choose between streaming write API and load jobs based on user preference or dynamic job properties . We could use streaming write in a batch pipeline if the data is small. We could use load jobs in streaming pipelines if the windows are large enough to make this practical.

* When issuing BigQuery load jobs, we could leave files in GCS if the import fails, so that data errors can be debugged.

* We should make per-window table writes scalable in batch.

Caveat, possibly blocker:
* (Beam-XXX): cleanup and temp file management. One advantage of the Google Cloud Dataflow implementation of BigQueryIO.Write is cleanup: we ensure that intermediate files are deleted when bundles or jobs fail, etc. Beam does not currently support this.",BigQueryIO.Write: reimplement in Java,1,,,dhalperi@google.com,True,peihe0@gmail.com,dhalperi@google.com
beam,BEAM-49,2016-02-25T14:56:40.000+0000,2016-10-20T18:26:48.000+0000,2016-10-20T18:26:49.000+0000,,Fixed,New Feature,Minor,['0.3.0-incubating'],,,,,,,,['io-java-gcp'],"['IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)']",BigQuery has added a new schema type: BYTES. We should support it.,BigQueryIO: support BYTES,,,,dhalperi@google.com,True,peihe0@gmail.com,dhalperi@google.com
beam,BEAM-48,2016-02-25T14:55:00.000+0000,2016-06-15T06:29:45.000+0000,2017-05-02T20:14:26.000+0000,,Fixed,New Feature,Major,['0.1.0-incubating'],,,,,,,,['io-java-gcp'],"['IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)']","BigQueryIO.Read is currently implemented in a hacky way: the DirectPipelineRunner streams all rows in the table or query result directly using the JSON API, in a single-threaded manner.

In contrast, the DataflowPipelineRunner uses an entirely different code path implemented in the Google Cloud Dataflow service. (A BigQuery export job to GCS, followed by a parallel read from GCS).

We need to reimplement BigQueryIO as a BoundedSource in order to support other runners in a scalable way.

I additionally suggest that we revisit the design of the BigQueryIO source in the process. A short list:

* Do not use TableRow as the default value for rows. It could be Map<String, Object> with well-defined types, for example, or an Avro GenericRecord. Dropping TableRow will get around a variety of issues with types, fields named 'f', etc., and it will also reduce confusion as we use TableRow objects differently than usual (for good reason).

* We could also directly add support for a RowParser to a user's POJO.

* We should expose TableSchema as a side output from the BigQueryIO.Read.

* Our builders for BigQueryIO.Read are useful and we should keep them. Where possible we should also allow users to provide the JSON objects that configure the underlying intermediate tables, query export, etc. This would let users directly control result flattening, location of intermediate tables, table decorators, etc., and also optimistically let users take advantage of some new BigQuery features without code changes.

* We could use switch between whether we use a BigQuery export + parallel scan vs API read based on factors such as the size of the table at pipeline construction time.",BigQueryIO.Read reimplemented as BoundedSource,3,,,dhalperi@google.com,True,peihe0@gmail.com,dhalperi@google.com
beam,BEAM-47,2016-02-25T14:28:48.000+0000,2019-06-01T03:50:01.000+0000,2019-06-01T03:50:01.000+0000,,Won't Do,New Feature,Major,['Not applicable'],,,,,,,,['io-ideas'],"['IO: ideas (proposals for new connectors, new designs, etc)']","BoundedSource has splitAtFraction and related code to enable large work items to be split dynamically and moved to other workers.

UnboundedSource does not have this support -- instead, runners are expected to allocate many splits initially and rebalance those between workers.

We should add dynamic work rebalancing to UnboundedSource. This will almost certainly be done by checkpointing the source, and then the dynamically produced splits will both inherit the checkpoint (or a split-adapted version of it).",UnboundedSource should support dynamic work rebalancing,2,,,dhalperi@google.com,True,,dhalperi@google.com
beam,BEAM-46,2016-02-25T14:21:58.000+0000,2016-08-16T02:02:24.000+0000,2016-08-16T02:02:24.000+0000,,Fixed,New Feature,Major,['0.3.0-incubating'],,,,,,,,['io-java-gcp'],"['IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)']","Google Cloud Bigtable is currently in Beta. https://cloud.google.com/bigtable/ A bounded sink is included in the initial code for Beam, and uses asynchronous row mutations (with bounded memory) for maximum throughput.

The unbounded sink code is in principle not too different. The key areas of focus are better connection management, thread management, and fault tolerance (e.g., so connections are not leaked if bundles fail) in the unbounded case in which there are hundreds of active threads and very small bundles.",Unbounded sink for Google Cloud Bigtable,1,,,dhalperi@google.com,True,ianzhou,dhalperi@google.com
beam,BEAM-45,2016-02-25T14:18:19.000+0000,2016-06-25T01:51:41.000+0000,2016-06-25T16:57:15.000+0000,,Later,New Feature,Major,['Not applicable'],['Not applicable'],,,,,,,['io-java-gcp'],"['IO: Java: Google Cloud Platform functionality (BigQueryIO, DatastoreIO, etc)']","Google Cloud Bigtable is currently in Beta. https://cloud.google.com/bigtable/ A bounded source is included in the initial code for Beam, and does a table scan (with an optional row filter) and dynamic work rebalancing.

The unbounded source should support initial splitting based on key ranges but then streaming along the timestamp dimension.",Unbounded source for Google Cloud Bigtable,,,,dhalperi@google.com,True,,dhalperi@google.com
beam,BEAM-42,2016-02-20T02:24:04.000+0000,2017-02-14T22:07:38.000+0000,2017-02-14T22:07:38.000+0000,,Fixed,New Feature,Minor,['0.6.0'],,,,,,,,['beam-model'],"['Beam Model: general programming model concepts, semantics']","Analogous to BagState's blind writes, and SetState's efficient membership check, MapState would add efficient key lookup and mutation, and other Map-related operations.","State for ""Map"" data, supporting efficient key lookup, etc.",5,,['State'],kenn,True,lzljs3620320,kenn
beam,BEAM-41,2016-02-20T02:22:05.000+0000,2017-02-14T22:07:24.000+0000,2017-04-07T01:13:37.000+0000,,Fixed,New Feature,Minor,['0.6.0'],,,,,,,,['beam-model'],"['Beam Model: general programming model concepts, semantics']","Analogous to how BagState supports efficient blind writes, a SetState would support efficient membership checking without reading the entire set.","State for ""Set"" data, supporting efficient membership checks",5,1,['State'],kenn,True,lzljs3620320,kenn
beam,BEAM-37,2016-02-19T22:53:35.000+0000,2016-10-20T18:33:58.000+0000,2016-11-28T21:19:53.000+0000,,Duplicate,New Feature,Major,['Not applicable'],,,,,,,,['runner-core'],['Runners: shared functionality for all runners'],"DoFnWithContext is an enhanced DoFn where annotations and parameter lists are inspected to determine whether it accesses windowing information, etc.

Today, each feature of DoFnWithContext requires implementation on DoFn, which precludes the easy addition of features that we don't have designs for in DoFn.",Run DoFnWithContext without conversion to vanilla DoFn,6,1,['backward-incompatible'],kenn,True,kenn,kenn
beam,BEAM-33,2016-02-19T22:34:17.000+0000,2016-06-22T17:00:24.000+0000,2016-06-25T16:57:47.000+0000,,Duplicate,New Feature,Major,['Not applicable'],,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']",Today DataflowAssert rewindows into the global window before performing a side input (as an implementation detail). This precludes support for other windowing schemes and triggers.,Make DataflowAssert more window-aware,2,,"['Triggers', 'Windowing']",kenn,True,tgroh,kenn
beam,BEAM-32,2016-02-19T22:31:06.000+0000,2017-06-20T18:18:24.000+0000,2017-06-20T18:18:24.000+0000,,Won't Fix,New Feature,Minor,['Not applicable'],,,,,,,,"['beam-model', 'runner-core']","['Beam Model: general programming model concepts, semantics', 'Runners: shared functionality for all runners']","Today, the ReduceFnRunner sets a timers and emits an empty ON_TIME pane as long as the trigger allows it. This could be controlled in a manner analogous to the empty final pane at window expiration (also owned by the ReduceFnRunner).",Consider not emitting empty ON_TIME pane unless requested,5,,"['Triggers', 'Windowing', 'backward-incompatible']",kenn,True,kenn,kenn
beam,BEAM-28,2016-02-19T22:04:34.000+0000,,2019-04-30T18:29:57.000+0000,,,New Feature,Major,,,,,,,,,['beam-model'],"['Beam Model: general programming model concepts, semantics']","Today, the side input window Sessions is undefined in the model (thus the Java implementation of #getSideInputWindow throws UnsupportedOperationException).",Support side inputs for Sessions,3,,['Windowing'],kenn,True,,kenn
beam,BEAM-27,2016-02-19T21:54:20.000+0000,2017-02-14T06:12:21.000+0000,2017-02-14T06:12:21.000+0000,,Implemented,New Feature,Major,['0.6.0'],,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']",Pipeline authors will benefit from a different factorization of interaction with underlying timers. The current APIs are targeted at runner implementers.,Add user-ready API for interacting with timers,7,,,kenn,True,kenn,kenn
beam,BEAM-26,2016-02-19T21:50:44.000+0000,,2019-06-12T08:52:25.000+0000,,,New Feature,Major,,,,,,,,,['runner-dataflow'],['Runners: Google Cloud Dataflow runner'],,Model for validation of user state upon pipeline update,4,,['State'],kenn,True,,kenn
beam,BEAM-25,2016-02-19T21:48:52.000+0000,2017-02-14T06:11:48.000+0000,2017-04-02T09:34:42.000+0000,,Implemented,New Feature,Major,['0.6.0'],,,,,,,,['sdk-java-core'],"['SDKs: Java: pipeline construction, core transformations']","Our current state API is targeted at runner implementers, not pipeline authors. As such it has many capabilities that are not necessary nor desirable for simple use cases of stateful ParDo (such as dynamic state tag creation). Implement a simple state intended for user access.

(Details of our current thoughts in forthcoming design doc: https://s.apache.org/beam-state)",Add user-ready API for interacting with state,15,3,['State'],kenn,True,kenn,kenn
beam,BEAM-23,2016-02-19T21:34:44.000+0000,2016-11-28T21:21:57.000+0000,2016-12-12T21:23:53.000+0000,,Duplicate,New Feature,Major,['Not applicable'],,,,,,,,['beam-model'],"['Beam Model: general programming model concepts, semantics']","To a key-partitioned ParDo, add the ability for a user's DoFn to, on a per-key-and-window basis:

 - read and write consistent state
 - set a timer to request a callback at a particular moment (in any time domain)
",User-facing state and timers for ParDo(DoFn),10,2,['State'],kenn,True,kenn,kenn
beam,BEAM-21,2016-02-18T09:34:22.000+0000,,2018-02-14T18:22:57.000+0000,,,New Feature,Major,,,,,,,,,['io-ideas'],"['IO: ideas (proposals for new connectors, new designs, etc)']",We can add the SocketStream as Flink to read data from net,Add SocketStream IO,2,1,,bakey,True,,bakey
beam,BEAM-16,2016-02-15T18:09:38.000+0000,,2017-05-30T08:56:02.000+0000,,,New Feature,Minor,,,,,,,,,['runner-spark'],['Runners: Spark runner'],This could be done by implementing a SparkSource.,Make Spark RDDs readable as PCollections,6,,,amitsela,True,,amitsela
beam,BEAM-14,2016-02-15T12:40:56.000+0000,,2019-04-30T18:30:18.000+0000,,,New Feature,Major,,,,,,,,,['sdk-ideas'],['SDKs: Coordination on new language-specific SDKs'],"Even if users would still be able to use directly the API, it would be great to provide a DSL on top of the API covering batch and streaming data processing but also data integration.
Instead of designing a pipeline as a chain of apply() wrapping function (DoFn), we can provide a fluent DSL allowing users to directly leverage keyturn functions.

For instance, an user would be able to design a pipeline like:

{code}
.from(“kafka:localhost:9092?topic=foo”).reduce(...).split(...).wiretap(...).map(...).to(“jms:queue:foo….”);
{code}

The DSL will allow to use existing pipelines, for instance:

{code}
.from(""cxf:..."").reduce().pipeline(""other"").map().to(""kafka:localhost:9092?topic=foo&acks=all"")
{code}

So it means that we will have to create a IO Sink that can trigger the execution of a target pipeline: (from(""trigger:other"") triggering the pipeline execution when another pipeline design starts with pipeline(""other"")). We can also imagine to mix the runners: the pipeline() can be on one runner, the from(""trigger:other"") can be on another runner). It's not trivial, but it will give strong flexibility and key value for Beam.

In a second step, we can provide DSLs in different languages (the first one would be Java, but why not providing XML, akka, scala DSLs).

We can note in previous examples that the DSL would also provide data integration support to bean in addition of data processing. Data Integration is an extension of Beam API to support some Enterprise Integration Patterns (EIPs). As we would need metadata for data integration (even if metadata can also be interesting in stream/batch data processing pipeline), we can provide a DataxMessage built on top of PCollection. A DataxMessage would contain:
structured headers
binary payload
For instance, the headers can contains an Avro schema to describe the payload.
The headers can also contains useful information coming from the IO Source (for instance the partition/path where the data comes from, …).
",Add declarative DSLs (XML & JSON),8,3,,jbonofre,True,eachsaj,jbonofre
beam,BEAM-13,2016-02-14T18:00:10.000+0000,2016-08-08T05:47:54.000+0000,2019-06-10T12:50:21.000+0000,,Fixed,New Feature,Major,['0.2.0-incubating'],,,,,,,,"['io-ideas', 'io-java-jms']","['IO: ideas (proposals for new connectors, new designs, etc)', 'IO: Java: JMS']",Work in progress: https://github.com/jbonofre/DataflowJavaSDK/tree/IO-JMS,Create JMS IO,2,,,jbonofre,True,jbonofre,jbonofre
beam,BEAM-10,2016-02-13T05:40:35.000+0000,,2019-04-30T18:30:25.000+0000,,,New Feature,Major,,,,,,,,,['runner-ideas'],['Runners: ideas for new Beam runners'],,Create OSGi/Karaf runner,4,2,,jbonofre,True,jbonofre,jbonofre
beam,BEAM-8,2016-02-12T19:43:58.000+0000,2017-02-07T02:15:32.000+0000,2017-02-07T02:15:32.000+0000,,Fixed,New Feature,Major,['Not applicable'],,,,,,,,['sdk-py-core'],['SDKs: Python'],"Google has an in progress Python SDK, which we will be contributing to Beam in the near future.",Import Python SDK,5,1,,frances,True,frances,frances
commons-beanutils,BEANUTILS-514,2018-10-08T17:28:26.000+0000,2018-10-12T20:47:59.000+0000,2018-10-12T20:47:59.000+0000,,Fixed,New Feature,Major,['2.0.0'],,,,,,,,,,Remove all code marked with {{@deprecated}} and {{@Deprecated}}.,Remove deprecated code for 2.0.0,1,,,ggregory,True,,ggregory
commons-beanutils,BEANUTILS-512,2018-10-05T19:58:00.000+0000,2018-10-05T19:59:24.000+0000,2018-10-05T19:59:25.000+0000,,Fixed,New Feature,Major,['2.0.0'],,,,,,,,,,Add Automatic-Module-Name entry to MANIFEST.MF.,Add Automatic-Module-Name entry to MANIFEST.MF,1,,,ggregory,True,ggregory,ggregory
commons-beanutils,BEANUTILS-511,2018-07-30T00:25:25.000+0000,2018-07-30T15:53:30.000+0000,2018-07-30T15:53:30.000+0000,,Duplicate,New Feature,Major,,"['1.9.4', '2.0.0']",,,3600,3600,3600,,"['Bean / Property Utils', 'Bean-Collections', 'ConvertUtils & Converters', 'DynaBean', 'Expression Syntax', 'Locale BeanUtils / Converters']","['BeanUtils/BeanUtilsBean, PropertyUtils/PropertyUtilsBean, MethodUtils, ConstructorUtils', 'Optional Bean-Collections package', 'Convert Utils and Converters (non-localized)', 'DynaBean, DynaClass and implementations', 'Expression Syntax', 'Localized Convert Utils and Converters']","Please add OWASP Dependency Check to the build (pom.xml). OWASP DC makes an outbound REST call to MITRE Common Vulnerabilities & Exposures (CVE) to perform a lookup for each dependant .jar to list any/all known vulnerabilities for each jar. This step is needed because a manual MITRE CVE lookup/check on the main component does not include checking for vulnerabilities in components or in dependant libraries.

OWASP Dependency check : https://www.owasp.org/index.php/OWASP_Dependency_Check has plug-ins for most Java build/make types (ant, maven, ivy, gradle).

Also, add the appropriate command to the nightly build to generate a report of all known vulnerabilities in any/all third party libraries/dependencies that get pulled in. example : mvn -Powasp -Dtest=false -DfailIfNoTests=false clean aggregate

Generating this report nightly/weekly will help inform the project's development team if any dependant libraries have a reported known vulnerailities. Project teams that keep up with removing vulnerabilities on a weekly basis will help protect businesses that rely on these open source componets.",Please add OWASP Dependency Check to the build (build.xml),1,,"['build', 'easyfix', 'security']",ABakerIII,True,,ABakerIII
commons-beanutils,BEANUTILS-425,2013-02-22T20:29:25.000+0000,2013-11-09T18:02:06.000+0000,2013-12-12T20:37:20.000+0000,,Fixed,New Feature,Minor,['1.9.0'],['1.8.3'],,,,,,,['Bean / Property Utils'],"['BeanUtils/BeanUtilsBean, PropertyUtils/PropertyUtilsBean, MethodUtils, ConstructorUtils']","So far BeanUtils can cope with properties conforming to the Java Beans specification. In some situations it makes sense to relax this requirement and allow the detection of other forms of get and set methods as well.

For instance, fluent APIs have become popular. Here you have a set method which does not return *void* and thus violates the Java Beans specification. Objects using such an API cannot be dealt with by BeanUtils currently.

For reasons of backwards compatibility the current behavior should remain the default. But it would be cool if there was an option to set a custom introspection policy. The policy would be invoked during property discovery and can decide which properties to include or not.",Support customization of introspection mechanism,2,,,oheger,True,oheger,oheger
commons-beanutils,BEANUTILS-424,2013-02-22T19:04:18.000+0000,,2013-12-05T21:05:42.000+0000,,,New Feature,Minor,['2.0.0'],['1.8.3'],,,,,,,['Bean / Property Utils'],"['BeanUtils/BeanUtilsBean, PropertyUtils/PropertyUtilsBean, MethodUtils, ConstructorUtils']","The method {{PropertyUtilsBean.copyProperties()}} allows copying all properties from a source bean to a target bean. There are use cases when only a subset of the properties available should be taken into account. It would be nice if there was an overloaded method accepting additional parameters to limit the set of properties that will be copied.

A pretty generic solution could define a typical filter interface, e.g. {{PropertyFilter}}. Then we can provide some default implementations, for instance inclusion or exclusion filters based on property names.",Allow filtering the properties to be copied,1,,,oheger,True,,oheger
commons-beanutils,BEANUTILS-407,2011-11-06T20:38:02.000+0000,,2013-12-05T21:05:45.000+0000,,,New Feature,Critical,['2.0.0'],['1.8.3'],,,,,,,['DynaBean'],"['DynaBean, DynaClass and implementations']","It would be very very cool if DynaBeans could be used with any framework doing reflection - not only the specially DynaBean-aware BeanUtils.

It turns out that using CGLIB this would be possible, I've tried and will attach a patch.","DynaBean & Byte Code generation, for interoperability",,,,vorburger,True,,vorburger
commons-beanutils,BEANUTILS-406,2011-11-06T12:31:44.000+0000,,2013-12-05T21:05:44.000+0000,,,New Feature,Major,['2.0.0'],['1.8.3'],,,,,,,['DynaBean'],"['DynaBean, DynaClass and implementations']","It could sometimes be very useful to create DynaClass definitions not only programmatically (as is possible today), but to define data structures in some textual format (a ""DSL""), and load that into DynaClass/DynaProperty and create DynaBeans from that.

This isn't very hard to add to BeanUtils (I've done it and will attach a patch) and would allow the following usage, given:

{noformat}Address  { 
	zip:  java.lang.Long 
}

Employee {
        firstName : java.lang.String
	lastName :java.lang.String   
	
	mainAddress  : Address
	boss         : Employee
	subordinates : Employee *
	address      : Address <>
}{noformat}

one could then use the new proposed DynaClassReader like so:

{noformat}DynaClassReader r = new DynaClassReader();
r.readClasspathResource(""/DynaClassReaderTest.domain.txt"");
DynaClass klass = r.getDynaClass(""Employee"");
{noformat}

This requires BEANUTILS-405.","DynaClassReader to read DynaClass definitions from a ""DSL""",3,,,vorburger,True,britter,vorburger
commons-beanutils,BEANUTILS-405,2011-11-05T14:38:44.000+0000,,2013-12-05T21:05:39.000+0000,,,New Feature,Major,['2.0.0'],['1.8.3'],,,,,,,['DynaBean'],"['DynaBean, DynaClass and implementations']","It would be useful if DynaBean would have support for ""nested dynamic types"", like e.g. EMF and SDO have.

Currently simple types (or the content types of indexed or mapped types) must ""be a Java language primitive (such as int, a simple object (such as a java.lang.String), or a more complex object whose class is defined either by the Java language"".  It turns out it wouldn't be very hard at all (I've tried and will attach a patch) to make some minor changes to relax this, and in addition also allow the following usage:

{noformat}    	DynaProperty[] adrProps = new DynaProperty[]{
            new DynaProperty(""zip"", Long.class)
          };
        BasicDynaClass adrDynaClass = new BasicDynaClass(""Address"", adrProps);

        DynaProperty[] empProps = new DynaProperty[]{
            new DynaProperty(""address"",     java.util.Map.class,  adrDynaClass),
            new DynaProperty(""subordinate"", java.util.List.class, DynaClass.class),
            new DynaProperty(""firstName"",   String.class),
            new DynaProperty(""lastName"",    String.class),
            new DynaProperty(""mainAddress"", adrDynaClass),
            new DynaProperty(""boss"",        DynaClass.class)
          };
        BasicDynaClass empDynaClass = new BasicDynaClass(""Employee"", empProps);
        empDynaClass.getDynaProperty(""boss"").setDynaType(empDynaClass);
        empDynaClass.getDynaProperty(""subordinate"").setDynaType(empDynaClass);
        
        // ---
        
        DynaBean address = adrDynaClass.newInstance();
        address.set(""zip"", new Long(9016));
        DynaBean subordinate = empDynaClass.newInstance();
        subordinate.set(""firstName"", ""Dino"");
        DynaBean boss = empDynaClass.newInstance();
        boss.set(""firstName"", ""Wilma"");
        DynaBean employee = empDynaClass.newInstance();
        employee.set(""firstName"", ""Fred"");
        employee.set(""lastName"", ""Flintstone"");
        employee.set(""mainAddress"", address);
        employee.set(""boss"", boss);
        PropertyUtils.setProperty(employee, ""boss.lastName"", ""Flintstone"");
        employee.set(""address"", new HashMap());
        PropertyUtils.setProperty(employee, ""address(home)"", address);
        employee.set(""subordinate"", new ArrayList());
        ((List)employee.get(""subordinate"")).add(subordinate);
{noformat}",DynaBean should support nested dynamic structures,,,,vorburger,True,,vorburger
commons-beanutils,BEANUTILS-389,2011-03-01T14:56:58.000+0000,,2013-12-05T21:05:32.000+0000,,,New Feature,Minor,['2.0.0'],,,,,,,,['Bean / Property Utils'],"['BeanUtils/BeanUtilsBean, PropertyUtils/PropertyUtilsBean, MethodUtils, ConstructorUtils']","Add a new method to copy a property. copyProperty(Object sourceBean, String sourceProperty, Object targetBean, String targetProperty);
If source and target beans are different type, this method will be useful.","Add a new method to copy a property. copyProperty(Object sourceBean, String sourceProperty, Object targetBean, String targetProperty);",2,,,maheshrpm,True,,maheshrpm
commons-beanutils,BEANUTILS-370,2009-12-06T17:44:39.000+0000,,2013-12-05T21:05:34.000+0000,,,New Feature,Major,['2.0.0'],,,,,,,,['Bean / Property Utils'],"['BeanUtils/BeanUtilsBean, PropertyUtils/PropertyUtilsBean, MethodUtils, ConstructorUtils']","Feature proposal to support querying or filtering collection of beans inside expression:

employee.addresses{city == 'Toulouse'}
employees{subordinate[0] != null}
...

I attach patch as a proof of concept.

Successfuly JUnit tests examples are :

listNestedBeans{intArray[0] == 0}.intArray[0]
listNestedBeans{intArray[0] != 0}
listNestedBeans{floatProperty == 1.1}.floatProperty
listNestedBeans{stringProperty == 'Yop 2'}.stringProperty
arrayNestedBeans{doubleProperty = -1.5}.doubleProperty
arrayNestedBeans{dateProperty != null}
arrayNestedBeans{dateProperty == "" + formatedDate + ""}

Modified classes :
org.apache.commons.beanutils.expression.Resolver
org.apache.commons.beanutils.expression.DefaultResolver
org.apache.commons.beanutils.PropertyUtilsBean
org.apache.commons.beanutils.PropertyUtils

Modified test classes:
org.apache.commons.beanutils.expression.DefaultResolverTestCase
org.apache.commons.beanutils.TestBean
org.apache.commons.beanutils.PropertyUtilsTestCase

Thank you for your feedbacks

Regards,

Pierrick HYMBERT",Querying or filtering collections expression feature,1,,,phymbert,True,,phymbert
commons-beanutils,BEANUTILS-335,2009-02-12T00:33:29.000+0000,,2013-12-05T21:05:46.000+0000,,,New Feature,Major,['2.0.0'],,,,,,,,['Bean / Property Utils'],"['BeanUtils/BeanUtilsBean, PropertyUtils/PropertyUtilsBean, MethodUtils, ConstructorUtils']","The attached patch allows users to easily define what I'm calling a ""fluid"" bean (though there might be a better name for it).

The idea here is to write a bean that doesn't follow the standard JavaBean convention.  Specifically, a ""fluid"" bean's setters return ""this,"" so you can ""chain"" calls to the setters, and the getters and setters don't start with ""get/set"" but are just the name of the property.  For example:

{code}public class Employee extends AbstractFluidBean {
  private String firstName, lastName;
  public String firstName() { return firstName; }
  public Employee firstName(String firstName) {
    this.firstName = firstName;
    return this;
  }
  public String lastName() { return lastName; }
  public Employee lastName(String lastName) {
    this.lastName = lastName;
    return this;
  }
}{code}

Fluid beans have some limitations: you can't use indexed or mapped properties with a fluid bean (because there's no way to disambiguate an indexed getter from a simple setter).  I think that's OK because indexed properties are a bit silly. (Why not just return a List or a Map?)

But I think they have substantial readability advantages.  With a fluid bean, you can write code like this:

{code}
HumanResources.hire(new Employee().firstName(""Dan"").lastName(""Fabulich""));
{code}

For an example of fluid chained setters in the wild, see (for example) Effective Java Second Edition by Joshua Bloch.  In Item 2 ""Consider a builder when faced with many constructor parameters"" Bloch defines a fluid bean with chained setters, so you can use it like this:

{code}
NutritionFacts cocoCola = new NutritionFacts.Builder(240, 8)
  .calories(100).sodium(35).carbohydrate(27).build();
{code}","Provide support for ""fluid"" beans",,,,dfabulich,True,,dfabulich
commons-beanutils,BEANUTILS-325,2008-08-07T15:28:43.000+0000,,2009-08-04T14:58:52.000+0000,,,New Feature,Major,['LATER THAN 1.8.4'],,,,,,,,['Locale BeanUtils / Converters'],['Localized Convert Utils and Converters'],"I have an web application used by users with different locales.

BeanUtils doesn't provide processing conversion according to given locale.

We need to convert numbers, dates according to user locale. For example, some users must have possibility to enter numbers with comma as decimal  delimiter. In opposite English users must have possibility to enter numbers with point as decimal delimiter.",support multiple locales per application,,,,siarhei,True,,siarhei
commons-beanutils,BEANUTILS-322,2008-07-12T16:03:27.000+0000,,2013-12-05T21:05:35.000+0000,,,New Feature,Major,['2.0.0'],['LATER THAN 1.8.4'],,,,,,,['Bean / Property Utils'],"['BeanUtils/BeanUtilsBean, PropertyUtils/PropertyUtilsBean, MethodUtils, ConstructorUtils']","While developing math related applications several times I have had the following use case:

1) I have a collection or array of a given Bean type.
2) I need to calculate some statistic e.g. min, max, stddev or avg over a numeric property of the Bean type above.

The solutions around this are ugly since the application would need writing each time code to extract the property of interest and put it into an e.g. double[] Another tempting antipattern solution for other users is to instead of keeping as state a collection of Beans of a well defined type, and to avoid the continuous conversion they decide to maintain and pass around a bunch of arrays or collections.

I have developed this facility that could be added to e.g. BeanUtils

**************************************************************************************************************************************************

	@SuppressWarnings(""unchecked"")
	public static <T> T[] getProperty(Object[] beansArray, String name, Class<T> target) {
		
		assert beansArray != null : ""'beansArray' must not be null"";
		assert beansArray.length > 0 : ""'beansArray' must not be empty"";
		assert name != null : ""'name' must not be null"";
                assert target != null : ""'target' must not be null"";
		
		T[] result = null;		
		try
		{
			ConvertUtilsBean converter = new ConvertUtilsBean();
			
			final int length = beansArray.length;
			
			result = (T[]) Array.newInstance(target, length);
			
			// we have zero element already ...
			for (int i = 0; i < length; i++) {				
				result[i] = (T) converter.convert(BeanUtils.getProperty(beansArray[i], name), target);
			}
		}
		// covert to RuntimeException and re-throw
		catch (Throwable exception) {
			throw new RuntimeException(exception);
		}
		
		return result;
       }
               
       public static <T> List<T> getProperty(List<Object> beansCollection, String name, Class<T> target) 		
                
                assert beansCollection!= null : ""'beansCollection' must not be null"";
		assert beansCollection.size() > 0 : ""beansCollection' must not be empty"";
		assert name != null : ""'name' must not be null"";
                assert target != null : ""'target' must not be null"";
                 
                return Arrays.asList(getProperty(beansCollection.toArray(), name, target));
       }

**************************************************************************************************************************************************


","BeanUtils to extract a property out of an Array or Collection of Beans i.e. static <T> T[] getProperty(Object[] beansArray, String propertyName, Class<T> targetType)",,,,bravegag,True,,bravegag
commons-beanutils,BEANUTILS-321,2008-07-09T11:16:26.000+0000,2008-08-19T17:19:57.000+0000,2011-02-24T23:28:34.000+0000,,Won't Fix,New Feature,Major,,['1.7.0'],,,,,,,,,"it seems that an isXXX() style property returning an java.lang.Boolean Object is NOT recognized as the getter peoperty -- at least it wont copy it.

Hence, the test case below will fail.
I suggest to handle these props as well, as for a user of BeanUtils this was/is quite surprising to me -- and probably others.

Thx

{code:java}
/**
 * @author tmenzel
 * 
 */
public class BeanUtilsTest extends TestCase {

  private class FooBean {
    Boolean booleanClass;

    boolean booleanPrimitive;

    public Boolean isBooleanClass() {
      return booleanClass;
    }

    public void setBooleanClass(Boolean booleanClass) {
      this.booleanClass = booleanClass;
    }

    public boolean isBooleanPrimitive() {
      return booleanPrimitive;
    }

    public void setBooleanPrimitive(boolean booleanPrimitive) {
      this.booleanPrimitive = booleanPrimitive;
    }

  }

  public void testCopyBooleanProps() throws Exception {

    FooBean a = new FooBean();
    a.setBooleanClass(false);
    a.setBooleanPrimitive(false);

    FooBean b = new FooBean();
    b.setBooleanClass(true);
    b.setBooleanPrimitive(true);

    BeanUtils.copyProperties(a, b);

    assertEquals(a.isBooleanPrimitive(), b.isBooleanPrimitive());
    assertEquals(a.isBooleanClass(), b.isBooleanClass());

  }
}
{code}",[beanutils] wont recognize isXXX() properties returning Boolean Object,,,,elonderin,True,,elonderin
commons-beanutils,BEANUTILS-320,2008-07-07T23:31:13.000+0000,2008-07-09T11:20:28.000+0000,2008-07-09T11:20:28.000+0000,,Duplicate,New Feature,Minor,,,,,,,,,['Bean / Property Utils'],"['BeanUtils/BeanUtilsBean, PropertyUtils/PropertyUtilsBean, MethodUtils, ConstructorUtils']","In my company (TomTom), we have internally developed a replacement of the equals, hashcode and toString methods.

The base idea is to use the annotation @BusinessObject at the class level and @BusinessField at field level. Then what developers normally do is to override the three methods delegating to the tomtom.BeanUtils methods:

      @Override
        public boolean equals(Object obj) {
            return BeanUtils.equals(this, obj);
        }

        @Override
        public int hashCode() {
            return BeanUtils.hashCode(this);
        }

        @Override
        public String toString() {
            return BeanUtils.toString(this);
        }

And i.e. the method signature of equals is:

    /**
     * Compare two @BusinessObject beans comparing only the {@link BusinessField}
     * annotated fields.
     * 
     * @param firstBean First bean to compare.
     * @param secondBean Second bean to compare.
     * @return The equals result.
     * @throws IllegalArgumentException If one of the beans compared is not an
     *         instance of a {@link BusinessObject} annotated class.
     */
    public static boolean equals(Object firstBean, Object secondBean);

In the last versions of EqualsBuilder now there is the new method reflectionEquals... but there is no a way to specify what to include in the comparison. With our two annotations we are able to let developers exactly define what need to be included in a Business comparison, as i.e. normally required by persistence framework like hibernate. 

The current implementation can also handle more complex case, comparing correctly totally different kind of objects.

For example if all my business logic cares only about the color, I can define:

@BusinessObject
public class Cat{

}

public class ColouredCat extends Cat{

@BusinessField
private String color;

getter/setter
}


@BusinessObject
public class SunSet{

@BusinessField
private String color=""red"";

getter/setter
}

and then compare any instance of ColouredCat with a Sunset instance, finding out that the redColouredCat is (for my business logic) equal to a default instance of a Sunset. And also more tricky cases are handled (different BusinessFields, no BusinessObject annotation and so on).

We intensively use Hibernate and the utility demonstrated to work fine with CGLIB proxies 


We always thought about the possibility to create a new Open Source project but then it was decided it would be better adding the feature to an already well know open source project...

If you are interested I can send you more details. How can we (me more one other developer) eventually became committers?


Thanks in advance,
Vicio.


P.s.: an utility method to automatically populate the BusinessFields of a BusinessObject is also implemented.","Implement equals, hashCode and toString replacement",1,,,vincenzo.vitale,True,,vincenzo.vitale
commons-beanutils,BEANUTILS-315,2008-05-20T15:56:36.000+0000,,2013-12-05T21:05:37.000+0000,,,New Feature,Minor,['2.0.0'],,,,,,,,['ConvertUtils & Converters'],['Convert Utils and Converters (non-localized)'],"I have developed a converter implementation that can convert XML Schema 'dateTime' (xs:dateTime) types into java.util.Date and would like to donate it to the project.

This Converter is being used with commons-digester successfully in one of my projects, and a series of posts on the commons-user list led me to believe that commons-beanutils was an appropriate place to file an enhancement request.

A proposed implementation is forthcoming.",Converter for XML Schema 'dateTime' type,,,,chris@christopherschultz.net,True,,chris@christopherschultz.net
commons-beanutils,BEANUTILS-313,2008-05-05T17:20:10.000+0000,,2013-12-05T21:05:33.000+0000,,,New Feature,Minor,['2.0.0'],,,,,,,,['Bean-Collections'],['Optional Bean-Collections package'],"For your consideration, attached is BeanComparatorChain, a Comparator implementation that maintains an o.a.c.collections.comparators.ComparatorChain of o.a.c.beanutils.BeanComparators.","[beanutils] BeanComparatorChain, yet another composite comparator based on bean properties",,,,heuermh@acm.org,True,,heuermh@acm.org
commons-beanutils,BEANUTILS-304,2008-02-07T16:23:44.000+0000,,2013-12-05T21:05:41.000+0000,,,New Feature,Major,['2.0.0'],['LATER THAN 1.8.4'],,,864000,864000,864000,,"['Bean / Property Utils', 'Bean-Collections', 'ConvertUtils & Converters', 'DynaBean']","['BeanUtils/BeanUtilsBean, PropertyUtils/PropertyUtilsBean, MethodUtils, ConstructorUtils', 'Optional Bean-Collections package', 'Convert Utils and Converters (non-localized)', 'DynaBean, DynaClass and implementations']","It would be great to have a way to compare two beans and capture the diff somehow.  This is just my initial brainstorm on the idea:

- The BeanDiff class encapsulates differences between two beans -of the same type-, the source bean and the target bean.
- A BeanDiff instance represents what property assignments would be necessary to turn the target bean into the source bean.
- A BeanDiff can be applied to a target bean, performing the necessary property assignments to make its property values exactly match the source bean.
- Two BeanDiff instances can be merged into a single BeanDiff instance, allowing multiple source beans to be merged into one target bean.
- If a conflict would occur during a merge (from a property being assigned two different values), a BeanDiffConflictException is thrown.
- The BeanDiffConflictException contains an array of BeanDiffConflict objects.
- Each BeanDiffConflict instance represents a single property and the two conflicting values that were to be assigned.

- All these actions would work on DynaBeans as well.
- +Converters would be able to account for type differences in source and target bean properties.+

An example of how this could be used is when dealing with ORM and optimistic locking.  Knowing exactly which properties have been modified would allow concurrent modification of a bean (record) without fear of reasserting the original values and destroying someone else's changes.","BeanDiff - encapsulate, apply, and merge bean differences",3,3,,malfunction84,True,,malfunction84
commons-beanutils,BEANUTILS-286,2007-06-30T03:02:20.000+0000,2007-06-30T03:06:13.000+0000,2008-03-25T08:08:06.000+0000,,Fixed,New Feature,Minor,['1.8.0'],['1.7.0'],,,,,,,['ConvertUtils & Converters'],['Convert Utils and Converters (non-localized)'],"Provide a ""facade"" Converter implementation that delegates to a specified Converter, but hides any public API other than that defined in the Converter interface.",New Facade converter implementation - hide non-Converter public APIs,,,,niallp,True,niallp,niallp
commons-beanutils,BEANUTILS-285,2007-06-29T23:51:36.000+0000,2007-07-02T02:04:47.000+0000,2008-03-25T08:08:06.000+0000,,Fixed,New Feature,Major,['1.8.0'],['1.8.0'],,,,,,,['ConvertUtils & Converters'],['Convert Utils and Converters (non-localized)'],"The Conversion improvements associated with BEANUTILS-258 potentially create compatibility issues - this was highlighted by Betwixt's tests failing recently in the gump run - see http://tinyurl.com/2mxbv8 for more details.

Quite a bit of effort has been put into making the new conversion facilities as painless as possible for existing users. However it is not fully backwards compatible in terms of behaviour (stiil binary compatible). Need to give this some consideration before a BeanUtils release - at the moment there are two options on the table (more welcome!):

1) The compatibility as it stands is good enough (covers most cases) - so do nothing
2) Provide a ""compatibility option"" - so that users can choose either the new behaviour or behaviour compatible with BeanUtils 1.7.0. This probably involves quite a bit of work - adding back the old behaviour alongside the new",Consider options for BeanUtils compatibility in light of Conversion improvements,,,,niallp,True,niallp,niallp
commons-beanutils,BEANUTILS-279,2007-05-19T22:15:45.000+0000,,2013-12-05T21:05:27.000+0000,,,New Feature,Minor,['2.0.0'],['1.7.0'],,,,,,,['ConvertUtils & Converters'],['Convert Utils and Converters (non-localized)'],"Currently only one Converter may be registered per destination class.  It may be prudent for an application to have the ability to use multiple ""named"" Converter instances to convert values to a destination type.  By adding a second-level Map keyed by Converter name, this can be accomplished without affecting existing behavior.  A ""default"" name is used for the default Converter which will allow for emulation of the previous single Converter implementation.

Patch files for both BeanUtils 1.7.0 and the current Head revision of the affected files has been provided.",Allowing Multiple Converters Per Destination Class,,,,bhandy,True,,bhandy
commons-beanutils,BEANUTILS-269,2007-02-01T03:41:14.000+0000,2007-05-20T15:53:27.000+0000,2008-07-09T11:16:26.000+0000,,Invalid,New Feature,Major,,['1.7.0'],,,,,,,,,"I have a bean class ""Cache"" which is generated from jaxb , which has the isEnable method, i can not use the xpath expression ""/cache/enable""to retrive the result, is that means BeanUtils only support setXXX getXXX? not support isXXX for boolean values?",Boolean Support,,,,maomaode,True,,maomaode
commons-beanutils,BEANUTILS-265,2006-11-23T09:28:13.000+0000,2008-08-20T02:11:33.000+0000,2009-07-26T07:04:33.000+0000,,Fixed,New Feature,Major,['1.8.0'],['1.7.0'],,,,,,,['Bean / Property Utils'],"['BeanUtils/BeanUtilsBean, PropertyUtils/PropertyUtilsBean, MethodUtils, ConstructorUtils']","Currently BeanUtils doesn't provide the possibility to access getters and setters in classes who are package-scoped!
---------------8<---------------
class HiddenBean {
	private String a;
	
	protected HiddenBean() {
		
	}
	
	public void setA(String a) {
		this.a = a;
	}
	
	public String getA() {
		return this.a;
	}
}

public class PublicBean extends HiddenBean {

}
---------------8<---------------",Allow access to non public class's public methods from a public sub-classes,,,,tomson,True,niallp,tomson
commons-beanutils,BEANUTILS-261,2006-11-09T09:20:51.000+0000,2006-11-23T05:13:11.000+0000,2008-03-25T08:08:06.000+0000,,Won't Fix,New Feature,Major,,,,,,,,,['Bean / Property Utils'],"['BeanUtils/BeanUtilsBean, PropertyUtils/PropertyUtilsBean, MethodUtils, ConstructorUtils']","Can provide a method in PropertyUtils to get nested properties without throwing a NestedNullException when any of the no-end property is null?
example:
/* code begin */
Employee employee=...
emplyee.setAddress(null);
String street=(String) PropertyUtils.getNestedProperty(employee, ""address.street"");// ####
/* code end */

The statement #### will throws a NestedNullException, but return null will make it easier to write code in some situation. So can please provide a method only return null?",Return null but not throw NestedNullException if the nested property is null,,,,justfly,True,,justfly
commons-beanutils,BEANUTILS-259,2006-11-08T04:07:11.000+0000,2007-05-20T08:53:19.000+0000,2013-05-02T02:29:08.000+0000,,Fixed,New Feature,Minor,['1.8.0'],['1.7.0'],,,,,,,['Expression Syntax'],['Expression Syntax'],"There are a number of outstanding bugs against the BeanUtils expression syntax with people wanting BeanUtils to support different variations. There is also a duplication of the ""expression evaluation"" code in various methods which can't be tested in isolation and is difficult to maintain as changes have to be applied uniformly to various places.

The main places where the code is duplicated:
   PropertyUtilsBean
              - getNestedProperty
              - setNestedProperty
              - getPropertyDescriptor
   BeanUtilsBean
              - copyProperty
              - setProperty

LocaleBeanUtils has also implemented an alternative mechanism - using a Descriptor object to resolve references. BeanUtils and PropertyUtils also work in slightly different ways. There are also other methods (e.g. PropertyUtilsBean's getIndexedProperty() method) which also have related code.

I propose to add a new ""expression resolver"" interface, which would be a singleton and everywhere would delegate to to resolve property expressions. This will allow easy testing as it can be tested in isolation and provide a uniform mechanism accross BeanUtils. It will also allow alternative syntax to be implemented if the resolver implementation can be configured.",Plugable Property Name Expression Resolver,,,,niallp,True,niallp,niallp
commons-beanutils,BEANUTILS-255,2006-11-02T08:16:54.000+0000,2007-05-20T08:19:03.000+0000,2011-01-26T22:22:16.000+0000,,Fixed,New Feature,Major,['1.8.0'],['1.7.0'],,,,,,,['ConvertUtils & Converters'],['Convert Utils and Converters (non-localized)'],It's impossible to convert a String to a java.util.Date (see commons-user mail thread 'Digester and using of java.util.Date').,Date and Calendar Converter implementations,,,,bayard,True,niallp,bayard
commons-beanutils,BEANUTILS-253,2006-10-31T01:17:24.000+0000,2006-11-02T17:40:11.000+0000,2008-03-25T08:08:05.000+0000,,Duplicate,New Feature,Major,,,,,,,,,,,As part of BEANUTILS-10 I suggested being able to register converters per bean property instead of per Java type. Here's a JIRA issue to suggest it.,Converters per property rather than per type,,,,bayard,True,,bayard
commons-beanutils,BEANUTILS-242,2006-06-26T22:35:20.000+0000,2007-05-20T08:17:48.000+0000,2008-03-25T08:08:05.000+0000,,Fixed,New Feature,Major,['1.8.0'],['1.7.0'],,,,,,,['ConvertUtils & Converters'],['Convert Utils and Converters (non-localized)'],"When using a method such as BeanUtils.copyProperty(), it would be good if it could automatically handle conversion between array types, rather than having to explicitly write an array converter for each target type.

For example, if I pass a String[] to BeanUtils to copy on to a property of type Integer[], then it would recognise the presense of an array, and then use the registered converters for the arrays' component types to decide how to map them together.

This would remove the need to essentially duplicate converters already written, just to add array support.",Add general array type conversion,,,,skaffman,True,niallp,skaffman
commons-beanutils,BEANUTILS-185,2006-05-07T11:22:23.000+0000,2007-05-24T20:42:47.000+0000,2008-03-25T08:08:03.000+0000,,Fixed,New Feature,Minor,['1.8.0'],['1.7.0'],,,,,,,['DynaBean'],"['DynaBean, DynaClass and implementations']","Hi,

I've done some modifications to the beanutils package to better support the use 
of DynaBeans with the JSTL tags. Some of the changes are discussed in this 
thread of the commons-user mailing list:

http://marc.theaimsgroup.com/?l=jakarta-commons-user&m=114669123403779&w=2

I attach the diff file that comprises changes to the RowSetDynaClass.java file 
and build.xml file (since I added a TestCase.) Note: Please try to filter 
carefully the diffs in the build.xml file since they include some local 
settings I have for compilation on my machine. :-(

Together with the diff file, I attach the new java files added to the package.

Regards,
Gabriel Belingueres
gaby@ieee.org",Map decorator for DynaBeans to allow BeanUtils to operate with technologies such as JSTL,1,,,gaby@ieee.org,True,niallp,gaby@ieee.org
commons-beanutils,BEANUTILS-244,2006-03-17T16:12:19.000+0000,,2013-12-05T21:05:40.000+0000,,,New Feature,Minor,['2.0.0'],,,,,,,,['Bean-Collections'],['Optional Bean-Collections package'],"Hi,

I recently needed to be able to sort a list of beans on many properties. So I
thought I would try and pick one of the collections comparators.

I had to to the sorting close to the sql way : be able to sort on n properties,
some ascending, some descending.

I haven't found what I'm looking for (:p), but I found some comparators in the
commons I used to do this : I used BeanComparator, NullComparator and
ComparatorChain to create a class : MultiPropertyBeanComparator.

Is there already something in one of the commons package that could be used
instead of it ? 

If not, I'd be glad to contribute the small piece of code if wanted. It has
dependencies against commons-beanutils (BeanComparator, which is moving from one
package to another at the moment, no ?) and commons-lang
(StringUtils.isBlank()). I think some things might not satisfactory for
everybody, but hey, could still be improved without problems, that's not big
work :p.

As adviced on the user mailing list, I'm posting the code on this bugzilla, so
as maybe one of the coder could take a look at it.

Here it is :

=======================================
import java.io.Serializable;
import java.util.ArrayList;
import java.util.Comparator;
import java.util.List;

import org.apache.commons.beanutils.BeanComparator;
import org.apache.commons.collections.ComparatorUtils;
import org.apache.commons.collections.comparators.NullComparator;
import org.apache.commons.collections.comparators.ReverseComparator;
import org.apache.commons.lang.StringUtils;

/**
 * This comparator lets you sort a list using a list of properties. You can
specify ascending or
 * descending order on these properties.
 * <p>
 * For example, if you want to sort with natural order a list of beans with
properties firstname,
 * nickname and address, sorting the firstname descending, you can do it this way:
 * </p>
 * <code>List l = ...</code>
 * <code>...</code>
 * <code>MultiPropertyBeanComparator multiPropBeanComp = new
MultiPropertyBeanComparator();</code>
 * <code>multiPropBeanComp.append(""firstname"",
true).append(""nickname"").append(""address"");</code>
 * <code>Collections.sort(l,multiPropBeanComp);</code>
 * 
 * @author Baptiste MATHUS
 */
public class MultiPropertyBeanComparator implements Comparator, Serializable
{
	private static final long serialVersionUID = -1431852774261001458L;

	private List comparatorList = new ArrayList();

	/**
	 * Use this method to add a comparator to the list.
	 * 
	 * @param property
	 *            the property on which to apply the given comparator.
	 * @param comparator
	 *            the comparator to be added. If null, natural order will be used.
	 * @param reverse
	 *            <p>
	 *            must be true if the given comparator must be used in opposite
order to sort. For
	 *            example, if the comparator is designed to sort in ascending
order, put this
	 *            parameter to <code>true</code> if you want descending order.
	 *            </p>
	 *            <p>
	 *            If the comparator is null, then the reversed natural order is used.
	 *            </p>
	 */
	public MultiPropertyBeanComparator append(String property, Comparator comparator,
			boolean reverse)
	{
		if (StringUtils.isBlank(property))
		{
			throw new IllegalArgumentException(""The given property is blank"");
		}

		// If the comparator is null, then compare only on the given property
		// with a natural sort.
		if (comparator == null)
		{
			comparator = new BeanComparator(property, new NullComparator(false));
		}
		// Else : compare on the property, but with given comparator.
		else
		{
			comparator = new BeanComparator(property, comparator);
		}
		// Here, the comparator cannot be null anymore, so reverse it if
		// necessary.
		if (reverse)
		{
			comparator = new ReverseComparator(comparator);
		}

		comparatorList.add(comparator);

		return this;
	}

	public MultiPropertyBeanComparator append(String property, Comparator c)
	{
		return append(property, c, false);
	}

	public MultiPropertyBeanComparator append(String property)
	{
		return append(property, null, false);
	}

	public MultiPropertyBeanComparator append(String property, boolean reverse)
	{
		return append(property, null, reverse);
	}

	/**
	 * Use this method to clear the
	 */
	public void clear()
	{
		comparatorList.clear();
	}

	/**
	 * Considered to be equal when all properties and comparators equal.
	 * 
	 * @see java.lang.Object#equals(java.lang.Object)
	 * @overrides
	 */
	public boolean equals(Object obj)
	{
		MultiPropertyBeanComparator comp = (MultiPropertyBeanComparator) obj;

		if (this.comparatorList.size() != comp.comparatorList.size())
		{
			return false;
		}

		for (int i = 0; i < comparatorList.size(); ++i)
		{
			if (!this.comparatorList.get(i).equals(comp.comparatorList.get(i)))
			{
				return false;
			}
		}
		return true;
	}

	/**
	 * @see Comparator#compare(T, T)
	 * @overrides
	 */
	public int compare(Object arg0, Object arg1)
	{
		return getComparator().compare(arg0, arg1);
	}

	private Comparator getComparator()
	{
		return ComparatorUtils.chainedComparator(comparatorList);
	}
}
=============",[collections] Add a multi property beancomparator,2,,,mathus.b@mipih.fr,True,,mathus.b@mipih.fr
commons-cli,CLI-293,2018-12-30T22:14:44.000+0000,,2018-12-31T02:01:37.000+0000,,,New Feature,Minor,,['1.5'],,,,,,,"['CLI-1.x', 'Options definition']","['', '']","By associating a priority with Option instances, the task of parsing each option in a prioritized manner is greatly simplified. This is the first time I have ever submitted a pull request to a project I did not create, but I did my honest best to keep aligned with the standards of this project. I submitted a pull request on github. It failed for openjdk6 on the Travis-CI test, but the logs lead me to believe that it due to a problem with the test and not my patch. The meat of the patch is only around 65 LOC so it is very minimally invasive. The pull request can be found [here|https://github.com/apache/commons-cli/pull/28]. Any feedback is much appreciated! ",Associate parse priority with Option flags,1,,"['newbie', 'pull-request-available', 'usability']",paroxayte,True,,paroxayte
commons-cli,CLI-290,2018-11-03T18:08:57.000+0000,,2018-11-03T18:08:57.000+0000,,,New Feature,Minor,,,,,,,,,['Validation'],['Issues related to the validation of the option arguments'],"I want to ad check if Options which marked as Required are available or not 

My use case is to have a method like below in CommandLine class
{code:java}
boolean hasAllRequiredOptions(){code}
 Check below given expected code, (code kept uncompiled purposefully)
{code:java}
import org.apache.commons.cli.*;

public class CLITester {
    public static void main(String[] args) throws ParseException {

        Options options = new Options();
        options.addRequiredOption(""p"", ""print"", false, ""Send print request to printer."")
                .addRequiredOption(""g"", ""gui"", false, ""Show GUI Application"")
                .addOption(""n"", ""nothing"", true, ""No. of copies to print"");

        CommandLineParser parser = new DefaultParser();
        CommandLine cmd = parser.parse(options, args);
        if (!cmd.hasAllRequiredOptions()) { // I want to have method like this which will check if all required options are available

            HelpFormatter formatter = new HelpFormatter();
            formatter.printHelp(""CLITester"", options);
        }
    }
}
{code}",check if all options are provided using CommandLine,1,,,mahadevkv,True,,mahadevkv
commons-cli,CLI-269,2017-02-22T20:05:30.000+0000,2017-03-09T11:17:32.000+0000,2017-03-13T14:12:43.000+0000,,Fixed,New Feature,Major,['1.4'],['1.3.1'],,,,,,,['Build'],[''],"For Maven there are several places where one can define the commandline arguments. These should be merged to 1 new commandline, but we need to have control over the order in which these are merged.
Best solution seems to be to introduce a CommandLineBuilder.",Introduce CommandLine.Builder,2,,,rfscholte,True,,rfscholte
commons-cli,CLI-267,2016-07-27T18:33:59.000+0000,2016-07-27T18:40:47.000+0000,2017-03-13T14:12:43.000+0000,,Fixed,New Feature,Major,['1.4'],,,,,,,,['Options definition'],[''],"Placeholder issue for https://github.com/apache/commons-cli/pull/7

{quote}
This pull request adds an addRequiredOption method, which creates an Option with setRequired(true).

This is really useful, given the amount of projects I saw doing things like:

{code}
Options options = new Options();
options.addOption( ""a"", ""all"", false, ""do not hide entries starting with ."" );
options.addOption( ""A"", ""almost-all"", false, ""do not list implied . and .."" );
options.addOption( ""b"", ""escape"", false, ""print octal escapes for nongraphic "" + ""characters"" );
// ... more addOptions like these and then
Option stuff = new Option( ""c"", ""stuff"", true, ""do not list implied . and .."" );
stuff.setRequired(true);
options.addOption( stuff );
// And many more required options like this one
{code}
This pull request proposes an auxiliary method to create a required option, so that these options could just be added with.
{code}
options.addRequiredOption( ""c"", ""stuff"", true, ""do not list implied . and .."" );
{code}
{quote}",Add an addRequiredOption method to Options,1,,['github'],britter,True,britter,britter
commons-cli,CLI-228,2012-10-31T09:27:47.000+0000,,2017-03-09T12:35:28.000+0000,,,New Feature,Minor,,,,,,,,,['CLI-1.x'],[''],"It would be great to be able to generate the bash completion script from the Options declared for Commons CLI.

Some documentation on this topic:
http://www.debian-administration.org/article/An_introduction_to_bash_completion_part_2
http://www.linuxjournal.com/content/more-using-bash-complete-command
",Bash auto completion,1,,,ebourg,True,,ebourg
commons-cli,CLI-219,2011-05-27T12:53:51.000+0000,,2017-03-09T12:35:29.000+0000,,,New Feature,Major,,['1.2'],,,,,,,['CLI-1.x'],[''],"I've explained the issue in that thread:
  http://www.mail-archive.com/user@commons.apache.org/msg06483.html

As hinted there, a solution might be to allow that the (single) argument of an ""Option"" (cf. ""hasArg()"") be split according to a user-defined regexp pattern. If given a split pattern, the code would convert a single-arg ""Option"" into a multiple-args one (where the arguments are the result of splitting the single-arg string with the pattern).
",Allow to specify options with a single argument that will be split into multiple arguments,1,,['features'],erans,True,,erans
commons-cli,CLI-218,2011-04-30T15:25:56.000+0000,2013-02-09T14:22:38.000+0000,2015-05-09T18:43:36.000+0000,,Fixed,New Feature,Major,['1.3'],['1.2'],,,,,,,['CLI-1.x'],[''],"Often, you can invoke command with the same option multiple time. For example
  java -cp test1 -cp test2

even though the option only accepts one value.
I believe, that the majority of programs then use the last value (test2 in this case).
However, getOptionValue() return the first value for some weired reason.

I believe, you should add a function called getLastOptionValue() (in class CommandLine) and getLastValue() (in class Option) to allow easy access to the last value.",Add function similar to getOptionValue() that returns last value,,,,sven.koehler,True,,sven.koehler
commons-cli,CLI-196,2010-03-15T21:12:39.000+0000,,2017-03-09T12:35:29.000+0000,,,New Feature,Major,,,,,,,,,['CLI-1.x'],[''],"This is a proposal for an alternative way of specifying command line options, using annotations.  It's implementation wraps Commons CLI code and uses reflection.

From a user perspective, the following is a example of a mythical program that takes three arguments:

-utc, reports the date in terms of UTC.
-offset <integer>, adds an offset to the reference date.
-reference <amount>, the reference amount (whatever that is).

The Java code for the options is as follows:

{code}
public class XyzOptions {

  private boolean utc;
  private int offset;
  private BigDecimal reference;

  @Option (name=""utc"", description=""Reports the date in terms of UTC"")
  void setUTC (boolean utc) {
    this.utc = utc;
  }

  @Option (name=""offset"", description=""Adds an offset to the reference date"")
  void setOffset (int offset) {
    this.offset = offset;
  }

  @Option (name=""reference"", required=true, description=""The reference amount"")
  void setReferenceAmount (BigDecimal amount) {
    this.reference = amount;
  }

  // get methods omitted
}
{code}

@Option is an annotation that provides CLI option details.  The @Option annotates set methods of the options class.

The Java code that uses these options is as follows:

{code}
public static void main (String[] args) {
  XyzOptions options = CommandLine.parse(args, XyzOptions.class, ""dateCommand"");
  
  // the program options can then be accessed using get methods or field access on the options object.
}
{code}

The ""CommandLine"" class is the class that builds the CLI Options instance using the @Option annotations and reflection.  

The @Option annotation allows the following to be specified:
* name.  The short name of the option.
* longForm.  The long form of the option.
* description.  The description of the option.  This is the description that appears in the help text.
* required.  true if this option is required.  This defaults to false, meaning the option is optional.
* argOptional.  true if the option argument is optional.  This defaults to false, meaning the option argument is mandatory.

The remaining information is obtained by reflection on the set method parameter.
* If the set method parameter is boolean or Boolean, the option is assumed to take no argument.  It is a simple switch.
* If the set method parameter takes any other type, the option is assumed to take an argument of that type.
** If the parameter type is String, the option argument is used as-is.
** If the parameter type is a primitive type (or the class equivalent), the option argument is converted to that type and passed to the set method.
** If the parameter type is something else (such as BigDecimal), the command line parser looks for a class constructor that takes a String argument.  If this is found, and instance of the object is created an passed to the set method.  Any class that has a constructor that takes a single String argument can be used.

Option arguments can be validated in the set methods.  For example, if a File needs to be a directory, the set method could be as follows:

{code}
  @Option (name=""dir"", description=""The output directory"")
  void setOutputDir (File file) {
    if (!file.isDirectory()) {
      throw new IllegalArgumentException (""'dir' option must specify a directory"");
    }
    this.file = file;
}
{code}

The code for what is described here is available if anyone wants it.

This is the idea.  There are some issues to resolve, but the first question is: does anyone think it worth pursuing this?
",Annotation based option specification,2,1,,kevin-au,True,,kevin-au
commons-cli,CLI-190,2009-11-06T15:07:27.000+0000,,2017-03-09T12:35:27.000+0000,,,New Feature,Minor,,['1.2'],,,,,,,['CLI-1.x'],[''],"Currently, CLI 1.x only supports classic options parsing with ""-o"" or ""--o"" notation.
Support for new style command syntax like in Ant or maven (maven -o --o install...) is sorely missing in CLI 1.x, but available in the not-released 2.x version.
I postponed rewriting my options parsing code to wait for CLI2, but that was sadly cancelled, so I hope for a backport to 1.x.
As it is now, CLI could not be used for Apache's own projects...,-)",Backport support for Commands from CLI-2 to 1.x,,1,,grexe,True,ebourg,grexe
commons-cli,CLI-168,2008-09-20T17:35:47.000+0000,,2009-05-26T05:06:15.000+0000,,,New Feature,Major,['2.0'],,,,,,,,['CLI-2.x'],[''],"It would be good to have at least a -BETA Releae of 2.0, so it could be used in Maven based builds. Otherwise it's complicated to use the commons-cli-2.0-DEV.
It should be distributed via an Maven Repository to have access to the release.",Maven Repository Distribution as at least as -BETA may be as SNAPSHOT,,,,khmarbaise,True,,khmarbaise
commons-cli,CLI-166,2008-07-28T15:31:22.000+0000,2009-05-28T16:23:16.000+0000,2009-05-28T16:23:16.000+0000,,Won't Fix,New Feature,Major,['1.3'],,,,,,,,['Help formatter'],[''],"HelpFormatter could be improved by adjusting automatically its width to the width of the terminal. The width of the terminal can be retrieved by the stty command on unix systems, on Windows it requires a native call. The JLine project on SourceForge already implements this logic.

http://jline.sourceforge.net

I suggest to add a setAutoWidth(boolean) method in HelpFormatter that will try to get the terminal width. If it fails it'll fall back to the width provided by getWidth().",Terminal width detection,1,,,ebourg,True,jimjag,ebourg
commons-cli,CLI-90,2003-10-23T16:29:53.000+0000,,2013-05-02T02:28:35.000+0000,,,New Feature,Minor,['2.1'],,,,,,,,['Options definition'],[''],"It would be useful to be able to generate a CLI from an xml file.  This would 
generate the necessary options and provide a CommandLine wrapper for type safe 
interrogation.  The xml might also be used to produce more in depth help than 
just the online version.",Defining command line options from an XML file,,,,roxspring,True,,roxspring
commons-cli,CLI-113,2002-07-08T10:21:55.000+0000,,2013-05-02T02:28:35.000+0000,,,New Feature,Minor,['2.1'],,,,,,,,,,"I'd like to suggest an enhancement to CLI - 

1) have CLI generate a schema of the command line options it expects
2) write some code to read the schema, and generate an ant task 

Any project using CLI would then be able to easily produce an ant task.",Generate an Ant task automatically from CLI,1,1,,pklein@lehman.com,True,,pklein@lehman.com
cocoon,COCOON-2338,2013-08-05T18:23:56.000+0000,,2013-11-30T14:39:29.000+0000,,,New Feature,Major,,,,,,,,,"['* Cocoon Core', '- Components: Sitemap']","['', '']","How difficult would it be to port the TikaGenerator classes from Cocoon3 to Cocoon 2.1.13?

Apache Tika is a powerful tool for extracting content and metadata from a wide variety of file formats.

See: https://issues.apache.org/jira/browse/COCOON3-128

Unfortunately, I don't have a clue how to do this: is there someone who would like to take the lead?

Classes are: TikaGenerator.java, and for testing TikaGeneratorTestCase.

The former's path is:

cocoon3/trunk/cocoon-optional/src/main/java/org/apache/cocoon/optional/pipeline/components/sax/tika/TikaGenerator.java

Thanks!

Dan",Port the Cocoon3 TikaGenerator to Cocoon 2.1.13,2,,,is49,True,,is49
cocoon,COCOON-2313,2011-08-09T14:41:24.000+0000,,2012-11-22T23:47:14.000+0000,,,New Feature,Major,,['2.1.11'],,,,,,,['* Cocoon Core'],[''],"Currently, the class name ""org.apache.cocoon.Cocoon"" is hardcoded in the CocoonServlet
Moreover, many of the Cocoon fields are private, making this class quite hard to extend without copying all the code",[2.1] Allow to subclass the Cocoon class,1,,,cedric,True,,cedric
cocoon,COCOON-2311,2011-07-16T13:28:02.000+0000,,2011-07-16T13:28:02.000+0000,,,New Feature,Major,,['2.1.11'],,,,,,,['* Cocoon Core'],[''],Why doesn't the XML serializer have a parameter for defining XML namespace. If you use the serializer to creeate XHTML you have to add the namespace declaration in e.g. Transformation step,Namespace parameter for XML serializer,1,,,awu5ima,True,,awu5ima
cocoon,COCOON-2288,2010-04-06T10:28:00.000+0000,2012-12-05T03:35:15.000+0000,2012-12-05T03:35:15.000+0000,,Fixed,New Feature,Major,['2.1.12'],,,,,,,,['* Cocoon Core'],[''],Attached are two classes allowing to use SLF4J as logging facade inside Cocoon.,Allow usage of SLF4J for traces,3,,,cedric,True,,cedric
cocoon,COCOON-2269,2009-09-22T15:42:58.000+0000,,2012-11-22T23:58:50.000+0000,,,New Feature,Major,,['2.1.12'],,,,,,,['* Cocoon Core'],[''],"JMagick is an open source Java interface of ImageMagick. It is implemented in the form of Java Native Interface (JNI) into the ImageMagick API.

JMagick does not attempt to make the ImageMagick API object-oriented. It is merely a thin interface layer into the ImageMagick API.

JMagick currently only implements a subset of ImageMagick APIs.

Till now JMagick has a LGPL (Lesser GNU Public License) license however they are willing to change the license to the apache license.

ImageMagick is free software delivered as a ready-to-run binary distribution or as source code that you may freely use, copy, modify, and distribute. Its license is compatible with the GPL. It runs on all major operating systems.",JMagick ImageReader based ImageMagick,,,,gaurav.kalia,True,,gaurav.kalia
cocoon,COCOON-2211,2008-06-14T11:38:04.000+0000,2009-04-27T21:34:52.000+0000,2013-01-20T16:42:47.000+0000,,Fixed,New Feature,Major,['2.2.1'],"['2.2', '2.2.1']",,,,,,,['Blocks: Templating'],['CTemplate (former JXTemplate)'],JXTemplate does n't support a jx:element instruction. This patch provides this support.,Support for jx:element,,,,kambha,True,grek,kambha
cocoon,COCOON-2205,2008-05-02T10:35:37.000+0000,2008-05-05T20:31:08.000+0000,2008-05-05T20:31:08.000+0000,,Fixed,New Feature,Major,,,,,,,,,['Corona (experimental)'],['Corona is an effort to implement completely reworked Cocoon core by following basic contracts.\n\nDevelopment happens in whiteboard section of repository and should be considered as experimental at the moment.'],"Provide a caching mechanism that performs cache updates asynchronously, if the cached value is no longer valid, and uses the previously cached result until the updated result is available.",Provide an asynchronous caching mechanism.,,,,steven.dolg,True,reinhard@apache.org,steven.dolg
cocoon,COCOON-2200,2008-04-18T16:38:15.000+0000,2008-04-20T21:15:10.000+0000,2008-04-20T21:15:10.000+0000,,Fixed,New Feature,Major,,,,,,,,,['Corona (experimental)'],['Corona is an effort to implement completely reworked Cocoon core by following basic contracts.\n\nDevelopment happens in whiteboard section of repository and should be considered as experimental at the moment.'],Support setting the mime-type for the FileReaderComponent from the sitemap.,Enable explicit mime-type setting for FileReaderComponent in the sitemap.,,,,steven.dolg,True,reinhard@apache.org,steven.dolg
cocoon,COCOON-2190,2008-03-30T12:22:09.000+0000,2008-04-01T09:28:42.000+0000,2008-04-01T20:20:28.000+0000,,Fixed,New Feature,Major,,,,,,,,,['Corona (experimental)'],['Corona is an effort to implement completely reworked Cocoon core by following basic contracts.\n\nDevelopment happens in whiteboard section of repository and should be considered as experimental at the moment.'],,Add caching  to corona,,,,steven.dolg,True,reinhard@apache.org,steven.dolg
cocoon,COCOON-2180,2008-03-21T13:33:04.000+0000,2008-03-21T17:18:46.000+0000,2008-03-21T17:18:46.000+0000,,Fixed,New Feature,Major,,,,,,,,,['* Cocoon Core'],[''],,Corona - A proposal for a reimplementation,,,,steven.dolg,True,reinhard@apache.org,steven.dolg
cocoon,COCOON-2154,2007-12-26T11:11:16.000+0000,2008-03-11T08:11:50.000+0000,2008-03-11T08:11:50.000+0000,,Fixed,New Feature,Major,,,,,,,,,['- Servlet service framework'],[''],"Using the servlet-protocol you can only define relative URIs which means that those URIs are only valid if they are resolved in the context of a particular servlet-service because they refer to the defines connections.

If you need globally resolveable URIs, there needs to be a way to define globally unique servlet:/ URIs. ",Servlet:/ protocol: Support absolute URIs,,,,reinhard@apache.org,True,reinhard@apache.org,reinhard@apache.org
cocoon,COCOON-2145,2007-11-09T13:37:45.000+0000,,2007-11-25T07:19:25.000+0000,,,New Feature,Major,,,,,,,,,['- Servlet service framework'],[''],"I needed an input module which returns me the full file path location of the base dir/resource of a block.

The implementation makes use of the blockcontext: protocol.

You can use it as 
{block-filepath:BLOCK/}",Input module to return absolute file location of a block,,,,thorsten,True,,thorsten
cocoon,COCOON-2137,2007-10-03T14:53:27.000+0000,,2009-05-07T12:53:57.000+0000,,,New Feature,Minor,,,,,,,,,['Blocks: Forms'],['Cocoon Forms'],"A common pitfall in developing cforms applications is the misuse of certain cforms tags.
A schema for the forms files (definition, binding, template) would greatly help in writing forms code faster and more accurate and minimizing syntax errors in the first place.

I have started writing such schemas for the binding and definition files and added notations out of the current (2.1.10) cforms documentation.  those schemas are quite completet (well almost).

I also added starting points for this schemas:
- i18n transformer
- jx template generator

furthermore form template generator (and form instance) would be interesting. the major problem with the template is that we are heavily mixing different xml gramars here and state-of-the-art xml editors (like oxygen) have their problems with providing auto-complete features here.
the code-completion (assistance) feature for the form-definition and form-binding works quite well. it also has the added benefit, that the important bits of the cocoon forms docs are being displayed.
",XSD Schemas for CForms Development,2,1,,ggruber,True,,ggruber
cocoon,COCOON-2122,2007-08-19T14:09:28.000+0000,2007-08-20T14:49:40.000+0000,2013-05-02T02:29:08.000+0000,,Fixed,New Feature,Major,['2.2'],['2.2'],,,,,,,['- Components: Sitemap'],[''],Pipeline component scope is going to be implemented as outlined here: http://article.gmane.org/gmane.text.xml.cocoon.devel/74714,Implement pipeline component scope,,,,grek,True,grek,grek
cocoon,COCOON-2118,2007-08-18T13:15:08.000+0000,,2013-05-02T02:29:10.000+0000,,,New Feature,Major,,['2.2'],,,,,,,['- Components: Sitemap'],[''],"Implement expression language (""map:"") that let's you to access sitemap variables.

Actually, it's already implemented in PreparedVariableResolver for years but the task is about implementing it as class imlementing org.apache.cocoon.components.expression.Expression interface.

It was proposed here[1] and clarified here[2].

[1] http://article.gmane.org/gmane.text.xml.cocoon.devel/73700
[2] http://article.gmane.org/gmane.text.xml.cocoon.devel/73760",Implement map: expression language,,,,grek,True,,grek
cocoon,COCOON-2116,2007-08-18T13:07:37.000+0000,,2007-12-31T12:45:04.000+0000,,,New Feature,Major,,['2.2'],,,,,,,"['- Components: Sitemap', '- Expression language']","['', ""It's developing component that will handle expressions in unified way across all the Cocoon's corners.""]","Sitemap execution information term needs clarification. Currenlty I have two things in mind:
  1. Sitemap variables (those coming from matching like {1})
  2. Component parameters (those coming from <map:parameter/> elements)",Put sitemap execution information on Object Model,,,,grek,True,,grek
cocoon,COCOON-2110,2007-08-10T21:56:47.000+0000,2007-08-20T14:57:37.000+0000,2007-08-20T14:57:37.000+0000,,Invalid,New Feature,Major,['2.2'],['2.2'],,,,,,,"['- Components: Sitemap', '- Expression language']","['', ""It's developing component that will handle expressions in unified way across all the Cocoon's corners.""]","Description of the task is relatively simple; all I want to achieve is to make possible following construct in sitemap:

<map:transform type=""xslt"">
  <map:parameter name=""request_param"" value=""${cocoon/request/some_param}""/>
</map:transform>

assuming that JXPath (used above) is default EL of choice. Of course one would be able to use following construct:

<map:transform type=""xslt"">
  <map:parameter name=""request_param"" value=""${jexl:cocoon.request.some_param}""/>
</map:transform>",Evaluate expressions defined in cocooon-expression-language-api in Sitemap engine,,,,grek,True,grek,grek
cocoon,COCOON-2083,2007-07-04T15:43:08.000+0000,2007-09-19T22:31:12.000+0000,2007-09-19T22:31:12.000+0000,,Fixed,New Feature,Major,['2.2'],['2.2'],,,,,,,"['* Cocoon Core', 'Blocks: Databases']","['', '']","In thread http://thread.gmane.org/gmane.text.xml.cocoon.devel/73925 we have came to conclusion that there is no way to access database connections defined as Spring beans. Since we have a lot of Avalon-managed components in Cocoon and Spring-based solutions are preferred now I think it's worth to implement a bridge that will connect both worlds.

The idea is implement Avalon's selector (DataSourceSelector) that will gather connections defined both in Avalon and Spring way and expose all of them. New Spring-based selector will replace old one and the old one will be renamed to stay still accessible for new selector.",Allow database connections defined as Spring beans to be used in Avalon components,,,,grek,True,grek,grek
cocoon,COCOON-2054,2007-04-25T16:24:23.000+0000,2007-07-23T14:06:19.000+0000,2007-07-23T14:06:19.000+0000,,Fixed,New Feature,Major,['2.2'],['2.2'],,,,,,,['- Components: Sitemap'],[''],"Since the default Store is no longer registered at the StoreJanitor, we must read and display it explicitly in the StatusGenerator. Previously the StatusGenerator iterated over all stores registered at the janitor, thus it no longer appears there.",StatusGenerator must now show default Store explicitly (because no longer part of StoreJanitor),,,,alexander.klimetschek,True,grek,alexander.klimetschek
cocoon,COCOON-2048,2007-04-19T11:55:37.000+0000,2007-12-31T15:28:44.000+0000,2007-12-31T15:28:44.000+0000,,Fixed,New Feature,Major,['2.2'],['2.2'],,,,,,,['Blocks: ImageOp'],[''],"New OverlayOperation that puts another image on top of the image used by the ImageOpReader. Useful to overcome problems with browsers like IE that cannot put a transparent <img> on top of a background image.

Example usage:

     <map:read type=""image-op-overlay"" src=""background-image.jpg"" >
          <map:parameter name=""overlay-offset-x"" value=""10"" />
          <map:parameter name=""overlay-offset-y"" value=""20"" />
          <map:parameter name=""overlay-source"" value=""overlay-image.png""/>
     </map:read>
",ImageOp: overlay a transparent image on another one,,,,alexander.klimetschek,True,lars3loff,alexander.klimetschek
cocoon,COCOON-2047,2007-04-18T22:40:42.000+0000,2007-04-19T12:56:04.000+0000,2007-04-19T12:56:04.000+0000,,Fixed,New Feature,Major,['2.2'],['2.2'],,,,,,,['- Components: Sitemap'],[''],Showing a list of all Spring beans in the status generator output might be very useful when setting up components.,StatusGenerator shows a list of Spring beans,,,,alexander.klimetschek,True,reinhard@apache.org,alexander.klimetschek
cocoon,COCOON-2046,2007-04-17T14:50:41.000+0000,,2007-12-31T12:23:42.000+0000,,,New Feature,Major,,['2.2'],,,,,,,['- Servlet service framework'],[''],"This issue is to cover all tasks needed to implement postable source. The idea itself has been discussed several times on the list. The most important threads are:
  * http://thread.gmane.org/gmane.text.xml.cocoon.devel/67477/focus=67480 - original proposal
  * http://thread.gmane.org/gmane.text.xml.cocoon.devel/70475 - discussion of caching issues related to postable source

First implementation is going to be very naive and will not support caching nor SAX events forwarding (XML will be serialized/deserialized).",Implement postable source,,,,grek,True,,grek
cocoon,COCOON-2037,2007-04-04T13:29:19.000+0000,,2012-11-23T00:16:23.000+0000,,,New Feature,Major,,['2.1.11'],,,,,,,['Blocks: Forms'],['Cocoon Forms'],"I created a new DynamicGroup widget, with it you can replace all the children of the group, so you can simply create a *dynamic form* without recreate the form each time. Usefull for a very huge form that need to show only some widget at time.
This approach is a little different with the CForms standard because every time that you replace the children of the dynamic-group you must do a bindLoad()/bindSave() if you need to preserve the data.",New DynamicGroup widget,,,,daniele,True,,daniele
cocoon,COCOON-2001,2007-02-03T07:54:34.000+0000,,2007-02-03T07:55:42.000+0000,,,New Feature,Major,,"['2.1.10', '2.1.11']",,,,,,,['Blocks: Forms'],['Cocoon Forms'],"having an xml datatype for form may be a good idea specially when 
using an XML native database like eXist with cocoon.

for example we cat get xml data from user with
some kind of javascript interface in a form and then write it directly to 
eXist.",xml datatype for forms,,,,abbasmousavi,True,,abbasmousavi
cocoon,COCOON-1996,2007-01-31T20:37:38.000+0000,2007-02-19T08:14:19.000+0000,2013-05-02T02:29:03.000+0000,,Fixed,New Feature,Minor,['2.2'],['2.2'],,,,,,,['- Components: Sitemap'],[''],"StringConcatMetaModule concats strings from configured module chain. It assumes that all modules have string attributes, on the contrary, it calls toString() method.
If null is returned by some module RuntimeException will be thrown.

For configuration of input module chain take a look at example on ChainMetaModule.",New input module: StringConcatMetaModule,,,,grek,True,,grek
cocoon,COCOON-1974,2006-12-21T23:44:36.000+0000,,2006-12-21T23:45:20.000+0000,,,New Feature,Minor,,['2.1.11'],,,,,,,['* Cocoon Core'],[''],"I was in need of something like this, so I created the ContextAttribute Input module. I would like to donate it to the Cocoon project.",Donating ContextAttributeInputModule,1,,,jeroenreijn,True,,jeroenreijn
cocoon,COCOON-1963,2006-12-05T17:30:08.000+0000,,2007-06-11T09:26:43.000+0000,,,New Feature,Major,,"['2.1.10', '2.2']",,,,,,,['Blocks: Ajax'],[''],"In some situations you want to redirect the browser to a different page inside a cforms action, eg. you have a REST-style interface and create something under the URL /new (which shows a form to enter your new data) and on save you want to redirect the user to a page where that new data is stored (e.g. /foobar42). To do so in an ajax-environment, where the save action will be answered with a browser-update XML snippet, you need a separate action in the browser update handler. This patch adds the handling of a simple ""redirect"" action to the BUHandler.js:

    <bu:document>
        <bu:redirect uri=""foobar42"" />
    </bu:document>

If you want to have a fallback solution for non-AJAX cases, you need to trigger a normal HTTP redirect from your pipeline. This must happen when this bu:redirect is inside the XML stream, otherwise all content should be serialized to the browser. That functionality is provided by the attached RedirectTransformer. The usage would be like:

    <select type=""ajax-request"">
        <when test=""false"">
            <transform type=""redirect"" />
        </when>
    </select>

The server-side javascript snippet for the save action should look like (form is the Form object and documentID=""foobar42""):

    if (newDocument) {
        form.getWidget().endProcessing(false);
        cocoon.redirectTo(""cocoon:/redirectTo/"" + documentID);
    }

There should be a pipeline that matches ""/redirectTo/*"" and that serves the bu:document like above (eg. via a jx template to insert the documentID).",Add a redirect action to the browser update handler,,,,alexander.klimetschek,True,,alexander.klimetschek
cocoon,COCOON-1955,2006-11-17T17:09:34.000+0000,2007-04-12T12:20:40.000+0000,2007-04-12T12:20:39.000+0000,,Fixed,New Feature,Major,['2.2'],['2.2'],,,,,,,"['* Cocoon Core', '- Servlet service framework']","['', '']","== Problem ==

When trying to integrate Apache Solr in our Cocoon webapplication, we got a problem with conflicting versions of jars. Our jackrabbit block needs an old lucene, whereas Solr wants a fresh new lucene nightly build.

== Solution ==

Since we integrate the SolrServlet (and the other servlets from solar) inside a BlockServlet to have them all inside our cocoon webapp with consistent URLs, we considered adding a ShieldingServlet. Thus the BlockServlet wraps the ShieldingServlet which in turn wraps the SolrServlet. The configuration for the BlockServlet bean looks like this:

	<bean id=""com.mindquarry.search.solr.solr-select-block""
		class=""org.apache.cocoon.blocks.BlockServlet"">
		<property name=""mountPath"" value=""/solr-select"" />

		<property name=""servletClass""
			value=""org.apache.cocoon.bootstrap.servlet.ShieldingServlet"" />

		<property name=""blockContextURL""
			value=""blockcontext:/mindquarry-solr-block/"" />

		<property name=""properties"">
			<map>
				<entry key=""servlet-class""
					value=""org.apache.solr.servlet.SolrServlet"" />
				<entry key=""bootstrap-classpath-file""
					value=""/paranoid-classpath.txt"" />
				<entry key=""bootstrap-classloader-debug"" value=""true"" />
				<entry key=""bootstrap-classloader-factory""
					value=""org.apache.cocoon.classloader.DefaultClassLoaderFactory"" />
			</map>
		</property>
	</bean>


The directory structure of the solr-block:

  src/main/resources/
                           COB-INF/
                                 paranoid-classpath.txt
                                 paranoid-lib/
                                        <shielded-jars>.jar
                                        (including the jar which contains the solr servlet)
                            META-INF/
                                  cocoon/
                                        spring/
                                             solr-blockservlet.xml


The paranoid-classpath.txt contains the following single line:

lib-dir: context:paranoid-lib


== The Patch ==

To get the ShieldingServlet work with the blocks-fw and the basic shielding that is done in the Cocoon webapp anyway (via a ShieldingListener in the web.xml), two things must be done:

a) The BootstrapClassLoaderManager in cocoon-bootstrap must not store the classloader it creates in a static variable so that in each call of getClassLoader() a new ClassLoader is created. That method is exactly called 2 times in our situation: first by the cocoon webapp shielding listener and then by the ShieldingServlet in our block.

b) The method BlockContext.getResourcePaths() must be implemented to list the resources in that block context (stuff under COB-INF of that block).

Lots of the work was done by my colleague Alexander Saar.",[Patch] Allow shielded class loading for a single block,,,,alexander.klimetschek,True,,alexander.klimetschek
cocoon,COCOON-1928,2006-10-04T14:05:39.000+0000,2008-03-03T02:21:35.000+0000,2008-03-03T02:21:35.000+0000,,Won't Fix,New Feature,Minor,,['2.2'],,,,,,,['* Cocoon Core'],[''],"I need - with forrest - to get the doctype-system and doctype-public in a parameter like following :
<map:serializer logger=""sitemap.serializer.xhtml"" mime-type=""text/html"" name=""xhtml"" pool-grow=""2"" pool-max=""64"" pool-min=""2"" 
        src=""org.apache.cocoon.serialization.XMLSerializer"">
        <parameter name=""doctype-public"" value=""-//W3C//DTD XHTML 1.0 Transitional//EN"" />
        <parameter name=""doctype-system"" value=""http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"" />
        <encoding>UTF-8</encoding>
        <indent>yes</indent>
	<omit-xml-declaration>no</omit-xml-declaration>
</map:serializer>

here is a patch which apply to the AbstractTextSerializer.
in case the doctype is also found in the childs, child have priority.

Regards,",Add the ability to pass the doctype in parameter for Serializer,1,,,duc,True,,duc
cocoon,COCOON-1908,2006-09-05T17:33:29.000+0000,2006-10-03T09:48:40.000+0000,2006-10-03T09:48:40.000+0000,,Later,New Feature,Major,,['2.2'],,,,,,,['Blocks: JCR'],[''],"The current implementation of the JCRSourceFactory is missing the support for JCR queries (XPath and SQL) and only allows direct paths (like ""jcr://root/folder/file"").

I have implemented a simple query handling (see patch). It supports both XPath (required for JCR Level 1 compliance, thus any JCR implementation) and SQL (optional JCR feature). The query strings given as Source-URIs must look like this:

XPath:

jcr://xpath!//root/*
(which maps to the xpath query ""//root/*"" which is passed on to the JCR Query)

jcr://!//root/*
(which is just a shorthand notation for the above, using Xpath)

SQL:

jcr://sql!select * from nt:base where jcr:path = '/root/myfile'
(well, which maps to the sql query ""select * from nt:base where jcr:path = '/root/myfile'"")

There is a restriction for the queries: the result to be returned must be a content-node with no children and some streamable content. The JCRNodeSource class does not allow collections to be returned. I am working on subclasses that will target this issue (using the XMLizable interface) but this is not yet finished.

I tried to make the patch complete, including updated javadocs for the JCRSourceFactory class.",Add query support to JCR source factory,,,,alexander.klimetschek,True,,alexander.klimetschek
cocoon,COCOON-1877,2006-07-06T05:00:31.000+0000,,2006-07-28T11:56:29.000+0000,,,New Feature,Major,,,,,,,,,['Blocks: Forms'],['Cocoon Forms'],"This patch provides simple pagination for the repeater widget using the binding.

It consists of 3 main parts: It extends the repeater-definition to configure the pages, adds a ChangePageAction for userinteraction and modifies the binding so savePage/loadPage can be performed after the action is called.


Speaking of a first demo with basic functionality there are some known issues and missing features:

- We plan to use some internal ""storage area"" inside the binding that contains the updated rows after a page-change. The real doSave() code should be called just once after the form was submitted. This should provide better integration to scenarios where persistence frameworks like hibernate are used.

- The userinterface needs to be extended to features like next/first/last-page buttons and the common ""first 1 2 3 4 5 next"" group of links to choose pages directly.

- Filtering and Sorting of the rows would be nice and could maybe accomplished extending the storage area implementation.
",[PATCH] Pageable Repeater,,,,mepheser,True,,mepheser
cocoon,COCOON-1874,2006-06-29T15:40:45.000+0000,,2006-06-29T20:54:01.000+0000,,,New Feature,Major,,['2.2'],,,,,,,['- Samples'],[''],"Create a Maven archetype for a simple example app, using CForms to edit data in an embedded database (an improved port of the Bricks example, see http://wiki.apache.org/cocoon/BricksCms).

See also http://marc.theaimsgroup.com/?l=xml-cocoon-dev&m=115141627713015&w=2",Bricks example app archetype,,,,bdelacretaz,True,,bdelacretaz
cocoon,COCOON-1835,2006-04-14T19:05:02.000+0000,2008-07-30T18:41:46.000+0000,2008-07-30T18:41:46.000+0000,,Won't Fix,New Feature,Major,,,,,,,,,['- OSGi integration'],['This component covers effort to make Cocoon (blocks) OSGi driven.'],"Write a bundle that can install, start, update and stop Cocoon blocks. It is configured by a deployment descriptor.

Internally it works based on the OSGi configuration service.

Some notes:
- go through the tutorial of PDE development in ""Eclipse Rich Client Platform""
- read Jeff McAffer on dev@felix: Installing bundles by reference (Feb 2006)
- make it configureable which bundles you want to install
  a) create an OSGi application
  b) create a WAR file that internally uses OSGi
- make the deployer check if all contracts are fullfilled (mail on dev@felix by Jeff McAffer, 30th of August)
",Deployment bundle,,1,,reinhard@apache.org,True,reinhard@apache.org,reinhard@apache.org
cocoon,COCOON-1834,2006-04-13T23:01:36.000+0000,2008-07-30T18:41:30.000+0000,2008-07-30T18:41:30.000+0000,,Won't Fix,New Feature,Major,,,,,,,,,['- OSGi integration'],['This component covers effort to make Cocoon (blocks) OSGi driven.'],Make Cocoon block artifacts valid OSGi bundles at build time (of course using Maven 2),Integrate OSGi extensions into build system,,1,,reinhard@apache.org,True,reinhard@apache.org,reinhard@apache.org
cocoon,COCOON-1833,2006-04-13T22:48:45.000+0000,,2007-09-02T20:05:32.000+0000,,,New Feature,Major,,,,,,,,,['- OSGi integration'],['This component covers effort to make Cocoon (blocks) OSGi driven.'],"We are not the first having this need :-)
We just have to use http://www.eclipse.org/equinox/incubator/server/",Make OSGi-based Cocoon useable from within servlet containers,,,,reinhard@apache.org,True,,reinhard@apache.org
cocoon,COCOON-1832,2006-04-13T22:37:49.000+0000,2006-04-14T19:00:59.000+0000,2006-04-14T19:00:59.000+0000,,Duplicate,New Feature,Major,,,,,,,,,['- Servlet service framework'],[''],"- the BlockContext should become an interface
- Make sure that all methods are implemented in the BlockContext that extends the servlet context
  (e.g. use the include() method in the dispatcher)
- provide some basic excalibur protcols (e.g. the context: protocol) as URL protocols",Finish blocks-fw implementation,,,,reinhard@apache.org,True,,reinhard@apache.org
cocoon,COCOON-1831,2006-04-13T22:35:07.000+0000,2008-03-11T08:13:07.000+0000,2008-03-11T08:13:07.000+0000,,Fixed,New Feature,Major,,,,,,,,,['- Servlet service framework'],[''],"When a servlet service request is created, parameters from the parent request are ignored. This means that the sub request is performed as a fresh and clean new call. This would avoid any possible side-effects, but is very inconvenient in practice because you don't even know the request header parameters from the original (external) request. Additionally you can only pass information which is part of the returned stream, which is e.g. a  blocker to use the servlet protocol together with the control flow implementations. Those make use of special request parameters to transport the model (""bizdata"") to the view layer. ",Passing parameters to sub calls,,,,reinhard@apache.org,True,reinhard@apache.org,reinhard@apache.org
cocoon,COCOON-1830,2006-04-13T22:32:32.000+0000,,2007-09-02T20:06:29.000+0000,,,New Feature,Major,,,,,,,,,['- OSGi integration'],['This component covers effort to make Cocoon (blocks) OSGi driven.'],,Implement OSGiSpringBridge,,,,reinhard@apache.org,True,,reinhard@apache.org
cocoon,COCOON-1806,2006-03-22T21:07:31.000+0000,2006-03-23T20:47:08.000+0000,2006-03-24T20:23:10.000+0000,,Fixed,New Feature,Major,"['2.1.9', '2.2']",['2.1.9'],,,,,,,['Blocks: Forms'],['Cocoon Forms'],"Implemented a custom java validator builder, very similar to the custom java listener builder. Also implemented a sample birthday validator and added it to form2 sample.",[PATCH] Java class custom validator builder,1,,,s.gianni,True,,s.gianni
cocoon,COCOON-1802,2006-03-18T18:31:58.000+0000,2006-03-30T19:14:58.000+0000,2006-03-30T19:14:58.000+0000,,Fixed,New Feature,Major,,['2.2'],,,,,,,['- Build System: Maven'],['Mavenization of Cocoon 2.2'],"See thread starting with http://www.mail-archive.com/dev@cocoon.apache.org/msg42233.html for discussion details.

Here's the info from the file README.txt:

What is it?
-----------
m10n-blocks.sh is a script which automates parts of the conversion from the
""old"" blocks to the ""new"" mavenized ones.


Configuration
-------------
Only 2 variables have to be adjusted:
blksrc:
Local directory where https://svn.apache.org/repos/asf/cocoon/blocks is checked out
blkdest:
Local directory where https://svn.apache.org/repos/asf/cocoon/trunk is checked out


Usage
-----
# ./m10n-blocks.sh <blockname>...

Example:
# ./m10n-blocks.sh asciiart faces


Conventions
-----------
The script assumes, that every block will consist of a parent block
(cocoon-<blockname>), an implementation block (cocoon-<blockname>-impl)
and a sample block (cocoon-<blockname>-sample).


TODOs
-----
The subversion commands are currently untested and commented-out.
The implementation pom has to be manually merged with the original pom of the
""old"" block. In many cases it is enough to add the dependencies.
The build section should not be merged, since the new directory structure uses
maven defaults and thus doesn't need a special configuration.
",Script for m10n of old blocks,,,,highstick,True,reinhard@apache.org,highstick
cocoon,COCOON-1801,2006-03-18T10:03:24.000+0000,2006-05-03T21:08:01.000+0000,2006-05-03T21:08:01.000+0000,,Fixed,New Feature,Major,,['2.1.9'],,,,,,,['Blocks: Forms'],['Cocoon Forms'],"Since I felt the need and there are many comments in the code about it, i've implemented a RepeaterListener. The listener is triggered on row addition, deletion, reordering and clear. The event also brings informations about the row index and the actual action that took place.

I've adapted the javascript listener to support this new listener, and in the meanwhile also noticed that lines 59 to 70 of it are useless, since i believe that code has been moved inside the JavaScriptHelper methods and those lines were left there.

I've added a sample in form1, in the contact repeater. It's better to use the flow version of the sample, since in the no-flow sample the repeater is always recreated, an all events are broadcasted again.

The usage is simply :
<fd:repeater>
  <fd:on-repeater-modified>
    <fd:javascript>....</fd:javascript>
    <fd:other-listener>...</fd:other-listener>
  </fd:on-repeater-modified>
</fd:repeater>

I took care to call the event after the row has been initalized and to provide two events (deleting and deleted) for row deletion, so that accessing the new or ""about to be deleted"" row inside the listener should not be a problem. The forms1.xml listener has an example on how to do this.

The only place where i'm not sure this will always work correctly is inside the initialize method, maybe the event broadcast should be moved to somewhere else, or at least after the super.initialize() call, just to make sure that when the listener gets the event everything is properly set up.

----
Please note that the patch includes modifications on the javascript listener (o.a.c.forms.events.impl.JavaScript*) which i already modified in COCOON-1781. Regarding this file this patch will conflict with the other one (since both contains the same modifications) and in part superseedes the other one (due to the lines i've removed). So please apply the COCOON-1781 first, then this one, and if having problems applying this one revert the o.a.c.forms.event.impl.JavaScript* classes and then retry.",[PATCH] Repeater events,,,,s.gianni,True,,s.gianni
cocoon,COCOON-1795,2006-03-08T21:46:35.000+0000,2006-10-02T18:40:17.000+0000,2006-10-02T18:40:17.000+0000,,Won't Fix,New Feature,Major,,,,,,,,,['- Components: Sitemap'],[''],"To get Apache FOP 0.91 beta working with Cocoon, a new Serializer is necessary (the FOP API changed considerably). I wrote a very simple but working Serializer (see attachment) and hope that helps a little for future releases of Cocoon. The Serializer doesn't accept any configuration except 'mime-type' yet.",PDF Serializer for FOP 0.91,,,,alochschmied,True,,alochschmied
cocoon,COCOON-1789,2006-03-02T08:19:52.000+0000,2006-03-12T04:18:20.000+0000,2006-03-12T04:18:20.000+0000,,Fixed,New Feature,Minor,,['2.1.9'],,,,,,,['Blocks: Forms'],['Cocoon Forms'],"Cocoon forms lacks a Char datatype. This is often needed in bean bindings for database oriented applications, where the char often appears in a field.

The first patch provides a char datatype with it's builder, it's char convertor, the second patch the changes to the configuration to add the datatype, the third a modification of the samples form2 definition, bean and binding to give an example of the char datatype adding a middleInitial char field.

Patches applied to 2.1.X branch, but tested on 2.1.6, 2.1.7 and trunk.

Thanks to Maurizio Pillitu for first implementation.",[PATCH] Char datatype,,,,s.gianni,True,,s.gianni
cocoon,COCOON-1768,2006-02-03T07:32:23.000+0000,2006-04-10T18:07:36.000+0000,2006-04-10T18:07:36.000+0000,,Won't Fix,New Feature,Minor,,,,,,,,,['- Servlet service framework'],[''],"The BlocksManager assumes that the wiring location points to unpacked blocks, implement support for packed blocks.  If we change this, the block deployer also has to support this.",JARed blocks,,,,reinhard@apache.org,True,,reinhard@apache.org
cocoon,COCOON-1767,2006-02-03T07:31:19.000+0000,,2006-02-03T07:31:19.000+0000,,,New Feature,Major,,,,,,,,,['- Servlet service framework'],[''],"There is sophisticated creation of error messages in the CocoonServlet, where is the right place for it in the blocks architecture? 

(by Daniel Fagerstrom: http://marc.theaimsgroup.com/?l=xml-cocoon-dev&m=113889468124728&w=2)",Error handling,,,,reinhard@apache.org,True,,reinhard@apache.org
cocoon,COCOON-1766,2006-02-03T07:29:59.000+0000,,2006-02-03T07:29:59.000+0000,,,New Feature,Major,,,,,,,,,['- Servlet service framework'],[''],"I think it makes most sense to let multi part MIME be centralized and at the start of the call chain, it doesn't make sense to have it at the block level AFAICS. So we could make it part of the BlocksManager, but I prefer to keep the BlocksManager focused, so a filter before the BlocksManager seem to be a good alternative.

That also means that the multi part MIME handler is reusable for others (or that we can use another implementation if we want). Setting up a filter isn't such a big deal, so it is easy to do even outside a servlet container. 

(by Daniel Fagerstrom: http://marc.theaimsgroup.com/?l=xml-cocoon-dev&m=113889468124728&w=2)",Multi part MIME handling,,,,reinhard@apache.org,True,,reinhard@apache.org
cocoon,COCOON-1765,2006-02-03T07:28:44.000+0000,,2007-09-03T06:45:58.000+0000,,,New Feature,Major,,,,,,,,,['- OSGi integration'],['This component covers effort to make Cocoon (blocks) OSGi driven.'],"[...] But my main thoughts is that logging need to be a centralized service, common for all blocks (separate logging solutions for each block would be a pain).

The logging implementation is contained in a block (that is installed early) and makes the logger available as a service that other block can get through the service manager. This way the logging implementation is chosen by the choice of logging block. Observe that I only is talking about the blocks fw, within a block an ordinary ECM can be set up and it will inject the logger in its managed objects through the usual Avalon style.

Using the same logger interface everywhere is also practical I guess we continue to use the o.a.avalon.framework.logger.Logger one. 

(by Daniel Fagerstrom: http://marc.theaimsgroup.com/?l=xml-cocoon-dev&m=113889468124728&w=2)",Logging,,,,reinhard@apache.org,True,,reinhard@apache.org
cocoon,COCOON-1764,2006-02-03T07:26:11.000+0000,2006-04-10T18:10:19.000+0000,2006-04-10T18:10:19.000+0000,,Won't Fix,New Feature,Major,,,,,,,,,['- Servlet service framework'],[''],"see http://marc.theaimsgroup.com/?l=xml-cocoon-dev&m=113813135727508&w=2

In the current implementation of blocks, each block has an own component manager (the choice of which type is configurable) for managing the components of the block. The block local CM has an InterBlockServiceManager as parent manager and through it it can access components from component managers in other blocks that it is *wired* to, (and not from the other blocks of the system). Let us call this behavior wiring based CM for later reference.

Now I'm starting to think that the above described behavior is unnecessarily complicated and has some other problems as well. So if no one protests I'm going to change it so that the blocks still has own component managers, but that they register their (exposed) components in a global registry and that the parent manager of the local component managers access components from the global registry and not only from the connected blocks.

Consequences
------------

A global registry is much more similar to the situation in our ""compile time"" blocks than the wiring based CM, so it should be easier to migrate, furthermore so is a global registry used in OSGi so it will be more future safe as well.

A global registry requires less configuration in block.xml and wiring.xml. It is enough to declare the dependency on the interfaces of the components in the POM. If one want to make certain that a certain block is used at run time, a run time dependency can declared in the POM. Connections in the block.xml is only used for declaring inter block servlet communication.

A possible disadvantage is that role names could collide in the global registry but by using URIs or package names it should be possible to distinguish between components from different manufacturers.

The global registry approach might give run time errors when components are missing instead of deploy time errors. But this depends to a large extent on what lookup strategy the components and the local CMs use. If most of the component lookup is done from within the components using a service manager the setup problems might be defered to runtime. But if configuration based dependency injection is used the problems can at least in principle be detected early.

Local vs Global CMs
-------------------

A question that not will matter much until we use OSGi is whether the CMs are used from within the block or from the outside.

The current design assumes that the CM is internal to each block. The reason for this is that with OSGi R3 it was, IIUC, the only way to be able to have the implementation classes for the components internal to the bundle. A global CM would have requried that all implementation packages to be exported.

With R4 there are some new possibilities, one can get a class loader from a bundle and use that for constructing components in a global CM. OSGi R4 uses this for the new declarative services manager. A bundle that want to use the declarative service manager signals that by pointing out its component declaration with a special manifest header, ""Service-Component"". We could have a similar global ECM++ manager for legacy support. It is rather probable that there will be a OSGi adapted Spring CM, following the same principles.

               --- o0o ---

AFAIR, we haven't discussed what lookup strategy we wanted for components in blocks, I implemented the wiring based strategy because it seemed more natural for me before. The OSGi CM bridge that Sylvain implemented used OSGis service registry as a global registry. Gianugo tried to convince me that global registry was better at ApacheCon, but I was obviously not ready for it then ",Component handling,,,,reinhard@apache.org,True,,reinhard@apache.org
cocoon,COCOON-1729,2006-01-11T21:04:10.000+0000,2006-03-10T18:41:23.000+0000,2006-03-10T18:41:23.000+0000,,Fixed,New Feature,Trivial,"['2.1.9', '2.2']",['2.1.8'],,,,,,,['Blocks: Forms'],['Cocoon Forms'],"To add a certain numbers of Repeater.Rows (more than 1) to a Repeater with one Action, use the attribute ""number-of-rows"" in the definition of a repeater-action.

Example: 
   <fd:repeater-action command=""add-row"" id=""add-many-rows-to-repeater"" number-of-rows=""5"" repeater=""repeater-name"">
      <fd:label> Add 5 rows</fd:label>
    </fd:repeater-action>

    <fd:repeater-action command=""add-row"" id=""add-one-row-to-repeater"" repeater=""repeater-name"">
      <fd:label>Add 1 row</fd:label>
    </fd:repeater-action>
",[PATCH] Add multiple rows to a Repeater (fd:repeater-action).,,,,rmetternich,True,jbq,rmetternich
cocoon,COCOON-1676,2005-11-03T04:49:51.000+0000,,2007-12-31T12:43:02.000+0000,,,New Feature,Major,,['2.2'],,,,,,,['Blocks: Portal'],[''],We need a way for deployment of portlets (and perhaps of coplets as well). We could either use the code from the pluto adminportlet or use the jetspeed2 components.,Portlet/Coplet deployment,,,,cziegeler,True,,cziegeler
commons-codec,CODEC-257,2019-03-22T15:16:04.000+0000,2019-03-22T17:19:34.000+0000,2019-03-22T17:19:34.000+0000,,Fixed,New Feature,Major,['1.13'],,,,,,,,,,Update from Java 7 to Java 8,Update from Java 7 to Java 8,1,,,ggregory,True,ggregory,ggregory
commons-codec,CODEC-251,2018-12-12T21:50:11.000+0000,2018-12-12T22:08:08.000+0000,2018-12-12T22:08:08.000+0000,,Fixed,New Feature,Major,['1.12'],,,,,,,,,,"Add SHA-3 methods in {{org.apache.commons.codec.digest.DigestUtils}} {{getSha3_*}} and {{sha3_*}}:
 - getSha3_224Digest()
 - getSha3_256Digest()
 - getSha3_384Digest()
 - getSha3_512Digest()
 - sha3_224(byte[])
 - sha3_224(InputStream)
 - sha3_224(String)
 - sha3_224Hex(byte[])
 - sha3_224Hex(InputStream)
 - sha3_224Hex(String)
 - sha3_256(byte[])
 - sha3_256(InputStream)
 - sha3_256(String)
 - sha3_256Hex(byte[])
 - sha3_256Hex(InputStream)
 - sha3_256Hex(String)
 - sha3_384(byte[])
 - sha3_384(InputStream)
 - sha3_384(String)
 - sha3_384Hex(byte[])
 - sha3_384Hex(InputStream)
 - sha3_384Hex(String)
 - sha3_512(byte[])
 - sha3_512(InputStream)
 - sha3_512(String)
 - sha3_512Hex(byte[])
 - sha3_512Hex(InputStream)
 - sha3_512Hex(String)

The underlying algorithms are available on Java 9 and above.",Add SHA-3 methods in DigestUtils,1,,,ggregory,True,ggregory,ggregory
commons-codec,CODEC-242,2017-10-11T23:10:53.000+0000,2017-10-11T23:14:51.000+0000,2017-10-11T23:14:51.000+0000,,Fixed,New Feature,Major,['1.11'],,,,,,,,,,Add {{Automatic-Module-Name}} manifest entry for Java 9: {{org.apache.commons.codec}},Add Automatic-Module-Name manifest entry for Java 9,1,,,ggregory,True,ggregory,ggregory
commons-codec,CODEC-240,2017-09-29T19:40:37.000+0000,2019-02-23T09:54:08.000+0000,2019-02-23T09:54:13.000+0000,,Fixed,New Feature,Minor,['1.12'],,,,,,,,,,"Similar but more generic than the URLCodec.

Reference: https://tools.ietf.org/html/rfc3986#section-2.1
",Add Percent-Encoding Codec (described in RFC3986 and RFC7578),3,,,sermojohn,True,,sermojohn
commons-codec,CODEC-238,2017-07-04T20:24:12.000+0000,,2019-03-22T17:19:21.000+0000,,,New Feature,Minor,['1.13'],,,,,,,,,,"Digests are the usual reason why I need Hex :-) 

",DigestUtils should make it easier to  generate hex strings from MessageDigests and Digest Streams ,1,,,sesuncedu,True,,sesuncedu
commons-codec,CODEC-236,2017-06-12T22:30:09.000+0000,2019-07-12T13:15:47.000+0000,2019-07-12T13:15:47.000+0000,,Fixed,New Feature,Minor,['1.13'],['1.10'],1200,1200,,,,100,,,"https://en.wikipedia.org/wiki/MurmurHash

Already exists an Apache implementation:

org.apache.hive.common.util.HashCodeUtil.murmurHash(byte[], int, int)",Add MurmurHash Implementation,3,,,belugabehr,True,,belugabehr
commons-codec,CODEC-235,2017-05-31T03:25:39.000+0000,,2017-05-31T17:50:36.000+0000,,,New Feature,Minor,,,,,,,,,,,"I have been dabbling in phonetic algorithms lately and it is pleasing to see that I can find something under the commons umbrella for this area as well so thanks a ton for that.

In regards to feature requests NYSIIS as is implemented here I believe falls under the original release in the 1970s. Not being savvy in this area it took me too long to realize that the results that I was seeing from Oracles implementation as referenced here https://docs.oracle.com/cd/E18150_01/javadocs/SunMasterIndex/com/sun/mdm/index/phonetic/impl/Nysiis.html differs from what exists in commons-codec 1.10.

A series of searches brings me to this cool page http://www.dropby.com/NYSIIS.html which illustrates the differences as each replacement occurs between the original NYSIIS and refined / alternate NYSIIS.

I would gladly put more research in regards to specifications, coming up with samples for tests, tests themselves, and even development if this was something the team wished to see become a part of commons-codec. Some other Google searches does yield some implementations that I am using for the time being but something that is already packaged into something I use daily would be gladly welcome in my books.",Revised / Alternate NYSIIS,2,,,juangarcia,True,,juangarcia
commons-codec,CODEC-233,2017-03-31T23:50:07.000+0000,2017-04-02T20:45:41.000+0000,2017-04-02T20:45:41.000+0000,,Fixed,New Feature,Major,['1.11'],,,,,,,,,,"The existing Soundex class was designed around the American Soundex algorithm.

Whilst it offers some flexibility with the mapping of letters to Soundex numbers, the list of the 'silent' letters H and W is built-in to the code. There is no provision for changing the set of silent (ignored) letters.

There is also no way to change the designation of HW from silent into consonant separator - i.e. code 0 - because that is how HW are currently encoded in the public API.

To fix this, the mapping can be enhanced to support an extra code for 'silent' letters.

A mapping which includes such a code did not have defined behaviour previously, so can be treated differently - there is no need to assume HW are silent.

This allows for the definition of alternative silent letters.

It can also be used to map HW as code '0' - as long as there is at least one 'silent' code. 

If there are no actual silent letters in the algorithm variant, then the code can be appended to the end of the mapping. This will not affect processing as only letters A-Z are passed to the method. 

An alternative would be to introduce yet another code as an alias for '0', and only treat HW as silent if they have code '0'.",Soundex should support more algorithm variants,,,,sebb,True,,sebb
commons-codec,CODEC-227,2017-02-20T17:52:41.000+0000,,2017-02-20T17:52:41.000+0000,,,New Feature,Major,,,,,,,,,,,"There are two related UTF-7 encodings which might be worth implementing:

[1] RFC 2152 - UTF-7 A Mail-Safe Transformation Format of Unicode
[2] RFC 3501 - INTERNET MESSAGE ACCESS PROTOCOL - VERSION 4rev1 (5.1.3)

[1] https://tools.ietf.org/html/rfc2152
[2] https://tools.ietf.org/html/rfc3501#section-5.1.3",Implement UTF-7 encodings?,,,,sebb,True,,sebb
commons-codec,CODEC-222,2016-05-19T11:32:01.000+0000,,2016-05-22T23:44:34.000+0000,,,New Feature,Major,,,,,,,,,,,"As with DigestUtils, HmacUtils has lots of similar methods, differing only by their name.

To simplify this, and unify the API for current standard and new algorithms, a fluent approach is proposed. This still allows for using the enum values defined in HmacAlgorithms but does not require the use of a different API for algorithms that are not supported by the enum.

The design is as for DigestUtils: create an instance of HmacUtils containing the Mac to be updated, and provide chaining instance methods to update it and generate the output.",Fluent interface for HmacUtils,,,,sebb,True,,sebb
commons-codec,CODEC-215,2016-05-14T11:38:59.000+0000,2016-05-19T15:34:26.000+0000,2016-05-19T15:35:30.000+0000,,Fixed,New Feature,Major,['1.11'],['1.11'],,,,,,,,,"Constructors must not call overrideable methods.

An object is not guaranteed fully constructed until the constructor exits, so the subclass override may not see the correct parent object.

This applies to:

PureJavaCrc32",Constructors must not call overrideable methods,,,,sebb,True,,sebb
commons-codec,CODEC-214,2016-05-14T11:34:03.000+0000,,2019-05-31T13:03:03.000+0000,,,New Feature,Major,,,1200,1200,,,,100,,,"Commons Compress uses @Immutable etc tags in Javadoc

Codec could do the same",Update POM to support @NotThreadSafe @Immutable Javadoc tags,2,,,sebb,True,,sebb
commons-codec,CODEC-213,2016-05-14T04:46:08.000+0000,2016-05-19T15:44:37.000+0000,2016-09-22T01:12:38.000+0000,,Fixed,New Feature,Major,['1.11'],,,,,,,,,,"Support JEP 287: SHA-3 Hash Algorithms: SHA3-224, SHA3-256, SHA3-384, SHA3-512.",Support JEP 287: SHA-3 Hash Algorithms,1,,,ggregory,True,,ggregory
commons-codec,CODEC-212,2016-05-14T01:35:37.000+0000,,2019-03-22T17:19:21.000+0000,,,New Feature,Major,['1.13'],,,,,,,,,,Create a minimal Digest command line utility: org.apache.commons.codec.digest.Digest.,Create a minimal Digest command line utility: org.apache.commons.codec.digest.Digest,1,,,ggregory,True,,ggregory
commons-codec,CODEC-211,2016-05-14T01:17:19.000+0000,2016-05-17T00:47:23.000+0000,2016-05-17T00:47:23.000+0000,,Won't Fix,New Feature,Major,,['1.10'],,,,,,,,,Create enum MessageDigestAlgorithm and deprecate class MessageDigestAlgorithms.,Create enum MessageDigestAlgorithm and deprecate class MessageDigestAlgorithms,1,,,ggregory,True,,ggregory
commons-codec,CODEC-210,2016-05-14T00:28:03.000+0000,2016-05-14T00:29:44.000+0000,2016-05-14T00:29:44.000+0000,,Fixed,New Feature,Major,['1.11'],,,,,,,,,,"Add DigestUtils.getDigest(String algo, MessageDigest):

{code:java}
    /**
     * Returns a <code>MessageDigest</code> for the given <code>algorithm</code> or a default if there is a problem getting the algorithm.
     *
     * @param algorithm
     *            the name of the algorithm requested. See <a
     *            href=""http://docs.oracle.com/javase/6/docs/technotes/guides/security/crypto/CryptoSpec.html#AppA""
     *            >Appendix A in the Java Cryptography Architecture Reference Guide</a> for information about standard
     *            algorithm names.
     * @param defaultMessageDigest The default MessageDigest.
     * @return A digest instance.
     * @see MessageDigest#getInstance(String)
     * @throws IllegalArgumentException
     *             when a {@link NoSuchAlgorithmException} is caught.
     * @since 1.11
     */
{code}","Add DigestUtils.getDigest(String, MessageDigest)",1,,,ggregory,True,ggregory,ggregory
commons-codec,CODEC-208,2016-05-14T00:15:45.000+0000,2016-05-14T00:17:38.000+0000,2016-09-22T01:11:18.000+0000,,Fixed,New Feature,Major,['1.11'],,,,,,,,,,"Make some DigestUtils private APIs public
- digest(MessageDigest, ByteBuffer)
- digest(MessageDigest, InputStream)",Make some DigestUtils APIs public,1,,,ggregory,True,ggregory,ggregory
commons-codec,CODEC-206,2016-05-13T20:24:04.000+0000,2016-05-18T13:55:24.000+0000,2016-05-18T13:55:24.000+0000,,Fixed,New Feature,Major,['1.11'],['1.10'],,,,,,,,,"The original ticket called for adding File APIs to DigestUtils:

- md2(File)
- md2Hex(File)
- md5(File)
- md5Hex(File)
- sha1(File)
- sha1Hex(File)
- sha224(File)
- sha224Hex(File)
- sha256(File)
- sha256Hex(File)
- sha384(File)
- sha384Hex(File)
- sha512(File)
- sha512Hex(File)

Instead, we only need two new APIs on MessageDigestAlgorithm:

- digest(File)
- digestHex(File)
",Add java.io.File APIs to DigestUtils,1,,,ggregory,True,ggregory,ggregory
commons-codec,CODEC-205,2016-01-18T02:30:58.000+0000,2016-01-18T02:34:05.000+0000,2016-01-18T02:34:05.000+0000,,Fixed,New Feature,Major,['1.11'],,,,,,,,,,"As a parallel to [CODEC-171], add a faster CRC32 implementation than what is in Java 6. The implementation is copied from Apache Hadoop Commons.",Add faster CRC32 implementation,1,,,ggregory,True,ggregory,ggregory
commons-codec,CODEC-203,2015-09-03T19:47:04.000+0000,2015-09-03T20:18:20.000+0000,2015-09-03T20:18:20.000+0000,,Fixed,New Feature,Major,['1.11'],['1.10'],,,,,,,,,Add convenience method decodeHex(String),Add convenience method Hex.decodeHex(String),1,,,ggregory,True,ggregory,ggregory
commons-codec,CODEC-195,2014-11-10T16:33:33.000+0000,2016-05-18T13:47:48.000+0000,2016-05-18T22:38:16.000+0000,,Fixed,New Feature,Major,['1.11'],['1.11'],,,,,,,,,"Support SHA-224 in DigestUtils on Java 8.

The new APIs throw IllegalArgumentException on Java versions prior to Java 8.",Support SHA-224 in DigestUtils on Java 8,1,,,ggregory,True,,ggregory
commons-codec,CODEC-194,2014-11-10T14:33:16.000+0000,2014-11-10T16:10:27.000+0000,2014-11-10T16:10:27.000+0000,,Fixed,New Feature,Major,['1.11'],,,,,,,,,,Support java.nio.ByteBuffer in org.apache.commons.codec.binary,Support java.nio.ByteBuffer in org.apache.commons.codec.binary.Hex,1,,,ggregory,True,ggregory,ggregory
commons-codec,CODEC-192,2014-10-30T10:59:40.000+0000,2014-11-04T02:18:53.000+0000,2014-11-10T21:18:07.000+0000,,Fixed,New Feature,Major,['1.10'],,,,,,,,,,"Add a new soundex variant: Daitch–Mokotoff, which is an improved version over the original one.

The description of the encoding is available here: http://www.avotaynu.com/soundex.htm

An implementation is available here: https://github.com/netomi/dm-soundex

I will additionally provide a proper patch for codec.
",Add Daitch–Mokotoff Soundex,2,,,tn,True,,tn
commons-codec,CODEC-188,2014-08-13T22:05:20.000+0000,2014-08-22T22:59:45.000+0000,2014-11-10T03:11:40.000+0000,,Fixed,New Feature,Major,['1.10'],,,,,,,,,,Add support for HMAC Message Authentication Code (MAC) digests like HmacMD5 or HMacSHA1 as defined in RFC 2104.,Add support for HMAC Message Authentication Code (MAC) digests,3,,['github'],salyh,True,ggregory,salyh
commons-codec,CODEC-173,2013-06-15T20:18:23.000+0000,2016-05-19T13:41:27.000+0000,2016-05-19T13:41:27.000+0000,,Won't Fix,New Feature,Major,,,,,,,,,,,"For applications that need to generate different digests, it would be useful to be able to provide the digest as a parameter to the process.

For example:

{code}
public static byte[] digest(MessageDigest messageDigest, byte[] bytes) {
    return updateDigest(messageDigest, bytes).digest();
}
public static byte[] digest(final String algorithm, byte[] bytes) {
    return digest(getDigest(algorithm), bytes);
}
public static String digestHex(MessageDigest messageDigest, byte[] bytes) {
    return Hex.encodeHexString(updateDigest(messageDigest, bytes).digest());
}
public static String digestHex(final String algorithm, byte[] bytes) {
    return digestHex(getDigest(algorithm), bytes);
}
{code}

The same 4 methods would also be useful for InputStream (and possibly String).

There is already a private method of this form:

byte [] digest(MessageDigest digest, InputStream data) throws IOException

",DigestUtils: extend updateDigest() methods to create the final digest in various forms,,,,sebb,True,,sebb
commons-codec,CODEC-161,2012-10-25T08:53:27.000+0000,2012-11-28T21:01:21.000+0000,2013-04-28T19:15:46.000+0000,,Fixed,New Feature,Minor,['1.8'],['1.6'],,,604800,604800,604800,,,,"I want to add MatchRatingApproach algorithm to the Lucene project via commons codec.
What I have at the moment is a class called org.apache.lucene.analysis.phoenetic.MatchRatingApproach implementing StringEncoder
I have a pretty comprehensive test file located at: org.apache.lucene.analysis.phonetic.MatchRatingApproachTests

It's not exactly existing pattern so I'm going to need a bit of advice here. Thanks! Feel free to email.

FYI: It my first contribution so be gentle :-)  C# is my native.
I had incorrectly added this to Lucene solution as LUCENE-4494 but received some good advice to move it to here. I'm doing that now.

Reference: http://en.wikipedia.org/wiki/Match_rating_approach",Add Match Rating Approach (MRA) phonetic algorithm encoder,5,,['newbie'],crice,True,,crice
commons-codec,CODEC-136,2012-03-27T22:33:05.000+0000,2012-03-28T16:31:13.000+0000,2017-03-26T16:56:33.000+0000,,Fixed,New Feature,Major,['1.7'],,,,,,,,,,"Use Charset objects when possible, create Charsets for required character encodings.","Use Charset objects when possible, create Charsets class for required character encodings",1,,,ggregory,True,ggregory,ggregory
commons-codec,CODEC-133,2012-02-06T21:17:16.000+0000,2012-04-20T15:37:42.000+0000,2012-09-14T20:19:23.000+0000,,Fixed,New Feature,Major,['1.7'],['1.6'],,,,,,,,,"The Linux libc6 crypt(3) function, which is used to generate e.g. the password hashes in /etc/shadow, is available in nearly all other programming languages (Perl, PHP, Python, C, C++, ...) and databases like MySQL and offers MD5/SHA1/SHA-512 based algorithms that were improved by adding a salt and several iterations to make rainbow table attacks harder. Thus they are widely used to store user passwords.

Java, though, has due it's platform independence, no direct access to the libc functions and still lacks an proper port of the crypt(3) function.

I already filed a wishlist bug (CODEC-104) for the traditional 56-bit DES based crypt(3) method but would also like to see the much stronger algorithms.
There are other bug reports like DIRSTUDIO-738 that demand those crypt variants for some specific applications so there it would benefit other Apache projects as well.

Java ports of most of the specific crypt variants are already existing, but they would have to be cleaned up, properly tested and license checked:
ftp://ftp.arlut.utexas.edu/pub/java_hashes/ 

I would be willing to help here by cleaning the source code and writing unit tests etc. but I'd like to generally know if you are interested and if there's someone who can do a code review (it's security relevant after all and I'm no crypto guy)

bye,

-christian-
",Add classes for MD5/SHA1/SHA-512-based Unix crypt(3) hash variants.,1,,"['MD5', 'SHA-512', 'crypt(3)', 'crypto', 'hash']",lathspell,True,,lathspell
commons-codec,CODEC-125,2011-06-24T15:31:51.000+0000,2011-12-20T14:27:08.000+0000,2015-12-03T23:09:23.000+0000,,Fixed,New Feature,Minor,['1.6'],,,,,,,,,,I have implemented Beider Morse Phonetic Matching as a codec against the commons-codec svn trunk. I would like to contribute this to commons-codec.,Implement a Beider-Morse phonetic matching codec,3,,,matthewpocock,True,ggregory,matthewpocock
commons-codec,CODEC-110,2011-01-23T06:28:05.000+0000,2011-01-25T01:17:15.000+0000,2011-11-10T18:10:03.000+0000,,Fixed,New Feature,Trivial,['1.5'],,,,,,,,,,"Add a String version of Base64.isArrayByteBase64().  Right now we just have a byte[] version.

We'll call it:  Base64.isStringBase64().",Add a String version of Base64.isArrayByteBase64(),,,,juliusdavies,True,juliusdavies,juliusdavies
commons-codec,CODEC-106,2010-09-15T07:45:01.000+0000,2010-09-17T04:46:18.000+0000,2011-01-23T22:03:32.000+0000,,Fixed,New Feature,Major,['1.5'],['1.4'],,,,,,,,,"I've implemented the ""Kölner Phonetik"" algorithm (cologne phonetic), which is a phonetic algorithm optimized for the German language. For a German description see: http://de.wikipedia.org/wiki/K%C3%B6lner_Phonetik . For an English description see the source-code comments.

If you want to add the following classes to commons-codec, feel free to adapt the sources to your guidelines and needs.

Latest source files (I wasn't able to attach files):

org.apache.commons.codec.language.ColognePhonetic - http://ubuntuone.com/p/Fzm/

org.apache.commons.codec.language.ColognePhoneticTest - http://ubuntuone.com/p/Fzn/

See also thread ""[codec] Kölner Phonetik (cologne phonetic)"" at dev@commons.apache.org (dev-thread.121001@commons.apache.org)","Add the ""Kölner Phonetik"" encoder (cologne phonetic) to codec.lang",1,,,it2mmeyerfa,True,,it2mmeyerfa
commons-codec,CODEC-104,2010-08-12T13:31:50.000+0000,2012-04-20T15:34:41.000+0000,2012-09-14T20:28:52.000+0000,,Duplicate,New Feature,Major,['1.7'],,,,,,,,,,"The Sun Java APIs lack a function for the classical Unix crypt(3) hash that was used in e.g. /etc/passwd or Apache htpasswd and is still widely used dispite the availablitity of better algorithms like MD5 or SHA.

Apart from me cursing Sun for producing monster crypto APIs but missing the little things that one really needs, there are already several Apache projects
that implemented UnixCrypt for their own:
  org.apache.directory.studio.ldapbrowser.core.utils.UnixCrypt
  org.apache.fulcrum.crypto.impl.UnixCrypt
  and maybe others 

bye,

-christian-",Add a function for the classical Unix crypt(3) hash,2,1,,lathspell,True,,lathspell
commons-codec,CODEC-88,2009-10-20T21:46:22.000+0000,2012-09-05T19:15:16.000+0000,2012-09-14T20:21:36.000+0000,,Fixed,New Feature,Minor,['1.5'],['1.x'],,,,,,,,,Any chance of getting Base32 encoding support along the lines of the existing Base64 encoder?,Base32 encoder,1,,,olandere,True,,olandere
commons-codec,CODEC-74,2009-01-20T13:14:53.000+0000,2009-01-20T20:54:06.000+0000,2011-03-10T22:27:49.000+0000,,Fixed,New Feature,Minor,['1.4'],,,,,,,,,,"Perhaps this is recurring. Sorry if this is the case.
As of now, Hex.encodeHex() will only use lowercase letters. There may be clients of this class who need the output in uppercase. Such clients who want to use the library, now need to do the upper-casing externally to the class. Being able to choose the output format will help reducing computing cycles, which is often an objective for library routines.
",Allow for uppercase letters output in Hex.encodeHex(),,,,dastoikov,True,ggregory,dastoikov
commons-codec,CODEC-63,2008-03-08T08:18:55.000+0000,2012-03-12T14:53:32.000+0000,2015-02-16T00:37:29.000+0000,,Fixed,New Feature,Major,['1.7'],['1.x'],,,,,,,,,http://en.wikipedia.org/wiki/NYSIIS,Implement NYSIIS,2,,,bayard,True,,bayard
commons-codec,CODEC-62,2008-03-08T08:14:06.000+0000,2008-05-30T06:25:57.000+0000,2008-05-30T06:25:57.000+0000,,Won't Fix,New Feature,Major,,,,,,,,,,,http://en.wikipedia.org/wiki/Daitch-Mokotoff_Soundex,Implement Daitch Mokotoff Soundex,,,,bayard,True,,bayard
commons-codec,CODEC-60,2008-01-20T08:17:15.000+0000,2008-03-08T08:59:49.000+0000,2008-03-08T08:59:49.000+0000,,Fixed,New Feature,Major,['1.4'],,,,,,,,,,http://en.wikipedia.org/wiki/Caverphone,Implement Caverphone,,,,bayard,True,,bayard
commons-collections,COLLECTIONS-719,2019-06-01T12:15:42.000+0000,2019-07-05T15:23:54.000+0000,2019-07-05T15:43:21.000+0000,,Fixed,New Feature,Major,['4.4'],,2400,2400,,,,100,['Properties'],[''],"Create a {{PropertiesFactory}} and {{SortedPropertiesFactory}} to create and load {{Properties}} and {{SortedProperties}} from standard sources.

 See https://github.com/apache/commons-collections/pull/75",Create a PropertiesFactory and SortedPropertiesFactory,1,,,ggregory,True,ggregory,ggregory
commons-collections,COLLECTIONS-715,2019-05-05T14:21:59.000+0000,2019-05-05T14:23:55.000+0000,2019-05-05T14:23:55.000+0000,,Fixed,New Feature,Major,['4.4'],,,,,,,,['Collection'],[''],Implement Collection's removeIf(),Implement Collection's removeIf(),1,,,ggregory,True,,ggregory
commons-collections,COLLECTIONS-706,2018-11-30T14:20:16.000+0000,2018-12-11T15:51:54.000+0000,2019-07-05T14:52:05.000+0000,,Fixed,New Feature,Major,['4.3'],,,,,,,,,,"Add an equivalent to the Arrays.asList() method for conveniently creating sets. A trivial implementation would be:

SetUtils.asSet(T... items) -> return Collections.unmodifiableSet(new HashSet<>(asList(items));",Add SetUtils.unmodifiableSet(T... items) method,3,,,rec,True,,rec
commons-collections,COLLECTIONS-700,2018-10-13T16:06:46.000+0000,,2018-10-31T14:46:17.000+0000,,,New Feature,Major,,,,,,,,,['Map'],[''],"Add a {{ConcurrentWeakHashMap}}: A concurrent {{[WeakHashMap|https://docs.oracle.com/javase/8/docs/api/java/util/WeakHashMap.html]}}. A need for this was found to best support BeanUtils's BEANUTILS-509

This issue is looking for a volunteer.",Add a ConcurrentWeakHashMap,2,,,ggregory,True,,ggregory
commons-collections,COLLECTIONS-693,2018-07-30T00:29:22.000+0000,,2018-07-30T01:18:56.000+0000,,,New Feature,Major,,"['4.2', '4.x', '5.0']",,,3600,3600,3600,,"['Collection', 'Core']","['', '']","Please add OWASP Dependency Check to the build (pom.xml). OWASP DC makes an outbound REST call to MITRE Common Vulnerabilities & Exposures (CVE) to perform a lookup for each dependant .jar to list any/all known vulnerabilities for each jar. This step is needed because a manual MITRE CVE lookup/check on the main component does not include checking for vulnerabilities in components or in dependant libraries.

OWASP Dependency check : https://www.owasp.org/index.php/OWASP_Dependency_Check has plug-ins for most Java build/make types (ant, maven, ivy, gradle).

Also, add the appropriate command to the nightly build to generate a report of all known vulnerabilities in any/all third party libraries/dependencies that get pulled in. example : mvn -Powasp -Dtest=false -DfailIfNoTests=false clean aggregate

Generating this report nightly/weekly will help inform the project's development team if any dependant libraries have a reported known vulnerailities. Project teams that keep up with removing vulnerabilities on a weekly basis will help protect businesses that rely on these open source componets.",Please add OWASP Dependency Check to the build (build.xml),2,,,ABakerIII,True,,ABakerIII
commons-collections,COLLECTIONS-692,2018-07-27T14:31:15.000+0000,2018-07-27T14:33:06.000+0000,2018-07-27T14:33:06.000+0000,,Fixed,New Feature,Major,['4.3'],,,,,,,,,,"Java 9 deprecates [SetUniqueList] Class#newInstance().

Fix two call sites:

- {{org.apache.commons.collections4.map.MultiValueMap.ReflectionFactory.create()}}
- {{org.apache.commons.collections4.list.SetUniqueList.createSetBasedOnList(Set<E>, List<E>)}}

The patch https://github.com/apache/commons-collections/pull/49 is incomplete and incorrect:

- Fix to {{org.apache.commons.collections4.map.MultiValueMap.ReflectionFactory.create()}} is missing
- Patch to {{org.apache.commons.collections4.list.SetUniqueList.createSetBasedOnList(Set<E>, List<E>)}} performs extra work by adding the initial values to the new set twice.

 ",Replace use of deprecated Class#newInstance() #49,1,,,ggregory,True,ggregory,ggregory
commons-collections,COLLECTIONS-671,2018-01-10T16:56:38.000+0000,2018-01-10T16:57:39.000+0000,2018-01-10T16:57:39.000+0000,,Fixed,New Feature,Major,['4.2'],,,,,,,,['Iterator'],[''],"Add org.apache.commons.collections4.IterableUtils.first(Iterable<E>):
{code:java}
    /**
     * Returns the <code>first</code> value in the <code>iterable</code>'s {@link Iterator}, throwing
     * <code>IndexOutOfBoundsException</code> if there is no such element.
     * <p>
     * If the {@link Iterable} is a {@link List}, then it will use {@link List#get(int)}.
     *
     * @param <T> the type of object in the {@link Iterable}.
     * @param iterable  the {@link Iterable} to get a value from, may be null
     * @return the first object
     * @throws IndexOutOfBoundsException if the request  is invalid
     * @since 4.2
     */
    public static <T> T first(final Iterable<T> iterable)
{code}",Add org.apache.commons.collections4.IterableUtils.first(Iterable<E>),1,,,ggregory,True,ggregory,ggregory
commons-collections,COLLECTIONS-670,2018-01-10T16:46:42.000+0000,2018-01-10T16:55:34.000+0000,2018-01-10T16:55:34.000+0000,,Fixed,New Feature,Major,['4.2'],,,,,,,,['Iterator'],[''],"Add org.apache.commons.collections4.IteratorUtils.first(Iterator<E>):
{code:java}
    /**
     * Returns the <code>first</code> value in {@link Iterator}, throwing
     * <code>IndexOutOfBoundsException</code> if there is no such element.
     * <p>
     * The Iterator is advanced to <code>index</code> (or to the end, if
     * <code>index</code> exceeds the number of entries) as a side effect of this method.
     *
     * @param <E> the type of object in the {@link Iterator}
     * @param iterator  the iterator to get a value from
     * @return the first object
     * @throws IndexOutOfBoundsException if the request is invalid
     * @since 4.2
     */
     public static <E> E first(Iterator<E> iterator)
{code}",Add org.apache.commons.collections4.IteratorUtils.first(Iterator<E>),1,,,ggregory,True,ggregory,ggregory
commons-collections,COLLECTIONS-668,2017-12-31T05:06:42.000+0000,2018-01-02T22:55:14.000+0000,2018-01-02T22:55:14.000+0000,,Fixed,New Feature,Minor,['4.2'],['4.2'],,,,,,,['Collection'],[''],Nice to add a CollectionUtils#containsAny which takes a Collection and a primitive array as an option.  Loop through every array item and check for existence within the Collection.,"Add CollectionUtils containsAny method for primitive array: org.apache.commons.collections4.CollectionUtils.containsAny(Collection<?>, T...)",2,,,belugabehr,True,,belugabehr
commons-collections,COLLECTIONS-667,2017-12-31T03:39:12.000+0000,,2019-07-05T15:40:10.000+0000,,,New Feature,Minor,,,,,,,,,['Collection'],[''],"Add a _concat_ method which returns a Collection of all the Collections in the argument list

{code}
public Collection concat(Collection c...)
{code}",CollectionUtils Concat Method,4,1,,belugabehr,True,,belugabehr
commons-collections,COLLECTIONS-664,2017-11-06T08:28:57.000+0000,2019-07-05T15:43:21.000+0000,2019-07-05T15:43:59.000+0000,,Fixed,New Feature,Major,['4.4'],,600,600,,,,100,,,"For example, In some interface, needs we have a {{java.util.Properties}} as a parameter. In common case, we have different profiles in maven under different environments that we can random selection. And use the current classloader, usually belongs to AppclassLoader to get it's inputstream. I propose we should add a functionality that can directly read a file name and return a {{java.util.Properties}} as a return value. ",Add class FileProperties to load file name directly,2,,,mingleizhang,True,,mingleizhang
commons-collections,COLLECTIONS-654,2017-07-25T22:14:06.000+0000,2017-07-25T22:20:42.000+0000,2017-07-25T22:21:16.000+0000,,Fixed,New Feature,Major,['4.2'],,,,,,,,['Properties'],[''],Add class {{SortedProperties}} to be used in place of {{java.util.Properties}} to sort keys,Add class SortedProperties to sort keys,1,,,ggregory,True,,ggregory
commons-collections,COLLECTIONS-607,2017-07-04T18:49:12.000+0000,,2019-07-05T15:45:26.000+0000,,,New Feature,Minor,,,,,1209600,1209600,1209600,,['BidiMap'],[''],"In the class Javadoc of http://svn.apache.org/viewvc/commons/proper/collections/trunk/src/main/java/org/apache/commons/collections4/bidimap/DualHashBidiMap.java?view=markup 
there is a mention that Collections would welcome a direct hash-based implementation of BidiMap interface.

I am working on such; my source is available at https://github.com/coderodde/BidirectionalHashMap

At this point it does not adhere to style/interfaces of Commons Collections (such as implementing the BidiMap interface), yet I believe that is a matter of simple rewrite.

Currently, it represents the ""collision chains"" as AVL-trees, thus guaranteeing O(log n) access/modification even on poor hash functions. If that is not required, rewriting would be trivial as well.

I have several questions, but I have to start from the most important: is there any acute need for such a data structure?",Adding a hash table based BidiMap,2,,"['features', 'newbie']",coderodde,True,,coderodde
commons-collections,COLLECTIONS-605,2017-06-06T22:51:10.000+0000,2019-01-22T21:42:23.000+0000,2019-01-22T21:42:23.000+0000,,Won't Fix,New Feature,Trivial,,['4.1'],,,,,,,,,"Java's {{java.util.Collections}} has:

{code}
nCopies(int n, T o)
Returns an immutable list consisting of n copies of the specified object.

addAll(Collection<? super T> c, T... elements)
Adds all of the specified elements to the specified collection.

fill(List<? super T> list, T obj)
Replaces all of the elements of the specified list with the specified element.
{code}

However, it does not have the ability to add nCopies directly to a Collection.",Add new CollectionUtils Feature - addNCopies,1,,,belugabehr,True,,belugabehr
commons-collections,COLLECTIONS-591,2016-06-16T20:11:28.000+0000,,2016-06-16T20:12:03.000+0000,,,New Feature,Minor,,,,,,,,,['List'],[''],"Hi all,

In one of my projects I wanted to use copy on write list. I slightly improved this implementation and thought it would be nice to add it to commons collections, as I couldn't find something like this.

Implementation details:
- the original list is wrapped with CopyOnWriteList,
- on modifications the original list is copied and copy is used instead,
- for iterators before modification, the cursor index is used and then new iterator is set on on required position (this can't be done for abstract collections)

I hope there is no flaw in my designs,

Best regards, 
Radek Smogura
",Copy on write list,1,,,rsmogura,True,,rsmogura
commons-collections,COLLECTIONS-575,2015-08-24T09:05:59.000+0000,2018-01-03T02:52:55.000+0000,2018-01-03T08:11:22.000+0000,,Fixed,New Feature,Major,['4.2'],['4.0'],,,,,,,['Collection'],[''],"There is no synchronized queue wrapper in QueueUtils like in MapUtils or ListUtils for example.
My own case: I want to synchronize CircularFifoQueue operations, for this I need synchronized queue wrapper.",Synchronized queue wrapper in QueueUtils,5,1,"['queue', 'synchronization', 'wrapping']",gsavinov,True,,gsavinov
commons-collections,COLLECTIONS-573,2015-07-11T11:12:23.000+0000,,2018-06-24T09:29:12.000+0000,,,New Feature,Major,,,,,,,,,,,Placeholder ticket for pull request: https://github.com/apache/commons-collections/pull/12,Add CollectionUtils#deepMerge method,3,,,kaching88,True,,kaching88
commons-collections,COLLECTIONS-568,2015-06-17T06:14:52.000+0000,,2018-01-03T03:00:15.000+0000,,,New Feature,Major,,['4.0'],,,,,,,['Map'],[''],Trie should implement NavigableMap,Add a NavigableTrie interface which implements NavigableMap,2,,,kpajak,True,,kpajak
commons-collections,COLLECTIONS-567,2015-06-09T08:52:43.000+0000,2015-11-15T15:09:01.000+0000,2015-11-27T21:11:14.000+0000,,Fixed,New Feature,Major,['4.1'],['4.0'],,,,,,,,,"Prior to the release of 4.0 there was a discussion about to change the Bag interface to make it compliant with the Collection contract.

The outcome was that the Bag interface should be kept as is to simplify migration of older code-bases.

Now, it would make sense to add a MultiSet interface that is basically similar to a Bag, but does comply to the Collection contract. The old Bag could then be deprecated.",Add a MultiSet interface / implementations that do not violate the Collection contract,,,,tn,True,,tn
commons-collections,COLLECTIONS-565,2015-05-29T07:47:37.000+0000,2015-11-15T10:14:23.000+0000,2015-11-27T21:11:10.000+0000,,Fixed,New Feature,Major,['4.1'],,,,,,,,,,As we moved on to Java 6 we can now add appropriate decorators for NavigableMap and NavigableSet.,Add support for NavigableSet interface,,,,tn,True,,tn
commons-collections,COLLECTIONS-564,2015-05-29T07:46:48.000+0000,,2017-02-08T07:52:31.000+0000,,,New Feature,Major,,,,,,,,,,,"As we moved on to Java 6, we can now also support the Deque interface.",Add support for Deque interface,3,,,tn,True,,tn
commons-collections,COLLECTIONS-561,2015-04-11T14:16:27.000+0000,,2015-12-05T06:10:47.000+0000,,,New Feature,Major,,,,,,,,,,,Placeholder ticket for pull request: https://github.com/apache/commons-collections/pull/10,Add CollectionUtils#containsQuietly method,1,1,,tn,True,,tn
commons-collections,COLLECTIONS-556,2015-02-17T21:36:37.000+0000,2015-05-25T19:19:47.000+0000,2015-11-27T21:11:10.000+0000,,Fixed,New Feature,Major,['4.1'],,,,,,,,,,"The default JDK does not provide an IdentityHashSet which can be quite handy sometimes.

collections already has a MapBackedSet, so we could easily provide one by wrapping an IdentityHashMap.",Add an IdentityHashSet,,,,tn,True,,tn
commons-collections,COLLECTIONS-552,2015-02-05T19:40:23.000+0000,2015-06-11T12:35:39.000+0000,2015-11-27T21:47:12.000+0000,,Won't Fix,New Feature,Major,,['4.0'],,,,,,,,,"An EquatorMap would use an Equator to compute the hashCode and make equality checks for the keys. Sometimes there is the use case to put objects into a map but their equals/hashCode implementation does not support that or one needs a custom version. The usual solution is to put the object in some kind of wrapper, but this requires additional memory.

By providing an Equator, one can create custom maps quite easily. The implementation could be similar to the previous IdentityMap.",Add an EquatorMap,,,,tn,True,,tn
commons-collections,COLLECTIONS-550,2015-01-24T15:14:02.000+0000,2015-02-02T23:12:27.000+0000,2015-11-27T21:11:17.000+0000,,Fixed,New Feature,Minor,['4.1'],['4.0'],,,,,,,['Collection'],[''],"Create an utility method that returns an arbitrary {{String}} representation of a given {{Iterable}}, where for each {{Iterable}} element one may require a {{String}} representation that is different from the element's own {{toString()}} result.

Additionally the client may also provide an optional delimiter, where the default one could be the common {{"", ""}}.

The transformation of each {{Iterable}} element into an arbitrary {{String}} could be implemented by the means of a {{Transformer}}.

Example (illustrative method in {{CollectionUtils}}):

{code}
static <C> String toString(Iterable<C> iterable, Transformer<C, String> transformer);
{code}

Consider the following illustrative class:

{code}
class SomeClass {

  private final String propertyOne;
  private final String propertyTwo;

  public SomeClass(String propertyOne, String propertyTwo) {
    this.propertyOne = propertyOne;
    this.propertyTwo = propertyTwo;
  }

  public String getPropertyOne() {
    return propertyOne;
  }

  public String getPropertyTwo() {
    return propertyTwo;
  }

  @Override
  public String toString() {
    return propertyOne;
  }

}
{code}

One could transform an {{Iterable}} containing elements of type {{SomeClass}} into a client provided {{String}} representation by calling:

{code}
// list contains elements of type SomeClass
String result = CollectionUtils.toString(list, new Transformer<SomeClass, String>() {
  @Override
  public String transform(SomeClass someClass) {
    return someClass.getPropertyTwo();
  }
});

// Will print ""propertyTwoA, propertyTwoB""
System.out.println(result);

result = CollectionUtils.toString(list, new Transformer<SomeClass, String>() {
  @Override
  public String transform(SomeClass someClass) {
    return someClass.getPropertyOne() + someClass.getPropertyTwo();
  }
});

// Will print propertyOneApropertyTwoA, propertyOneBpropertyTwoB
System.out.println(result);

{code}
",Provide a simple way for creating an arbitrary String representation of a given Iterable,2,,"['collection', 'util']",gonmarques,True,,gonmarques
commons-collections,COLLECTIONS-546,2015-01-21T12:59:20.000+0000,,2015-06-23T13:37:50.000+0000,,,New Feature,Major,,['4.x'],,,,,,,['Collection'],[''],"It's very useful collection wrapper for map with expiration - PassiveExpiringMap.
Can you implement queue with expiration like already exist PassiveExpiringMap?",Implement expiring queue like already exist PassiveExpiringMap,4,1,"['collection', 'expiration', 'queue']",gsavinov,True,,gsavinov
commons-collections,COLLECTIONS-541,2014-12-30T18:33:41.000+0000,,2015-06-29T20:34:32.000+0000,,,New Feature,Minor,['4.x'],,,,,,,,,,Placeholder ticket for pull request: https://github.com/apache/commons-collections/pull/5,Add CollectionUtils.getRandom() method,,,,tn,True,,tn
commons-collections,COLLECTIONS-533,2014-10-09T18:10:37.000+0000,,2015-06-01T22:01:07.000+0000,,,New Feature,Major,['4.x'],,,,,,,,['Map'],[''],Placeholder ticket for https://github.com/apache/commons-collections/pull/3,Add a MultiValuedLinkedHashMap to preserve insertion order,3,,['github'],britter,True,,britter
commons-collections,COLLECTIONS-532,2014-05-30T10:27:24.000+0000,2014-06-21T14:36:37.000+0000,2015-11-27T21:47:15.000+0000,,Won't Fix,New Feature,Major,,['4.0'],,,,,,,"['Collection', 'Map']","['', '']","I expected that MapUtils.predicatedMap() includes those entries of the specified map that match the specified key-predicate and specified value-predicate.I expected that entries that do not match either of the predicates not included i the returned map. 
But in as per the code of org.apache.commons.collections4.map. {code}
PredicatedMap.validate()
 protected void validate(final K key, final V value) {
        if (keyPredicate != null && keyPredicate.evaluate(key) == false) {
            throw new IllegalArgumentException(""Cannot add key - Predicate rejected it"");
        }
        if (valuePredicate != null && valuePredicate.evaluate(value) == false) {
            throw new IllegalArgumentException(""Cannot add value - Predicate rejected it"");
        }
    }
{code}
 if evaluation of key or value predicate fails an IllegalArgumentException is thrown.

Predicates are passed to input map to test if an entry is to be included or not. If the evaluation of this predicate itself throws an exception based on result of predicate, then this method is useless and cannot be used to filter entries of a map(using predicates)","MapUtils.predicatedMap(map, keypredicate,valuepredicate) is not working as expected",2,,,bps,True,,bps
commons-collections,COLLECTIONS-520,2014-05-01T12:04:06.000+0000,,2015-01-23T09:57:25.000+0000,,,New Feature,Major,['4.x'],['4.0'],,,,,,,['Map'],[''],"Java provides implementations for List's and Set's but Map is missing.

Jersey uses an implementation ([com.sun.jersey.client.impl.CopyOnWriteHashMap|https://java.net/projects/jersey/sources/svn/content/trunk/jersey/jersey-client/src/main/java/com/sun/jersey/client/impl/CopyOnWriteHashMap.java]) but it doesn't fully implement Map because entrySet/keySet/values are not mutable.

I implemented a CaseInsensitiveCopyOnWriteMap in Mule and while it serves our purposes it's not 100% complete and isn't thread-safe, and it would be good to see an implementation in commons.  (BTW: I plan to extend from AsbtractHashedMap once we moved to 4.0/4.1).  

See:  [org.mule.util.CopyOnWriteCaseInsensitiveMap|https://github.com/mulesoft/mule/blob/716893e6231be47dccd3cfba9619762e146b4b84/core/src/main/java/org/mule/util/CopyOnWriteCaseInsensitiveMap.java]",Add a CopyOnWrite implementation of Map,1,,,dfeist,True,,dfeist
commons-collections,COLLECTIONS-511,2014-04-04T19:01:01.000+0000,2015-01-01T16:15:20.000+0000,2015-11-27T21:11:10.000+0000,,Fixed,New Feature,Trivial,['4.1'],,,,,,,,['Collection'],[''],"I recently needed a way to use a Predicate to select things from a list, but I also wanted to know which ones failed the predicate test.

I wanted the following, but with one iteration instead of two:
{code}
List originalList = (...)

Predicate p = (...)

Collection selected = CollectionUtils.select(originalList, p);
Collection rejected = CollectionUtils.selectRejected(originalList, p);
// handle the selected cases (save them or whatnot)
// now throw an error message or handle the rejected cases
{code}

This is what I came up with based on the CollectionUtils.select(...) method:
{code:java}
	public static <O, R extends Collection<? super O>> void bisect(
			final Iterable<? extends O> inputCollection,
			final Predicate<? super O> predicate, 
			final R selectedCollection,
			final R rejectedCollection) {

		if (inputCollection != null && predicate != null) {
			for (final O item : inputCollection) {
				if (predicate.evaluate(item)) {
					selectedCollection.add(item);
				}else{
					rejectedCollection.add(item);
				}
			}
		}
	}
	
	public static void main(String[] args){
		// this will test the bisection code
		List<String> original = Arrays.asList(
				""testString1"",
				""testString2"",
				""testString3"",
				""String1"",
				""String2"",
				""String3"",
				""testString4"",
				""String4"",
				""testString5""
				);
		
		List<String> selected = new ArrayList<String>();
		List<String> rejected = new ArrayList<String>();
		
		Predicate<String> beginsWithTestPredicate =
		new Predicate<String>() {
			public boolean evaluate(String object) {
				return object.startsWith(""test"");
			}
		};
		
		bisect(original, beginsWithTestPredicate, selected, rejected);
		
		System.out.println(""Size of selected (should be 5):"" 
				+ selected.size());
		System.out.println(""Size of rejected (should be 4):""
				+ rejected.size());
	}
{code}

This will of course throw a NullPointerException if either output collection is null.  This seems appropriate since we need to return two outputs anyway.

Not sure if *bisect* is the best name, but this method will split the original into two pieces. https://www.google.com/#q=define+bisect","CollectionUtils.bisect(...), this would be a combination of Select and SelectRejected",3,,,panchonb,True,,panchonb
commons-collections,COLLECTIONS-505,2013-12-10T23:33:37.000+0000,2013-12-11T19:12:25.000+0000,2013-12-11T20:39:21.000+0000,,Not A Problem,New Feature,Minor,,['4.0'],,,,,,,"['Collection', 'List']","['', '']","Remove the duplicate elements from the given collection. Generally, to implement this, the list would be copied to a set and back to list, which removes all the duplicate elements. A utility method for this would be a good-to-have feature in the CommonsCollection.",Removing duplicates from a collection,2,,,palvali,True,,palvali
commons-collections,COLLECTIONS-503,2013-11-22T15:14:53.000+0000,2013-12-08T11:42:59.000+0000,2015-11-27T21:11:16.000+0000,,Fixed,New Feature,Trivial,['4.1'],,,,,,,,['Functor'],[''],"Just thought a basic ifTransformer that performs operations based on a predicate would be useful.  I know this functionality can be accomplished via a switchTransformer, but sometimes it would just be easier and more clear to have an ifTransformer.

I attached a draft of what it could look like.  Let me know if this is something that should be included and I can polish it and write some tests.",IfTransformer,1,,,CainJ13,True,,CainJ13
commons-collections,COLLECTIONS-497,2013-11-11T19:42:42.000+0000,2013-11-11T20:29:19.000+0000,2014-11-09T14:31:43.000+0000,,Fixed,New Feature,Major,['4.0'],['4.0-alpha1'],,,,,,,,,Add a CollectionSortedBag adapter similar to CollectionBag for SortedBag implementations.,Add a CollectionSortedBag adaptor,,,,tn,True,,tn
commons-collections,COLLECTIONS-479,2013-07-15T16:55:12.000+0000,,2016-02-16T15:01:16.000+0000,,,New Feature,Minor,['4.x'],,,,,,,,,,"An order statistic tree http://en.wikipedia.org/wiki/Order_statistic_tree provides two useful properties. The ability to rank arbitrary keys relative to keys existing in the tree AND the ability to retrieve elements from the tree with the given rank.

This can be used to find the percentile rank of a key for example.

This functionality is not yet provided yet by any of the major libraries AFAIK.",An Order Statistic Tree,3,,,ajo.fod,True,,ajo.fod
commons-collections,COLLECTIONS-471,2013-05-14T21:16:47.000+0000,2013-11-27T22:58:41.000+0000,2015-11-27T21:11:15.000+0000,,Fixed,New Feature,Major,['4.1'],,,,,,,,,,It would be nice to have a decorator which bounds the number of elements returned by the decorated iterator.,Add a BoundedIterator which returns at most limit elements,1,,,tn,True,,tn
commons-collections,COLLECTIONS-469,2013-05-14T11:05:38.000+0000,,2013-06-24T19:34:25.000+0000,,,New Feature,Minor,['4.x'],,,,,,,,['Map'],[''],"A copy from COLLECTIONS-467.

Two remarks about current trunk implementation of PassiveExpiringMap.

You should additionally keep the value of the next element timeout (the youngest timeout) in object field, so that you don't need to iterate through whole map to find elements for expiration on each call. It improves performance by calculating only simple if for most cases:

{code}
public class PassiveExpiringMap {
  long youngestTimeoutMs = 0; // assert to have next timeout time for youngest element here always, or 0 if map is empty
  
  private void removeAllExpired(final long now) {
    if (youngestTimeoutMs>0 && youngestTimeoutMs<=now) {
      // do the cleanup
    }
  }
}
{code}

The second remark concerns the refreshing policy. As I can see there's no refreshing policy in current implementation, there should be at last two policies: NoRefresh (working as the current) and RefreshOnHit - which updates element expiration time when the element is ""hit"" (eg. by get()), so that the map always removes the least used resources.",PassiveExpiringMap performance improvement and refreshing policy,1,,,l0co,True,,l0co
commons-collections,COLLECTIONS-467,2013-05-08T13:10:12.000+0000,,2015-06-08T21:23:26.000+0000,,,New Feature,Minor,['4.x'],['3.2'],,,,,,,,,"If you use LRUMap with objects that require doing some disposal when they are dropped (eg. close()) and you hold these objects only in LRUMap, you cannot do it with current implementation. I propose to add onRemove() and onAdd() methods to the implementation, so that you can create anonymous inherited class and be able to react to these events.",LRUMap remove callback,1,,,l0co,True,,l0co
commons-collections,COLLECTIONS-460,2013-05-06T09:09:02.000+0000,2013-05-14T18:37:03.000+0000,2014-11-09T14:31:55.000+0000,,Fixed,New Feature,Major,"['4.0-alpha1', '4.0']",,,,,,,,['Iterator'],[''],"An IteratorQueue is equivalent to an IteratorChain with the difference that the iterators to use are put into a queue instead of an list, and that they are removed from the queue if an iterator is exhausted.

The IteratorChain keeps the iterators in the list even if they are exhausted, thus they can not be gc'ed until the whole chain is done. The interface of IteratorChain also contains a getIterators method, which returns the list of iterators used by this chain. So instead of changing the IteratorChain, I propose to add a separate class IteratorQueue with the described behavior.",Change IteratorChain to use a Queue and remove iterators after use,,,,tn,True,,tn
commons-collections,COLLECTIONS-457,2013-04-29T20:39:55.000+0000,2013-05-03T08:06:44.000+0000,2015-11-27T21:47:14.000+0000,,Won't Fix,New Feature,Major,,,,,,,,,,,"After the recently added SequencesComparator it is would be straightforward to create a simple diff-like tool (see http://en.wikipedia.org/wiki/Diff) for String sequences (List<String>).

The DiffUtils should support for the beginning the traditional diff and edit script output which can be achieved with implementing a quite simple CommandVisitor.",Add a new DiffUtils class,1,,,tn,True,,tn
commons-collections,COLLECTIONS-456,2013-04-29T19:50:39.000+0000,2013-04-29T20:09:12.000+0000,2014-11-09T14:31:30.000+0000,,Fixed,New Feature,Major,"['4.0-alpha1', '4.0']",,,,,,,,,,"Using the recently added SequencesComparator, the implementation is trivial.","Add ListUtils.longestCommonSubsequence(List, List)",,,,tn,True,,tn
commons-collections,COLLECTIONS-440,2013-02-01T20:35:32.000+0000,2014-05-02T09:17:29.000+0000,2015-11-27T21:47:18.000+0000,,Won't Fix,New Feature,Major,['4.x'],,,,,,,,,,http://code.google.com/p/java-heaps/,Consider including heap implementations from java-heaps project,,,,tn,True,,tn
commons-collections,COLLECTIONS-476,2012-12-28T06:20:18.000+0000,2015-01-24T14:19:37.000+0000,2015-11-27T21:47:18.000+0000,,Won't Fix,New Feature,Minor,,,,,,,,,"['List', 'Set']","['', '']","Would it be possible to add something like this to commons lang? Commons collection looks like it's pretty much dead, and the only alternative for this kind of stuff is Google's horribly designed and massively confusing Guava library.

Below are a couple of quick and dirty (untested) examples of the kind of wrappers I'm talking about. Note the isLocked/locked methods and the viewBlacklist method. Using these methods, it's easy for a component receiving a list/set to identify that it's currently unmodifiable and that the collection restricts certain values (e.g. null? maybe others?).

{code}
public final class LockableBlacklistableList<T> implements List<T> {

    private boolean locked;
    private List<T> list;
    private Set<T> blacklist;

    public LockableBlacklistableList(List<T> backingList, T... blacklist) {
        if (backingList == null) {
            throw new NullPointerException();
        }

        if (!backingList.isEmpty()) {
            throw new IllegalArgumentException();
        }
        this.blacklist = new HashSet<>(Arrays.asList(blacklist));
        this.list = backingList;
    }

    public void lock() {
        locked = true;
    }

    public boolean isLocked() {
        return locked;
    }

    public Set<T> viewBlacklist() {
        return Collections.unmodifiableSet(blacklist);
    }

    @Override
    public int size() {
        return list.size();
    }

    @Override
    public boolean isEmpty() {
        return list.isEmpty();
    }

    @Override
    public boolean contains(Object o) {
        return list.contains(o);
    }

    @Override
    public Iterator<T> iterator() {
        final Iterator<T> it = list.iterator();
        return new Iterator<T>() {
            @Override
            public boolean hasNext() {
                return it.hasNext();
            }

            @Override
            public T next() {
                return it.next();
            }

            @Override
            public void remove() {
                if (locked) {
                    throw new IllegalStateException();
                }
                it.remove();
            }
        };
    }

    @Override
    public Object[] toArray() {
        return list.toArray();
    }

    @Override
    public <T> T[] toArray(T[] a) {
        return list.toArray(a);
    }

    @Override
    public boolean add(T e) {
        if (locked) {
            throw new IllegalStateException();
        }
        return list.add(e);
    }

    @Override
    public boolean remove(Object o) {
        if (locked) {
            throw new IllegalStateException();
        }
        return list.remove(o);
    }

    @Override
    public boolean containsAll(Collection<?> c) {
        return list.containsAll(c);
    }

    @Override
    public boolean addAll(Collection<? extends T> c) {
        if (locked) {
            throw new IllegalStateException();
        }
        for (T item : c) {
            if (blacklist.contains(item)) {
                throw new IllegalArgumentException();
            }
        }
        return list.addAll(c);
    }

    @Override
    public boolean addAll(int index, Collection<? extends T> c) {
        if (locked) {
            throw new IllegalStateException();
        }
        for (T item : c) {
            if (blacklist.contains(item)) {
                throw new IllegalArgumentException();
            }
        }
        return list.addAll(index, c);
    }

    @Override
    public boolean removeAll(Collection<?> c) {
        if (locked) {
            throw new IllegalStateException();
        }
        return list.removeAll(c);
    }

    @Override
    public boolean retainAll(Collection<?> c) {
        if (locked) {
            throw new IllegalStateException();
        }
        return list.retainAll(c);
    }

    @Override
    public void clear() {
        if (locked) {
            throw new IllegalStateException();
        }
        list.clear();
    }

    @Override
    public boolean equals(Object o) {
        return list.equals(o);
    }

    @Override
    public int hashCode() {
        return list.hashCode();
    }

    @Override
    public T get(int index) {
        return list.get(index);
    }

    @Override
    public T set(int index, T element) {
        if (locked) {
            throw new IllegalStateException();
        }
        if (blacklist.contains(element)) {
            throw new IllegalArgumentException();
        }
        return list.set(index, element);
    }

    @Override
    public void add(int index, T element) {
        if (locked) {
            throw new IllegalStateException();
        }
        if (blacklist.contains(element)) {
            throw new IllegalArgumentException();
        }
        list.add(index, element);
    }

    @Override
    public T remove(int index) {
        if (locked) {
            throw new IllegalStateException();
        }
        return list.remove(index);
    }

    @Override
    public int indexOf(Object o) {
        return list.indexOf(o);
    }

    @Override
    public int lastIndexOf(Object o) {
        return list.lastIndexOf(o);
    }

    @Override
    public ListIterator<T> listIterator() {
        final ListIterator<T> backingListIt = list.listIterator();
        return new ListIterator<T>() {
            @Override
            public boolean hasNext() {
                return backingListIt.hasNext();
            }

            @Override
            public T next() {
                return backingListIt.next();
            }

            @Override
            public boolean hasPrevious() {
                return backingListIt.hasPrevious();
            }

            @Override
            public T previous() {
                return backingListIt.previous();
            }

            @Override
            public int nextIndex() {
                return backingListIt.nextIndex();
            }

            @Override
            public int previousIndex() {
                return backingListIt.previousIndex();
            }

            @Override
            public void remove() {
                if (locked) {
                    throw new IllegalStateException();
                }
                backingListIt.remove();
            }

            @Override
            public void set(T e) {
                if (locked) {
                    throw new IllegalStateException();
                }
                if (blacklist.contains(e)) {
                    throw new IllegalArgumentException();
                }
                backingListIt.set(e);
            }

            @Override
            public void add(T e) {
                if (locked) {
                    throw new IllegalStateException();
                }
                if (blacklist.contains(e)) {
                    throw new IllegalArgumentException();
                }
                backingListIt.add(e);
            }
        };
    }

    @Override
    public ListIterator<T> listIterator(int index) {
        final ListIterator<T> backingListIt = list.listIterator(index);
        return new ListIterator<T>() {
            @Override
            public boolean hasNext() {
                return backingListIt.hasNext();
            }

            @Override
            public T next() {
                return backingListIt.next();
            }

            @Override
            public boolean hasPrevious() {
                return backingListIt.hasPrevious();
            }

            @Override
            public T previous() {
                return backingListIt.previous();
            }

            @Override
            public int nextIndex() {
                return backingListIt.nextIndex();
            }

            @Override
            public int previousIndex() {
                return backingListIt.previousIndex();
            }

            @Override
            public void remove() {
                if (locked) {
                    throw new IllegalStateException();
                }
                backingListIt.remove();
            }

            @Override
            public void set(T e) {
                if (locked) {
                    throw new IllegalStateException();
                }
                if (blacklist.contains(e)) {
                    throw new IllegalArgumentException();
                }
                backingListIt.set(e);
            }

            @Override
            public void add(T e) {
                if (locked) {
                    throw new IllegalStateException();
                }
                if (blacklist.contains(e)) {
                    throw new IllegalArgumentException();
                }
                backingListIt.add(e);
            }
        };
    }

    @Override
    public List<T> subList(int fromIndex, int toIndex) {
        LockableBlacklistableList<T> ret = new LockableBlacklistableList<>(
                list.subList(fromIndex, toIndex));
        ret.locked = locked;
        ret.blacklist = blacklist;
        return ret;
    }
}
{code}

{code}
public final class LockableBlacklistableSet<T> implements Set<T> {
    private boolean locked;
    private Set<T> set;
    private Set<T> blacklist;

    public LockableBlacklistableSet(Set<T> backingSet, T... blacklist) {
        if (backingSet == null) {
            throw new NullPointerException();
        }

        if (!backingSet.isEmpty()) {
            throw new IllegalArgumentException();
        }
        this.blacklist = new HashSet<>(Arrays.asList(blacklist));
        this.set = backingSet;
    }
    
    public void lock() {
        locked = true;
    }

    public boolean isLocked() {
        return locked;
    }

    public Set<T> viewBlacklist() {
        return Collections.unmodifiableSet(blacklist);
    }

    public int size() {
        return set.size();
    }

    public boolean isEmpty() {
        return set.isEmpty();
    }

    public boolean contains(Object o) {
        return set.contains(o);
    }

    public Iterator<T> iterator() {
        final Iterator<T> it = set.iterator();
        return new Iterator<T>() {
            @Override
            public boolean hasNext() {
                return it.hasNext();
            }

            @Override
            public T next() {
                return it.next();
            }

            @Override
            public void remove() {
                if (locked) {
                    throw new IllegalStateException();
                }
                it.remove();
            }
        };
    }

    public Object[] toArray() {
        return set.toArray();
    }

    public <T> T[] toArray(T[] a) {
        return set.toArray(a);
    }

    public boolean add(T e) {
        if (locked) {
            throw new IllegalStateException();
        }
        if (blacklist.contains(item)) {
            throw new IllegalArgumentException();
        }
        return set.add(e);
    }

    public boolean remove(Object o) {
        if (locked) {
            throw new IllegalStateException();
        }
        return set.remove(o);
    }

    public boolean containsAll(Collection<?> c) {
        return set.containsAll(c);
    }

    public boolean addAll(Collection<? extends T> c) {
        if (locked) {
            throw new IllegalStateException();
        }
        for (T item : c) {
            if (blacklist.contains(item)) {
                throw new IllegalArgumentException();
            }
        }
        return set.addAll(c);
    }

    public boolean retainAll(Collection<?> c) {
        if (locked) {
            throw new IllegalStateException();
        }
        return set.retainAll(c);
    }

    public boolean removeAll(Collection<?> c) {
        if (locked) {
            throw new IllegalStateException();
        }
        return set.removeAll(c);
    }

    public void clear() {
        if (locked) {
            throw new IllegalStateException();
        }
        set.clear();
    }

    public boolean equals(Object o) {
        return set.equals(o);
    }

    public int hashCode() {
        return set.hashCode();
    }
}
{code}

",Collection wrappers to for unmodifiable / nonnull-safe collections,3,,,offbynull,True,,offbynull
commons-collections,COLLECTIONS-428,2012-07-26T18:55:26.000+0000,,2018-01-03T03:06:06.000+0000,,,New Feature,Major,['4.x'],,,,,,,,,,"Hello,
a while ago I wrote a GeneralizedSuffixTree (based on the Ukkonen's paper ""On-line construction of suffix trees"" http://www.cs.helsinki.fi/u/ukkonen/SuffixT1withFigs.pdf), that I later open sourced at https://github.com/abahgat/suffixtree.

Thomas Neidhart did several improvements here: https://github.com/netomi/suffixtree and I merged them back in the 'commons' branch of my own repository.

As per Thomas's suggestion, I am submitting the library for evaluation, as it may be a good fit for this project.

Regards,
Alessandro",Contribution: GeneralizedSuffixTree,2,,,abahgat,True,,abahgat
commons-collections,COLLECTIONS-422,2012-07-12T15:42:59.000+0000,2013-04-21T15:18:36.000+0000,2014-11-09T14:31:45.000+0000,,Fixed,New Feature,Minor,"['4.0-alpha1', '4.0']",['4.x'],,,,,,,['Collection'],[''],Here's a patch providing a tool to get every permutation of a collection. ,Proposal for a permutation generator (Permutator),1,,"['newbie', 'patch']",magauss,True,,magauss
commons-collections,COLLECTIONS-399,2012-03-08T01:43:27.000+0000,2012-06-19T21:06:27.000+0000,2014-11-09T14:31:48.000+0000,,Fixed,New Feature,Major,"['4.0-alpha1', '4.0']",,,,,,,,,,"Since the BoundedFifoBuffer is implemented as an array, it should be simple to implement random access to any element of the buffer.

[This functionality was requested on the Commons user list]

For example:

// get nth entry; i.e. get(0) is the same as get()
// NoSuchElementException if i >= numEntries
// if i < 0, could either throw an Exception or treat as reverse index from end of buffer
E get(int i)

This would also work for the sub-class CircularFifoBuffer.",Add indexed get to CircularFifoQueue,1,,,sebb,True,,sebb
commons-collections,COLLECTIONS-396,2012-03-02T20:44:39.000+0000,2013-02-28T18:31:20.000+0000,2014-11-09T14:31:44.000+0000,,Fixed,New Feature,Minor,"['4.0-alpha1', '4.0']",,,,3600,3600,3600,,['Iterator'],[''],"I would like to submit my DynamicIterator class for inclusion in Commons Collections.

Description from JavaDoc:

Iterates over the elements of an inner iterator provided by nextIterator() Once the inner iterator's hasNext() method returns false, nextIterator() will be called to obtain another iterator, and so on until nextIterator() returns null.

The use case I had in mind for this class was the paged web service. A web service returns a list of results, which are paginated. Once a page's results are exhausted, nextIterator() can go fetch the next page for iteration.

This class, and accompanying test class are my original works. I have released them into the public domain, feel free to use the appropriate ASF license.",New LazyIteratorChain class.,1,,"['dynamic', 'iterator', 'new']",jeffrey.rodriguez,True,,jeffrey.rodriguez
commons-collections,COLLECTIONS-393,2012-02-29T04:15:10.000+0000,2012-08-16T20:51:29.000+0000,2014-11-09T14:31:53.000+0000,,Fixed,New Feature,Major,"['4.0-alpha1', '4.0']",,,,86400,86400,86400,,['Collection'],[''],"Returns consecutive sublists of a list, each of the same size (the final list may be smaller). For example, partitioning a list containing [a, b, c, d, e] with a partition size of 3 yields [[a, b, c], [d, e]] -- an outer list containing two inner lists of three and two elements, all in the original order. 

The outer list is unmodifiable, but reflects the latest state of the source list. The inner lists are sublist views of the original list, produced on demand using List.subList(int, int), and are subject to all the usual caveats about modification as explained in that API. Adapted from http://code.google.com/p/google-collections/

Inspired by Lars Vogel",Split / Partition a collection into smaller collections,1,,,hamedshayan@gmail.com,True,,hamedshayan@gmail.com
commons-collections,COLLECTIONS-370,2011-01-11T22:42:19.000+0000,2013-02-24T21:16:21.000+0000,2015-11-27T21:47:19.000+0000,,Implemented,New Feature,Minor,,['3.2'],,,,,,,['Map'],[''],"Add method MapUtils.putIfNotNull that adds the given value to the map only if it is not null.

Possible implementation:

public static <K, V> void putIfNotNull(Map<K, V> map, K key, V value) {
	if (value != null) {
		map.put(key, value);
	}
}",Add method MapUtils.putIfNotNull,,,,tompasik,True,,tompasik
commons-collections,COLLECTIONS-368,2010-12-27T20:56:59.000+0000,2011-02-04T15:27:11.000+0000,2015-11-27T21:47:14.000+0000,,Invalid,New Feature,Minor,,,,,,,,,['List'],[''],Sometimes it is very useful to be able to create new and populate List objects in the same way that can be done with arrays by using the braces initialisation. My patch is an implementation of this for ListUtils with corresponding unit tests. Hoping for inclusion as soon as possible. Any comments are welcome.,New method in ListUtils to create Lists populated with var-args elements,,,,eldakkar,True,,eldakkar
commons-collections,COLLECTIONS-366,2010-11-02T19:10:16.000+0000,2015-01-24T13:04:44.000+0000,2015-11-27T21:47:20.000+0000,,Won't Fix,New Feature,Major,,['3.2'],,,,,,,,,"Sometimes there is a demand too have a list, that represents numbers within some range (say, [5..10]). If the range is big (millions of records), creating a dummy list that holds all instances of objects is too expensive.
The provided implementation (attached to this issue) solves this problem. Nice to have in commons collections.",A light-weight list of integers,2,,,dma_k,True,,dma_k
commons-collections,COLLECTIONS-356,2010-06-23T09:33:38.000+0000,,2013-11-24T16:01:34.000+0000,,,New Feature,Minor,['4.x'],,,,,,,,['Collection'],[''],"Currently, BidiMap supports one to one relationship. However, in real life, there are many many-to-many relationships. Take an example of student and course. One student can take many courses while one course contain many students. It would be useful to have a class to manipulate such situation.

The work around currently is using 2 hash maps.",Request for a Many-to-Many BidiMap,1,,,franzwong,True,,franzwong
commons-collections,COLLECTIONS-354,2010-02-25T15:39:42.000+0000,2013-05-13T22:31:59.000+0000,2015-11-27T21:47:16.000+0000,,Won't Fix,New Feature,Major,,,,,,,,,['Map'],[''],"MINA has some useful Collection utilities, for example CopyOnWriteMap.

http://svn.apache.org/repos/asf/mina/trunk/core/src/main/java/org/apache/mina/util/CopyOnWriteMap.java

Several of the other utilities look useful.",Add Collection utils from MINA: e.g. CopyOnWriteMap,1,,,sebb,True,,sebb
commons-collections,COLLECTIONS-347,2009-12-18T21:15:36.000+0000,,2013-06-04T23:27:53.000+0000,,,New Feature,Minor,['4.x'],,,,,,,,['Map'],[''],"The {{LimitedSizeFifoMap}} never exceeds a given element limit by deleting old entries, whenever new ones are inserted. It may be of use for caches holding memory-intense objects (it is for me, anyway).

The patch contains:
* the interface {{LimitedSizeMap}}
* the fully documented implementation {{LimitedSizeFifoMap}}, which deletes the oldest items first
* the accompanying unit test

If you find anything that needs improvement, please let me know.",Proposal for adding a LimitedSizeFifoMap decorator,1,,,mthomas,True,,mthomas
commons-collections,COLLECTIONS-345,2009-10-20T19:34:22.000+0000,2010-06-19T19:46:09.000+0000,2010-06-19T19:46:09.000+0000,,Won't Fix,New Feature,Minor,,,,,900,900,900,,['Collection'],[''],Adds the ability to turn a Jdbc ResultSet into a java.util.Collection,Creates a Collection from a JDBC ResultSet,1,,,hdiwan,True,,hdiwan
commons-collections,COLLECTIONS-342,2009-10-15T09:58:08.000+0000,,2015-06-09T20:48:52.000+0000,,,New Feature,Minor,,,,,,,,,['Map'],[''],The project {{mina-core}} contains a very useful map implementation: {{org.apache.mina.util.ExpiringMap}}. I'd suggest to adopt it for the {{commons-collections}} project.,Add ExpiringMap,2,,,marc.rohlfs,True,,marc.rohlfs
commons-collections,COLLECTIONS-306,2008-11-08T12:09:54.000+0000,2011-05-24T16:12:35.000+0000,2014-11-09T14:31:44.000+0000,,Fixed,New Feature,Major,"['4.0-alpha1', '4.0']",,,,2700,2700,2700,,['Collection'],[''],"It is good idea to use Predicate in subtract method, I've developed myself the mentioned method and now I am testing it. I mean we should have following methods:
The one already exist is:
public static Collection subtract(Collection a, Collection b)
I offer to have one more which is:
public static Collection subtract(Collection a, Collection b, Predicate predicate)",Use Predicate in subtract,,,,hamedshayan@gmail.com,True,,hamedshayan@gmail.com
commons-collections,COLLECTIONS-301,2008-07-05T18:48:13.000+0000,2008-07-07T14:55:20.000+0000,2011-09-07T20:50:45.000+0000,,Won't Fix,New Feature,Major,,['3.2'],,,,,,,['Map'],[''],"I miss an efficient implementation of a SingleValueMap.
The idea goes as follows:
Map<String, String> map = new SingleValueMap<String, String>();

map.put(""hello"",""greeting"");
map.put(""hola"",""greeting"");
map.put(""hallo"",greeting"");

a normal map would now hold three copies of ""greeting"". A SingleValueMap would contain only one copy of this value. All keys would map to one copy, identically to a N:1 relaton.",Provide SingleValueMap,,,,,,,michael-o
commons-collections,COLLECTIONS-296,2008-05-17T04:48:50.000+0000,2013-04-18T21:08:17.000+0000,2014-11-09T14:31:47.000+0000,,Fixed,New Feature,Minor,"['4.0-alpha1', '4.0']",,,,,,,,['Core'],[''],"Is there any interest in this?

    /**
     * Returns a new ArrayList where sorted Collection a and sorted Collection b
     * are combined such that ordering of the elements according to
     * Comparator c is retained.  Uses the standard O(n) merge algorithm
     * for combining two sorted lists.
     *
     * @param a Object to combine with sorted Collection b.  Must implement Comparable.
     * @param b Sorted Collection to combine with Object a.
     * @param c Comparator by which Collection a and Collection b have been sorted, or null
     *          if the Collections are sorted according to their natural ordering.
     * @return a new sorted ArrayList
     */
    public static ArrayList merge(Collection a, Collection b, Comparator c);
",Introduce SortedUtils.merge() for merging sorted collections,1,1,,juliusdavies,True,,juliusdavies
commons-collections,COLLECTIONS-287,2008-03-13T21:21:54.000+0000,2008-03-19T06:59:04.000+0000,2013-02-28T19:52:39.000+0000,,Won't Fix,New Feature,Minor,,['3.2'],,,,,,,['Collection'],[''],It would be nice to retrieve the size of collections and not care whether the collection is null -- returning 0 otherwise. ,Null-safe Collections.size,,,,pbenedict,True,,pbenedict
commons-collections,COLLECTIONS-286,2008-02-19T15:52:00.000+0000,2013-02-28T19:33:36.000+0000,2014-11-09T14:31:55.000+0000,,Fixed,New Feature,Major,"['4.0-alpha1', '4.0']",,,,,,,,,,"    public <T> T getSingleton(List<T> singletonList)
    {
        if (singletonList.size() != 1)
        {
            throw new IllegalArgumentException(""The singletonList size (""
                    + singletonList.size() + "") should be 1."");
        }
        return singletonList.get(0);
    }",New util method: CollectionUtils.extractSingleton(Collection<T> coll) which returns the single object in the collection or throws an exception,1,1,,ge0ffrey,True,,ge0ffrey
commons-collections,COLLECTIONS-283,2008-01-28T03:31:21.000+0000,2013-02-24T21:24:36.000+0000,2015-11-27T21:47:21.000+0000,,Implemented,New Feature,Major,,,,,,,,,['Map'],[''],The HashedMap class lacks a method to do a deep clone() of the Object. This patch adds it and adds a test to make sure it works.,HashedMap deepClone,,,,hdiwan,True,,hdiwan
commons-collections,COLLECTIONS-275,2007-11-08T11:53:44.000+0000,2013-03-02T12:58:52.000+0000,2014-11-09T14:31:29.000+0000,,Fixed,New Feature,Major,"['4.0-alpha1', '4.0']",,,,,,,,['Collection'],[''],"I propose that an IndexedCollection is added, which has the following properties

* IndexedCollection<K,C> implements Collection<C>, Map<K,C>
* Contains a Transformer<C,K> for determining the keys
* Presents as an unmodifiableMap (all manipulation must be done through the collection)
* It will need constructors for unique indexes (Map) and non-unique (Map<K,Collection<V>> / MultiMap).  
",IndexedCollection would be a useful addition to Commons Collections,,,,shammah,True,shammah,shammah
commons-collections,COLLECTIONS-270,2007-10-08T09:39:53.000+0000,2014-10-29T21:11:51.000+0000,2015-11-27T21:47:15.000+0000,,Won't Fix,New Feature,Major,,['3.2'],,,,,,,['List'],[''],"Here is a new List implementation named BackedList, to use for transparently accessing values of a generic data source as it were a normal List.
This List implementation dynamically accesses data source values through a proper strategy object, keeping in memory only a limited, configurable, buffer of values.
So, it is useful for:
1) Providing a List-like view of different data source values.
2) Avoiding to pre-load all values in a standard list, providing better performances and less memory consumption.

As a side note, this list implementation is already used in production for implementing pagination over a relational data source.",Proposal for a new List implementation: BackedList,,,,sbtourist,True,,sbtourist
commons-collections,COLLECTIONS-263,2007-08-29T22:04:46.000+0000,2013-04-25T20:39:28.000+0000,2014-11-09T14:31:41.000+0000,,Fixed,New Feature,Minor,"['4.0-alpha1', '4.0']","['3.1', '3.2']",,,,,,,"['KeyValue', 'Map']","['', '']","I purpose extending the MultiHashMap to create an object filter which will filter a given collection of objects by a given field value.

For example:

I have a collection of 5 objects of class X. X has an int field i as shown below:

x1 i = 1
x2 i = 2
x3 i = 2
x4 i = 5
x5 i = 5

The extended MultiHashMap will filter those objects by the field i and store each sorted object into the map using the value of the field, i, as the key, such that the resulting MultiHashMap looks as follows:

key | values
1     | x1
2     | x2, x3
5     | x4, x5

I have a class which does this and I find it invaluable in my day to day work. I don't Collections has a similar one.

Kind regards,

John.",Extend the MultiHashMap to create an object filter by value of given field,2,1,,johnhunsley,True,,johnhunsley
commons-collections,COLLECTIONS-258,2007-06-26T14:39:20.000+0000,2013-02-28T20:59:34.000+0000,2014-11-09T14:31:48.000+0000,,Fixed,New Feature,Trivial,"['4.0-alpha1', '4.0']",,,,,,,,['BidiMap'],[''],"I recently needed a BidiMap that also maintained an insertion iteration order.  I didn't find one in Commons Collections, but I did find a good framework for the creation of one.

I basically took the code from DualHashBidiMap and changed all the references from HashMap to LinkedHashMap.  This made it trivial to create.

Thanks for the great package!
",DualLinkedHashBidiMap,,,,panchonb,True,,panchonb
commons-collections,COLLECTIONS-248,2007-04-16T14:13:17.000+0000,2015-05-24T11:08:31.000+0000,2015-11-27T21:47:21.000+0000,,Won't Fix,New Feature,Major,,,,,,,,,['Collection'],[''],"Sometimes you need to keep track of what was added and what was removed
from a collection, and that is what we created TrackingCollection for.

Additionally, since we were implementing lazy-loading of collections of
persistent objects, we introduced a CollectionLoader that is used by the
TrackingCollection when it has to access all of its elements. It is
still possible however to use the TrackingCollection without a loader,
and it will simply start empty and be considered loaded.

    Sherpa Solutions is offering the Apache Software Foundation the
    attached source code, to be distributed by the projet under the
    Apache License terms.",TrackingCollection,1,,,elecnix,True,,elecnix
commons-collections,COLLECTIONS-241,2007-02-16T18:44:08.000+0000,2012-08-24T05:10:07.000+0000,2014-11-09T14:31:52.000+0000,,Implemented,New Feature,Minor,"['4.0-alpha1', '4.0']",,,,,,,,['Map'],[''],"This is a Map decorator which passively evicts expired keys once their expiry time has been reached.

When putting a key-value pair in the map, this decorator calls expiryTime(key, value), passing the key and the value as parameters, and uses the returned value as the expiry time for that key.

When getting the value for a key, its expiry time is checked, and if it's greater than the current time, the value is returned. Otherwise, the key is removed from the decorated map, and null is returned.

Doing so, there's no need to have a separate, active thread (hence the name 'passive') to check expiry times - the check is performed on demand.
",[contribution] PassiveTimeOutMap,2,,,elifarley@yahoo.com,True,brentworden,elifarley@yahoo.com
commons-collections,COLLECTIONS-235,2006-11-20T16:49:44.000+0000,2013-02-28T19:36:43.000+0000,2014-11-09T14:31:22.000+0000,,Fixed,New Feature,Major,"['4.0-alpha1', '4.0']",['3.2'],,,,,,,,,"Similar to this method on CollectionUtils:

  public static Object find(Collection c,Predicate p)

there should be the following method on ListUtils:

  public static int indexOf(List l,Predicate p)

which returns the index of the first Object in the List where the Predicate evaluates true.","Missing static int ListUtils.indexOf(List,Predicate) method",2,,,negge,True,,negge
commons-collections,COLLECTIONS-225,2006-09-25T19:18:29.000+0000,2013-06-16T15:59:24.000+0000,2014-11-09T14:31:41.000+0000,,Fixed,New Feature,Major,"['4.0-alpha1', '4.0']",,,,,,,,['Map'],[''],"We (Roger Kapsi & I) would like to contribute a Patricia tree.  The tree implements the Map & SortedMap interface, meaning it can be used as a replacement for any arbitrary map.  It also implementes a new 'Trie' interface, allowing other implementations or other varieties of Tries to be added.  The tree is currently written for generics, but that can easily be removed.  We have used the tree as the structure backing a route table in a new Kademlia-based DHT, as the structure backing an IP filter (storing IP addresses & IP ranges, allowing retrieval/searching in nanoseconds), and have tested it with Strings by storing all of 'hamlet' and comparing it against a TreeSet.  The tree is also ready to implement NavigableMap whenever Java 1.6 becomes available.

I will attach the files in an update to this issue",Contribution: A Patricia Tree,6,9,,sberlin,True,,sberlin
commons-collections,COLLECTIONS-224,2006-09-25T18:55:13.000+0000,2013-04-17T18:37:03.000+0000,2015-11-27T21:47:19.000+0000,,Won't Fix,New Feature,Major,,,,,,,,,['Buffer'],[''],"Patch contains three new files:
KeyedPriorityBuffer - class based on PriorityBuffer; with additional support for fast search and remove
KeyedBuffer - general interface for all buffers containing key-identified elements
TestKeyedPriorityBuffer - JUnit test cases

Rationale for introducing new types: I needed a buffer with additional fast search (both get and remove) of elements. I wanted a buffer with search characteristics of a HashMap. But one cannot simply put together a Buffer with a HashMap since the Map needs to have an access to the internal buffer data structures to manage fast remove(key). 
KeyedPriorityBuffer is mixin of a PriorityBuffer with a internal HashMap managing fast search operations. 
KeyedBuffer is a general interface for all such buffers supporting key operations. It extends Map rather than Collection since imho it is more natural.

",[PATCH] Proposal for a new collection type KeyedBuffer,1,,,mkrzemien,True,,mkrzemien
commons-collections,COLLECTIONS-8,2006-05-14T22:38:32.000+0000,2010-10-15T20:46:59.000+0000,2014-11-09T14:31:36.000+0000,,Fixed,New Feature,Minor,"['4.0-alpha1', '4.0']",['3.1'],,,,,,,['Functor'],[''],"A predicate that makes use of a Comparator object for evaluation is, in my 
opinion, missing in the functors package.",Comparator Predicate,1,2,,runepeter@online.no,True,brentworden,runepeter@online.no
commons-collections,COLLECTIONS-181,2004-12-23T00:49:43.000+0000,,2015-06-08T21:38:50.000+0000,,,New Feature,Minor,['4.x'],['3.1'],,,,,,,['Map'],[''],"When I was implementing a new part of our large system I needed a sorted map,
where keys and corresponding values could also be accessed directly by index (as
in array or List). Both keyed and indexed access methods had to be efficient,
while insertions or removals needn't to be.
I was amazed to find absolutely no ready-to-use implementations, that's why I
decided to create my own.
I created a sorted map, which uses array as a backend storage. I borrowed some
internal implementation ideas from java.util.ArrayList, java.util.TreeMap and
org.apache.commons.collections.map.AbstractHashedMap and LinkedMap.
See attachment.
I hope you will find the idea useful.",Provide maps with direct indexed access to the entries [PATCH],,1,,sergei_ivanov@vpost.ru,True,,sergei_ivanov@vpost.ru
commons-collections,COLLECTIONS-152,2004-07-18T01:54:38.000+0000,,2008-04-02T06:16:16.000+0000,,,New Feature,Minor,['4.x'],,,,,,,,['BidiMap'],[''],"I've implemented SynchronizedBidiMap which was listed in the task list for
Commons Collections. Implementing this required that I also implement
SynchronizedMap. As usual, I tried to keep the format of the code similar to
other code in Commons Collections. I've also included JUnit tests for both of
these new classes and have included the .obj files suitable for placement in
data/test so you don't have to bother generating them yourself.","Implementation of SynchronizedBidiMap and SynchronizedMap, with tests",,1,,ccesc@eonomine.com,True,,ccesc@eonomine.com
commons-collections,COLLECTIONS-165,2004-06-28T03:35:35.000+0000,2013-01-26T20:35:14.000+0000,2015-11-27T21:47:18.000+0000,,Implemented,New Feature,Minor,,['3.1'],,,,,,,"['List', 'Map', 'Set']","['', '', '']","In the project I work I have found that it would be efficient (at a small cost
of memory) to use filtered maps, lists, and sets instead of creating a new copy
of the decorated entities that contains only the filtered elements.

To begin with I have implemented a FilteredMap and plan to implement
FilteredList and FilteredSet.  I will post the code for these in a couple of
days and it would be great if Collections library could accept these contributions.","Filtered Maps, Lists, and Sets",1,1,,comfortably007numb-python@yahoo.com,True,,comfortably007numb-python@yahoo.com
commons-collections,COLLECTIONS-196,2003-08-31T23:07:13.000+0000,2006-07-19T22:16:03.000+0000,2006-07-19T22:16:03.000+0000,,Won't Fix,New Feature,Minor,,,,,,,,,['Set'],[''],An AVL Tree/Set implementation submitted to the list by Dieter Wimberger.,AVLTree/Set,,,,scolebourne@joda.org,True,,scolebourne@joda.org
commons-configuration,CONFIGURATION-745,2019-05-21T00:16:27.000+0000,2019-05-21T21:52:33.000+0000,2019-05-21T21:52:33.000+0000,,Fixed,New Feature,Major,['2.5'],,1200,1200,,,,100,,,"Add the special key ""includeoptional"" for properties files.

If the file is missing, the rest of the file is processed normally,","Add the special key ""includeoptional"" for properties files.",2,1,,ggregory,True,ggregory,ggregory
commons-configuration,CONFIGURATION-704,2018-05-30T08:43:00.000+0000,,2018-06-05T20:04:45.000+0000,,,New Feature,Major,,['2.2'],,,,,,,,,"Hi,

 

I want an option when loading a properties file from the include functionality (or in the same file) to disable the list feature that when entering the same key it creates a list instead of override the old value.

for example when entering these properties:

colors.pie = #FF0000

colors.pie = #00FF00

colors.pie = #0000FF

instead of creating a string with seperator like this  colors.pie =#FF0000,#00FF00,#0000FF

 

i want only the last value colors.pie = #0000FF",an option to override property key instead of creation of lists,3,,,danalon,True,,danalon
commons-configuration,CONFIGURATION-676,2017-11-24T15:17:15.000+0000,2017-11-29T14:27:34.000+0000,2017-11-29T14:27:34.000+0000,,Invalid,New Feature,Major,,,,,,,,,,,"Handling configuration with Environment Variables have become more commonplace with the advent of containers and it would be very nice if it were to be supported by commons-configuration.

An example project doing this would be Spring Boot where where an configuration property such as *colors.background* could be picked up as the environment variable *COLORS_BACKGROUND*.

Would this make sense for commons-configuration?

Reference: https://docs.spring.io/spring-boot/docs/current/reference/html/boot-features-external-config.html",Configuration with OS Environment Variables,2,,,rbjorklin,True,,rbjorklin
commons-configuration,CONFIGURATION-674,2017-11-06T06:58:37.000+0000,,2017-11-06T10:37:55.000+0000,,,New Feature,Major,,,,,,,,,['File reloading'],[''],"For example, In some interface, needs we have a {{java.util.Properties}} as a parameter. In common case, we have different profiles in maven under different environments that we can random selection. And use the current classloader, usually belongs to AppclassLoader to get it's inputstream. I propose we should add a functionality that can directly read a file name and return a {{java.util.Properties}} as a return value. ",Add class FileProperties to load file name directly,1,,,mingleizhang,True,,mingleizhang
commons-configuration,CONFIGURATION-664,2017-04-20T03:11:51.000+0000,2017-04-20T03:18:17.000+0000,2017-04-25T17:14:04.000+0000,,Fixed,New Feature,Major,['2.2'],,,,,,,,,,"Add API org.apache.commons.configuration2.tree.ImmutableNode.getChildren(String):

{code:java}
    /**
     * Returns a list with the children of this node. This list cannot be
     * modified.
     * @param name the node name to find
     *
     * @return a list with the child nodes
     */
    public List<ImmutableNode> getChildren(final String name)
    {
        final List<ImmutableNode> list = new ArrayList<>();
        if (name == null) {
            return list;
        }
        for (final ImmutableNode node : children) 
        {
            if (name.equals(node.getNodeName()))
            {
                list.add(node);
            }
        }
        return list;
    }
{code}",Add API org.apache.commons.configuration2.tree.ImmutableNode.getChildren(String),2,,,ggregory,True,ggregory,ggregory
commons-configuration,CONFIGURATION-662,2017-04-10T05:44:23.000+0000,,2017-04-10T05:44:44.000+0000,,,New Feature,Major,,,,,,,,,['Type conversion'],[''],Add API org.apache.commons.configuration2.DataConfiguration.getPath(String) methods. Uses the Java 7 NIO Path class.,Add API org.apache.commons.configuration2.DataConfiguration.getPath(String) methods,1,,,ggregory,True,,ggregory
commons-configuration,CONFIGURATION-661,2017-04-10T05:43:35.000+0000,2017-04-10T21:50:34.000+0000,2017-04-10T21:50:34.000+0000,,Fixed,New Feature,Major,['2.2'],['2.1.1'],,,,,,,,,"Update platform requirement from Java 6 to 7.
",Update platform requirement from Java 6 to 7,1,,,ggregory,True,ggregory,ggregory
commons-configuration,CONFIGURATION-658,2017-04-05T03:17:35.000+0000,2017-04-10T05:42:42.000+0000,2017-04-10T05:42:42.000+0000,,Fixed,New Feature,Major,['2.2'],,,,,,,,,,"Add org.apache.commons.configuration2.DataConfiguration APIs:

- getURI(String)
- getURI(String, URI)
- getURIArray(String)
- getURIArray(String, URI[])
- getURIList(String)
- getURIList(String, List<URI>)",Add API org.apache.commons.configuration2.DataConfiguration.getURI(String) methods,1,,,ggregory,True,ggregory,ggregory
commons-configuration,CONFIGURATION-656,2017-04-04T23:57:21.000+0000,2017-06-22T17:45:16.000+0000,2017-10-12T20:23:43.000+0000,,Fixed,New Feature,Major,['2.2'],,,,,,,,['Format'],['Issues related to the implementation of a configuration format'],"Like the missing JSON Configuration, we really should also support YAML.",YAML Configuration,4,1,,ggregory,True,,ggregory
commons-configuration,CONFIGURATION-624,2016-04-27T14:38:45.000+0000,2016-05-15T17:52:55.000+0000,2016-08-21T19:39:32.000+0000,,Fixed,New Feature,Major,['2.1'],,,,,,,,,,"For Commons Configuration 1.0 a Spring modules factory bean (org.springmodules.commons.configuration.CommonsConfigurationFactoryBean) was available to directly use Commons Configuration with Spring's PropertyPlaceholderConfigurer. As this is no longer maintained, similar classes should become part of Commons Configuration 2.0 (with optional Spring dependency).

See mailinglist discussion:
http://mail-archives.apache.org/mod_mbox/commons-user/201604.mbox/%3CCACZkXPy-zQvxz98Z6GGcd8BBHKZ5cukFfgtihSDt97wnoktxyA%40mail.gmail.com%3E",Support Commons Configuration as PropertySource in Spring,3,,,deki,True,,deki
commons-configuration,CONFIGURATION-569,2014-02-14T03:24:50.000+0000,,2014-02-23T18:47:24.000+0000,,,New Feature,Trivial,,['1.9'],,,2419200,2419200,2419200,,['Format'],['Issues related to the implementation of a configuration format'],"An API to insert comments into saved to disk configurations would be nice.  That would make it possible to write a default configuration file in code for first-time use.

The comments should be attached to particular config keys so that the comments appear right before the config key/value (or node in the tree) on disk.

The goal is to have some metadata to explain what values should/could be used for a particular configuration setting.",Add API call to insert comments into save configuration,2,,,rbeede,True,,rbeede
commons-configuration,CONFIGURATION-565,2014-02-01T21:18:07.000+0000,2014-11-11T20:54:50.000+0000,2016-03-24T20:20:52.000+0000,,Fixed,New Feature,Major,['2.0'],['2.x'],,,,,,,['Expression engine'],['Issues related to the use of an expression engine to query a property'],"Hi guys,

it would be really awesome if the {{Configuration}} could be created with a passphrase (another constructor which accepts a string, let's say 'mypassphrase') and then you also add a method in the interface (let's say getEncodedString('database.password')) so we could have encoded properties in our properties files. Internally the method just needs to delegate to jasypt's  {{StandardPBEStringEncryptor}} for instance
{code}
 StandardPBEStringEncryptor encryptor = new StandardPBEStringEncryptor();
            encryptor.setPassword(""mypassphrase"");
encryptor.decrypt(""database.password"");
{code}
decrypt the value and then return the decrypted value. This will allow for more secure property files.
",Add a getEncryptedString method in the Configuration interface,2,1,,paranoiabla,True,,paranoiabla
commons-configuration,CONFIGURATION-559,2013-11-17T17:12:34.000+0000,2014-05-24T13:46:47.000+0000,2016-03-24T20:20:46.000+0000,,Fixed,New Feature,Major,['2.0'],['1.9'],,,,,,,,,"In version 1.x it was possible to set a few default values for the properties of configuration instances (e.g. the list delimiter or the expression engine for XML configurations). These settings were used if they were not overridden for a specific configuration instance. This mechanism was mainly based on static fields which is unflexible and thread-hostile.

In the new design, configuration builders are used to create configuration objects. Parameter objects can be passed to the builder allowing a fine-grained configuration of the instances to be created. Because there are many settings it would be helpful if there was a way to define certain default values. An application creating multiple configuration instances with the same settings could define the used defaults once and then reuse them.

Default settings should be possible for all parameters object supported by the different builder implementations. No static fields should be used to avoid the drawbacks of the current design.
",Support setting default values for configurations,1,,,oheger,True,,oheger
commons-configuration,CONFIGURATION-526,2013-02-19T20:06:16.000+0000,2013-02-23T19:55:50.000+0000,2013-10-28T06:54:37.000+0000,,Fixed,New Feature,Minor,"['2.0', '1.10']",,,,,,,,,,"I'm loading a complex XML document, where properties are are nested using the Java 1.5 properties XML format. This is in contrast of the issue CONFIGURATION-209, where an XML-based format has been introduced. I require following signatures:

  void load(Element element) throws ConfigurationException
  void save(Document document, Node parent)

Please find attached a patch implementing this requirement.
",Support loading from and saving to DOM nodes,2,,['patch'],koppor,True,,koppor
commons-configuration,CONFIGURATION-517,2012-11-25T17:14:52.000+0000,2012-11-26T20:53:45.000+0000,2016-03-24T20:20:50.000+0000,,Fixed,New Feature,Major,['2.0'],['1.9'],,,,,,,,,"So far, there is no direct way for a hierarchical configuration to obtain all elements contained in a given section (i.e. the child elements of a specific key). This can be achieved by operating on a {{ConfigurationNode}} object directly, but this is a low-level API, and for further processing of the element's content it might be necessary to create a {{SubNodeConfiguration}} around the node again.

Therefore, it makes sense to extend the API of {{HierarchicalConfiguration}}. A method can be added - similar to {{configurationsAt()}} - which expects a key, locates the corresponding node, and then creates {{SubNodeConfiguration}} objects for all direct children of this node.

It would also be useful to provide a way to query a configuration for the name of its root element. This makes it easier to deal with dynamic configuration structures.
",Add methods to HierarchicalConfiguration to obtain sub configurations for child elements,1,,,oheger,True,,oheger
commons-configuration,CONFIGURATION-514,2012-11-08T21:08:49.000+0000,2012-11-12T20:42:09.000+0000,2016-03-24T20:20:56.000+0000,,Fixed,New Feature,Major,['2.0'],['1.9'],,,,,,,,,"Currently it is possible to declare simple beans in configuration files. These beans are created using a standard constructor; then properties can be set. This functionality is used internally (mainly by {{DefaultConfigurationBuilder}}), but is provided to client code as well.

When the library design goes more in the direction of immutable objects it will be necessary to support constructor invocation as well. I expect that this will be required for a new version of {{DefaultConfigurationBuilder}}, so it makes sense to implement this feature.",Add support for constructor invocation in bean declarations,1,,,oheger,True,,oheger
commons-configuration,CONFIGURATION-512,2012-11-04T19:48:48.000+0000,2012-11-07T21:16:08.000+0000,2016-03-24T20:20:43.000+0000,,Fixed,New Feature,Major,['2.0'],['1.9'],,,,,,,,,"The {{Configuration}} interface allows manipulating configuration objects. There are use cases in which it is not necessary or even forbidden to change the content of a Configuration. To support such use cases, a new interface {{ImmutableConfiguration}} should be introduced which allows only querying of configuration data. In addition, it should be possible to transform a mutable configuration into an immutable one.",Add support for immutable configurations,1,,,oheger,True,,oheger
commons-configuration,CONFIGURATION-498,2012-05-20T22:15:35.000+0000,,2016-03-24T20:12:56.000+0000,,,New Feature,Major,['2.x'],['1.8'],,,,,,,['File reloading'],[''],"I'm rather new to commons-configuration, so I don't have a lot of personal experience with it, so please close this issue if it is naive or poorly matched to the goals of the project.  :-)

As part of the OSGi spec, a service called ""Configuration Admin"" is available to provide access to properties-based configuration that is stored in the container.  Providing access to that facility via commons-configuration seems like an ideal situation since it can further abstract the container that the client code is running on, now including OSGi containers such as Karaf or Felix.",Add OSGi Configuration Admin Service as a source for configuration data,2,,,topping,True,,topping
commons-configuration,CONFIGURATION-414,2010-04-02T16:08:48.000+0000,2010-04-03T14:44:22.000+0000,2010-04-03T14:44:38.000+0000,,Not A Problem,New Feature,Major,,,,,,,,,,,"Some time ago I created a contribution for jconfig which completed the work described here: http://www.jconfig.org/ConfigurationInheritance.html

Jconfig seems however to be no longer maintained and I never found a home for the code.

Basically jconfig would allow for a -D option to be specified in the JVM which would allow an application to decide which xml config file to start with per environment and at that point inheritance would take over.  In each config file it supported a 2 level hierarchical structure. Therefore you can group certain properties into categories. There is one default category called ""general"".

so how it would work is as follows:
you ask for property ""myapplication:theproperty"" first it would look in the first config file pointed to from the -D option and see if a category ""myapplication"" and a property within ""theproperty"" was defined.. If not it would follow the extends xml config to determine the same for that category and property and so on and so forth.. In addition if after looking specifically for the category property combination it is not resolved then it would look for the same property in the general category once again starting with the first config and following the extends.

In addition if a property value contained a place holder i:e ${mycategory2:myproperty2} it would resolve that using the same hierarchical procedure before returning that value in the config.

The usecase here of this in practice is as follows...

On the app server classpath we maintained a config xml file that contained the general category for storing properties that were specific to environment vs application.
This xml config would extend app config file in the application itself

the app config file in the application has a category that was specific to the application i.e: myapplication and properties that were application specific
this file then extended another library config file which kept default properties that were specific to frameworks in the general category

so hence properties for app, framework or libraries can be defined or overridden at any level. In the environment, in the application or in the global library configuration

It lead to great flexibility..

Is there anything quite like this is Commons Configuration and if not how likely would you be to work with me to get the code I have based on jconfig to port to this solution?

",Support inheritance for configuration,1,,,garpinc,True,,garpinc
commons-configuration,CONFIGURATION-380,2009-04-11T06:12:49.000+0000,2009-04-11T06:50:30.000+0000,2011-09-08T09:20:13.000+0000,,Fixed,New Feature,Major,['1.7'],['1.6'],,,,,,,['Expression engine'],['Issues related to the use of an expression engine to query a property'],"We have a need to use String manipulation functions such as are provided by commons lang StringUtils on values in DefaultConfigurationBuilder's configuration file. For example, we use the MDCStrLookup provided by SLF4J and need to call

right(${mdc:myKey}, 2) in a file pattern.",Allow expressions during lookup processing.,,,,rgoers,True,rgoers,rgoers
commons-configuration,CONFIGURATION-371,2009-03-13T12:22:35.000+0000,2009-03-30T19:32:06.000+0000,2011-09-08T09:20:05.000+0000,,Fixed,New Feature,Major,['1.7'],['1.6'],,,,,,,['Format'],['Issues related to the implementation of a configuration format'],"PropertiesConfiguration can read property files where there is no whitespace around the property separator (name=value). However, when the same configuration is written back to disk whitespace is always used (name = value).
Can an option be added to enable or disable using whitespace when writing property files so that the original properties file can be reproduced.",Add option to PropertiesConfiguration.PropertiesWriter to enable/disable using whitespace around = separator,,,,mark@panonet.net,True,oheger,mark@panonet.net
commons-configuration,CONFIGURATION-370,2009-03-10T23:26:57.000+0000,2009-03-21T20:21:37.000+0000,2011-09-08T09:20:14.000+0000,,Fixed,New Feature,Major,['1.7'],['1.6'],,,,,,,,,"Currently Commons Configuration does not offer support to handle non-standard property file parsing. Inparticular, you can't parse properties where the key has whitespace.

Taken from thread on mailing list:
{quote}
Howdy,

We've got legacy/shared property files for our config (i.e. can't change the property files to solve this). We're trying to use commons-config but it has difficulty dealing with our properties files as the key's contain whitespace.

Any help on how I can resolve this would be most welcomed :)

Example Property:

Welcome Message=Hello World

Cheers.
{quote}
Response:
{quote}
The current code does not seem to support whitespace in property keys.

PropertiesConfiguration uses PropertiesConfigurationLayout.load() for loading and parsing configuration files, which in turn delegates to the inner class PropertiesConfiguration.PropertiesReader. This class interprets all whitespace as end markers of the property key.

It is probably not trivial to inject your own reader for properties files. Maybe you can create a custom layout class derived from PropertiesConfigurationLayout that overrides the load() method. You would have to override PropertiesConfiguration.createLayout() to activate your new layout class.

Given these problems it would probably be a good idea if the library offered better support for customizing the properties reader. If you consider this helpful, feel free to open an enhancement ticket in Jira [1].

Oliver
{quote}
",Customizing the Properties Reader,,,,ahhughes,True,oheger,ahhughes
commons-configuration,CONFIGURATION-367,2009-02-18T19:12:18.000+0000,2009-06-19T11:27:52.000+0000,2016-03-24T20:20:55.000+0000,,Fixed,New Feature,Major,['2.0'],['1.6'],,,,,,,,,Current JNDIConfiguration implementation is readonly. It would be nice to have write access too.,Writable JNDIConfiguration,,,,vkopichenko,True,,vkopichenko
commons-configuration,CONFIGURATION-353,2008-12-18T22:03:09.000+0000,2008-12-18T22:32:09.000+0000,2009-06-26T10:10:28.000+0000,,Fixed,New Feature,Major,['1.6'],['1.6'],,,,,,,,,"SystemConfiguration allows the system properties to be accessed as a configuration and used in a combined configuration. However, no mechanism is available to set system properties from an external file. Such a facility would alleviate having to place a bunch of -D options on the command line.",Allow Commons Configuration to set system properties,,,,rgoers,True,,rgoers
commons-configuration,CONFIGURATION-350,2008-11-14T07:47:50.000+0000,2008-12-08T23:23:18.000+0000,2008-12-08T23:23:18.000+0000,,Fixed,New Feature,Major,['1.6'],['1.5'],,,,,,,,,"Commons Configuration doesn't support environments where a different configuration needs to be used for each ""client"".",Add multitenancy support,,,,rgoers,True,,rgoers
commons-configuration,CONFIGURATION-333,2008-07-09T20:30:07.000+0000,2008-07-12T15:26:30.000+0000,2016-03-24T20:20:43.000+0000,,Fixed,New Feature,Minor,['2.0'],['1.5'],,,,,,,,,"The configurationAt() method of hierarchical configurations has a supportsUpdates parameter. If set to true, the sub-configuration returned is aware of updates of its parent configuration. The configurationsAt() method should support a similar mechanism, too.

Checking for updates of the parent configuration requires the sub-configuration to be constructed with a unique key. So far it was not possible to determine such a key in the configurationsAt() method because the key passed to this method can select multiple nodes. So a way has to be implemented to determine a unique key for a given configuration node.

Having unique keys for all properties defined could be useful for other use cases, too. Maybe an extended version of the getKeys() method makes sense that returns unique keys? So each key returned by this method is guaranteed to map to exactly one node.",Support update-aware sub-configurations for the configurationsAt() method,,,,oheger,True,oheger,oheger
commons-configuration,CONFIGURATION-312,2008-02-21T22:55:54.000+0000,,2011-10-24T20:28:38.000+0000,,,New Feature,Major,['2.x'],,,,,,,,['Format'],['Issues related to the implementation of a configuration format'],"This is a RFE to implement a configuration backed by the windows registry.

It can be implemented using an external library such as the ICE JNI Registry :
http://www.trustice.com/java/jnireg

It could also be implemented by hacking around the private WindowsPreferences class from the JDK (with no guarantee it will work on later versions, on other VMs, or on environments with a SecurityManager). Here is an example using this approach and displaying the user's data directory :

{code:java}
Preferences winPrefs = Preferences.userRoot();
Class cls = winPrefs.getClass();

Method openKey = cls.getDeclaredMethod(""openKey"", byte[].class, int.class, int.class);
openKey.setAccessible(true);
Method closeKey = cls.getDeclaredMethod(""closeKey"", int.class);
closeKey.setAccessible(true);
Method winRegQueryValue = cls.getDeclaredMethod(""WindowsRegQueryValueEx"", int.class, byte[].class);
winRegQueryValue.setAccessible(true);
Method winRegEnumValue = cls.getDeclaredMethod(""WindowsRegEnumValue1"", int.class, int.class, int.class);
winRegEnumValue.setAccessible(true);
Method winRegQueryInfo = cls.getDeclaredMethod(""WindowsRegQueryInfoKey1"", int.class);
winRegQueryInfo.setAccessible(true);

Integer keyHandle = (Integer) openKey.invoke(winPrefs, toByteArray(""Volatile Environment""), KEY_READ, KEY_READ);

byte[] array = (byte[]) winRegQueryValue.invoke(winPrefs, keyHandle, toByteArray(""APPDATA""));
closeKey.invoke(winPrefs, keyHandle);

System.out.println(new String(array));
{code}
",WindowsConfiguration,,,,ebourg,True,,ebourg
commons-configuration,CONFIGURATION-307,2008-02-04T16:23:48.000+0000,2008-06-01T20:24:45.000+0000,2009-08-22T19:36:51.000+0000,,Fixed,New Feature,Minor,['1.6'],['1.5'],,,,,,,,,With XMLConfiguration it is currently not possible to control the treatment of whitespace in XML elements. The XML specification defines the xml:space attribute for this purpose (http://www.w3.org/TR/REC-xml/#sec-white-space). XMLConfiguration should evaluate this attribute and avoid any trim operations on an element's content if it is present.,XMLConfiguration should support the xml:space attribute,1,,,oheger,True,oheger,oheger
commons-configuration,CONFIGURATION-290,2007-08-08T13:05:48.000+0000,2007-09-22T19:14:04.000+0000,2009-08-22T19:36:50.000+0000,,Fixed,New Feature,Major,['1.5'],['1.4'],,,,,,,,,"When trying to parse a XMLConfiguration from behind a firewall. Configuration will quit due to a refused connection when trying to resolve a Public ID to an external entity. (web.xml DTD in this case)

With Digester I'd know what to to, simply register() a local copy of the file in question)

It would be nice to do the same for Configuration wouldn't it?
(It would save me from writing my own EntityResolver)",Ability to register() Entity URLs for Public IDs like in Digester,,,,kkiefer,True,oheger,kkiefer
commons-configuration,CONFIGURATION-284,2007-07-06T06:45:37.000+0000,2007-10-23T20:11:14.000+0000,2009-10-06T04:56:21.000+0000,,Fixed,New Feature,Minor,['1.5'],,,,,,,,['Interpolation'],[''],"suggestion:
interpolation with system environment, i.e.
${sys:user.name} reads from systemproperties (as of today)
${sysenv:TEMP} reads from OS environment

these can be parsed from a supported set of OS's - windows read from a set in a ""cmd /c"", unix/linux from an ""env"" etc. (just once on demand)",ability to read OS environment variables,2,1,,aksel.schmidt,True,oheger,aksel.schmidt
commons-configuration,CONFIGURATION-276,2007-05-26T16:16:07.000+0000,2008-04-19T15:20:37.000+0000,2016-03-24T20:21:00.000+0000,,Fixed,New Feature,Minor,['2.0'],['1.4'],,,,,,,['Format'],['Issues related to the implementation of a configuration format'],"Hi,

I have read that Commons Configuration is intended to be compatible with Java 1.3 but would it be possible to create a configuration backend for the Preferences API provided since Java 1.4?

In my project I would like to use a very low-overhead configuration mechanism as is the Preferences API while still having the flexibility and power offered by Commons Configuration. In fact, being able to swap in a config file for testing purposes without changing the code would be wonderful. However, I do not like the idea of providing the end user with such a file or even a database, changing application settings should be done through the UI.

If it is not possible to add a preferences backend to the code base due to the backward compatibility policy, could such a backend then possibly be provided in another branch or in a subproject?

Best regards,
Michael",java.util.prefs Configuration,1,,,javaprog,True,oheger,javaprog
commons-configuration,CONFIGURATION-273,2007-05-23T17:20:46.000+0000,2007-10-27T18:36:48.000+0000,2009-08-22T19:36:49.000+0000,,Fixed,New Feature,Major,['1.5'],['1.4'],,,,,,,,,"It will be very nice if you'll add the ability to save a configuration file with the interpolation data.
so if my config file is :
my_home=127.0.0.1
my_place=Is ${my_home}

when I save the configuration (let's say with save(true)) it will look like
my_home=127.0.0.1
my_place=Is 127.0.0.1

Thank you!",Saving with interpolation,,,,paradoxpixel,True,oheger,paradoxpixel
commons-configuration,CONFIGURATION-271,2007-05-22T16:32:21.000+0000,,2016-03-24T20:13:01.000+0000,,,New Feature,Minor,['2.x'],['1.4'],,,,,,,,,"In addPropertyDirect(String key, Object value) the method adds the new value to the property.
If the property has the same value in the list, it will get duplicated.
The method should check if the list contains the value and only if the result is false add the value. 
There is no logic in saving a multi value key with more than one instance of a value.
",BaseConfiguration duplicates multi value keys values,,,,paradoxpixel,True,,paradoxpixel
commons-configuration,CONFIGURATION-262,2007-04-10T15:10:02.000+0000,,2016-08-10T23:40:22.000+0000,,,New Feature,Minor,['2.x'],,,,,,,,['Format'],['Issues related to the implementation of a configuration format'],"With OS X 10.4 Apple introduced a variant of the plist format that is compressed using a binary format. That would be nice to support this format with a BinaryPropertyListConfiguration class.

I haven't found the specification of this format. 

The plutil tool can be used to convert between xml and binary plist files.",Binary property list format,2,2,,ebourg,True,,ebourg
commons-configuration,CONFIGURATION-258,2007-03-10T11:20:00.000+0000,2017-06-22T17:47:25.000+0000,2017-10-12T20:23:41.000+0000,,Fixed,New Feature,Minor,['2.2'],['1.3'],,,,,,,['Format'],['Issues related to the implementation of a configuration format'],"JSON  is a lightweight data-interchange format

{code}
{""menu"": {
  ""id"": ""file"",
  ""value"": ""File"",
  ""popup"": {
    ""menuitem"": [
      {""value"": ""New"", ""onclick"": ""CreateNewDoc()""},
      {""value"": ""Open"", ""onclick"": ""OpenDoc()""},
      {""value"": ""Close"", ""onclick"": ""CloseDoc()""}
    ]
  }
}}
{code}

All references can be located at
http://www.json.org/",JSON configuration,7,2,,taorg,True,,taorg
commons-configuration,CONFIGURATION-247,2007-01-03T13:21:59.000+0000,2007-01-08T10:46:43.000+0000,2007-01-08T10:46:43.000+0000,,Won't Fix,New Feature,Major,,,,,,,,,,,"Properties-Files included in other properties-Files should be recognized by the event-listener / reloading-strategie set on the main properties-File. This is necessary  in case of modifications in these included Files.

for instance:

main.properties
include=sample.properties
....

sample.properties
mail.address=info@localhost
...

when i change the value of mail.address i definitly need a restart of my application.
",recognize changes in included Propertiefiles,,,,mirkowolf,True,,mirkowolf
commons-configuration,CONFIGURATION-237,2006-11-07T10:13:42.000+0000,2007-02-13T20:57:05.000+0000,2009-08-22T19:36:46.000+0000,,Fixed,New Feature,Minor,['1.4'],['Nightly Builds'],,,,,,,,,"This patch adds a reloading strategy based on management request, typically from a JMX console. The Strategy implements a MBean interface so can be exported as a JMX Managed bean with no code change.",Contrib : managed reloading strategy,,,,nicolas de loof,True,,nicolas de loof
commons-configuration,CONFIGURATION-236,2006-10-31T22:42:57.000+0000,2006-11-21T18:01:47.000+0000,2009-08-22T19:36:46.000+0000,,Fixed,New Feature,Major,['1.4'],['1.3'],,,,,,,,,"I have a CombinedConfiguration created from two XmlConfiguration objects. I want to output the combined XML of the CombinedConfiguration, but this doesn't currently seem possible.

The CombinedConfiguration, through the NodeCombiner, seems to merge the hierarchy internally in any case, so I don't really see massive problem with outputting it as an XML. Logically the test would be:


String xml = CombinedConfiguration.toXml();
CombinedConfiguration == XmlConfiguration(xml)

So the that outputted XML of the CombinedConfiguration would be logically the same as the CombinedConfiguration. 
",Outputting CombinedConfiguration as XML,1,,,wanderingwalrus,True,oheger,wanderingwalrus
commons-configuration,CONFIGURATION-218,2006-06-28T11:57:39.000+0000,2006-07-09T23:03:26.000+0000,2018-09-20T18:44:33.000+0000,,Duplicate,New Feature,Major,,['1.2'],,,,,,,,,"It would be cool to retain all formatting of the original properties file when writing back some new entry. If properties file is large and have many commented entries which are there as a remainder of all possible options that can be set, those commented entries should noot be deleted when saving configuration.

As mentioned in http://issues.apache.org/jira/browse/CONFIGURATION-170, a formatter class of some kind would be the solution.",Formatter for properties file,1,,,borutb,True,,borutb
commons-configuration,CONFIGURATION-164,2005-07-22T00:34:00.000+0000,,2008-02-18T10:00:28.000+0000,,,New Feature,Minor,['2.x'],['Nightly Builds'],,,,,,,,,"Detecting automatically the charset of the file based configurations would be a
nice addition. When the file has no byte order mark defining the charset, we
might apply a detection algorithm similar to the one implemented in Mozilla.
There is at least one Java library providing this feature, jchardet :

http://sourceforge.net/projects/jchardet",Charset detection,,,,ebourg,True,,ebourg
commons-configuration,CONFIGURATION-211,2005-03-25T01:15:10.000+0000,,2013-10-21T23:27:01.000+0000,,,New Feature,Minor,,['Nightly Builds'],,,,,,,['Format'],['Issues related to the implementation of a configuration format'],"I implemented a simple configuration taking the properties from the command
line. I don't know if it is worth adding in the main code base, but I'm
submitting it here if ever someone find it useful. Comments are welcome :)",CommandLineConfiguration,,,,ebourg,True,,ebourg
commons-configuration,CONFIGURATION-201,2004-12-11T00:54:00.000+0000,,2008-04-30T11:15:27.000+0000,,,New Feature,Minor,['2.x'],,,,,,,,['Format'],['Issues related to the implementation of a configuration format'],"This is a RFE to implement a file configuration based on the YAML format:

Example file:

{code}
  --- !clarkevans.com/^invoice
  invoice: 34843
  date   : 2001-01-23
  bill-to: &id001
      given  : Chris
      family : Dumars
      address:
          lines: |
              458 Walkman Dr.
              Suite #292
          city    : Royal Oak
          state   : MI
          postal  : 48046
  ship-to: *id001
  product:
      - sku         : BL394D
        quantity    : 4
        description : Basketball
        price       : 450.00
      - sku         : BL4438H
        quantity    : 1
        description : Super Hoop
        price       : 2392.00
  tax  : 251.42
  total: 4443.52
  comments: >
      Late afternoon is best.
      Backup contact is Nancy
      Billsmer @ 338-4338.
{code}

Reference:
http://www.yaml.org/
http://www.yaml.org/spec
http://www.yaml.org/refcard.html

Java implementations:
http://helide.com/g/yaml/
http://homepages.ihug.com.au/~zenaan/zenaan/files/",YAML configuration,1,3,,ebourg,True,,ebourg
commons-configuration,CONFIGURATION-198,2004-12-11T00:45:35.000+0000,,2008-04-30T11:15:41.000+0000,,,New Feature,Minor,['2.x'],,,,,,,,['Format'],['Issues related to the implementation of a configuration format'],"This is a RFE to implement a file configuration based on the OGDL format.

Example file:

{code}
network
  eth0
    ip   192.168.0.10
    mask 255.255.255.0
    gw   192.168.0.1

hostname crispin
{code}

Reference:
http://ogdl.sourceforge.net/
http://ogdl.sourceforge.net/spec/",OGDL configuration,,,,ebourg,True,,ebourg
commons-daemon,DAEMON-321,2014-07-10T20:16:44.000+0000,2014-07-12T14:31:00.000+0000,2017-11-23T09:48:27.000+0000,,Duplicate,New Feature,Major,['1.1.0'],['1.0.15'],,,300,300,300,,['Jsvc'],['Unix java service wrapper'],"We just need to add 5 lines into the /commons-daemon-1.0.16-src/src/native/unix/support/apsupport.m4 file:

s390x)
    CFLAGS=""$CFLAGS -DCPU=\\\""s390x\\\""""
    supported_os=""s390x""
    HOST_CPU=s390x
    ;;

Preferably right after the s390 platform.   ",Support the zLinux system,4,,['features'],jianle4att,True,,jianle4att
commons-daemon,DAEMON-314,2014-02-09T12:49:48.000+0000,,2014-02-09T13:15:12.000+0000,,,New Feature,Major,,,,,,,,,['Jsvc'],['Unix java service wrapper'],"I have been used Apache Commons Daemon, and jsvc in product.
If we stop processes with ""jsvc stop"", sometimes process doesn't shutdown.

I think it's more beautiful to let parent process to force shutdown child process than kill parent process and child process manually.

Does it make sense and is it possible?",[daemon] feature request: jsvc stop with force option,2,,,kabhwan,True,,kabhwan
commons-daemon,DAEMON-303,2013-08-12T19:32:47.000+0000,2019-06-13T10:08:13.000+0000,2019-06-13T10:08:13.000+0000,,Fixed,New Feature,Minor,['1.1.1'],['1.0.15'],,,,,,,['Procrun'],['Windows native daemon'],"Since Windows Vista, ""Automatic (Delayed Start)"" has been offered as an option for starting a service.

prunsrv should provide that option","prunsrv --Startup should offer ""Automatic (Delayed Start)"" option for service startup",2,1,,russ.noseworthy,True,,russ.noseworthy
commons-daemon,DAEMON-293,2013-04-05T12:12:00.000+0000,2019-06-11T16:09:28.000+0000,2019-06-11T16:09:35.000+0000,,Won't Fix,New Feature,Minor,,,,,,,,,['Procrun'],['Windows native daemon'],"After creating a service with a basic .bat file (from the example on the site), I changed some settings with the prunmgr executable. It seems a good feature to be able to create/export a .bat file based on the configurations made in prunmgr.
I suggest generating a script over some kind of configuration file; for two reasons: if you just want to clone settings, copying registry keys will do the trick as well. And a script can be modified to request input from users, or do some automatic tests to find paths.",Export Prunmgr settings to batch script.,2,3,"['configuration', 'prunmgr']",neduz,True,,neduz
commons-daemon,DAEMON-264,2012-11-01T13:56:57.000+0000,2012-11-21T10:05:01.000+0000,2012-11-21T13:00:34.000+0000,,Fixed,New Feature,Minor,['1.0.11'],['1.0.10'],,,,,,,['Jsvc'],['Unix java service wrapper'],"It's good practice for daemons to change their current working directory to / as this ensures they don't prevent file systems being mounted/unmounted by having a cwd wherever their controlling terminal was when it spawned them.

It would be great if jsvc could chdir to / when detatching from its controlling terminal.",Change current working directory to / when daemonising,4,,,dtn-cfl,True,,dtn-cfl
commons-daemon,DAEMON-262,2012-10-16T09:53:56.000+0000,2012-11-21T12:15:18.000+0000,2012-11-21T12:15:18.000+0000,,Fixed,New Feature,Minor,['1.0.11'],['1.0.10'],,,,,,,['Procrun'],['Windows native daemon'],"The service configuration window needs a refresh button (or a background thread to automatically refresh) to show the correct start of the service.
If the service is started or stopped by some other tool, or the service doesn't stop immediately when the stop button is pressed, then the ""Start""/""Stop""/""Pause""/""Restart"" button states can end up all greyed out.  The only way to re-enable them is to exit and restart.  A refresh button would query the state of the running service and update the button states correctly (and service status).",Service configuration dialog needs a refresh button,2,,,ibeaumont,True,mturk@apache.org,ibeaumont
commons-daemon,DAEMON-245,2012-03-03T23:00:30.000+0000,2012-10-22T07:07:44.000+0000,2012-12-01T23:51:57.000+0000,,Fixed,New Feature,Major,['1.0.11'],['1.0.10'],,,,,,,['Procrun'],['Windows native daemon'],"Windows introduced User Access Control which limits things users can do even if they are logged in as administrators. Management of services is one of those things. Usually the recommendation was to switch the UAC feature off, but it is not to hard to support UAC for procrun.

Any binary can include information about a needed privilege escalation. If you start such a binary, the usualy Windows confirmation popup of UAC will show and you can confirm the operation.

To support this, the manifest included in the exe files need to be enhanced. The following is an example for prunsrv:

<?xml version=""1.0"" encoding=""UTF-8"" standalone=""yes""?>
<assembly xmlns=""urn:schemas-microsoft-com:asm.v1"" manifestVersion=""1.0"">
<assemblyIdentity version=""3.1.0.0"" processorArchitecture=""*"" name=""Apache.Procrun.Prunsrv"" type=""win32""></assemblyIdentity>
<description>Apache Procrun Service Manager</description>
<dependency>
<dependentAssembly>
<assemblyIdentity type=""win32"" name=""Microsoft.Windows.Common-Controls"" version=""6.0.0.0"" processorArchitecture=""*"" publicKeyToken=""6595b64144ccf1df"" language=""*""></assemblyIdentity>
</dependentAssembly>
</dependency>
<trustInfo xmlns=""urn:schemas-microsoft-com:asm.v3"">
<security>
<requestedPrivileges>
<requestedExecutionLevel level=""requireAdministrator"" uiAccess=""false""></requestedExecutionLevel>
</requestedPrivileges>
</security>
</trustInfo>
</assembly>

I used this manifest and the same one with the string ""Prunsrv"" replaced by ""Prunmgr"" successfully on Windows 7.

I do not know, whether such an extended manifest has negative cnsequences on older Windows.

If accepted, the manifest should be added to both binaries. Currently only one of them contains a manifest.

Regards,

Rainer
",Support privilege escalation on Windows (UAC),3,2,,rainer.jung@kippdata.de,True,,rainer.jung@kippdata.de
commons-daemon,DAEMON-243,2012-03-01T16:25:11.000+0000,2019-06-12T17:55:43.000+0000,2019-07-04T13:11:12.000+0000,,Fixed,New Feature,Minor,['1.1.0'],['1.0.10'],1800,1800,,,,100,['Procrun'],['Windows native daemon'],"The MS Windows Service Controller provides functionality to perform some basic failure recovery actions: Restart service or execute any command or restart the machine. This can be configured using the Service Controller GUI or in a shell with SC.EXE (commands FAILURE and FAILUREFLAG).

It would be convenient if this could be configured also using procrun.",Support for Failure Recovery,4,2,,peter.ehrbar,True,,peter.ehrbar
commons-daemon,DAEMON-216,2011-09-06T19:06:43.000+0000,2011-11-07T15:55:37.000+0000,2011-11-07T15:55:37.000+0000,,Fixed,New Feature,Minor,['1.0.8'],,,,,,,,['Procrun'],['Windows native daemon'],"I have a Java Service and it is creating a tasktray. This only works with the switch ""--type interactive"" but its creating a not needed console window to fetch user input.
I wish a switch to hide the unwanted window.
Thank you for your efforts.
Kurt","Add command line switch to hide console window when switch ""--type interactive"" is used.",,1,,jaku1,True,,jaku1
commons-daemon,DAEMON-209,2011-06-17T12:42:20.000+0000,2011-06-21T06:32:13.000+0000,2011-06-21T06:32:13.000+0000,,Fixed,New Feature,Major,['1.0.6'],['1.0.5'],,,172800,172800,172800,,['Procrun'],['Windows native daemon'],"Starting from Windows XP SP1 Windows has a new SetDllDirectory API that allow to specify the search path for LoadLibrary calls. Those calls are used by JVM to find the libraries during Runtime.loadLibrary calls. Without that the default mechanism is used which usually requires setting the system environment PATH variable to include those directories.
The new install option will allow to specify additional path that will be used to call SetDllDirectory allowing user to specify the custom location for dll loading without the need to modify the system PATH environment variable.
",Allow setting additional path for LoadLibrary,,,,mturk@apache.org,True,mturk@apache.org,mturk@apache.org
commons-daemon,DAEMON-189,2010-12-13T19:30:40.000+0000,2010-12-13T20:18:29.000+0000,2010-12-13T20:18:29.000+0000,,Won't Fix,New Feature,Major,,,,,,,,,['Procrun'],['Windows native daemon'],"Originally reported against Tomcat [1], this enhancement request is to have the service start not exit until the daemon has started. As far as I can tell, this will require changes to procrun for this to work with Tomcat - hence this enhancement request.


[1] https://issues.apache.org/bugzilla/show_bug.cgi?id=5329",Align windows service start with daemon start,,,,markt,True,,markt
commons-daemon,DAEMON-180,2010-10-20T06:08:12.000+0000,2010-10-20T06:10:00.000+0000,2010-10-20T06:10:11.000+0000,,Fixed,New Feature,Major,['1.0.4'],,,,,,,,['Jsvc'],['Unix java service wrapper'],"Add new generic wrapper that would allow to run the standard application that do not have required
Daemon methods by using a special wrapper class.
The wrapper class calls main method if the -start class is prefixed with ampersand.
eg. @foo.bar.Application will call foo.bar.Application's main(String[] args) method instead
trying to load the init(String[]) method.
Stop method inside wrapper calls System.exit, so applications should have some
mechanism (usually by using ShutdownHook) to perform a shutdown cleanup ",Allow running basic applications as daemons,,,,mturk@apache.org,True,,mturk@apache.org
commons-daemon,DAEMON-179,2010-10-20T06:02:15.000+0000,2010-10-20T06:02:51.000+0000,2010-10-20T06:02:51.000+0000,,Fixed,New Feature,Major,['1.0.4'],,,,,,,,['Procrun'],['Windows native daemon'],"Allow using java.lang.System as --ShutdownClass with mandatory exit as --ShutdownMethod
Applications that are not designed with specific init and shutdown methods cannot be run effectively without that 
Using System.exit allow any application to be run with procrun given that application itself
installs some sort of ShutdownHook that will cleanup its stuff on System.exit
",Allow using System.exit to shutdown services,,,,mturk@apache.org,True,mturk@apache.org,mturk@apache.org
commons-daemon,DAEMON-176,2010-10-02T08:56:48.000+0000,2010-10-02T09:00:08.000+0000,2010-10-02T09:00:08.000+0000,,Fixed,New Feature,Minor,['1.0.4'],['1.0.3'],,,,,,,['Jsvc'],['Unix java service wrapper'],"Create config.nice during configure stage.
This is standard way most configure systems work.
Inside configure separate CFLAGS to CFLAGS and includes so that all cmdline parameters can
be saved for future configure calls",Create config.nice on configure,,,,mturk@apache.org,True,mturk@apache.org,mturk@apache.org
commons-daemon,DAEMON-173,2010-09-14T19:08:07.000+0000,2010-10-19T19:51:32.000+0000,2010-10-19T19:51:32.000+0000,,Fixed,New Feature,Minor,['1.0.4'],['1.0.3'],,,,,,,['Procrun'],['Windows native daemon'],"--StartMode/--StopMode 'java' assumes to use JDK and expects --JavaHome to be set. It should be a common requirement to use plain JRE to run services. In this case procrun should extract binary path from registry. Same way like mode 'jvm'.
To recognize this mode StartMode/StopMode could use a new value ('jre') or JavaHome could understand a special keyword ('jre').",StartMode/StopMode 'jre' should work without need for explicite JavaHome,,,,mhagedor,True,,mhagedor
commons-daemon,DAEMON-140,2010-03-14T17:16:35.000+0000,2010-03-15T07:44:32.000+0000,2010-08-02T19:57:54.000+0000,,Fixed,New Feature,Major,['1.0.3'],['1.0.3'],,,,,,,['Jsvc'],['Unix java service wrapper'],"Add missing java6 program launcher options:
-enableassertions
-da | -disableassertions
-esa | -enablesystemassertions
-dsa | -disablesystemassertions
-showversion
",Add Java6 mission options,,,,mturk@apache.org,True,mturk@apache.org,mturk@apache.org
commons-daemon,DAEMON-138,2010-03-14T06:35:23.000+0000,2010-03-15T07:44:11.000+0000,2010-08-02T19:57:56.000+0000,,Fixed,New Feature,Major,['1.0.3'],['1.0.3'],,,,,,,['Procrun'],['Windows native daemon'],"Currently there is no way to easily determine the process id of the service file.
Add --PidFile configuration option that will store the process id in file.",Create pidfile for storing the process id.,,,,mturk@apache.org,True,mturk@apache.org,mturk@apache.org
commons-daemon,DAEMON-137,2010-03-14T06:32:47.000+0000,2010-03-15T07:43:44.000+0000,2010-08-02T19:57:55.000+0000,,Fixed,New Feature,Major,['1.0.3'],['1.0.3'],,,,,,,['Procrun'],['Windows native daemon'],Currently procrun always uses vprintf callback. Add --LogJniMessages configuration option that would allow to bypass that.,Allow to configure JNI message logging,,,,mturk@apache.org,True,mturk@apache.org,mturk@apache.org
commons-daemon,DAEMON-125,2009-06-08T16:14:43.000+0000,2010-02-06T12:30:49.000+0000,2010-02-26T16:50:30.000+0000,,Fixed,New Feature,Major,['1.0.2'],,,,,,,,,,"Is it possible to modify jsvc so that it can be compiled as a 64 bit on Mac OS X. At the moment JDK 1.6 is 64 bit only on this platform so cannot be launched by JSVC.

I am building the source using the macports.org package.

Thanks,
Paul",JDK 1.6 (64 bit) support on Mac OS X ,,,,p_d_austin,True,,p_d_austin
commons-daemon,DAEMON-104,2007-10-26T14:52:49.000+0000,2010-02-11T17:36:57.000+0000,2010-02-11T17:36:57.000+0000,,Not A Problem,New Feature,Minor,,,,,,,,,,,"Hi to all,
Is possible to add a feature to configure the system tray icon of the procrunw program?

Thank you",Cange the minimized procrunw icon,,,,mbm99,True,,mbm99
commons-daemon,DAEMON-89,2006-11-26T20:00:18.000+0000,2017-11-14T14:59:46.000+0000,2017-11-15T10:53:30.000+0000,,Won't Fix,New Feature,Minor,,['Nightly Builds'],,,,,,,['Jsvc'],['Unix java service wrapper'],"The checks for JVMs such as Kaffe or SableVM should be carried out at run-time. Currently the support for these VMs is configured at compile time, and support is conditioned on macros HAVE_KAFFEVM and HAVE_SABLEVM (see location.c and java.c). This is useless for operating systems such as Debian, which support several JVMs.

Instead, jsvc should scan for all supported VMs on the platform at run-time.

This should be a matter of simply removing the #ifdefs for the above macros, but I haven't tested yet.
",JVM detection should use run-time check,1,1,,marcusb,True,,marcusb
commons-dbcp,DBCP-546,2019-05-09T00:34:04.000+0000,2019-05-09T00:44:29.000+0000,2019-05-09T00:44:29.000+0000,,Fixed,New Feature,Major,['2.7.0'],,,,,,,,,,Avoid NPE when calling DriverAdapterCPDS.toString().,Avoid NPE when calling DriverAdapterCPDS.toString(),1,,,ggregory,True,,ggregory
commons-dbcp,DBCP-506,2018-06-16T23:14:03.000+0000,2018-06-18T21:19:38.000+0000,2018-06-18T21:19:38.000+0000,,Fixed,New Feature,Major,['2.5.0'],,,,,,,,,,"Add support for JDCB 4.2.

See also https://docs.oracle.com/javase/8/docs/technotes/guides/jdbc/jdbc_42.html",Support JDBC 4.2,1,,,ggregory,True,,ggregory
commons-dbcp,DBCP-489,2018-05-01T15:07:33.000+0000,,2018-06-10T19:11:35.000+0000,,,New Feature,Major,,,,,,,,,,,,Add ability to export/import messages from file in CLI,2,,,martyntaylor,True,,martyntaylor
commons-dbcp,DBCP-409,2013-12-24T10:04:33.000+0000,2013-12-24T10:33:44.000+0000,2015-01-03T19:31:37.000+0000,,Invalid,New Feature,Major,,,,,,,,,,,"HI,
How to create DBCP Connection pooling for Spymemcached server.

Ps: Spymemcached server is not a database.

please reply..
Thanks in advance, Sekhar
",How to create DBCP Connection Pooling for memcache server?,1,,,msekharmca@gmail.com,True,,msekharmca@gmail.com
commons-dbcp,DBCP-368,2011-10-10T22:46:38.000+0000,2014-02-04T19:26:29.000+0000,2014-05-31T04:11:36.000+0000,,Fixed,New Feature,Major,['2.0'],,,,,,,,,,"Situation: a Firebird-Classic database (each connection has independent metadata cache), in a metadata-heavy environment; many ""quick"" actions will never force the database server to load full metadata, but a few will, in our case resulting in 200MB of metadata cache in memory (takes about 3 seconds to load, visible latency to the user); any connection that has done this is ""more valuable"" to us, and we'd prefer to keep those connections in the pool, and remove idle connections which haven't incurred this cost yet and are relatively cheap to recreate later if our load increases. Doing this is preferable to forcing ALL connections to load metadata at start, as this means the pool will always have a startup cost (in time), and the server will likely use more memory than really necessary.
Proposal: add new configuration parameter; if set, run the SQL string and get back a single-column, single-row result-set containing an arbitrary ""value"" as determined by the connection; prefer to close the lowest(?)-value connections first, to get down to the configured minIdle. Would require querying each open connection, when the pool is over-full, to determine which ones are most valuable. If not set, use current algorithm (based on age?).",determine which connections to hold in pool by relative value,1,,,unordained2,True,,unordained2
commons-dbcp,DBCP-361,2011-05-16T22:02:40.000+0000,2014-03-05T11:46:37.000+0000,2014-05-31T04:10:53.000+0000,,Won't Fix,New Feature,Major,,,,,,,,,,,"It would be nice to not automatically enlist connections in a transaction.  I have found automatic enlistment can be problematic when using another transaction API such as Spring's declarative transactions (JtaTransactionManager).  It appears Spring may create a second, wrapping transaction.  With Oracle this leads to: ORA-02089: COMMIT is not allowed in a subordinate session.

E.g. see Bitronix setAutomaticEnlistingEnabled http://btm.codehaus.org/api/1.3.3/bitronix/tm/resource/common/ResourceBean.html#setAutomaticEnlistingEnabled(boolean)",BasicManagedDataSource optional transaction enlistment,,,,aaron,True,,aaron
commons-dbcp,DBCP-325,2010-02-23T14:54:55.000+0000,2013-10-30T14:55:20.000+0000,2015-02-24T03:23:10.000+0000,,Won't Fix,New Feature,Trivial,['2.0'],,,,,,,,,,"It would be nice to have a timeout that specifies how long a connection, on which close() has been called, should be held in the pool, even if this connection exceeds the maxIdle limit. This would make it possible to say something like ""I know this is one more idle connection than I configured but keep it around for just a few more seconds in case we have a second peak so we don´t need to construct the connection again"".

If this has a chance of getting into the pool, I would try to provide a patch that implements this so that, in best case, you only would have to do a code review.",Allow idle connection count to exceed maxIdle for configurable time period,,,"['connection', 'timeout']",pkranz,True,,pkranz
commons-dbcp,DBCP-297,2009-07-15T09:29:41.000+0000,2012-03-05T21:53:09.000+0000,2014-05-31T04:14:33.000+0000,,Won't Fix,New Feature,Major,['2.0'],,,,,,,,,,It would be nice to allow ciphered passwords with pluggable codec mechanisms.,Ciphered passwords,,,,jlmonteiro,True,,jlmonteiro
commons-dbcp,DBCP-292,2009-05-19T08:23:13.000+0000,2013-12-11T10:40:55.000+0000,2015-02-24T03:23:22.000+0000,,Fixed,New Feature,Minor,['2.0'],['1.2.2'],,,,,,,,,"The attached patch adds an mbean that exposes the following metrics of a BasicDataSource via jmx: numActive, numIdle, maxActive, maxIdle, minIdle & maxWait.

I have put it in a new package: org.apache.commons.dbcp.management but feel free to put it in any package you see fit.

If using Spring the bean can be configured a little something like this:
<bean id=""dataSource"" class=""org.apache.commons.dbcp.BasicDataSource"">
 ...
</bean>

<bean id=""dataSourceMBean"" class=""org.apache.commons.dbcp.management.BasicDataSourceMBean"">
	<constructor-arg ref=""dataSource""/>
</bean>

<bean id=""mbeanExporter"" class=""org.springframework.jmx.export.MBeanExporter"">
	<property name=""beans"">
		<map>
			<entry key=""DBCP:name=dataSource"" value-ref=""dataSourceMBean""/>
		</map>
	</property>
</bean>

",Adds an mbean for exposing metrics around a BasicDataSource via JMX,4,3,,dhorne,True,,dhorne
commons-dbcp,DBCP-261,2008-02-09T09:47:35.000+0000,2013-10-30T15:00:58.000+0000,2014-05-31T04:15:01.000+0000,,Won't Fix,New Feature,Major,['2.0'],,,,,,,,,,"need some injection point of custom implementation replacing pathetically inefficient ""org.apache.commons.pool.impl.GenericObjectPool"".

thanks",How can I use custom object pool?,,,,navis,True,,navis
commons-dbcp,DBCP-249,2007-11-14T18:17:00.000+0000,2014-02-25T12:26:08.000+0000,2014-05-31T04:14:10.000+0000,,Fixed,New Feature,Major,['2.0'],['1.2.2'],,,,,,,,,"Request to add support for testOnCreate, to validate connections only when they are created, much less frequent than on borrow.

This will solve issues where for example driver logs you into master instead of db on the connection string because db is not available (e.g. Sybase) and then rest of the queries fail.  This only happens on first login.  Hence validating the correct db using sql can test such a case so that borrowers are guaranteed correct db without having to testOnBorrow every time.

May also be worthwhile to support different validation queries for testing at different times.

Thanks.
Parag",Validate connection only on create.,1,,,paragmehta,True,,paragmehta
commons-dbcp,DBCP-231,2007-07-12T16:14:02.000+0000,2014-02-11T10:38:47.000+0000,2015-02-24T03:23:13.000+0000,,Won't Fix,New Feature,Minor,,['1.3'],,,,,,,,,"It would be nice if BasicDataSource could automatically ""restart"" after changing a connection property. For example, in our application, we sometimes have to change the connection url at runtime (i.e. we are connected to one database and then, under certain conditions, we switch to another database). Currently, we workaround this limitation by manually calling BasicDataSource.close() after calling BasicDataSource.setUrl().

Looking at the July 11, 2007 snapshot of the source code for BasicDataSource, it appears that the author was _starting_ to implement this feature. If you look at many of the setters such as setUrl(), setUsername() and setPassword(), you will see this line of code after the corresponding instance variable is set:

  this.restartNeeded = true;

Furthermore, there is this private restart() method (notice the comment ""not used currently""):

  /**
    * Not used currently
    */
  private void restart() {
      try {
          close();
      } catch (SQLException e) {
          log(""Could not restart DataSource, cause: "" + e.getMessage());
      }
  }

To finish implementing this, I think you would only need to add the following snippet at the very top of createDataSource():

  if (restartNeeded) {
    restart();
  }

Some users might not like this feature because it might possibly cause active connections to be killed abruptly (I'm not sure though because I haven't really looked at the implementation of close() very closely). To calm their fears, perhaps you could make this auto-restart feature optional by adding a boolean property called  ""restartable"". Then you could modify my snippet to this:

  if (restartable && restartNeeded) {
    restart();
  }

Anyway, just my two cents. I think the class is already pretty useful.",Automatic Restart of BasicDataSource After Changing Connection Properties Such as Url,1,,,cyboc,True,,cyboc
commons-dbcp,DBCP-230,2007-07-11T23:45:00.000+0000,2007-07-13T13:12:36.000+0000,2008-03-25T08:11:36.000+0000,,Fixed,New Feature,Major,['1.3'],,,,,,,,,,This patch creates an extension to the BasicDataSource which creates ManagedConnection.  This class allows easy usage of the ManagedDataSource from environments that configure via JavaBeans properties.,[DBCP] BasicManagedDataSource,,,,dain,True,,dain
commons-dbcp,DBCP-229,2007-07-09T12:19:13.000+0000,2013-07-25T13:10:32.000+0000,2014-05-31T04:13:42.000+0000,,Fixed,New Feature,Major,['2.0'],,,,,,,,,,"Lately we got the following exception
org.apache.commons.dbcp.SQLNestedException: Cannot get a connection, pool exhausted
        at
org.apache.commons.dbcp.PoolingDataSource.getConnection(PoolingDataSource.java:103)
        at
org.apache.commons.dbcp.BasicDataSource.getConnection(BasicDataSource.java:540)

The reason for that was that some piece of code opened a connection, but never closed it. Tracking the active connections (and the callers of the getConnection method) would it make it easier to find such erroneous code.
One possible approach would be to add the connection returned by BasicDataSource.getConnection together with the stacktrace in a Map holding all active connections. And removing the connection from the map during PoolableDataSource.close().
",Track callers of active connections for debugging,,,,arminha,True,,arminha
commons-dbcp,DBCP-228,2007-07-04T00:56:03.000+0000,2007-07-05T14:13:20.000+0000,2008-03-25T08:11:36.000+0000,,Fixed,New Feature,Major,,,,,,,,,,,"This patch adds support for pooling of ManagedConnections.  A managed connection is responsible for managing a database connection in a transactional environment (typically called ""Container Managed"").  A managed connection opperates like any other connection when no gloabal transaction (a.k.a. XA transaction or JTA Transaction) is in progress.  When a global transaction is active a single physical connection to the database is used by all ManagedConnections accessed in the scope of the transaction.  Connection sharing means that all data access during a transaction has a consistent view of the database.  When the global transaction is committed or rolled back the enlisted connections are committed or rolled back.  

This patch supports full XADataSources and non-XA data sources using local transaction semantics.  non-XA data sources commit and rollback as part of the transaction but are not recoverable in the case of an error because they do not implement the two-phase commit protocol.

The patch includes test cases and javadoc comments.",[dbcp] Managed Connection support,,,,dain,True,,dain
commons-dbcp,DBCP-223,2007-05-11T00:12:27.000+0000,2013-12-10T11:02:14.000+0000,2014-05-31T04:13:26.000+0000,,Fixed,New Feature,Major,['2.0'],['1.2.2'],,,,,,,,,Is there a way that the DBCP automatically alerts critical events such as db down or max # of connections reached too frequently? I was wondering if there's something written or this should be a nice feature for the DBCP. Our ops team really need to be alerted right away if the DBCP has some critical events.,Auto-alert of connection pool critical events?,,,,bliu,True,,bliu
commons-dbcp,DBCP-220,2007-05-11T00:00:20.000+0000,2007-05-11T05:11:49.000+0000,2008-03-25T08:11:36.000+0000,,Won't Fix,New Feature,Major,,['1.2.2'],,,,,,,,,"Assume the DBCP is used in a servlet environment. Is there a UI component such as a Servlet that shows the stats of the connection pool? We want the ability to keep track of number of connections being used, current number of idle and active connections, etc. We also like an easy way to dynamically change the parameters of the connection pool on the fly (e.g. max number of active connections). Just wondering if there is something that has been developed or been considered by the developers?",Is there any UI developed for the connection pool?,,,,bliu,True,,bliu
commons-dbcp,DBCP-219,2007-05-10T23:56:05.000+0000,2014-02-13T11:48:17.000+0000,2014-05-31T04:13:34.000+0000,,Fixed,New Feature,Major,['2.0'],['2.0'],,,,,,,,,"Hi:

This is really not a bug but probably a desired feature and something I am not aware of.

We use the Apache connection pool in a Servlet environment. We need the feature to kill or close a connection forcefully when it is being used by a servlet. The reason can be various: an evil servlet that holds a connection forever, db issue, network issue, etc. How can we do that? I notice that datasource.getConnection() always returns a new connection. How do we keep track of connections that are being used so we can close them if needed? Currently we can only kill one at the database side by a DBA.

Should we write a a util to keep track of connection objects that are borrowed by clients?",how to kill a connection from the connection pool without shutting down the connection pool,4,1,,bliu,True,,bliu
commons-dbcp,DBCP-204,2006-12-04T09:20:55.000+0000,2009-11-21T20:10:24.000+0000,2010-02-15T04:10:20.000+0000,,Fixed,New Feature,Minor,['1.3'],['1.2.1'],,,,,,,,,"Hello =)

I had explored DBCP source code to cook for my project need recently.
As it's said that the Database Connection Pool (DBCP) component can be used in applications where JDBC resources need to be pooled. 
""Apart from JDBC connections, this provides support for pooling Statement and PreparedStatement instances as well.""

I am curious that why it does not support for pooling CallableStatement. 
Does anybody know why the developer did not implement this furture?Are there some special consideration or some reason else?",BasicDataSource doesn't include CallableStatement Pooling,,,,welch.chen,True,psteitz,welch.chen
commons-dbcp,DBCP-203,2006-12-04T01:15:38.000+0000,2011-03-22T20:46:57.000+0000,2015-02-24T03:23:10.000+0000,,Fixed,New Feature,Minor,['1.3'],['1.2.1'],,,,,,,,,"I have an application that will be configured to use a JDBC driver at run time.  It would be helpful if I could set the ClassLoader that BasicDataSource uses to load the JDBC driver.  

I don't know what the procedure is for submitting improvements, but this is something that I could add myself. the code would look like this:

private ClassLoader driverLoader = getClass().getClassLoader();
...
public ClassLoader getDriverLoader() {
    return driverLoader();
}

public void setDriverLoader(ClassLoader newValue) {
    driverLoader = newValue;
}
...

Change the statement that in createDataSource from
    Class.forName(driverClassName);
to be
    Class.forName(driverClassName, true, driverLoader);",Want to provide a ClassLoader for BasicDataSource to use for loading a JDBC driver.,1,,,mgrand,True,markt,mgrand
commons-dbutils,DBUTILS-127,2016-02-15T18:29:42.000+0000,2016-02-17T23:38:29.000+0000,2016-02-17T23:38:29.000+0000,,Duplicate,New Feature,Major,,['1.6'],,,604800,604800,604800,,,,"Supporting iterables using batch DB queries is extremely important when you want to iterate over a large set of data without loading all of it in memory and waiting for DB to load all the query result.
This will enable cursor loading from DB.

I have written a patch to add more handlers supporting iterables and modified where appropriate to propagate the change, I have also added tests for all the new handlers.",Iterable and batch support,2,,"['features', 'patch', 'test']",mina.asham,True,,mina.asham
commons-dbutils,DBUTILS-126,2015-08-12T18:49:25.000+0000,,2016-02-17T23:41:04.000+0000,,,New Feature,Major,,['1.5'],,,,,,,,,"Currently, DbUtils allows to load all data at once into memory. In many cases, when dealing with 100 000+ records, this will cause an OOME. I have to iterate over 100 000 lines with 10 columns and retrieve subdata which are in turn indexed with Lucene.

Because I cannot iterate over the entire dataset without memory problems, I have to retrieve them pagewise. (fiddling with rownums in Oracle).

An iterator interface which streams this down from the DB to the client would solve such a problem, very much like [Spring JdbcTemplate extension that returns iterators|https://github.com/alexkasko/springjdbc-iterable]. This works very well in a Spring app downloading a table with two million records directly to the HTTP client without any buffers.",Add iterable interface approach,2,,,michael-o,True,,michael-o
commons-dbutils,DBUTILS-124,2015-04-17T20:06:05.000+0000,2015-04-20T06:34:15.000+0000,2018-04-20T19:26:46.000+0000,,Fixed,New Feature,Major,['1.7'],,,,,,,,,,"The column types and property types handled by {{BeanProcessor}} are hard coded to the processor.  We already use a common return type, so we could add a services approach using the spi built into the jdk.  This should also allow other types to be handled outside of {{commons-dbutils}}.","Introduce SPI to add more column, property handlers",2,,,thecarlhall,True,thecarlhall,thecarlhall
commons-dbutils,DBUTILS-97,2012-08-06T10:48:33.000+0000,2012-09-21T14:02:35.000+0000,2014-07-20T08:48:08.000+0000,,Fixed,New Feature,Major,['1.6'],['1.6'],,,,,,,,,"According to the DRY principle (Don't Repeat Yourself), repeating {{resultSet}} variable inside the {{ResultSetHandler#handle(ResultSet)}} over and over for each iteration can get a little tedious.
It would be helpful adding a support class, named {{AbstractResultSetHandler}}, which implicitly gives users access to {{ResultSet}}'s methods. _For example_, we could extend {{AbstractResultSetHandler}} and rewrite the mapping below:

{code}
new ResultSetHandler<Collection<Map<String, Object>>> {

    @Override
    public Collection<Map<String, Object>> handle(ResultSet rs) throws SQLException {
        Collection<Map<String, Object>> result = new LinkedList<Map<String, Object>>();

        while (rs.next()) {
            Map<String, Object> current = new HashMap<String, Object>();
            for (int i = 1; i <= rs.getMetaData().getColumnCount(); i++) {
                current.put(rs.getMetaData().getColumnName(i), rs.getObject(i));
            }

            result.add(current);
        }

        return result;
    }

}
{code}

as:

{code}
new AbstractResultSetHandler<Collection<Map<String, Object>>> {

    @Override
    protected Collection<Map<String, Object>> handle() throws SQLException {
        Collection<Map<String, Object>> result = new LinkedList<Map<String, Object>>();

        while (next()) {
            Map<String, Object> current = new HashMap<String, Object>();
            for (int i = 1; i <= getMetaData().getColumnCount(); i++) {
                current.put(getMetaData().getColumnName(i), getObject(i));
            }

            result.add(current);
        }

        return result;
    }

}
{code}
",Add an Abstract ResultSetHandler implementation in order to reduce redundant 'resultSet' variable invocation,2,,,simone.tripodi,True,simone.tripodi,simone.tripodi
commons-dbutils,DBUTILS-89,2012-04-11T09:47:16.000+0000,2015-04-14T20:24:59.000+0000,2015-04-26T16:54:41.000+0000,,Fixed,New Feature,Major,['1.7'],['1.4'],,,,,,,,,"I really miss a method in BeanProcessor that would populate an *existing* bean with data from ResultSet, eg:

BeanProcessor.populateBean(ResultSet resultSet, Object bean) .",Add method in BeanProcessor to populate an existing bean ,3,,,adeon,True,thecarlhall,adeon
commons-dbutils,DBUTILS-87,2012-02-01T06:54:02.000+0000,2012-08-08T18:54:28.000+0000,2014-07-20T08:48:08.000+0000,,Implemented,New Feature,Major,['1.6'],,,,,,,,,,It would be useful to have an insert method on QueryRunner that returns the id of the new record.,Return generated key on insert,2,,,mwanji,True,wspeirs,mwanji
commons-dbutils,DBUTILS-83,2011-11-09T17:01:37.000+0000,,2014-10-28T19:17:32.000+0000,,,New Feature,Minor,,['Nightly Builds'],,,,,,,,,"I have created a BatchUploader class in one of my project, and thought I might contribute it to DbUtils.

It is similar to DBUTILS-78, but is different. It is basically a ""producer-consumer"" queue, where you application produce rows, and BatchUploader ""consumes"" the rows and write them back to the database. DBUTILS-78 also uses multi-threading, but it doesn't have the ""producer-consumer"" queue. So you could imagine cases where you add too many rows in a batch before run executeBatch() and cause ""Java Out of Memory"". Also, BatchUploader only uses simple Thread class, so it's more compatible to other Java libraries compared to using ExecutorService/Future.

Here's how you use BatchUploader in you application:

uploader = new BatchUploader(threadName, dbConnection, sqlStmt);
uploader.start();
while (...) {
  uploader.put(...); //  here you put each row in the queue
}
uploader.accomplish();
// some other code ...
uploader.join(); // wait the uploader to finish before moving to the rest of the code.
// continue running ...


The code attached is directly copied from my project. If people here think it should go into DbUtils, I can add more javadoc and remove the specific things that were used in my project to make it more general.
","Add ""BatchUploader"" (related to DBUTILS-78 but different)",1,,,danithaca,True,,danithaca
commons-dbutils,DBUTILS-78,2011-07-29T01:20:33.000+0000,2011-09-26T21:02:18.000+0000,2011-11-10T18:07:36.000+0000,,Fixed,New Feature,Minor,['1.4'],,,,,,,,,,"I propose a new QueryRunner class, AsyncQueryRunner, which changes the return type of batch, query, and update methods. Instead of returning their respective return types, the methods would return a RunnableFuture. This would allow callers to either execute the RunnableFuture in a thread or via an CompletionService like the ExecutorCompletionService.

I have attached a first cut at this class.","Add asynchronous batch, query, and update calls",1,,,wspeirs,True,simone.tripodi,wspeirs
commons-dbutils,DBUTILS-76,2010-10-30T11:06:12.000+0000,2012-07-06T14:48:07.000+0000,2012-07-20T19:34:51.000+0000,,Fixed,New Feature,Major,['1.5'],,,,300,300,300,,,,"As a result of issue DBUTILS-69, I would like to StringColumnListHandler class was in the standard package DbUtils.",New handler StringColumnListHandler,2,,,stas.aga,True,,stas.aga
commons-dbutils,DBUTILS-67,2010-01-08T10:23:39.000+0000,2011-11-23T16:32:24.000+0000,2012-07-20T19:34:50.000+0000,,Fixed,New Feature,Major,['1.5'],['1.3'],,,,,,,,,Based on code which has been posted here a long time ago I have built a generic BeanMapHandler based on the AbstractKeyedHandler. See the patch. Modify as you think fit.,Add a BeanMapHandler,1,,,michael-o,True,,michael-o
commons-dbutils,DBUTILS-50,2009-02-25T16:58:34.000+0000,2015-04-17T01:39:51.000+0000,2015-04-26T16:53:18.000+0000,,Fixed,New Feature,Major,['1.7'],,,,,,,,,,"Using fillStatement and the new fillStatementWithBean, you can use a CallableStatement like a PreparedStatement, retrieve its ResultSet and handle it with a ResultSetHandler.  But we don't yet support registering ""out"" parameters on the CallableStatement in a convenient way and retrieving them back into an object.

DBUTILS-28 requests stored procedure support and provides a patch, but I don't like the patch.  Regardless, we really should support a few helpers like:

{code}
void registerOutParameters(CallableStatement stmt, int... sqlTypes)

Object[] getOutParameters(CallableStatement stmt)

void registerOutParameters(CallableStatement stmt, Class<?> beanClass)

<T> T getOutParameters(CallableStatement stmt, Class<T> beanClass)
{code}

You should be able to write code like this:

{code}
CallableStatement stmt = runner.prepareCall(myString);
helper.registerOutParameters(stmt, MyBean.class);
queryRunner.fillStatement(stmt, foo, bar, baz);
stmt.executeUpdate();
MyBean bean = helper.getOutParameters(stmt, MyBean.class);
{code}

Or like this:

{code}
CallableStatement stmt = runner.prepareCall(myString);
helper.registerOutParameters(stmt, TINYINT, DECIMAL);
queryRunner.fillStatement(stmt, foo, bar, baz);
stmt.executeUpdate();
Object[] result = helper.getOutParameters(stmt);
{code}","Support CallableStatement ""out"" parameters",3,1,,dfabulich,True,thecarlhall,dfabulich
commons-dbutils,DBUTILS-48,2008-12-01T22:05:32.000+0000,2009-11-03T01:00:01.000+0000,2009-11-06T14:27:49.000+0000,,Fixed,New Feature,Critical,['1.3'],,,,,,,,,,"Well, DButils was a great step forward when managing a database. But Java 5 came and added a lot of new features to the language, including generics, which I think is easy to put in place.

The base change of all this is changing the signature of ResultSetHandler:

public interface ResultSetHandler<T> {
  public T handle(ResultSet rs) throws SQLException;
}

With this code, we can easily provide directly from the QueryRunner a specific object without having to cast it.

Furthermore, we can also make the queries much more efficient, and declare only 2 method instead of three as it is currently the case:

	public <T> T query(String sql, ResultSetHandler<T> handler, Object... params) throws SQLException {
		Connection connection = this.prepareConnection();
		try {
			return this.query(connection, sql, handler, params);
		} finally {
			close(connection);
		}
	}

	public <T> T query(Connection connection, String sql, ResultSetHandler<T> handler, Object... params) throws SQLException {
		PreparedStatement statement = null;
		ResultSet resultSet = null;

		try {
			statement = this.prepareStatement(connection, sql);
			this.fillStatement(statement, params);
			resultSet = this.wrap(statement.executeQuery());
			return handler.handle(resultSet);
		} catch (SQLException e) {
			throw this.nestException(e, sql, params); // nestException creates an exception to be thrown here, instead of rethrow(e, sql, params);
		} finally {
			try {
				close(resultSet);
			} finally {
				close(statement);
			}
		}
	}

I don't know for you, but I find that code much more readable and maintainable. In fact, I already did this implementation and use it in prod without any issue, since it is only basic language adaptation.

And using it is also very easy:

Book book = queryRunner.query(""SELECT * FROM book WHERE title = ? AND author = ?"", new BeanHandler(Book.class), title, author);

instead of 

Book book = (Book)queryRunner.query(""SELECT * FROM book WHERE title = ? AND author = ?"", new BeanHandler(Book.class), new Object[]{title, author});

However, I wrote an original time estimation of 1 day, because all classes have to be adapted, especially those included in org.apache.common.dbutils.handlers.

As I estimate, it is minor, it is just a request in order to have more beautiful code, for a downside of providing a Java 1.5 compatible version of DBUtils. Maintaining this should be very rough as well, since DBUtils is not really big (but it's amazing how so little code can provide so much code improvements!)",Java 1.5 generics and varargs,,1,,ogregoire,True,,ogregoire
commons-dbutils,DBUTILS-47,2008-12-01T21:40:46.000+0000,2009-02-26T03:22:49.000+0000,2009-03-07T06:04:50.000+0000,,Won't Fix,New Feature,Minor,,['1.1'],,,,,,,,,"When using the QueryRunner, in conjunction with the ResultSetHandler interface, we got awesome results. Everything seems to be easier... except when it comes to update an object to the database.

To solve this, what I request is adding a new interface that allows to easily fill a statement with a bean, instead of a list of values (Object).

The interface should be something like this:

public interface StatementFiller {
  public void fillStatement (Statement stmt, Object bean) throws SQLException;
}

and can be called in an update() like this:

	public int update(Connection connection, String sql, StatementFiller filler, Object bean)
	        throws SQLException {
		PreparedStatement statement = null;
		try {
			statement = this.prepareStatement(connection, sql);
			filler.fillStatement(statement, bean);
			return statement.executeUpdate();
		} catch (SQLException e) {
			throw this.nestException(e, sql, bean);
		} finally {
			close(statement);
		}
	}

What is clearly the advantage of this? Simply that we can use only one QueryRunner and any amount of statement fillers, just like there exist a lot of ResultSetHandler. There is no more need to override the QueryRunner's statementFiller.

To keep backward compatibility, QueryRunner may implement that interface and be provided as the default Filler, so the current behavior is not lost.

(Sorry for not showing the actual sources of DButils: I don't have them right here, I used my tweaked version instead).","Add a StatementFiller to be able to provide a Bean for updates, instead of bean values.",,,,ogregoire,True,,ogregoire
commons-dbutils,DBUTILS-43,2008-01-13T12:02:44.000+0000,2009-02-26T03:23:07.000+0000,2009-03-07T06:04:50.000+0000,,Won't Fix,New Feature,Major,,['1.1'],,,,,,,,,"PreparedStatementProcessor class is the main point. As it is documented :

""""
PreparedStatementProcessor defines a prepared statement and execute it for each set of given parameters.

PreparedStatementProcessor could be used with the producer/consumer pattern, like the following example:
 
{code}
Object[] line = null;
CvsReader reader = ...;
PreparedStatementProcessor processor = ...;
while(reader.read(line)) {
  processor.executeUpdate(line);
}
processor.close();
{code}
""""
 
",an helper set of classes for JDBC processing with prepared statement,,,,dmdevito,True,,dmdevito
commons-digester,DIGESTER-158,2011-11-05T22:44:54.000+0000,,2012-02-10T19:16:38.000+0000,,,New Feature,Major,['3.3'],['3.2'],,,,,,,,,Implement a [http://docs.oracle.com/javase/6/docs/api/javax/annotation/processing/Processor.html] to process Digester annotations rules and generate {{RulesModule}} instances at compile time.,Use Java6 annotation processing to generate RulesModule instances at compile-time,,,,simone.tripodi,True,simone.tripodi,simone.tripodi
felix,FELIX-6131,2019-05-16T16:19:03.000+0000,2019-05-16T16:35:06.000+0000,2019-06-18T15:46:07.000+0000,,Fixed,New Feature,Major,['configadmin-1.9.16'],,,,,,,,['Configuration Admin'],['The Configuration Admin from the OSGi R7 specification (Section 104).'],"By default configurations are persisted in the file system. There are some use cases where the instance is started and never restarted. For such use cases there is no need to store the configurations in the file system, especially as all of them are hold in memory, too.
Therefore we should provide a simple in-memory persistence manager for these use cases to avoid creating unnecessary files in the file system",Add in-memory persistence manager,1,,,cziegeler,True,cziegeler,cziegeler
felix,FELIX-6129,2019-05-13T15:13:37.000+0000,2019-05-15T19:25:50.000+0000,2019-05-20T07:10:39.000+0000,,Fixed,New Feature,Major,"['healthcheck.api 2.0.2', 'healthcheck.core 2.0.6']",,,,,,,,['Health Checks'],[''],"ResultLog (and even more so FormattingResultLog) provide the ability to add entries to the health check result. By default this is not also logged to the regular log file (as usually this is consumed by the health check client). For certain health checks it can be useful to log the exact same messages to a regular log file - that way it is not necessary to duplicate logs in the HC implementation classes (today it is quite common to log-file-logging combined with HC-result-logging in one HC which bloats the code unnecessarily).

To ensure the log messages arrive synchronously in the log file the logging has to happen in [1] (and not in the HC executor that only gets to see the result once it's completed). 

The shall be implemented opt-in (to not surprise existing users) via a configuration property in the HC executor (to get that config status to ResultLog a system property can be used). The be able to fine control what health checks log messages are logged at what level, the the prefix ""healthchecks."" shall be used, e.g. ""healthchecks.com.mycorp.myplatform.mymodule.ModuleCheck"". That way it is easy to generate log file with all health check messages or with health check messages of certain modules.

[1] http://svn.apache.org/viewvc/felix/trunk/healthcheck/api/src/main/java/org/apache/felix/hc/api/ResultLog.java?view=markup&pathrev=1853224#l112",Allow to configure logging of health check result entries to log file,1,,,henzlerg,True,henzlerg,henzlerg
felix,FELIX-6025,2019-01-11T00:13:40.000+0000,2019-01-17T20:54:24.000+0000,2019-02-27T16:15:21.000+0000,,Fixed,New Feature,Major,['healthcheck.generalchecks 2.0.0'],,,,,,,,['Health Checks'],[''],"Allow to create health via script where the script source is either
* the OSGi configuration itself
* via url reference (would usually be a file url)

It should support the same languages as the felix script console [1] does.

[1]
http://felix.apache.org/documentation/subprojects/apache-felix-script-console-plugin.html",Create ScriptedHealthCheck,1,,,henzlerg,True,,henzlerg
felix,FELIX-6024,2019-01-10T22:05:17.000+0000,2019-01-10T23:55:27.000+0000,2019-02-27T16:15:22.000+0000,,Fixed,New Feature,Major,['healthcheck.generalchecks 2.0.0'],,,,,,,,['Health Checks'],[''],"Generic http requests that can execute http requests and validate the response: 

* The OSGi configuration should allow for a list of URLs
* Should allow configuration of additional request options (like method or headers) in curl-like syntax (since this is well known)
* Has to allow for host relative URLs (using the host/port from HttpService) and full URLs
* The response should be checkable against
** Http response code (equals)
** Response text (via RegEx)
** Response timing (via SimpleConstraintChecker)
** Response header (via SimpleConstraintChecker)
** Value in JSON response (via SimpleConstraintChecker)
* The result status shall be configurable for the case any of the checks fail
* connect/read timeouts shall be configurable",Create HttpRequestsCheck ,1,,,henzlerg,True,,henzlerg
felix,FELIX-6019,2019-01-09T08:51:31.000+0000,2019-01-21T20:31:13.000+0000,2019-01-23T19:29:42.000+0000,,Won't Do,New Feature,Major,,,,,,,,,,,"The plugin removes parameters from bundle headers.

See bnd issue [#2429|https://github.com/bndtools/bnd/issues/2429] and SLING-8178.

Example instruction (for bnd file):
{noformat}
-plugin:\
  org.apache.felix.bnd.plugins.header.RemoveParametersPlugin;\
    'Require-Capability'='osgi.service;filter:=""(objectClass=org.osgi.service.event.EventAdmin)"";effective:=active,osgi.service;filter:=""(objectClass=org.osgi.service.event.EventHandler)"";effective:=active;cardinality:=multiple'
{noformat}

Initial source: https://svn.apache.org/repos/asf/felix/sandbox/olli/org.apache.felix.bnd.plugins/",Provide a bnd plugin to remove header parameters,6,,,olli,True,olli,olli
felix,FELIX-6018,2019-01-08T16:58:03.000+0000,2019-01-09T20:37:42.000+0000,2019-02-27T16:15:21.000+0000,,Fixed,New Feature,Major,['healthcheck.core 2.0.0'],,,,,,,,['Health Checks'],[''],"Servlet Filter to answer arbitrary http requests with 503 if certain HC tags are in non-OK result:

* Can be used together with ongoing deployment filter FELIX-6017 to answer all http requests with 503 during deployment
* Can replace sling startup filter SLING-2347 for sling platforms",Servlet Filter to answer arbitrary http requests with 503 if certain HC tags are in non-OK status,1,,,henzlerg,True,,henzlerg
felix,FELIX-6017,2019-01-08T16:56:05.000+0000,2019-01-09T20:32:42.000+0000,2019-02-27T16:15:24.000+0000,,Fixed,New Feature,Major,['healthcheck.core 2.0.0'],,,,,,,,['Health Checks'],[''],"On configurable paths/headers/request parameters, a servlet filter is to be implemented that dynamically adds a HC service with status TEMPORARILY_UNAVAILABLE (or other status, configurable) while the request is active. 

This can be used for both bundle/config update/deployment http requests to Felix web console (the plain Felix way) or other http deployment mechanisms as used by products built on top of Felix.

",Ongoing deployment servlet filter that controls a dynamic health check ,1,,,henzlerg,True,,henzlerg
felix,FELIX-6013,2019-01-04T15:07:49.000+0000,,2019-01-04T15:31:52.000+0000,,,New Feature,Major,,,,,,,,,['Web Console'],['A web based management console'],"It looks like there is currently not yet a @ComponentPropertyType annotation for web console plugins. Instead of writing 
{code}
@Component(service=Servlet.class, property={
        ""felix.webconsole.label=myplugin"",
        ""felix.webconsole.title=My Plugin"",
        ""felix.webconsole.category=Main"",
        ""felix.webconsole.css=/path/to/cssfile.css""
})
{code}
it should be possible to just write 
{code}
@Component(service=Servlet.class)
@WebConsolePlugin(label=""myplugin"", title=""My Plugin"", category=""Main"", ...)
{code}

",Create annotation for Web Console Plugins leveraging the OSGi R7 component property types,1,,,henzlerg,True,,henzlerg
felix,FELIX-6012,2019-01-04T12:13:09.000+0000,2019-01-04T14:56:02.000+0000,2019-02-27T16:15:22.000+0000,,Fixed,New Feature,Major,['healthcheck.generalchecks 2.0.0'],,,,,,,,['Health Checks'],[''],"Generally it is best practice to include HC implementations along with the bundle that contains the aspects being checked. 

However there are a few basic checks that should be provided in an ootb bundle:

* All bundles started (allowing for an exclude list)
* Available heap space not under a certain threshold
* Disk space not under a certain threshold
* CPU/thread usage","Introduce module ""generalchecks"" with basic general purpose checks",1,,,henzlerg,True,,henzlerg
felix,FELIX-6004,2018-12-19T21:34:35.000+0000,2019-01-08T16:30:15.000+0000,2019-04-01T11:20:22.000+0000,,Fixed,New Feature,Major,['healthcheck.annotations 2.0.0'],,,,,,,,['Health Checks'],[''],"In the same way as it was possible for Sling HCs [1] (based on felix SCR plugin), there should be annotations to set health check specific SCR properties in a type-safe manner. This is supported since OSGi R7 via component property types [2] and is already used by the http whiteboard [3].

[1] https://sling.apache.org/documentation/bundles/sling-health-check-tool.html#slinghealthcheck-annotation
[2] https://blog.osgi.org/2018/03/osgi-r7-highlights-declarative-services.html
[3] https://blog.osgi.org/2018/05/osgi-r7-highlights-http-whiteboard.html
",Create a health check annotation based on OSGi R7 component property types,1,,,henzlerg,True,,henzlerg
felix,FELIX-5984,2018-11-20T05:28:12.000+0000,,2018-11-20T05:28:12.000+0000,,,New Feature,Major,,,,,,,,,['Framework'],"['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).']","Currently capabilities are only read from bundle manifest. There should be a way to register and unregister capabilities, like {{BundleContext#registerCapability(...)}}

See thread [Capabilities|https://lists.apache.org/thread.html/6e130df81e61070ce093bb9042e5d5e7d390c26682476316fa67304d@%3Cdev.sling.apache.org%3E] on dev@sling.",Allow registration of (dynamic) capabilities at runtime,1,,,olli,True,,olli
felix,FELIX-5983,2018-11-20T05:28:09.000+0000,,2018-11-20T05:28:09.000+0000,,,New Feature,Major,,,,,,,,,['Framework'],"['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).']","Capabilities should be exposed read-only for usage outside of resolver.

See thread [Capabilities|https://lists.apache.org/thread.html/6e130df81e61070ce093bb9042e5d5e7d390c26682476316fa67304d@%3Cdev.sling.apache.org%3E] on dev@sling.",Expose capabilities from (Stateful) Resolver,1,,,olli,True,,olli
felix,FELIX-5961,2018-10-11T14:09:19.000+0000,2018-10-12T10:00:44.000+0000,2019-04-01T08:12:30.000+0000,,Fixed,New Feature,Major,['rootcause-0.1.0'],,,,,,,,"['rootcause', 'System Ready']","['', '']",Like announced on the list I extract root cause analysis into its own projects. So it can be used without felix systemready.,Extract root cause analysis into its own project,2,,,cschneider,True,cschneider,cschneider
felix,FELIX-5952,2018-10-04T22:27:48.000+0000,2018-12-19T21:09:21.000+0000,2019-02-27T16:15:24.000+0000,,Fixed,New Feature,Major,"['healthcheck.api 2.0.0', 'healthcheck.core 2.0.0', 'healthcheck.webconsole 2.0.0']",,,,,,,,['Health Checks'],[''],"Sling Health Checks [1] allow to check a system's health manually (humans) or technically (load balancers, Kubernetes, etc.). Since Sling HCs have minimal dependencies to Sling, they should be brought to Felix to make them available to a broader audience and to to be able to use the executor runtime for the systemready checks - see [2] for the discussion on the Sling mailing list.

[1] https://sling.apache.org/documentation/bundles/sling-health-check-tool.html
[2] http://apache-sling.73963.n3.nabble.com/hackathon-health-checks-td4086283.html",Felix Health Checks,5,,,henzlerg,True,,henzlerg
felix,FELIX-5951,2018-10-04T15:42:36.000+0000,2018-10-04T16:10:45.000+0000,2018-10-04T21:17:42.000+0000,,Fixed,New Feature,Minor,['configadmin-1.9.10'],,,,,,,,['Configuration Admin'],['The Configuration Admin from the OSGi R7 specification (Section 104).'],"I'd like to propose adding the service properties:
{{osgi.command.scope = 'cm'}} and 
{{osgi.command.function = ConfigurationAdmin.class.getDeclaredMethods() }} to the ConfigurationAdmin service.

Here are some examples that this would enable:
{code:bash}
# store a pid for reuse
g! pid = ""foo""
## CM - get configurations
g! cm:listconfigurations ""(service.pid=$pid)""
## CM - get single configuration
g! config = (cm:listconfigurations ""(service.pid=$pid)"") 0
## CM - create configuration
g! config = cm:getconfiguration $pid ""?""
## CM - delete configuration
g! $config delete
## CM - list configuration properties
g! $config properties
## CM - update configuration from existing properties
g! props = ($config properties)
g! $props put ""foo"" 20
g! $config update $props
## CM - update configuration with new properties
g! props = (new java.util.Hashtable)
g! $props put ""foo"" 20
g! $config update $props
{code}",Automatically add ConfigurationAdmin service as a shell command,2,,,rotty3000,True,rotty3000,rotty3000
felix,FELIX-5923,2018-09-13T10:03:57.000+0000,2018-09-13T10:09:59.000+0000,2018-09-13T10:09:59.000+0000,,Fixed,New Feature,Major,['converter-1.0.2'],['converter-1.0.0'],,,,,,,['Converter'],[''],"As the Felix Converter is the RI for the OSGi Converter project, this bug came in here: https://osgi.org/bugzilla/show_bug.cgi?id=227",Conversion from Map to single element Annotation type does not incorporate PREFIX_,2,,,bosschaert,True,bosschaert,bosschaert
felix,FELIX-5900,2018-08-06T12:45:57.000+0000,2018-08-21T12:57:24.000+0000,2018-09-23T07:10:15.000+0000,,Fixed,New Feature,Major,['osgicheck-maven-plugin 0.1.0'],,,,,,,,,,"after collected a series of feedbacks from  dev@sling.a.o , I am here to propose a couple of new Maven MOJOs to be included in the Felix codebase, able to generate final-user markdown documentation from SCR and Metatype medata descriptors.

Advantages of producing such documentation, are:
 * for an internal use, having such catalogue could reduce the development efforts, maybe there are services already available for certain operations that don’t need to be re-implemented; moreover, it can improve/simplify heterogeneous teams integration work.
 * from customers point of view, it would be good to know what solutions are already offered, to develop their needs on top of our solutions; moreover, under a security PoV, admins can have an overall view to identify which are potential entry-points that can be attacked.

If you want to have a look at the output, I tested the MOJOs against a couple of Apache Sling projects and collected all of them under a private public GitHub repo[1], it should be easy enough understanding how traverse rendered data.

How it works: it is a couple of plain-old Maven3 MOJOs which can be configured directly in the POM, I packaged already all the sources in order to be donated to the ASF, I just would like to start the discussion in order to understand if the community is interested on that tool and which steps are required in order to have it accepted. 
I identified the osgicheck-maven-plugin[2] as the best candidate in order to host the new codebase.

[1] https://github.com/simonetripodi/mddoc-samples
[2] https://github.com/apache/felix/tree/trunk/tools/osgicheck-maven-plugin",Donating a tool able to generate markdown documentation for SCR and Metatype,3,,,simone.tripodi,True,cziegeler,simone.tripodi
felix,FELIX-5881,2018-07-04T03:14:23.000+0000,,2018-07-13T14:44:35.000+0000,,,New Feature,Major,['log-1.2.2'],,,,,,,,['Log Service'],['The Log Service from the OSGi R4 specification (Section 101).'],,Implement stream support from Log Service Specification 1.4,1,,,rotty3000,True,rotty3000,rotty3000
felix,FELIX-5880,2018-07-04T02:54:28.000+0000,2018-07-04T03:11:34.000+0000,2018-07-04T03:11:34.000+0000,,Fixed,New Feature,Major,['log-1.2.0'],,,,,,,,"['Felix Logback', 'Log Service']","['', 'The Log Service from the OSGi R4 specification (Section 101).']","When what you need is logging at the earliest possible moment you want to place the Log Service beside the framework. Creating a framework extension will make this possible. This framework extension is targeted specifically a the felix framework (since equinox has it's own LogService embedded).

This framework extension will also trigger the loading of Felix logback (if it's found).",Add a framework extension fragment which helps load Felix Log Service and Felix Logback when they are on the framework classpath,1,,,rotty3000,True,rotty3000,rotty3000
felix,FELIX-5862,2018-06-08T12:28:06.000+0000,2018-06-14T14:58:39.000+0000,2018-06-14T14:58:39.000+0000,,Not A Bug,New Feature,Major,,['scr-2.1.0'],,,,,,,['Declarative Services (SCR)'],['The Declarative Services from the OSGi R7 specification (Section 112).'],"Hello,

my Component ServiceA exists one for each configuration. (foo and bar)

 
{code:java}
@Component(service = ServiceA.class, immediate = true)
@Designate(factory = true, ocd = ConfigA.class)
public class ServiceA {

  @ObjectClassDefinition
  public @interface ConfigA {

    String ident();
  }

  private ConfigA config;

  private void activate(ConfigA config) {
    this.config = config;
    System.out.println(""ServiceA activated with ident: "" + config.ident());

  }

  public String getIdent() {
    return config.ident();
  }
}

---
    ""my.test.cfgservicetest.ServiceA~foo"": {\
        ""ident:String"": ""foo""\
    },\
    ""my.test.cfgservicetest.ServiceA~bar"": {\
        ""ident:String"": ""bar""\
    },\
{code}
 

 

When i use a static target filter ""(ident=foo)"" felix could find the correct service by because the value is set in Component Configuration Properties.
{code:java}
@Component(service = ServiceB.class, immediate = true)
public class ServiceB {

  @Reference(target = ""(ident=foo)"")
  public void bindServiceA(ServiceA serviceA) {
    System.out.println(""Service B -> Service A (ident): "" + serviceA.getIdent());
  }
}
{code}
 

It would be great if there is a more flexible way, using a placeholder like *$CCP*

{serviceARefIdent} in the targetfilter. When Felix tryes bind the ServiceA to ServiceC it could lookup for the *C*omponent *C*onfiguration *P*roperties of ServiceC and replace the placeholder in the target filter before searching fore the right ServiceA.
 
  
 
 
{code:java}
@Component(service = ServiceC.class, immediate = true)
 @Designate(factory = true, ocd = ConfigC.class)
 public class ServiceC {
 
   @ObjectClassDefinition
   public @interface ConfigC {     String serviceARefIdent();   }
 
   private ConfigC config;
 
 
   @Reference(target = ""(ident=$CCP{serviceARefIdent}
)"")
   public void bindServiceA(ServiceA serviceA)
{     System.out.println(""Service B -> Service A (ident): "" + serviceA.getIdent());   }
}
— config
""my.test.cfgservicetest.ServiceC~foo"":
{\
""serviceARefIdent:String"": ""foo""\ 
 },\
""my.test.cfgservicetest.ServiceC~bar"":{\
         ""serviceARefIdent:String"": ""bar""\
 }\{code}
 

I couldn't find information about this in the OSGi Spec.

 

Is there any way to do this at the moment?

Is there a better way to bind 2 factory-configured services to each other (without Servicetracker)?

Is there any way to add this to Felix SCR(direct or as kind of scr-plugin?)

 

It also would be nice to filter with SystemEnviromentPropertys
{code:java}
  @Reference(target = ""(ident=$ENV{serviceARefIdent})"")
{code}
 

 

 

Regards

Stefan",Bind a Service with an configurable target/placeholder in @Referance annotation,2,,,bischofs@jena.de,True,,bischofs@jena.de
felix,FELIX-5848,2018-05-09T11:49:01.000+0000,2018-05-09T11:53:36.000+0000,2018-05-09T11:56:16.000+0000,,Fixed,New Feature,Minor,,,,,,,,,['System Ready'],[''],"Contribute System Ready to felix.
Mail thread: https://www.mail-archive.com/dev@felix.apache.org/msg45559.html ",[contribution] Finalize System Ready contribution,1,,['contrib'],andrei.dulvac,True,andrei.dulvac,andrei.dulvac
felix,FELIX-5839,2018-04-20T10:48:43.000+0000,2018-04-20T13:56:13.000+0000,2018-05-01T19:47:32.000+0000,,Fixed,New Feature,Major,['utils-1.11.0'],,,,,,,,,,,Add xml and json repositories implemenation,1,,,gnt,True,gnt,gnt
felix,FELIX-5837,2018-04-19T18:04:47.000+0000,2018-04-19T18:09:06.000+0000,2018-04-19T18:09:06.000+0000,,Fixed,New Feature,Major,['gogo.jline-1.1.0'],,,,,,,,['Gogo JLine'],[''],,[gogo][jline] Improve styling support,1,,,gnt,True,gnt,gnt
felix,FELIX-5836,2018-04-19T18:03:07.000+0000,2018-04-19T18:09:23.000+0000,2018-04-19T18:09:23.000+0000,,Fixed,New Feature,Major,"['gogo.jline-1.1.0', 'gogo.runtime-1.1.0', 'gogo.shell-1.1.0', 'gogo.command-1.1.0']",,,,,,,,"['Gogo Command', 'Gogo JLine', 'Gogo Runtime', 'Gogo Shell']","['OSGi and Felix related gogo commands', '', 'RFC-147 shell runtime', 'scripting support and non-OSGi built-in commands']",,Upgrade to OSGi r6,1,,,gnt,True,gnt,gnt
felix,FELIX-5835,2018-04-19T18:02:15.000+0000,2018-04-19T18:09:46.000+0000,2018-04-19T18:09:46.000+0000,,Fixed,New Feature,Major,"['gogo.jline-1.1.0', 'gogo.runtime-1.1.0', 'gogo.shell-1.1.0', 'gogo.command-1.1.0']",,,,,,,,"['Gogo Command', 'Gogo JLine', 'Gogo Runtime', 'Gogo Shell']","['OSGi and Felix related gogo commands', '', 'RFC-147 shell runtime', 'scripting support and non-OSGi built-in commands']",,Upgrade to JDK 8,1,,,gnt,True,gnt,gnt
felix,FELIX-5834,2018-04-19T18:00:17.000+0000,2018-04-19T18:08:34.000+0000,2018-04-19T18:08:34.000+0000,,Fixed,New Feature,Major,['gogo.jline-1.1.0'],,,,,,,,['Gogo JLine'],[''],,Upgrade to JLine 3.7.0,1,,,gnt,True,gnt,gnt
felix,FELIX-5833,2018-04-19T17:58:51.000+0000,2018-04-19T18:08:47.000+0000,2018-04-19T18:08:47.000+0000,,Fixed,New Feature,Major,['gogo.jline-1.1.0'],,,,,,,,['Gogo JLine'],[''],,Support for completion of quoted arguments,1,,,gnt,True,gnt,gnt
felix,FELIX-5791,2018-02-23T22:56:22.000+0000,2018-03-17T01:08:21.000+0000,2018-03-17T01:08:21.000+0000,,Fixed,New Feature,Major,['framework-6.0.0'],['framework-5.6.10'],,,,,,,['Framework'],"['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).']","Now that the R7 spec is completed we need to look into getting the framework to support R7.

Development will be done against the copy in the osgi-r7 sub dir for now. The update of the osgi classes and the switch to the resolver supporting R7 has already been done in the initial copy commit.",Support OSGi R7 framework features,2,,,karlpauls,True,karlpauls,karlpauls
felix,FELIX-5749,2017-11-21T15:12:33.000+0000,2017-11-22T11:47:13.000+0000,2017-11-22T11:47:13.000+0000,,Invalid,New Feature,Major,,['scr-2.0.12'],,,,,,,,,"When desigining the scope of a bundle you sometimes have an optional part that could be externalized into its own bundle but you decide to keep it in your bundle to limit the number of bundles. In this case you have to use an optional import and make sure the code that depends on this import only runs when this import is wired. This code is often quite awkward and often also buggy.

We discussed on osgi-dev that you can make such code a lot simpler to write by using DS. 

This is how the code would look like:
You externalize the code that depends on the optional import into one or more components. These components offer a service interface that is not dependent on the optional import. Inside the component you can work freely with the optional packages. You have to make sure this component is disabled by default. Then you write a ""starter"" component that enables the component if the package is available.

I think scr could support such ""optional"" components without the disabled trick. We could load the component class and if it fails disable the component. If it works we enable it. 
So if the package is wired later and we get a refresh this approach would activate the component without any additional effort from the developer side.

----
Below I am copying a snippet from Ray that details what they did.

Given your component which has the optional import package (doesn't matter how it's used):

import com.liferay.demo.foo.Foo; // The optional package

@Component(
    enabled = false // disable by default so DS ignores it
)
public class OptionalPackageConsumer implements Foo {...}


Make sure the component is disabled by default. This will prevent SCR from classloading the component class.
Second, you construct a ""starter"" component who's job it is to check for the available package:

@Component
public class OptionalPackageConsumerStarter {
   @Activate
    void activate(ComponentContext componentContext) {
        try {
            Class.forName(com.liferay.demo.foo.Foo.class.getName());

            componentContext.enableComponent(OptionalPackageConsumer.class.getName());
        }
        catch (Throwable t) {
            _log.warn(""Could not find {}"", t.getMessage());
        }
    }
}
",Allow to use components that depend on optional imports,5,,,cschneider,True,,cschneider
felix,FELIX-5732,2017-10-26T21:44:31.000+0000,,2017-11-14T08:18:33.000+0000,,,New Feature,Minor,,,,,,,,,,,We wish to contribute this utility to Felix File Install subproject. :),Contribute Bundle Archive Installer Extension to File Install subproject,2,,,jkmarz,True,,jkmarz
felix,FELIX-5694,2017-09-21T14:08:35.000+0000,,2018-01-11T16:50:23.000+0000,,,New Feature,Major,,,,,,,,,['Maven Bundle Plugin'],[''],"Hi,

the current maven-scr-plugin uses the fully qualified class name as a name for the xml file that will contain the generated service description.

Unfortunately the Eclipse IDE uses the service name as the name for the xml file, i.e. for
{code:java}
package org.mycompany;

@Component(name = ""org.topic.myservice"")
public class MyClassName {
{code}

it would create ""org.topic.myservice.xml"" inside the OSGI-INF directory.

At the moment your maven-scr-plugin would create ""org.mycompany.MyClassName.xml"".

if you use both, maven and the Eclipse IDE on the same workspace this will result in duplicate service descriptions.

Would it be possible for you to implement a new option that allows for using the service name (if set) as a name for the generated xml file?",maven-scr-plugin use service name property as filename.xml,4,,"['features', 'maven']",triller-telekom,True,,triller-telekom
felix,FELIX-5678,2017-08-16T05:39:50.000+0000,,2019-06-03T07:01:46.000+0000,,,New Feature,Major,,,,,,,,,['Converter'],[''],"Given a typed object O1 and a ""partial"" representation of an object O2 (for instance in the form of a Map), allow O2 to be merged into O1.

Example:

{code}
public class Foo {
  public String a;
  public String b;
  public String c;
}

Foo f = new Foo();
a = ""Eh!"";
b = ""Be cool."";
c = ""See you later?"";

Map<String, String> m = new Map<>();
m.put(""b"", ""Be there or be square"");

Foo f2 = Converter.convert(f).merge(m);
{code}

I am sure there are many ways to skin this cat.

If the Converter API cannot be changed, what would be the best way to tackle this problem?

(In the meantime, while awaiting comments form [~bosschaert], I'll try to run a few experiments to see if I can come up with something reasonable.)",Allow merging of objects,2,,,dleangen,True,dleangen,dleangen
felix,FELIX-5608,2017-04-07T14:21:20.000+0000,2017-04-07T14:41:11.000+0000,2017-12-06T13:48:29.000+0000,,Fixed,New Feature,Major,['utils-1.10.0'],,,,,,,,['Utils'],[''],,New TypedProperties object to support typed properties file,2,,,gnt,True,gnt,gnt
felix,FELIX-5482,2017-01-06T17:05:23.000+0000,2018-07-03T12:44:12.000+0000,2018-07-03T12:44:12.000+0000,,Fixed,New Feature,Major,['resolver-2.0.0'],,,,,,,,['Resolver'],['Implementation of the OSGi resolver service specification'],The OSGi R7 specification includes updates to the resolver implementation.  I have a contribution to implement the proposed Resolver v1.1 specification.,Contribute Resolver 1.1 implementation for OSGi R7,1,,,tjwatson,True,tjwatson,tjwatson
felix,FELIX-5425,2016-11-24T15:08:17.000+0000,,2016-11-24T15:08:17.000+0000,,,New Feature,Minor,,['org.apache.felix.dependencymanager-r8'],,,,,,,['Dependency Manager Lambda'],['A java8 builder library on top of existing dependency manager API.'],"When you use the new DM-lambda API, and when a component needs to get the result of an asynchronous operation before being activated and registered in the OSGi service registry, you can now use a new ""future dependency"" which allows to define a CompletableFuture (jdk8), and when the CF completes, then the result is injected in the component (like if it was a ServiceDependency), then the component is started and registered.

For example, if your component needs to download a web page before being activated and registered, you can do this:

{code}
 public class Activator extends DependencyManagerActivator {
   public void init(BundleContext ctx, DependencyManager dm) throws Exception {  
      // Download a web page asynchronously, using a CompletableFuture:
        	
      String url = ""http://felix.apache.org/"";
      CompletableFuture<String> page = CompletableFuture.supplyAsync(() -> downloadSite(url));				

      // The component depends on a log service and on the content of the Felix site.
      // The lambda passed to the ""withFuture"" method configures the callback that is 
      // invoked with the result of the CompletableFuture (the page content).
      
      component(comp -> comp
          .provides(MyService.class)
          .impl(MyComponent.class)
          .withService(LogService.class)
          .withFuture(page, result -> result.complete(MyComponent::setPage)));
   }
 }
 
 public class MyComponent implements MyService {
   volatile LogService log; // injected.
   
   void setPage(String page) {
      // injected by the FutureDependency.
   }
   
   void start() {
      // the web page has been downloaded and we can now register in the OSGI registry.
   }

   @Override
   void doService() {
        ...
   }
 }
{code}

Now, it should be investigated if the DM-lambda API could also support the OSGI Promise API (in addition to the jdk8 CompletableFuture).
",Add support for OSGi Promise in DM-lambda,1,,,pderop,True,pderop,pderop
felix,FELIX-5410,2016-11-12T03:30:23.000+0000,,2017-01-09T21:55:23.000+0000,,,New Feature,Major,,,,,,,,,['Web Console'],['A web based management console'],"h4. Feature
Add a new view/plugin to the standard webconsole that helps to pin point which bundles, services or components are the true source for inactive bundles or services.

* For *bundles* the underlying assumption would be a healthy system with all bundles active, and thus any inactive can be shown and analyzed as being problematic.
* For *services/components* one can look at inactive _immediate_ services that fail because of unsatisfied references. For others, the user might need to enter the ""problematic"" service or component they expect to be running to start the analysis.

h4. Motivation
In a larger OSGi application with many bundles and components, it can be difficult to find out the root cause why certain bundles do not start or why a service is not active, especially for folks new to OSGi or with limited knowledge about the application. I have seen many people fail, and thus ""not like"" OSGi because of such hurdles during development, where it is easy to update on bundle but miss out on crucial dependencies.

Figuring out is possible through the current web console, but only for experts, if you click through the bundle or service details. This is usually tedious work, if for example a lower level bundle is the problem, and 200 others are not active because of it.
",Web console plugin for troubleshooting wiring issues,2,,,alexander.klimetschek,True,,alexander.klimetschek
felix,FELIX-5367,2016-09-30T12:59:35.000+0000,2016-09-30T13:01:21.000+0000,2016-10-08T14:15:18.000+0000,,Fixed,New Feature,Major,['http.jetty-3.4.0'],,,,,,,,['HTTP Service'],['The HTTP Service and the Http Whiteboard Service from the OSGi R7 specification (Section 102 and 140).'],,Update Jetty to 9.3.12.v20160915,1,,,cziegeler,True,cziegeler,cziegeler
felix,FELIX-5354,2016-09-19T06:57:56.000+0000,2017-11-02T06:38:55.000+0000,2018-04-30T08:13:48.000+0000,,Fixed,New Feature,Major,['scr-2.1.0'],,,,,,,,['Declarative Services (SCR)'],['The Declarative Services from the OSGi R7 specification (Section 112).'],Tracking updates as defined in RFC 222,Implement Declarative Service Updates for R7 (RFC 222),2,,,cziegeler,True,cziegeler,cziegeler
felix,FELIX-5349,2016-09-15T16:39:05.000+0000,2016-09-20T05:47:22.000+0000,2016-09-20T06:11:58.000+0000,,Fixed,New Feature,Minor,"['http.base-3.0.16', 'http.jetty-3.4.0', 'http.bridge-3.0.16']","['http.base-3.0.12', 'http.jetty-3.2.4']",,,,,,,['HTTP Service'],['The HTTP Service and the Http Whiteboard Service from the OSGi R7 specification (Section 102 and 140).'],"I need to run multiple HTTP services, with different configurations.

I was surprised to find that felix.http.jetty does not provide a ManagedServiceFactory and so were others as indicated in this stack overflow conversation: http://stackoverflow.com/questions/20074211/osgi-http-bundle-bind-to-two-ports

I have experimented with creating a ManagedServiceFactory for the HTTP service myself. It is difficult to do externally to the http.jetty bundle, as it requires access to internal packages.

I have therefore created a small patch to revision 1760954 which adds a ManagedServiceFactory, without changing any of the existing functionality of the ManagedService. This allows multiple http.jetty instances to be configured using WebConsole, fileinstall or other config admin clients.

I have also added a new configuration property: org.apache.felix.http.name,
which is added as a service property to the HTTP runtime instance. This makes it easier to target a specific HTTP whiteboard service than filtering on the port:
HTTP_WHITEBOARD_TARGET + ""=(org.apache.felix.http.name=web1)""

I would be grateful if you would consider adding this functionality.",Add ManagedServiceFactory to HTTP service,2,,,db82407,True,cziegeler,db82407
felix,FELIX-5339,2016-09-05T06:57:36.000+0000,2016-09-07T07:56:40.000+0000,2016-09-07T07:56:40.000+0000,,Fixed,New Feature,Minor,,,,,,,,,"['Dependency Manager', 'Dependency Manager Shell']","['Provides dynamic service and component dependency management.', 'The gogo shell  for Felix Dependency Manager']","The 'dm wtf' command only displays the missing 'out of the box' dependencies (service, resource, bundle, configuration). There are cases however, when the missing dependency is a custom dependency. In this case, the missing dependency is not displayed by the command, even if there are unregistered components due to this. It may be useful to enhance the diagnostics API to retrieve all missing custom dependencies and to use this in the 'dm wtf' command.",Enhance the diagnostics API to retrieve missing custom dependecies,2,,,ralucav,True,pderop,ralucav
felix,FELIX-5336,2016-08-26T08:09:32.000+0000,2018-10-17T14:10:23.000+0000,2018-10-22T09:02:15.000+0000,,Fixed,New Feature,Major,['org.apache.felix.dependencymanager-r13'],,,,,,,,"['Dependency Manager', 'Dependency Manager Annotations', 'Dependency Manager Lambda', 'Dependency Manager Runtime']","['Provides dynamic service and component dependency management.', 'The Dependency Manager annotations API', 'A java8 builder library on top of existing dependency manager API.', 'Activates components declared using Dependency Manager annotations']","In the users mailing list, there is a wish to add support in DM4 for OSGi prototype scope services, which allows any service consumer to get its own instance of a given service dependency.

See http://www.mail-archive.com/users@felix.apache.org/msg17473.html
",Add support for prototype scope services in DM4,4,1,,pderop,True,pderop,pderop
felix,FELIX-5335,2016-08-25T05:43:03.000+0000,2016-09-08T07:32:27.000+0000,2016-09-08T07:32:27.000+0000,,Won't Fix,New Feature,Major,,,,,,,,,['Converter'],[''],"Just to keep a record of this somewhere... here is the request I posted to the mail list.
----
bq. Is there a simple way to transform a DTO into an immutable value object?

To put my question into context: on the bndtools list, I asked this question, and got this response Peter K.:

bq.Not in enRoute and not in the new spec that was inspired by this service. In general you pass copies so they can do whatever they like with them.

Fine. But I am not seeing a simple way of creating such copy. Is there a “clone” method of some sort, or do I just convert an object to another object of its own type? Like this:

 MyDTO copy = cnv.convert( dto ).to( MyDTO.class );

If making copies is indeed how this service is intended to be used, it would be nice to have a more explicit method for this, maybe:

 MyDTO copy = cnv.copy( dto );
",Addition of copy() method to API,2,,,dleangen,True,,dleangen
felix,FELIX-5332,2016-08-24T07:30:02.000+0000,2016-09-26T12:48:38.000+0000,2016-09-27T07:46:27.000+0000,,Fixed,New Feature,Major,,,,,,,,,['Converter'],[''],"Test case and a bit of code to show how a Serializer could work.

To work as a Serializer, the type information needs to be embedded into the output.",Schematizer,2,,,dleangen,True,bosschaert,dleangen
felix,FELIX-5329,2016-08-19T19:59:33.000+0000,2017-02-16T10:04:04.000+0000,2017-10-18T16:04:51.000+0000,,Fixed,New Feature,Minor,['framework-5.6.2'],['framework-5.4.0'],,,,,,,['Framework'],"['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).']","People are starting to experiment with Java 9, but I don't believe we've created a Java 9 package definition yet in default.properties so that the system bundle knows what to export.

I can't remember how we did this previously, but I think we used bnd to analyze and come up with the packages and their ""uses"" constraints.",[Framework] Fix Java 8 packages and add Java 9 packages in default.properties,6,,,rickhall,True,karlpauls,rickhall
felix,FELIX-5326,2016-08-18T09:05:57.000+0000,2016-09-19T23:12:37.000+0000,2016-09-20T07:44:00.000+0000,,Duplicate,New Feature,Major,,,,,,,,,['Converter'],[''],"The Converter service does a lot of introspection and parsing of the DTO data structure. In many cases, a DTO is a very simple object structure. However, it can also be a very complex structure, too.

According to my understanding of the objectives of the Converter, one important goal is to be able to persist data. The idea is that the DTO describes the data structure. Thus, it is the ideal way to ship the state of the system off to PersistenceLand.

If we do buy into this vision, then we may be missing out on a few great opportunities here. When data gets persisted, we often need to understand the relationships between the embedded objects. Or, we may want to be able to create an index on the data. These are a few of the reasons why we would want to have some kind of x-ray vision on the data structure. Since we already go through all the trouble of parsing the data structure in order to convert it, and since this is ~95% of the work, it would be really nice to provide access to this information in order to easily link in services that require this intimate knowledge. Otherwise, all the parsing would have to be done over and over again for each service.

I believe that it would only take a few methods to be able to leverage all the parsing work done by the Converter. I can think of:

 DataTree Converter.toTree(DTO dto); // DataTre gives a tree view of the structure
 Object tree.valueAt(DTO dto, String path); // Dot-separated path value within the tree structure
 void tree.set(DTO dto, String path, Object value); // Set the value at the given location in the tree structure
 void process(DTO dto, Consumer<?> function); // Visit each node for some kind of processing

Those are just some examples. Perhaps a new API would be necessary, but my main point here is that since we are going through all this work of implementing a parser, this is the IDEAL time to create this type of view on the data.

Also, one of the properties of DTOs is that the DTOs are really, in a way, nothing more than schema. Because of this, it should be (and is) trivial to convert to JSON, XML, YAML, or whatever. If the DTO *is* the data structure, then it should also be trivial to convert the type descriptor (or tree, or whatever) to some kind of schema, like JSON Schema, DTD, XML Schema, RDF…

That fits well with one of the features of the Converter: codecs to convert
to/from serialized types. RFC 215 defines two portable codecs: JSON and
YAML but other implementations can add their own codecs too. We could do the same not just for the live data instance, but for the data schema as well. (Note that this schema generation is not required: we could decide only to implement the data tree structure, and have an outside process generate the scheme, but at least the data tree would enable this).

I do understand that it is a step away from a simple “Converter”, but the parsing is essentially the same. Since the hard work is already being done, why not take advantage of it here? Even if this tree view ends up being a completely different service, the same code base could easily serve the two.
",Add data structure descriptor,2,,,dleangen,True,bosschaert,dleangen
felix,FELIX-5272,2016-06-01T11:12:06.000+0000,2017-10-16T14:37:02.000+0000,2017-10-16T14:37:02.000+0000,,Fixed,New Feature,Major,"['gogo.shell-1.0.0', 'gogo.jline-1.0.0', 'gogo.runtime-1.0.0']",,,,,,,,['Gogo Runtime'],['RFC-147 shell runtime'],"* improved gogo parser (fully parse the command line, reporting missing terminators)
* redirections support
* job control
* improved parameter expansion
* jline shell with telnet, ssh, editor, etc...",New gogo features,1,,,gnt,True,gnt,gnt
felix,FELIX-5258,2016-05-18T07:59:14.000+0000,2016-05-18T09:02:15.000+0000,2016-07-18T08:24:33.000+0000,,Fixed,New Feature,Major,['maven-bundle-plugin-3.2.0'],['maven-bundle-plugin-3.0.1'],,,,,,,['Maven Bundle Plugin'],[''],"In my company it happens sometimes that people using the _Maven Bundle Plugin_ to produce the bundle misinterpret the configuration settings, producing a {{MANIFEST}} file with invalid OSGi entries, i.e. given a project with the following structure:

{noformat}
myproject
├── src
│   ├── main
│   │   ├── java
│   │   │   └── org
│   │   │       └── apache
│   │   │           ├── acme
│   │   │           │   ├── utils
{noformat}

and the {{pom.xml}} is wrongly configured as:

{noformat}
<plugin>
                <groupId>org.apache.felix</groupId>
                <artifactId>maven-bundle-plugin</artifactId>
                <extensions>true</extensions>
                <configuration>
                    <instructions>
                        <Export-Package>
                            nothing
                        </Export-Package>
                    </instructions>
                </configuration>
            </plugin>
{noformat}

it makes the {{Export-Package}} header resulting as {{Export-Package: nothing}}.

A MOJO, invoked during the {{verify}} phase, would be very useful to check the target bundle integrity and prevent this kind of wrong exports.

Patch is coming.",Add a new MOJO which verifies the bundle integrity,4,,,simone.tripodi,True,cziegeler,simone.tripodi
felix,FELIX-5240,2016-04-25T12:44:59.000+0000,,2016-07-14T20:19:07.000+0000,,,New Feature,Major,['maven-bundle-plugin-future'],['maven-bundle-plugin-3.0.1'],,,,,,,['Maven Bundle Plugin'],[''],"Karaf 4 commands are identified by the annotation
org.apache.karaf.shell.api.action.lifecycle.Service

For performance reasons karaf only processes commands in packages listed in the Manifest property:
Karaf-Commands: 

We could spare people from adding the header above by hand by creating a small bnd plugin that scans for the above annotations and adds the header.

I can provide a pull request for this.
",Bnd plugin for karaf commands,2,1,,cschneider,True,,cschneider
felix,FELIX-5239,2016-04-20T19:55:29.000+0000,2016-11-14T16:02:54.000+0000,2017-02-06T15:15:54.000+0000,,Won't Fix,New Feature,Major,,['dependencymanager-4.3.0'],,,,,,,['Dependency Manager'],['Provides dynamic service and component dependency management.'],"I would need a way to express a dependency between components.
I suppose I could duplicate all its dependencies, but that's not really the point and would duplicate all tracking events handling.",Express a dependency between components,3,,,gnt,True,,gnt
felix,FELIX-5233,2016-04-13T15:50:40.000+0000,2016-04-14T14:20:10.000+0000,2016-09-13T13:47:48.000+0000,,Fixed,New Feature,Major,['maven-bundle-plugin-3.2.0'],,,,,,,,['Maven Bundle Plugin'],[''],"When dealing with JPA, requirements can be generated when injecting EntityManager / EntityManagerFactory objects.
However, the maven plugin does not automatically generate those, which leads to mismatch and unresolved requirements.

This plugin would analyse the bundle and look for the {{Meta-Persistence}} header, find the persistent units and generate capabilities / requirements accordingly.
",New JPA analyser for the maven bundle plugin,1,,,gnt,True,gnt,gnt
felix,FELIX-5154,2015-12-31T14:15:11.000+0000,,2015-12-31T14:18:24.000+0000,,,New Feature,Minor,,['maven-bundle-plugin-3.0.0'],,,,,,,['Maven Bundle Plugin'],[''],I'd like to use the maven-bundle-plugin in projects with a custom project type different from jar and bundle which are hard-coded in the maven-bundle-plugin's mojos. It's not easily possible at the moment because the supported project types can't be configure from the command-line. The attached patch solves the issue.,Allow configuring supported project types from command line,1,,,mkrizmanic,True,,mkrizmanic
felix,FELIX-5152,2015-12-28T06:39:53.000+0000,2016-01-27T01:44:14.000+0000,2016-01-27T01:44:14.000+0000,,Fixed,New Feature,Major,,,,,,,,,['Declarative Services (SCR)'],['The Declarative Services from the OSGi R7 specification (Section 112).'],"The preferred (and only way for most) to enable felix extension for ds components is with extension attributes in the component xml.  This provides a way of generating these using bnd.

e.g. adding 

@DSExt.PersistentFactoryComponent

at class level adds the appropriate namespace 

xmlns:felix=""http://felix.apache.org/xmlns/scr/extensions/v1.0.0""

and the attribute

felix:persistentFactoryComponent=""true""

to the bnd-generated component xml.

",[DS] bnd meta-annotation to generate felix extension flags in component descriptor,1,,,djencks,True,djencks,djencks
felix,FELIX-4955,2015-07-08T21:51:33.000+0000,,2015-07-10T13:05:56.000+0000,,,New Feature,Major,,,,,,,,,['Dependency Manager'],['Provides dynamic service and component dependency management.'],"(this is an experimental issue)

Whilst Apache Felix Dependency Manager is a powerful tool that allows to manage OSGi service components, it is not standard. On the other hand, at Felix we have SCR which is an implementation of the standard OSGi Declarative Service (DS) specification.

The intent of this experimental issue is to unify and align both DS and DM in order to provide a Declarative Service library based on Dependency Manager.

This new DS library will bring the following benefits:

- For those who love DM, this new library will provide a unified solution that align both DM and DS, where each DS component would be actually implemented as a DM component, browsable from the DM shell.
- provide some DM extension to DS component: for example, the ability to add some extra dependencies dynamically from DS component's ""init"" method.

This experimentation can be split in the following steps:

h5. STEP 1: build current SCR in a DM submodule using bndtools

Create a new DM dependencymanager/org.apache.felix.dependencymanager.ds module that includes the current Apache SCR implementation. As DM is based on bndtools, the current SCR implementation will have to be de-mavenified and built using bndtools. The new module will also include the current SCR junit tests, but not integration tests because current released bndtools requires to have separate modules
for integration tests (if I'm correct).

h5. STEP 2: create a new DM submodule for SCR integration tests.

Create a new DM dependencymanager/org.apache.felix.dependencymanager.ds.itest sub module that includes the current Apache SCR integration tests.

Pax-exam seems to not work well within bndtools. So, it has to be replaced by a simpler tool that allows to dynamically build a ""tiny"" bundle under bndtools. That tool will be simply based on bndlib.

All tests will also have to be reworked in order to cleanup possibly created test configurations because in bndtools, the framework is not restarted between tests.

h5. STEP 3: update the dependency manager load test suite

Add a new loadtest suite based on current Apache Felix SCR
Add a new loadtest suite based on the new org.apache.felix.dependencymanager.ds bundle.

This will allow to compare and make sure that the new DS library will be at minimum as fast as the current SCR implementation. It will also be used to observe the gain of using the actor thread model of DM with DS components.

h5. STEP 4: rework SCR with DM

That will be of course the most difficult part to do: rework
dependencymanager/org.apache.felix.dependencymanager.ds in order to map each DS components to DM components (internally).

After a quick look over the current implementation, the following things could be done:

- modify the classes derived from org.apache.felix.scr.impl.manager.AbstractComponentManager and transform them into DM component.
- Rework the org.apache.felix.scr.impl.manager.DependencyManager and implements it as a DM dependency (by extending the org.apache.felix.dm.context.AbstractDependency class).

h5. STEP 5: build everything in java8

Java7 is in end of life and the DM loadtest suite requires java8.
So, for consistency reasons, build every DM submodules using java8.
",DS based on Dependency Manager,2,,,pderop,True,pderop,pderop
felix,FELIX-4947,2015-07-02T09:54:54.000+0000,2015-07-14T08:19:18.000+0000,2015-07-14T08:19:18.000+0000,,Fixed,New Feature,Minor,,,,,,,,,['Web Console'],['A web based management console'],"Implement a webconsole plugin to manage OSGi subsystems. The console should be similar in look and feel and behaviour to the bundles web console page and allow the user to install, start, stop, uninstall and inspect the available subsystems.",Webconsole plugin for OSGi Subsystems,1,,,bosschaert,True,bosschaert,bosschaert
felix,FELIX-4942,2015-06-26T12:32:29.000+0000,2015-07-13T15:25:39.000+0000,2015-08-31T05:14:45.000+0000,,Fixed,New Feature,Major,"['framework-5.2.0', 'resolver-1.6.0']",,,,,,,,,,,Optimise the resolver,2,,,gnt,True,gnt,gnt
felix,FELIX-4926,2015-06-12T15:02:52.000+0000,2015-08-19T07:52:32.000+0000,2015-08-31T05:15:36.000+0000,,Fixed,New Feature,Major,['resolver-1.6.0'],,,,,,,,['Resolver'],['Implementation of the OSGi resolver service specification'],"The resolver algorithm for {{Candidates#populate}} currently uses a recursive algorithm.
At first glance, the number of recursion can amount to the number of resources to resolve.  This limits the size of the resolution.

I'd like to investigate replacing the recursion with a loop.  

This may also allow using a fork/join design to leverage multiple cores for the resolution.  The fork/join could also be used for the main resolution loop.
It may make things slightly harder to keep the reproducibility of the algorithm if things are not always considered in the same order.  Though there may be some way around.",Investigate rewriting the resolver algorithm to use loops instead of recursion ,2,,,gnt,True,gnt,gnt
felix,FELIX-4853,2015-04-15T08:38:55.000+0000,2015-04-20T08:16:03.000+0000,2015-04-20T08:16:03.000+0000,,Not A Problem,New Feature,Major,,['dependencymanager-3.2.0'],,,,,,,['Dependency Manager'],['Provides dynamic service and component dependency management.'],"The invoke method would go like this:
{code:title=TCCLServiceDependencyImpl.java|borderStyle=solid}

        @Override
	@SuppressWarnings(""rawtypes"")
	public void invoke(
		Object[] callbackInstances, DependencyService dependencyService,
                ServiceReference reference, Object service, String name) {

		Bundle bundle = reference.getBundle();

		BundleWiring bundleWiring = bundle.adapt(BundleWiring.class);

		ClassLoader bundleClassLoader = bundleWiring.getClassLoader();

		Thread currentThread = Thread.currentThread();

		ClassLoader contextClassLoader = currentThread.getContextClassLoader();

		currentThread.setContextClassLoader(bundleClassLoader);

		try {
			super.invoke(
				callbackInstances, dependencyService, reference, service, name);
		}
		finally {
			currentThread.setContextClassLoader(contextClassLoader);
		}
	}
{code}

If you think this is useful I can provide a patch. Which version and which repo should I use for it?",Create a new ServiceDependency that sets the TCCL to the incoming servicereference bundle's classloader before invoking callbaks,2,,,csierra,True,,csierra
felix,FELIX-4807,2015-02-23T11:57:34.000+0000,2015-02-23T11:59:17.000+0000,2015-03-19T06:04:34.000+0000,,Fixed,New Feature,Major,['org.apache.felix.dependencymanager-r1'],,,,,,,,['Dependency Manager'],['Provides dynamic service and component dependency management.'],"This issue describes the new concurrency principles that form the basis for the new upcoming Dependency Manager 4.0 version:
\\

* In order to support highly concurrent systems, the Dependency Manager has been totally reworked using a kind of actor thread model, where all external events that influence the state of a component are handled serially (in FIFO order), but not using synchronizations, blocking locks, or any read/write locks.
Instead of that, a lock-free serial queue is internally maintained for each component, and all events (component dependency events, bundle events) are then scheduled in the component queue, which is executed by the first thread that triggers an event for a given component.

* The new thread model may sometimes make it easier to develop components, because dependency injection callbacks are now serially invoked, and there is no need to synchronize callbacks anymore. However if a dependency is injected while another thread may use the component, then it is still necessary to store injected service dependencies in thread safe datastructure or volatiles variable.

* Optionally, a shared threadpool can be registered in the registry by any management agent bundle, in order to allow the parallel execution of component queues. However, all events scheduled in a given component queue are still executed in FIFO order, but independent components may each be managed and activated concurrently with respect to each other (in other worlds, the components remains ""single threaded"", but multiple components can be started concurrently).

* In order to enable parallel activation of components, you first have to register an instance of the org.apache.felix.dm.ComponentExecutorFactory service. That factory will then be used by DependencyManager to create an Executor of your choice for each Component, typically a shared threadpool configured by yourself.

* If you want to ensure that all Components are initialized after the ComponentExecutorFactory is registered in the OSGI registry, you can use the {{org.apache.felix.dependencymanager.parallel}} OSGi system property which specifies the list of components which must wait for the ComponentExecutorFactory service. This property value can be set to a wildcard {{*}}, or a list of components implementation class prefixes (comma separated). So, all components whose class name starts with the specified prefixes will be cached until the ComponentExecutorFactory service is registered (In this way, it is not necessary to use the StartLevel service if you want to ensure that all components are started concurrently).
Notice that if the ComponentExecutorFactory implementation and all its dependent services are defined using the Dependency Manager API, then you have to list the package of such components with a ""!"" prefix, in order to indicate that those components must not wait for a ComponentExecutorFactory service (since they are part of the ComponentExecutorFactory implementation !).

See dependencymanager-samples/org.apache.felix.dependencymanager.samples/bnd.bnd and org.apache.felix.dependencymanager.samples/src/org/apache/felix/dependencymanager/samples/tpool/ for a concrete example.
",New thread model for Dependency Manager,1,,,pderop,True,pderop,pderop
felix,FELIX-4760,2015-01-08T08:49:10.000+0000,2015-01-08T08:51:47.000+0000,2015-03-17T06:54:09.000+0000,,Fixed,New Feature,Major,"['utils-1.8.0', 'fileinstall-3.5.0']",,,,,,,,"['File Install', 'Utils']","['A bundle that watches a directory set by the felix.fileinstall.dir property; the default is ""load"". JAR files in this directory are installed and uninstalled when they are no longer there. Updating the JAR file in the directory will update the bundle. The bundle allows you to use downloads from a browser, drag and drop, or the command line copy function, for OSGi management.', '']",This should be done using {{:-}} and {{:+}} operators as done in {{gogo}} and {{bash}}.,Support default/alternate values for variable substitution,1,,,gnt,True,gnt,gnt
felix,FELIX-4759,2015-01-08T08:47:31.000+0000,2015-01-08T08:51:53.000+0000,2015-03-17T06:54:20.000+0000,,Fixed,New Feature,Major,"['utils-1.8.0', 'fileinstall-3.5.0']",,,,,,,,"['File Install', 'Utils']","['A bundle that watches a directory set by the felix.fileinstall.dir property; the default is ""load"". JAR files in this directory are installed and uninstalled when they are no longer there. Updating the JAR file in the directory will update the bundle. The bundle allows you to use downloads from a browser, drag and drop, or the command line copy function, for OSGi management.', '']",,Support env:XXX subtitution for environment variables,1,,,gnt,True,gnt,gnt
felix,FELIX-4712,2014-11-27T13:06:40.000+0000,2014-11-27T15:15:21.000+0000,2014-11-27T15:15:21.000+0000,,Fixed,New Feature,Major,['connect-0.1.0'],,,,,,,,['Connect'],['A service registry that enables OSGi style service registry programs without using an OSGi framework'],,Upgrade connect to OSGi r5,1,,,gnt,True,gnt,gnt
felix,FELIX-4700,2014-11-14T16:43:38.000+0000,2017-10-19T07:57:20.000+0000,2017-10-19T07:57:21.000+0000,,Won't Fix,New Feature,Minor,,['fileinstall-3.4.2'],,,2419200,2419200,2419200,,['File Install'],"['A bundle that watches a directory set by the felix.fileinstall.dir property; the default is ""load"". JAR files in this directory are installed and uninstalled when they are no longer there. Updating the JAR file in the directory will update the bundle. The bundle allows you to use downloads from a browser, drag and drop, or the command line copy function, for OSGi management.']","Hi,
I am currently working on implementing Felix Support for Apache HBase's coprocessor feature in order to allow dynamic loading and unloading.
One of the issues for this is to provide HDFS Support for Apache Felix, in order to allow fileInstall to work with HDFS targets. I started implementing this in the HBase context, however it would probably make more sense to incorporate this into Felix, directly. While I will be able to provide the actual code, once Its finished, I'm wondering if there is interest from the Felix community to incorporate this into Felix.",Add HDFS Support to Felix fileInstall,1,,"['features', 'newbie']",juwi,True,,juwi
felix,FELIX-4689,2014-11-11T08:12:43.000+0000,2016-01-31T23:28:30.000+0000,2016-03-06T19:08:55.000+0000,,Fixed,New Feature,Major,['org.apache.felix.dependencymanager-r8'],,,,,,,,['Dependency Manager'],['Provides dynamic service and component dependency management.'],"I wonder if the DependencyManager API could be made a bit more fluent.
Technically it already uses the fluent builder pattern
but all the builder verbs still look a lot like traditional setters.

I know what I propose is mostly syntactic sugar but I think the result
looks more readable and crisp. See below for some ideas.

There is the concern about auto adding the component() to manager as it would acrivate the not fully configured component. We could perhaps overcome this by adding the component to a list of pending components first and then moving them to the active components after the init method.

The camel DSL solves this similarly.

This is from samples.dependonservice:
    public void init(BundleContext context, DependencyManager manager)
throws Exception {
        manager.add(createComponent()
            .setImplementation(DataGenerator.class)
            .add(createServiceDependency()
                .setService(Store.class)
                .setRequired(true)
            )
            .add(createServiceDependency()
                .setService(LogService.class)
                .setRequired(false)
            )
        );
    }

Why not make it look like this:
    public void init(BundleContext context, DependencyManager manager)
throws Exception {
        component()
            .implementation(DataGenerator.class)
            .add(serviceDependency(Store.class).required())
            .add(serviceDependency(LogService.class))
            );
        );
    }

component() could create and add the component.

Or for configuration:
    public void init(BundleContext context, DependencyManager manager)
throws Exception {
        manager.add(createComponent()
            .setImplementation(Task.class)
            .add(createConfigurationDependency()
                .setPid(""config.pid"")
                // The following is optional and allows to display our
configuration from webconsole
                .setHeading(""Task Configuration"")
                .setDescription(""Configuration for the Task Service"")
                .add(createPropertyMetaData()
                     .setCardinality(0)
                     .setType(String.class)
                     .setHeading(""Task Interval"")
                     .setDescription(""Declare here the interval used to
trigger the Task"")
                     .setDefaults(new String[] {""10""})
                     .setId(""interval""))));
    }

could be:
    public void init(BundleContext context, DependencyManager manager)
throws Exception {
        component().implementation(Task.class)
            .configuration(""config.pid"")
                .add(meta(""Task Configuration)
                    .description(""Configuration for the Task Service"")
                    .add(property(""interval"")
                            .cardinality(0)
                            .type(String.class)
                            .heading(""Task Interval"")
                            .description(""Declare here the interval used
to trigger the Task"")
                            .default(""10""))
    }

",Create a more fluent syntax for the dependency manager builder,2,,,cschneider,True,pderop,cschneider
felix,FELIX-4687,2014-11-09T19:28:01.000+0000,2015-10-14T06:20:31.000+0000,2016-02-12T16:16:51.000+0000,,Fixed,New Feature,Major,['metatype-1.1.0'],['metatype-1.0.10'],,,,,,,['Metatype Service'],['The Metatype Service from the OSGi R6 specification (Section 105).'],"Support the R6 v 1.3 metatype schema version.  The changes currently are:
- new namespace
- AD are now optional (not currently validated on earlier versions)
- AD type ""Char"" spelling corrected to ""Character"".

In line with the Eclipse metatype service I'm going to allow both ""Character"" and 'Char"" in any namespace.  If anyone objects I'm happy to change this to validate which namespace each is valid under.",Support v. 1.3 metatype namespace,4,,,djencks,True,djencks,djencks
felix,FELIX-4671,2014-10-15T13:15:49.000+0000,2015-01-26T10:52:15.000+0000,2015-03-17T07:05:22.000+0000,,Fixed,New Feature,Major,['gogo.runtime-0.16.2'],,,,,,,,['Gogo Runtime'],['RFC-147 shell runtime'],"Gogo provides no way to do arithmetic operations or to provide boolean operators.
This makes writing scripted loops or conditions extremely difficult.",Provide an expression parser,3,,,gnt,True,gnt,gnt
felix,FELIX-4669,2014-10-12T12:17:29.000+0000,,2014-10-14T14:39:22.000+0000,,,New Feature,Major,,,,,,,,,['Maven Bundle Plugin'],[''],"When embedding 3rd party JARs into a bundle, one ends up with Import-Package entries for all referenced packages in those JARs. The entries are generated the same, irrespective of whether the package can be resolved in the Maven dependencies or not. 

In order to avoid a potentially long list of explicit import statements like 

{code:xml}
<Import-Package>
	COM.jrockit.reflect;
	resolution:=optional,
	COM.newmonics.PercClassLoader;
	resolution:=optional,
	com.google.gson;
	resolution:=optional,
	...,
	*
</Import-Package>
{code}

it would be convenient if the imports were generated with resolution:=optional by default if they cannot be resolved in the Maven dependencies.",Packages that cannot be resolved in Maven dependencies should have resolution:=optional,1,1,,ralfsteppacher,True,,ralfsteppacher
felix,FELIX-4641,2014-09-17T06:40:28.000+0000,,2014-09-17T06:43:46.000+0000,,,New Feature,Major,,['maven-bundle-plugin-2.5.3'],,,,,,,['Maven Bundle Plugin'],[''],"The baselining feature is nice, however it reports in a file for each module separately.

Similarly to what is done in some other plugin, it would be nice to be able to aggregate all these reports (xml or text) into one single reports (this could be seen as  a code quality metrics.)",Adding the capability to aggregate baselining reports.,1,4,,jdelaire,True,,jdelaire
felix,FELIX-4624,2014-08-28T23:45:36.000+0000,,2014-08-28T23:46:49.000+0000,,,New Feature,Minor,,['eventadmin-1.4.0'],,,,,,,['Event Admin'],['The Event Admin from the OSGi R6 specification (Section 113).'],Event Admin Async and Sync thread pools could be rebalanced based on actual application performance.  This could be done by periodically measuring calls to send and post and recomputing the ratios.  This could be used to adjust the async thread pools up or down.  Although this adjustment should be bound by at least current number of sync threads minus 1 since if async = sync the application may deadlock.,Event Admin - Retune Async to Sync ThreadPool ratios based on actual post/send ratio.,1,,,bobpaulin,True,,bobpaulin
felix,FELIX-4623,2014-08-28T23:37:36.000+0000,2014-08-29T13:57:54.000+0000,2014-09-14T10:19:38.000+0000,,Fixed,New Feature,Minor,['eventadmin-1.4.2'],['eventadmin-1.4.0'],,,,,,,['Event Admin'],['The Event Admin from the OSGi R6 specification (Section 113).'],The current Event Admin ThreadPool assumes an even split between post and send operations. However this is not the case with many applications leaning heavily one way or the other.  This ratio should be configurable for the application.,Event Admin - Make Async to Sync ThreadPool Ratio Configurable.,2,,,bobpaulin,True,cziegeler,bobpaulin
felix,FELIX-4607,2014-08-18T05:58:34.000+0000,2016-05-21T05:06:32.000+0000,2016-07-08T08:05:15.000+0000,,Fixed,New Feature,Major,['scr-2.0.4'],['scr-2.0.0'],,,,,,,['Declarative Services (SCR)'],['The Declarative Services from the OSGi R7 specification (Section 112).'],"RFC 190 has the unfortunate limitation that annotations used for configuration cannot have annotation field values.  This is easy to remedy if we pick a format for mapping typed data trees into a map.  I'm going to implement this following James Strachan's system for creating an xml DSL for spring configuration for ActiveMQ (see also geronimo's xbean-spring and xbean-blueprint) and inspired by xpath. 

Here's an example of how the mapping works.  Lets say you have annotations
A { B[] b();} and B { String foo();}.  The configuration map will have entries with keys of the form:
b.0.foo
b.1.foo
b.2.foo

b.<arrayIndex>.foo

<element name of outer class>.<arrayIndex>.<element name of inner class>

Obviously this can be extended to any level of nesting.  Non array annotation valued elements will correspond to arrayIndex 0.

To extend support to interfaces with inheritance, it's also necessary to include the class of the desired sub-interface in the source map.  I don't have a proposal yet for the name of the sub key.",[DS] Configure with nested annotations/interfaces,2,,,djencks,True,djencks,djencks
felix,FELIX-4604,2014-08-11T05:54:09.000+0000,2014-08-22T06:49:10.000+0000,2014-08-25T09:20:21.000+0000,,Fixed,New Feature,Major,['eventadmin-1.4.0'],,,,,,,,['Event Admin'],['The Event Admin from the OSGi R6 specification (Section 113).'],There are some use cases for letting the event admin simply drop events for a specific topic and not deliver them.,Add a configuration to ignore certain events,1,,,cziegeler,True,cziegeler,cziegeler
felix,FELIX-4599,2014-08-05T12:11:43.000+0000,2017-08-03T06:04:21.000+0000,2017-08-08T14:00:56.000+0000,,Fixed,New Feature,Major,,,,,,,,,['Configuration Admin'],['The Configuration Admin from the OSGi R7 specification (Section 104).'],"Currently it is not possible to define configuration properties, the contents of which should be automatically encrypted upon save by the configuration admin.

An example would be a mail server configuration, where SMTP credentials must specified and the password should be encrypted upon saving the configuration. The encrypted password should then be accessible and decryptable in the component to which the configuration is bound.",Support Encryption Of Configuration Properties,8,,,dominique.jaeggi,True,,dominique.jaeggi
felix,FELIX-4591,2014-07-30T08:46:34.000+0000,2014-08-21T12:32:50.000+0000,2015-08-01T11:30:10.000+0000,,Fixed,New Feature,Major,['maven-bundle-plugin-2.5.2'],['maven-bundle-plugin-2.5.0'],,,,,,,['Maven Bundle Plugin'],[''],"The new baselining feature is nice, unfortunately it compare to a specific version without the option to compare specific classifier, if we don't generate an OSGi bundle by default it doesn't work.

For example I'm working on a project that is generating NetBeans modules and OSGi bundle for the same artifact.

But since the OSGi bundle is generated as <classifier>bundle</classifier> and the baseline feature only compare the jar generated by the project default packaging (which in this case is a jar without the packageinfo and with a netbeans module manifest) so the baselining fail.

Adding the ability to specify the classifier in the plugin configuration would be good.",add the ability to specify the classifier in the new baseline configuration of the plugin.,3,5,,jdelaire,True,cziegeler,jdelaire
felix,FELIX-4550,2014-07-05T08:54:17.000+0000,2015-01-29T08:37:43.000+0000,2015-02-02T08:06:43.000+0000,,Fixed,New Feature,Major,['http-2.4.0'],,,,,,,,['HTTP Service'],['The HTTP Service and the Http Whiteboard Service from the OSGi R7 specification (Section 102 and 140).'],"Currently HTTP Service embeds Jetty 8.1.14.v20131031 which supports Servlet 3.0. Current Jetty version 9.2.1.v20140609 supports Servlet 3.1 which provided access to some new NIO based features. Further it also provides lots of new features like Spdy support , Websocket support etc [1]

[1] https://webtide.com/jetty-9-features/

",Upgrade embedded Jetty to version 9.2.x in Felix Http Jetty,5,1,,chetanm,True,jajans,chetanm
felix,FELIX-4517,2014-05-19T06:20:54.000+0000,2014-05-20T09:13:16.000+0000,2015-08-01T11:28:40.000+0000,,Fixed,New Feature,Major,['maven-bundle-plugin-2.5.0'],,,,,,,,['Maven Bundle Plugin'],[''],,Generate generic capabilities and requirements for services from blueprint and scr descriptors,3,,,gnt,True,gnt,gnt
felix,FELIX-4512,2014-05-11T14:21:04.000+0000,2014-06-13T15:45:06.000+0000,2014-07-30T08:46:54.000+0000,,Fixed,New Feature,Major,['maven-bundle-plugin-2.5.0'],['maven-bundle-plugin-2.4.0'],,,,,,,['Maven Bundle Plugin'],[''],"The BND 2.2.0 library contains an extraordinary tool called [Baseline|http://www.aqute.biz/Bnd/Versioning] (see Baselining paragraph) that compares the public API of a bundle with the public API of another bundle.

It would be really useful to have this tool in the {{maven-bundle-plugin}} in order to keep track of APIs modifications during development time.",Add a new Mojo to invoke the BND Baseline tool,11,,,simone.tripodi,True,cziegeler,simone.tripodi
felix,FELIX-4507,2014-04-30T16:44:05.000+0000,2014-08-06T02:04:01.000+0000,2015-08-11T05:09:38.000+0000,,Fixed,New Feature,Major,['scr-2.0.0'],['scr-1.8.0'],,,,,,,['Declarative Services (SCR)'],['The Declarative Services from the OSGi R7 specification (Section 112).'],"The lifecycle constraints of factory components make them generally useless.  This introduces a different kind of factory component where:
- the componentFactory service is always registered irrespective of whether any references are available
- the components created by newInstance act like regular components created by config admin factory pids; the component is present whether or not references are available and the instance comes and goes as references come and go.  Calling dispose removes the component.
- a modify method is added to the ComponentInstance returned from newInstance. (the ComponentInstance implements a new, extension, interface with the modify method)

There are some consequences, such as ComponentInstance.getInstance() may return null and may not always return the same object.

This behavior can be turned on for a particular component with a ""extension"" attribute in the component descriptor
xmlns:felix=""http://felix.apache.org/xmlns/scr/extensions/v1.0.0""
felix:persistentFactoryComponent=""true""
","[DS] ""persistent"" factory components",1,,,djencks,True,djencks,djencks
felix,FELIX-4506,2014-04-30T16:35:06.000+0000,2014-08-06T02:04:31.000+0000,2015-08-11T05:09:33.000+0000,,Fixed,New Feature,Major,['scr-2.0.0'],['scr-1.8.0'],,,,,,,['Declarative Services (SCR)'],['The Declarative Services from the OSGi R7 specification (Section 112).'],"Currently the felix optional extensions are controlled either through a global property or through a felix namespace for the component description xml.  I'm going to introduce a namespace (I propose ""http://felix.apache.org/xmlns/scr/extensions/v1.0.0"") and attribute names to control these per component in a normal DS namespace.  These will be easy to add using a bnd annotation (not yet committed, but soon).  The attribute names I have so far are:
configurableServiceProperties (cf FELIX-4149)
persistentFactoryComponent (no issue yet)
deleteCallsModify (cf FELIX-2523)
obsoleteFactoryComponentFactory (per-component version of global ds.factory.enabled property)
configureWithInterfaces (no issue yet, allow interfaces as well as annotations to configure 1.3 components)
delayedKeepInstances (per-component version of global ds.delayed.keepInstances property)",[DS] Control felix optional extensions through xml attributes in component descriptor,1,,,djencks,True,djencks,djencks
felix,FELIX-4500,2014-04-25T15:51:40.000+0000,2014-08-21T11:38:16.000+0000,2015-01-05T09:53:02.000+0000,,Fixed,New Feature,Major,['webconsole-event-plugin-1.1.2'],['webconsole-event-plugin-1.1.0'],,,,,,,['Web Console'],['A web based management console'],"If not, all transient events are ignored (starting, stopping, etc...)
See https://github.com/apache/felix/blob/trunk/webconsole-plugins/event/src/main/java/org/apache/felix/webconsole/plugins/event/internal/EventListener.java#L29",EventListener should implement SynchronousBundleListener,2,,,gnt,True,v_valchev,gnt
felix,FELIX-4499,2014-04-25T15:49:50.000+0000,2014-08-21T11:36:03.000+0000,2015-01-05T09:53:01.000+0000,,Fixed,New Feature,Major,['webconsole-event-plugin-1.1.2'],['webconsole-event-plugin-1.1.0'],,,,,,,['Web Console'],['A web based management console'],See https://github.com/apache/felix/blob/trunk/webconsole-plugins/event/src/main/java/org/apache/felix/webconsole/plugins/event/internal/converter/BundleEventConverter.java#L68,BundleEventConverter reports UNINSTALLED for UNRESOLVED events,2,,,gnt,True,v_valchev,gnt
felix,FELIX-4476,2014-04-03T11:54:23.000+0000,2014-05-06T17:17:09.000+0000,2014-05-12T09:25:23.000+0000,,Fixed,New Feature,Major,"['ipojo-runtime-1.12.0', 'ipojo-manipulator-1.12.0']",,,,,,,,['iPOJO'],['iPOJO service-oriented component framework'],,Java 8 Support,2,1,,sauthieg,True,clement.escoffier,sauthieg
felix,FELIX-4446,2014-03-06T13:44:42.000+0000,2014-09-23T14:06:24.000+0000,2014-09-23T14:06:24.000+0000,,Won't Fix,New Feature,Major,,,,,,,,,['Event Admin'],['The Event Admin from the OSGi R6 specification (Section 113).'],"EventAdmin is great because various bundles do send events, so it's really nice for an audit trail of what happened.
However, in order for the audit trail to be useful, it's missing a few properties such as the timestamp and the user performing the action.
It would be nice if event admin add those automatically to avoid modifying various projects to add those properties.",Optionally enhance event admin to add a timestamp and the authenticated subject before dispatching events,5,,,gnt,True,gnt,gnt
felix,FELIX-4439,2014-02-27T10:03:50.000+0000,,2015-06-18T10:46:41.000+0000,,,New Feature,Major,,"['configadmin-1.6.0', 'configadmin-1.8.0']",,,345600,345600,345600,,['Configuration Admin'],['The Configuration Admin from the OSGi R7 specification (Section 104).'],"According to the OSGi compendium specifications 4.3.0, section 104.8.1 (page 88), the ConfigurationAdmin should send events through the EventAdmin when it is available.

Copy of the text:

Configuration events must be delivered asynchronously by the Configuration Admin implementation, if present. The topic of a configuration event must be:

org/osgi/service/cm/ConfigurationEvent/<event type>

The <event type> can be any of the following:
- CM_DELETED
- CM_UPDATED
- CM_LOCATION_CHANGED

The properties of a configuration event are:
• cm.factoryPid – (String) The factory PID of the associated Configuration object, if the target is a
Managed Service Factory. Otherwise not set.
• cm.pid – (String) The PID of the associated Configuration object.
• service – (ServiceReference) The Service Reference of the Configuration Admin service.
• service.id – (Long) The Configuration Admin service's ID.
• service.objectClass – (String[]) The Configuration Admin service's object class (which must
include org.osgi.service.cm.ConfigurationAdmin)
• service.pid – (String) The Configuration Admin service's persistent identity, if set.

I'd like to use the EventAdmin for this information, because it is guaranteed to have no side effects and to be informative only.

Now of course I could use the ConfigurationListener for this, but it seems that this has some unwanted side effects. Unbounded configurations now seem to bind to my listenening bundle, while I only want to present some configuration status on a Web UI. There is another bundle to which the configuration should bind, but I use DS to do so. Maybe this is another issue...",ConfigurationAdmin should send events through the EventAdmin,3,,['feature'],marcdejonge,True,,marcdejonge
felix,FELIX-4426,2014-02-17T11:25:05.000+0000,2014-09-30T20:50:05.000+0000,2015-03-19T06:05:03.000+0000,,Fixed,New Feature,Major,['org.apache.felix.dependencymanager-r1'],['dependencymanager-3.2.0'],,,,,,,['Dependency Manager'],['Provides dynamic service and component dependency management.'],"DM has great support for single-cardinality dependencies, allowing you to only declare the dependency as (volatile) field. For multiple-cardinality dependencies, no such support is present, forcing you to always manually implement this using callbacks.

It would be great if I could declare multiple-cardinality dependencies like:

{code}
private volatile List<MyService> m_services;
{code}

and let DM manage the list for me.

Note that you need some additional reflection mojo to obtain the actual collection type. An example of how this can work can be found in a utility class for Swagger in the Amdatu-Web project, see https://bitbucket.org/amdatu/amdatu-web/src/master/org.amdatu.web.rest/src/org/amdatu/web/rest/doc/swagger/SwaggerUtil.java?at=master#cl-304
",Allow DM to manage collections of services,3,,,jajans,True,pderop,jajans
felix,FELIX-4421,2014-02-06T14:24:11.000+0000,2014-02-07T16:12:58.000+0000,2014-06-13T09:24:01.000+0000,,Fixed,New Feature,Major,['http-2.3.0'],['http-2.2.1'],,,,,,,['HTTP Service'],['The HTTP Service and the Http Whiteboard Service from the OSGi R7 specification (Section 102 and 140).'],"Currently, the Jetty version embedded in Felix Http Jetty is version 7.6.13.v20130916. In order to leverage features available in Servlet API 3.0 only, I'd like to change the embedded version to 8.1.14.v20131031.

A quick test shows that increasing this version number in parent/pom.xml alone requires some small changes to JettyService, deployment of a fresh build of Http Jetty into an OSGI container succeeds, but no request can be handled, because Jetty internally calls methods on interfaces and classes that were only introduced in Servlet API 3.0.

This implies that the servlet version supported should be upped to 3.0 as well. In order to be backward compatible with bundles expecting a 2.x version, one could also export a virtual version (e.g. 2.6) to satisfy those.",Embed Jetty 8 in Felix Http Jetty,7,,,dpfister,True,,dpfister
felix,FELIX-4397,2014-01-21T16:54:07.000+0000,2014-01-22T18:27:29.000+0000,2015-08-01T09:22:23.000+0000,,Fixed,New Feature,Major,['http-2.2.2'],['http-2.2.1'],,,,,,,['HTTP Service'],['The HTTP Service and the Http Whiteboard Service from the OSGi R7 specification (Section 102 and 140).'],It would be nice to configure the maximum form size in Jetty through the OSGI Jetty Bridge.,Allow to configure Max Form Size,2,,,dpfister,True,cziegeler,dpfister
felix,FELIX-4276,2013-10-09T16:59:22.000+0000,,2014-03-12T08:17:15.000+0000,,,New Feature,Major,,,,,,,,,['iPOJO'],['iPOJO service-oriented component framework'],,Let stereotypes have attributes,1,,,clement.escoffier,True,sauthieg,clement.escoffier
felix,FELIX-4271,2013-10-04T09:21:11.000+0000,2016-05-11T06:59:22.000+0000,2016-07-12T04:54:05.000+0000,,Won't Fix,New Feature,Major,,['maven-scr-plugin 1.14.0'],,,,,,,['SCR Tooling'],"['Tooling for Maven, Ant, and Bnd including SCR annotations']","When testing a system based on SCR it is necessary to generate SCR descriptors for test components. Currently the plugin only considers src/main/java and target/classes.

Perhaps the plugin could offer full control of the component source directories.",Allow processing of test components ,2,,,tdiesler,True,,tdiesler
felix,FELIX-4267,2013-10-02T16:27:40.000+0000,2013-10-02T16:31:33.000+0000,2013-10-08T17:17:49.000+0000,,Fixed,New Feature,Major,['ipojo-runtime-1.11.0'],,,,,,,,['iPOJO'],['iPOJO service-oriented component framework'],"To ease the provisioning of iPOJO in Apache Karaf, we should provide a 'feature' file defining the most common set of bundles we deploy.",Define Apache Karaf features for iPOJO,1,,,clement.escoffier,True,clement.escoffier,clement.escoffier
felix,FELIX-4265,2013-10-02T14:51:20.000+0000,2013-10-02T19:37:53.000+0000,2013-10-02T19:37:53.000+0000,,Fixed,New Feature,Major,['ipojo-runtime-1.11.0'],,,,,,,,['iPOJO'],['iPOJO service-oriented component framework'],"We now have QueueListener to see what happen in the QueueService.
It will be useful to provide a dedicated service that will record the bootstrap events for future usage (statistics, ...).",Provides a recorder for startup events,1,,,sauthieg,True,sauthieg,sauthieg
felix,FELIX-4231,2013-09-17T11:52:13.000+0000,2013-09-17T12:21:09.000+0000,2013-10-08T17:17:03.000+0000,,Fixed,New Feature,Major,['ipojo-runtime-1.11.0'],,,,,,,,['iPOJO'],['iPOJO service-oriented component framework'],Service Binding interceptors are the last interception level on service dependencies. They intercept when a dependency is retrieve a service object from a provider and can modify it. They are also called when a service is unbound.,Provide service binding interceptors,1,,,clement.escoffier,True,clement.escoffier,clement.escoffier
felix,FELIX-4227,2013-09-12T16:02:02.000+0000,2015-10-30T08:12:49.000+0000,2016-09-22T10:44:44.000+0000,,Duplicate,New Feature,Major,,['framework-4.2.1'],,,,,,,['Framework'],"['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).']","Currently you define the system package exports using:
org.osgi.framework.system.packages: allows to completely redefine the packages
org.osgi.framework.system.packages.extra: allows to add packages to the system package exports

I am missing a way to mostly keep the exports as is but remove some package and add some others.
So some property like ""org.osgi.framework.system.packages.remove"" would be nice.

A typical scenario is cxf that wants to replace some JRE API and IMPLs with their own jars.

Currently this can only be solved by replacing all exports. To do this correctly the exports even have to be different for each jdk version. So it is a lot of effort.

The requested feature would be most useful if it is a OSGi standard property so it works on all frameworks.",There should be a way to remove packages from the system package exports without redefining all exports,2,2,,cschneider,True,,cschneider
felix,FELIX-4215,2013-09-04T17:20:52.000+0000,2013-09-04T17:34:34.000+0000,2013-10-08T17:17:53.000+0000,,Fixed,New Feature,Major,['ipojo-runtime-1.11.0'],['ipojo-runtime-1.10.1'],,,,,,,['iPOJO'],['iPOJO service-oriented component framework'],"The current manipulation metadata do not contains the argument names. However, they may be useful to determine property's name or whatever aspect configured using a parameter annotation.",Extend manipulation metadata with argument names,1,,,clement.escoffier,True,clement.escoffier,clement.escoffier
felix,FELIX-4194,2013-08-14T14:31:19.000+0000,,2013-08-14T14:31:19.000+0000,,,New Feature,Major,,,,,,,,,['Web Console'],['A web based management console'],"We should add some sort of authorization support to the web console, to further restrict the possibilities. The main idea is to support the reading and the ""writing"" use cases, so some user should only get read (informational) access, while others have the ability to change things.
Started a thread in the mailing list to discuss the details further",Add authorization support to web console,3,1,,cziegeler,True,,cziegeler
felix,FELIX-4147,2013-06-24T13:37:16.000+0000,2013-06-25T17:24:40.000+0000,2013-10-08T17:17:51.000+0000,,Fixed,New Feature,Major,['ipojo-runtime-1.11.0'],,,,,,,,['iPOJO'],['iPOJO service-oriented component framework'],"Add a method to access the represented provided service object from ProvidedServiceDescription.

In additon, change the modifier of the register and unregister methods from ProvidedService to let external entity the choice to register and unregister the service.",Add getProvidedService in ProvidedServiceDescription and allow external service management,1,,,clement.escoffier,True,clement.escoffier,clement.escoffier
felix,FELIX-4146,2013-06-24T09:56:39.000+0000,2013-06-26T17:04:34.000+0000,2013-10-08T17:17:57.000+0000,,Fixed,New Feature,Major,['ipojo-runtime-1.11.0'],['ipojo-runtime-1.10.1'],,,,,,,['iPOJO'],['iPOJO service-oriented component framework'],"Add the following methods to the `Factory` service interface:

* List<ComponentInstance> getInstances() returning the list of created (and living) instances
* List<String> getInstanceNames() returning the names of the created instances",Add getInstances and getInstanceNames in the Factory interface,2,,,clement.escoffier,True,clement.escoffier,clement.escoffier
felix,FELIX-4137,2013-06-18T09:00:41.000+0000,2013-11-15T12:59:04.000+0000,2014-02-12T12:41:42.000+0000,,Fixed,New Feature,Major,['http-2.2.1'],['http-2.2.0'],,,,,,,['HTTP Service'],['The HTTP Service and the Http Whiteboard Service from the OSGi R7 specification (Section 102 and 140).'],"Support WAB deployment: if a bundle has a header ""Web-ContextPath"" or ""Webapp-Context"", deploy it inside the jetty servlet container under that servlet context.",Web application bundle (WAB) deployment,4,,,dpfister,True,fmeschbe,dpfister
felix,FELIX-4131,2013-06-15T07:02:23.000+0000,2013-06-15T07:12:12.000+0000,2013-06-17T05:47:43.000+0000,,Fixed,New Feature,Major,['ipojo-runtime-1.10.1'],['ipojo-runtime-1.10'],,,,,,,['iPOJO'],['iPOJO service-oriented component framework'],"when a configuration (from the configuration admin) is null, set the location to the bundle managing it.",Explicitly set configuration's location when the configuration is null,1,,,clement.escoffier,True,clement.escoffier,clement.escoffier
felix,FELIX-4130,2013-06-14T17:35:05.000+0000,2013-06-14T17:45:47.000+0000,2013-06-17T05:47:43.000+0000,,Fixed,New Feature,Major,['ipojo-runtime-1.10.1'],['ipojo-runtime-1.10'],,,,,,,['iPOJO'],['iPOJO service-oriented component framework'],This would be an easy way to retrieve the component instance from the instance description.,Allow retrieving the component instance from the instance description,1,,,clement.escoffier,True,clement.escoffier,clement.escoffier
felix,FELIX-4125,2013-06-12T12:34:28.000+0000,2013-06-12T12:47:10.000+0000,2013-06-17T05:47:42.000+0000,,Fixed,New Feature,Major,['ipojo-runtime-1.10.1'],['ipojo-runtime-1.10'],,,,,,,['iPOJO'],['iPOJO service-oriented component framework'],"To avoid confusion, we should support 'component' and 'components' commands. Indeed in code the factories are declared using the @Component annotation.

The new commands are equivalent to `factory` and `factories`.",Provide 'components' and 'component' commands,1,,,clement.escoffier,True,clement.escoffier,clement.escoffier
felix,FELIX-4120,2013-06-12T10:09:18.000+0000,2013-06-12T12:21:08.000+0000,2013-06-17T05:47:43.000+0000,,Fixed,New Feature,Major,['ipojo-runtime-1.10.1'],['ipojo-runtime-1.10'],,,,,,,['iPOJO'],['iPOJO service-oriented component framework'],"When iPOJO is tracking services, it should let extenral entities to intercept service resolution to determine the services to use.

With this feature it is possible to:

* mark services
* change filters, even build filter chains
* update the selected services dynamically

Two interception are supported:

* tracking : allow filtering out and transforming references. Transformation changes the properties of the reference
* ranking : sort the set of references

Both interceptors are exposed as services and are plugged on the service dependency dynamically. Obviously they can leave and arrive at runtime. To manage a dependency, they expose a service property named 'target' which is a LDAP filter selecting a set of dependencies.",Allow external entity to interact during the service resolution,1,,,clement.escoffier,True,clement.escoffier,clement.escoffier
felix,FELIX-4116,2013-06-11T07:58:42.000+0000,2013-06-14T17:31:12.000+0000,2013-06-17T05:47:42.000+0000,,Fixed,New Feature,Major,['ipojo-runtime-1.10.1'],['ipojo-runtime-1.10'],,,,,,,['iPOJO'],['iPOJO service-oriented component framework'],"iPOJO offers the ability to be notified when a service arrives/is modified/leaves a dependency. However this notifications happens _inside_ the component, via dependency callbacks (@Bind, @Unbind, @Modified).

What would be cool is to listen to this events _externally_, with listeners on the DependencyModel.
Is is possible right now to do this with a hack of the DependencyCallbacks (lots of reflection, ugly code).
So what just lacks is the API to register/unregister listeners + the listener interface.

Same thing for service providings : we should be able to be notified when a component start/stop to provide a service. It is possible to listen to all services with the good ""instance.name"", but this is not really elegant, and there might be issues with isolated ServiceContext (composite). Registering a listener on the ProvidedService seems a better approach IMO.

Same point for configuration, like the @Updated callback, but external.

For sure there are lots of other component things to listen to... ;)","Ability to listen for component service dependencies, providings, configuration properties, ...",2,,,pierre.bourret,True,clement.escoffier,pierre.bourret
felix,FELIX-4095,2013-06-01T19:57:17.000+0000,2013-06-12T12:06:01.000+0000,2013-06-12T12:06:01.000+0000,,Fixed,New Feature,Major,['ipojo-manipulator-1.10.1'],['ipojo-manipulator-1.10.0'],,,,,,,['iPOJO'],['iPOJO service-oriented component framework'],"I'd like to provide a @Stereotype meta-annotation that will decorate a user-defined annotation.
All annotations decorating that user-defined annotation are recorded and replayed when the stereotyped annotation is found

Example: 
@Component(architecture = false)
@Instantiate
@Stereotype
public @interface IntantiableComponent {}

Usage:
@InstantiableComponent
public class MyComponent {}

Is the same as:
@Component(architecture = false)
@Instantiate
public class MyComponent {}

but reusable :)",Add CDI-like @Stereotype,1,,,sauthieg,True,sauthieg,sauthieg
felix,FELIX-4060,2013-05-10T15:46:52.000+0000,2015-05-27T12:58:15.000+0000,2016-08-22T10:01:12.000+0000,,Fixed,New Feature,Major,['http.base-3.0.0'],,,,,,,,['HTTP Service'],['The HTTP Service and the Http Whiteboard Service from the OSGi R7 specification (Section 102 and 140).'],"The OSGi early draft 2013-03 contains a version of RFC 189, the HTTP service update.

This issue is for keeping track of the changes and their implementations",Implement HTTP Whiteboard Service (RFC-189),4,,,cziegeler,True,cziegeler,cziegeler
felix,FELIX-4059,2013-05-09T15:08:47.000+0000,2013-05-09T15:10:52.000+0000,2013-05-28T04:10:19.000+0000,,Fixed,New Feature,Major,['ipojo-manipulator-1.10.0'],['ipojo-manipulator-1.8.6'],,,,,,,['iPOJO'],['iPOJO service-oriented component framework'],Provide a simple CLI tool to manipulate iPOJO bundle.,Provide a CLI tool to manipulate iPOJO bundles,1,,,clement.escoffier,True,clement.escoffier,clement.escoffier
felix,FELIX-4034,2013-04-24T14:57:48.000+0000,2013-04-24T15:10:18.000+0000,2013-05-28T04:12:21.000+0000,,Fixed,New Feature,Major,['ipojo-runtime-1.10'],,,,,,,,['iPOJO'],['iPOJO service-oriented component framework'],"Provides a DSL to declare instances without relying on XML.

This DSL looks like a fluent API using in classes annotated with the @Configuration annotation:

@Configuration
public class MyInstances {
        Instance i1 = instance().of(""my.factory"")
                .with(""simple"").setto(""simple"");

        Instance i2 = instance()
                .of(""my.factory"")
                .with(""simple"").setto(""simple"")

                .with(""list"").setto(list(1, 2, 3))
                .with(""list2"").setto(list().with(1).with(2).with(3))

                .with(""map"").setto(map().with(pair(""entry"", ""value"")))
                .with(""map"").setto(map()
                .with(pair(""entry2"", list(""aaa"", ""bbb""))));
}

Methods returning instances should also be called (receiving an optional bundle context). Thus we can add some logic computing the instance configuration.

The DSL creates an instance declaration and not just an instance. It's equivalent to the <instance/> XML declaration. ",Instance configuration DSL,1,,,clement.escoffier,True,clement.escoffier,clement.escoffier
felix,FELIX-4009,2013-04-01T02:20:50.000+0000,2016-05-18T09:06:19.000+0000,2017-04-04T19:24:04.000+0000,,Fixed,New Feature,Major,['maven-bundle-plugin-3.2.0'],['maven-bundle-plugin-2.3.7'],,,,,,,['Maven Bundle Plugin'],[''],"Stuart:

1) currently, to integrate maven-bundle-plugin into m2e, one must  use tycho configurator
https://github.com/sonatype/m2eclipse-tycho

which adds needles complexity, and makes few strange assumptions
https://github.com/sonatype/m2eclipse-tycho/blob/master/org.sonatype.tycho.m2e/src/org/sonatype/tycho/m2e/internal/OsgiBundleProjectConfigurator.java#L74

which no one it seems is there to care to correct
https://github.com/sonatype/m2eclipse-tycho/pull/8

2) instead, I suggest maven-bundle-plugin to provide direct integration with eclipse via
http://wiki.eclipse.org/M2E_compatible_maven_plugins#BuildContext

which I would expect to be more customizable / flexible for the end user.

I recently made similar switch with my DS plugin, and life got easier :-)
https://github.com/carrot-garden/carrot-maven/tree/master/carrot-maven-scr-plugin

thanks.

Andrei.
",maven bundle plugin should be integrated directly with eclipse,7,,['m2e'],andrei.pozolotin,True,cziegeler,andrei.pozolotin
felix,FELIX-3997,2013-03-26T08:58:30.000+0000,2013-03-26T09:11:11.000+0000,2015-03-17T06:48:18.000+0000,,Fixed,New Feature,Major,['utils-1.4.0'],,,,,,,,['Utils'],[''],,Provide an abstract bundle extender,1,,,gnt,True,gnt,gnt
felix,FELIX-3885,2013-02-01T07:55:01.000+0000,,2013-02-01T15:51:52.000+0000,,,New Feature,Minor,,,,,,,,,['Bundle Repository (OBR)'],[''],"It would be great to remove certain bundles like snaphot bundles from obr. Looking at the current documentation the bundle plugin only provides clean and remote-clean, which removes missing bundles. A target like uninstall would be great. My objective is, at some points i want to cleanup the obr by removing SNAPSHOT-Versions or remove specific bundle version that roots from a wrong build.",Remove bundle from obr,3,1,,cinhtau,True,,cinhtau
felix,FELIX-3866,2013-01-23T11:32:11.000+0000,2013-01-23T13:00:49.000+0000,2013-01-23T13:01:11.000+0000,,Fixed,New Feature,Minor,,['dependencymanager-3.0.0'],,,,,,,['Dependency Manager'],['Provides dynamic service and component dependency management.'],"The dependency manager ""AspectService"" annotation does not support the new ""swap"" parameter that has been added in the createAspect method from the API.","Missing swap attribute in dependency manager ""AspectService"" annotation",1,,,pderop,True,pderop,pderop
felix,FELIX-3863,2013-01-22T17:40:08.000+0000,2013-01-22T18:02:25.000+0000,2013-01-22T18:02:46.000+0000,,Fixed,New Feature,Minor,,['dependencymanager-3.0.0'],,,,,,,['Dependency Manager'],['Provides dynamic service and component dependency management.'],"This change request concerns the dependency manager maven annotation plugin and is related to the following old post in the user mailing list:

http://www.mail-archive.com/users@felix.apache.org/msg11621.html

Basically, the purpose of the change request is to allow the maven plugin to generate the dependencymanager annotation metadata files (META-INF/dependencymanager/*) not only in the
bundle file but also in the maven project folder hierarchy, i.e.,
com.myannotation.project/com.myannotation.testbundle/META-INF/dependencymanager/...

",Generate DependencyManager Annotation MetaData In Project Folder,1,,,pderop,True,pderop,pderop
felix,FELIX-3818,2012-12-13T15:03:52.000+0000,,2012-12-13T16:21:43.000+0000,,,New Feature,Major,,,,,,,,,"['Installer', 'Remote Shell']","['Anything related to the Felix installers or Felix installation.', 'Simple remote access for the Felix shell']","I created a script to daemonize a felix instance, being executed in background.
My daemon script is based on linux initscripts and screen.

After installing this daemon, you would access felix CLI by the command 'screen -r OSGi' or if you have installed remote shell for felix, telnet localhost 6666. 

Best regards, 
Marotta",Felix as a daemon,1,,['features'],marcelo.m87,True,,marcelo.m87
felix,FELIX-3780,2012-11-23T10:08:22.000+0000,2015-01-15T08:44:11.000+0000,2016-01-16T15:03:05.000+0000,,Fixed,New Feature,Minor,['deploymentadmin-0.9.8'],,,,,,,,['Deployment Admin'],"['The Deployment Admin service (OSGi 4.1 compendium chapter 114) standardizes the access to some of the responsibilities of the management agent: that is, the lifecycle management of interlinked resources on an OSGi Service Platform.']",I wanted to put org.apache.felix.deploymentadmin.stopunaffectedbundle=false to external file instead of having to declare it as system property but noticed that the configuration admin spec isn't supported. ,Allow using configuration admin in addition to system properties,2,,,tuomas_kiviaho,True,jajans,tuomas_kiviaho
felix,FELIX-3770,2012-11-21T01:42:43.000+0000,2012-12-02T18:51:33.000+0000,2013-06-06T15:34:57.000+0000,,Fixed,New Feature,Major,['webconsole-4.2.0'],,,,,,,,['Web Console'],['A web based management console'],It's time to upgrade jquery-ui used by Web Console. JQuery UI 1.9.1 has new features like a menu widget that can be used to implement FELIX-3769. JQuery core should probably be upgraded as well.,Upgrade jquery-ui to 1.9.1,2,,,hsaginor@gmail.com,True,fmeschbe,hsaginor@gmail.com
felix,FELIX-3769,2012-11-21T01:36:58.000+0000,2012-12-28T15:32:15.000+0000,2013-06-06T15:34:56.000+0000,,Fixed,New Feature,Minor,['webconsole-4.2.0'],['webconsole-4.0.0'],,,,,,,['Web Console'],['A web based management console'],"Something needs to be done about growing number of web console plugins. Currently the UI displays a tab for each plugin. This makes the UI cluttered as more and more plugins are added. To address the tabs should be replaced with a tree structure or a drop-down menu.    

Felix Meschberger has proposed the following on the dev list to implement this.

""* Plugins registered as services may have a ""felix.webconsole.category"" property
indicating the category. Plugins not registering this property will be placed in
the default category
* AbstractWebConsolePlugin is ammended with a getCategory() method, which may
overwritten by implementations. The default implementation in the
AbstractWebConsolePlugin class returns the default category
* A default category can be configured
* Categories are simple strings such as ""OSGi"" or relative paths such as
""Sling/Main"". Relative paths define multi-level trees. I think in general a
single level is probably enough. Maybe we can start with just supporting a
single level (so just plain strings).
* Translation of categories is such that each segment in the path (or the
complete string if there is no sub-categories) is converted into a translation
label by prefixing with ""category."". So the translation for the ""OSGi"" category
would be found with the translation string ""category.OSGi"".
* The plugin navigation is refactored to move it to the left and render it as a
tree structure (I assume we can use the JQuery treetable plugin).""",Improve the way Web Console UI manages growing number of plugins.,2,,,hsaginor@gmail.com,True,fmeschbe,hsaginor@gmail.com
felix,FELIX-3711,2012-10-13T08:49:25.000+0000,,2012-10-30T15:00:00.000+0000,,,New Feature,Major,,['webconsole-4.0.0'],,,,,,,['Web Console'],['A web based management console'],"Many REST urls already work if created manually, but are not yet available as links in the UI:
http://localhost:8080/system/console/bundles/org.apache.felix.webconsole
http://localhost:8080/system/console/bundles/org.apache.felix.webconsole:4.0.1.SNAPSHOT
http://localhost:8080/system/console/configMgr/org.apache.felix.webconsole.plugins.event.internal.PluginServlet
http://localhost:8080/system/console/components/com.test.scr.ClassB

It would be easy to put the appropriate link behind the property values in bundles/configuration/services detail views. 

I use the REST Urls a lot as they are really valuable in email/instant messenger/JIRA/WIKI to bring people with one click to the desired location, therefore it would be nice to not have to construct them manually.
",Make more REST Urls available in UI (component id/name and service pid is often not clickable),4,1,,henzlerg,True,,henzlerg
felix,FELIX-3705,2012-10-10T05:26:13.000+0000,2012-10-18T10:12:42.000+0000,2014-02-17T07:53:10.000+0000,,Fixed,New Feature,Minor,['jaas-0.0.2'],,,,,,,,['JAAS'],['OSGi support for JAAS'],"Bundle to simplify usage of JAAS within OSGi environment.

It supports following features

1. It can work both in Standalone and AppServer deployments i.e. in those environment where global JAAS configuration might be used by other applications and our usage of JAAS should not affect them 
2. It enables usage of OSGi Configuration support to dynamically configure the login modules. 
3. It allows LoginModule instances to be created via factories registered in OSGi Service Registry 
4. It does not require the client to depend on any OSGi API 
5. It works well with the dynamic nature of the OSGi env 6. Implementation depends only on Core OSGi API and ConfigAdmin 

Complete details are provided at [1]. Discussion thread on Felix dev mailing list is at [2]

[1] https://github.com/chetanmeh/c/wiki/JAAS-in-OSGi
[2] http://markmail.org/thread/6k2lw6ieczyy5db2 ",Bundle to simplify JAAS usage in OSGi,2,,,chetanm,True,fmeschbe,chetanm
felix,FELIX-3699,2012-10-03T08:34:49.000+0000,2013-04-10T12:09:58.000+0000,2013-05-28T04:10:21.000+0000,,Fixed,New Feature,Major,['ipojo-manipulator-1.10.0'],,,,,,,,['iPOJO'],['iPOJO service-oriented component framework'],"Custom components are well handled in iPOJO using the metadata.xml, for example:

<CustomComponent classname=""MyClass"" name=""myCustomComponent"">
    <requires field=""service""/>
</CustomComponent>

The custom component definition will, for example, add handlers and so on. It works very well and is nice that iPOJO allows that kind of extensions. 

But we can't declare a custom component by using annotations, for example doing:

@CustomComponent
public class MyClass {...}

It will be a nice if iPOJO allows to do it using annotations, since the same functionality must be possible with the metadata.mxl and with annotations.",Allow annotations to handle custom component definitions.,3,,,torito,True,clement.escoffier,torito
felix,FELIX-3693,2012-09-30T08:15:18.000+0000,2012-10-05T18:36:50.000+0000,2013-09-28T17:40:04.000+0000,,Fixed,New Feature,Major,['http-2.2.1'],['http-2.2.0'],,,,,,,['HTTP Service'],['The HTTP Service and the Http Whiteboard Service from the OSGi R7 specification (Section 102 and 140).'],"Problem: A an OSGi based application (e.g. an Apache Sling instance) operates behind an HTTP Proxy. Clients contact the proxy with HTTPS (SSL) while the proxy contacts the application over plain HTTP. The information that the client is using HTTPS/SSL is lost along the way.

From the POV of the application all requests are not secured since the ServletRequest.isSecure() method always returns false.

This creates some trouble particularly when sending absolute links (including the scheme) or redirects back to the client. Another issue is cookies which should be set to ""secure"" if the client is using HTTPS.

The general concept is as follows:

(1) The proxy is configured to set a request header when being the SSL endpoint (talking SSL to clients and talking plain HTTP to application) for the application to act as if handling a secure request:
(1a) X-Forwarded-SSL: on (see Making HTTPS Redirects Work With a Reverse Proxy at http://www.turbogears.org/1.0/docs/Install/RedirectHttpsRequests.html)
(1b) Optionally set other headers to provide the cipher_suite, key_size, and ssl_session_id. If the proxy is not able to derive these values from, the information just cannot be provided, which is not problematic

(2) A servlet filter is implemented to act upon the headers provided by the proxy, creating a request wrapper as follows:
(2a) overwrite ServletRequest.getScheme, ServletRequest.isSecure, HttpServletRequest.getRequestURL to indicate HTTPS
(2b) Set the request attributes defined by the Servlet API spec if the respective information is available from the dispatcher. Otherwise the attributes remain undefined

This issue is about implementing the second part as a servlet filter to support a proxy configured as described in the first part.",Filter for servers running behind a SSL-endpoint proxy,4,,,fmeschbe,True,fmeschbe,fmeschbe
felix,FELIX-3600,2012-07-18T10:22:40.000+0000,2012-09-28T13:43:25.000+0000,2012-09-28T13:43:25.000+0000,,Fixed,New Feature,Minor,,,,,,,,,['User Admin'],['The User Admin from the OSGi R4 specification (Section 107).'],"I've written a new implementation of the UserAdmin specification that I want to donate to the Felix project. 

This implementation is backed by an extensive test suite (200+ tests) to keep an eye on regression in the various parts of the implementation. Of course, it is compliant to the UserAdmin specification. 

In addition, it offers the possibility to use another backend repository for the storage of roles. By default, it uses an file-based repository, making it a fully self-contained service.

I am willing to maintain this implementation in the future if accepted in the main source tree of Felix.
",New UserAdmin implementation,3,,['patch'],jajans,True,jajans,jajans
felix,FELIX-3544,2012-06-11T17:32:35.000+0000,2012-06-11T17:34:48.000+0000,2015-03-17T06:50:08.000+0000,,Fixed,New Feature,Major,['utils-1.2.0'],,,,,,,,,, ... without synchronization issues,Add a BaseManagedServiceFactory to help writing such factories,1,,,gnt,True,gnt,gnt
felix,FELIX-3537,2012-06-05T13:23:07.000+0000,,2013-06-14T17:45:33.000+0000,,,New Feature,Major,,,,,,,,,['iPOJO'],['iPOJO service-oriented component framework'],"Currently, ComponentInstance works great with iPOJO's Factory:
With the Factory, you can create a new ComponentInstance, and using the ComponentInstance returned, you have some control over the instance (start/stop/dispose/reconfigure).

This is very nice, but the limitation is that the ComponentInstance is only known within your ""session"" with the Factory, you cannot access it outside.

That means, for example, that an instance created from the metadata.xml will never be startable, stoppable or reconfigurable.

So I would like to have a way to obtain ComponentInstance(s) from the iPOJO API.",Make ComponentInstance more easily accessible,4,,,sauthieg,True,ggezer,sauthieg
felix,FELIX-3524,2012-05-27T22:58:59.000+0000,2012-09-02T19:54:39.000+0000,2012-11-20T10:58:14.000+0000,,Fixed,New Feature,Minor,['scr-1.6.2'],[' scr-1.6.0'],,,,,,,"['Declarative Services (SCR)', 'Specification compliance']","['The Declarative Services from the OSGi R7 specification (Section 112).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']","This issue is about implementing the new ""configuration-pid"" component attribute specified by the OSGi 4.3 compendium. So far, components were using the component name in order to retrieve the component configuration from config admin. But In the section 112.4.4,  a component can now define an optional specific configuration-pid, in order to use a PID which is different from the component name.

I have attached to this issue a proposed candidate patch, if someone could review it and hopefully commit it.
I have not yet made a unit test, but if my propose patch seems reasonable and is accepted, then I will try to learn to implement the corresponding test.

Here is a brief description of the patch:

- first the parser (XmlHandler/ComponentMetaData) has been modified in order to parse the new configiration-pid attribute from the component element (the parsing fails if the version is not greater or equals to DS 1.2).

- next, in ComponentRegistry.java, we are now holding the mapping between configuration pids and their respective components: a new method getComponentHoldersByPid(String pid) method has been added, and returns the iterator on the ComponentHolders which must be configured with the given pid. Please see comments in patch.

- in ConfigurationSupport.java:  when a configuration update is detected from config admin, then the updated is notified to all components whose configuration-pid are referencing the updated pid.

thanks.

",SCR configuration-pid from compendium 4.3,2,,,pderop,True,fmeschbe,pderop
felix,FELIX-3507,2012-05-14T23:50:57.000+0000,2012-05-17T13:40:07.000+0000,2012-11-20T10:58:18.000+0000,,Fixed,New Feature,Major,['scr-1.6.2'],[' scr-1.6.0'],,,,,,,"['Declarative Services (SCR)', 'Specification compliance']","['The Declarative Services from the OSGi R7 specification (Section 112).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']","The compendium 4.3 has a policy-option for references with new behavior for ""greedy"".  I've implemented this.",scr greedy and reluctant policy options from compendium 4.3,2,,,djencks,True,fmeschbe,djencks
felix,FELIX-3504,2012-05-11T17:23:37.000+0000,2012-06-16T19:20:18.000+0000,2012-06-16T19:20:18.000+0000,,Fixed,New Feature,Major,['framework-4.2.0'],['framework-4.0.2'],,,,,,,['Framework'],"['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).']","We should move to the latest OSGi R5 API in the framework. There isn't too much new functionality in the R5 API, mostly just adding the resource model used by the new OBR resolver. Moving to the R5 API will make it possible to play with the new OBR resolver as well as make it easier to implement the handful of new R5 features as we move forward.",[Framework] Move to OSGi R5 packages,1,,,rickhall,True,rickhall,rickhall
felix,FELIX-3489,2012-04-25T20:06:56.000+0000,,2012-04-25T20:06:56.000+0000,,,New Feature,Minor,,['fileinstall-3.2.2'],,,,,,,['File Install'],"['A bundle that watches a directory set by the felix.fileinstall.dir property; the default is ""load"". JAR files in this directory are installed and uninstalled when they are no longer there. Updating the JAR file in the directory will update the bundle. The bundle allows you to use downloads from a browser, drag and drop, or the command line copy function, for OSGi management.']","It would be nice to have the possibility to specify per directory whether configuration changes should be written back to the configuration file or not.

This has been discussed on the mailing list:

http://www.mail-archive.com/users@felix.apache.org/msg12279.html ",Make it possible to specify the felix.fileinstall.enableConfigSave per directory,1,,,rodehav,True,,rodehav
felix,FELIX-3462,2012-04-15T06:22:34.000+0000,2014-06-14T10:40:47.000+0000,2015-08-01T11:28:55.000+0000,,Fixed,New Feature,Major,['maven-bundle-plugin-2.5.0'],['maven-bundle-plugin-2.3.7'],,,,,,,['Maven Bundle Plugin'],[''],"Version 2.3.6 introduced the nicer formatting for manifests where after each e.g. Import package a newline should happen. This never worked for me. I already asked on the original issue but never got an answer.
",Nicer Manifest formatting,2,2,,cschneider,True,gnt,cschneider
felix,FELIX-3342,2012-02-09T04:08:48.000+0000,,2017-11-10T12:33:30.000+0000,,,New Feature,Minor,,['framework-4.0.2'],,,,,,,['Framework'],"['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).']","Currently when a resource is not found inside a bundle, an IOException is thrown with the message: ""Resource does not exist"" . It would be more useful to throw instead a FileNotFound exception. This behavior happens outside OSGi and also in Equinox. For example look the stack trace in Equinox:

java.io.FileNotFoundException: /resource/myresource.stg
at
org.eclipse.osgi.framework.internal.protocol.bundleresource.Handler.findBundleEntry(Handler.java:51)
at
org.eclipse.osgi.framework.internal.core.BundleResourceHandler.openConnection(BundleResourceHandler.java:175)
at java.net.URL.openConnection(URL.java:945)
at java.net.URL.openStream(URL.java:1010)

This small fix would improve compatibility with 3rd parties that rely on this behavior, and also improve compatibility with the behavior in equinox. ",Throw FileNotFound exception when trying to get a resource that does not exist,,,,irvingc,True,,irvingc
felix,FELIX-3312,2012-01-21T14:01:03.000+0000,,2012-01-21T14:01:03.000+0000,,,New Feature,Minor,,['framework-4.0.2'],,,,,,,['Framework'],"['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).']",While discussing a problem launching multiple instances of a framework within the same JVM (see http://markmail.org/message/gby3k46mqcjjpmjm) it came out that Felix could add a framework ID to the bundle: URLs.,Add a framework ID on bundle: URLs,,,,rasteele,True,,rasteele
felix,FELIX-3295,2012-01-05T11:23:56.000+0000,2012-01-05T11:30:30.000+0000,2013-09-28T17:40:02.000+0000,,Fixed,New Feature,Major,['http-2.2.1'],['http-2.2.0'],,,,,,,['HTTP Service'],['The HTTP Service and the Http Whiteboard Service from the OSGi R7 specification (Section 102 and 140).'],"We have a need that we want our Http Service based application to not always run on the root context path.

The use case is that we have a single reverse proxy in front of multiple instances of our application and we forward to the different instances based on the URL path. For simplicity we don't want to rewrite the paths and thus the easiest thing is to run the instances with different context paths.

The proposed solution is simple: We add a new configuration property ""org.apache.felix.http.context_path"" (named after the request property ""javax.servlet.include.context_path"" indicating the context path on RequestDispatcher.include). This defaults to ""/"" and may be any path with a leading slash (enforced if missing) and no trailing slash (removed if provided).

The value of this property (or the default) is then used to create the Context for Jetty.

This is also referred to by a post on the users list [1].

[1] http://markmail.org/message/gwnct4675htaf7jc",Allow configuration of Http Service context path,,,,fmeschbe,True,fmeschbe,fmeschbe
felix,FELIX-3248,2011-11-28T16:14:48.000+0000,,2013-05-02T02:30:52.000+0000,,,New Feature,Minor,,['webconsole-3.1.8'],,,,,,,['Web Console'],['A web based management console'],"This depends on FELIX-2896. Once we are able to add bundle info provider, I think it would be nice to have a provider reading the embedded pom and displaying the maven coordinates etc.",Display maven information for a bundle,,,,cziegeler,True,,cziegeler
felix,FELIX-3241,2011-11-23T13:57:26.000+0000,,2011-11-23T14:00:27.000+0000,,,New Feature,Major,,,,,,,,,['File Install'],"['A bundle that watches a directory set by the felix.fileinstall.dir property; the default is ""load"". JAR files in this directory are installed and uninstalled when they are no longer there. Updating the JAR file in the directory will update the bundle. The bundle allows you to use downloads from a browser, drag and drop, or the command line copy function, for OSGi management.']","At the Amdatu project we need a way to use metatype configuration files in conjunction with fileinstall. We use metatype for provisioning using autoconf deployment based on Apache Ace. In some (development) cases however we also use fileinstall to supply configuration and it makes no sense for us to use cfg file there.

To support this we created a fileinstall ArtifactInstaller extension that handles metatype. It's roughly modeled after the configinstaller andf autoconf code. As it is a rather generic use case we would be happy to transfer the code to Apache Felix.

I'll attach the initial code. If there is interest in adopting this I'de be happy to update it.",Fileinstall metatype artifactInstaller,,,,bramk,True,,bramk
felix,FELIX-3237,2011-11-18T23:44:09.000+0000,2011-11-19T00:14:23.000+0000,2013-09-28T17:40:03.000+0000,,Fixed,New Feature,Major,['http-2.2.1'],['http-2.2.0'],,,,,,,['HTTP Service'],['The HTTP Service and the Http Whiteboard Service from the OSGi R7 specification (Section 102 and 140).'],"To check whether Whiteboard Filter, Servlet, and HttpContext service registration properly works a Web Console plugin should be added to the Http Service Whiteboard support bundle.

In addition a ConfigurationPrinter is to be added to get a simple information dump.",Provide Web Console plugin for the Http Whiteboard support,,,,fmeschbe,True,fmeschbe,fmeschbe
felix,FELIX-3225,2011-11-15T07:49:35.000+0000,2011-11-15T08:04:59.000+0000,2013-05-02T02:29:46.000+0000,,Invalid,New Feature,Major,,['http-2.2.0'],,,,,,,['HTTP Service'],['The HTTP Service and the Http Whiteboard Service from the OSGi R7 specification (Section 102 and 140).'],"The servlet registration whiteboard pattern is great. But it lacks functionality to use a custom HttpContext implementation. This is particularly annoying in situations where a custom implementation is required e.g. for authentication purposes.

The idea is:
  * HttpContext services are registered with an identification property, e.g. felix.context.id
  * Servlet services are registered with the same identification property, where the whiteboard support will merge the two
  * If a Servlet service is registered for which not HttpContext is available, the servlet is not registered with the HttpService (yet).
  * When an HttpContext is registered any registered servlet services are checked for whether they refer to the newly registered HttpContext
  * Likewise on HttpContext unregistration, related services must be unregistered from the HttpContext again",Support flexible use of HttpContext implementations for Servlet whiteboard pattern,,,,fmeschbe,True,,fmeschbe
felix,FELIX-3210,2011-11-09T09:16:12.000+0000,2011-11-18T23:28:09.000+0000,2011-12-02T15:44:16.000+0000,,Fixed,New Feature,Major,['maven-bundle-plugin-2.3.6'],,,,900,900,900,,['Maven Bundle Plugin'],[''],remote-clean goal demands user to confirm repository modification. It is not appropriate for automated builds. A configuration option would solve the issue.,Maven bundle plugin remote-clean goal without user confirmation,,,,hipa,True,mcculls,hipa
felix,FELIX-3208,2011-11-08T14:10:11.000+0000,2011-11-28T15:06:34.000+0000,2011-12-02T15:44:15.000+0000,,Fixed,New Feature,Major,['maven-bundle-plugin-2.3.6'],,,,,,,,['Maven Bundle Plugin'],[''],Would be useful to just use <_sources>true</_sources> and get the project's source bundled automatically. Also need to investigate if it's possible to include dependency sources.,Provide default value for sourcepath when using BND -sources=true,,,,mcculls,True,mcculls,mcculls
felix,FELIX-3170,2011-10-17T15:20:27.000+0000,2012-08-31T14:05:01.000+0000,2012-08-31T14:05:12.000+0000,,Duplicate,New Feature,Major,,,,,,,,,['SCR Tooling'],"['Tooling for Maven, Ant, and Bnd including SCR annotations']","Hello;

SITUATION:

I am using scr-maven-plugin in eclipse 3.7 with m2e incremental mode;
project has some 1000 java files and 100 scr components;

on every java file save, eclipse runs JDT builder and then maven invokes SRC plugin;

as project progressed, the responsiveness of eclipse continued to dwindle down,
now to 3...5 seconds delay on each java file edit;

QUESTIONS:

1) is it feasible to support incremental build in scr plugin?
for this, plugin would have to:
a) keep scr components parse/build cache
b) support a new goal, when only a single file with scr annotations is requested for processing
but still produces complete scr xml;

2) has anyone devised a better solution to this?

thank you;

Andrei.
",scr-maven-plugin : incremental build mode for eclipse development?,3,2,,andrei.pozolotin,True,,andrei.pozolotin
felix,FELIX-3165,2011-10-15T09:53:51.000+0000,2011-10-16T17:56:33.000+0000,2011-12-02T15:44:14.000+0000,,Fixed,New Feature,Major,['maven-bundle-plugin-2.3.6'],,,,,,,,['Maven Bundle Plugin'],[''],,Add an option to the manifest goal that will copy any BND generated resources to the target folder,,,,mcculls,True,mcculls,mcculls
felix,FELIX-3156,2011-10-11T16:36:35.000+0000,2011-11-20T19:46:48.000+0000,2011-11-20T19:46:49.000+0000,,Fixed,New Feature,Major,['framework-4.0.2'],['framework-4.0.0'],,,,,,,['Framework'],"['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).']","Implement Bundle::getDataFile and Bundle::compareTo.
Both are very low hanging fruit, I am willing to post a patch for both if needed.",Implement Bundle::getDataFile and Bundle::compareTo,,,,lgalfaso,True,rickhall,lgalfaso
felix,FELIX-3152,2011-10-10T08:44:49.000+0000,,2016-10-27T18:19:21.000+0000,,,New Feature,Major,,,,,,,,,['Web Console'],['A web based management console'],The attached file is a source code that implement JMX client as felix web console.,JMX as web console feature,6,3,"['jmx', 'webconsole']",christanto,True,,christanto
felix,FELIX-3128,2011-09-22T20:40:28.000+0000,2011-11-18T09:41:25.000+0000,2011-11-18T09:41:25.000+0000,,Fixed,New Feature,Minor,,,,,,,,,['Web Console'],['A web based management console'],"Hi all,

I'd like to suggest/contribute the attached project.

The project aims at easing diagnostics of OSGi services and finding about missing dependencies.

Typically in a large system with many cascading dependencies managed by different trackers such as DeclarativeService, DependencyManager or others, tracking the root cause of a top level service not being started can become very cumbersome. When building service oriented architectures, it is often the case that a single missing requirement will lock a full stack of services, but to find that one requirement is like finding a needle in a haystack!

The basic idea here is to ask each dependency manager instance about its unresolved dependencies, merge all answers and filter the result to keep only the root causes.

Typically, if A depends on B which depends on C which depends on D, and D is nowhere to be found, I need only show the ""C -> D"" missing requirement; if D is resolved, then the whole stack is unlocked.

Similarly, if D is known by another dependency management system, but unregistered because it depends on E which is missing, then only the ""D -> E"" requirement is relevant.

The proposed code is composed as follows:

* servicediagnostics: the API package. It holds the main service interface as well as the plugin interface, to extend to other dependency management systems

* servicediagnostics.impl: the implementation package. It contains plugins implementations for org.apache.felix.scr and org.apache.felix.dependencymanager, as well as the main service implementation.

* servicediagnostics.webconsole: a Felix WebConsole plugin that displays a graphical view of the diagnostics result. Alternatively it can also show a graph of all services currently registered in the service registry and the bundles using them. See Screenshot-notavail.png and Screenshot-all.png. Using the Raphael graffle library (http://raphaeljs.com/graffle.html) allows for nice interactive views of the service registry. 

The attached package is provided as a maven project, along with a sample runner. Simply untar the files, run mvn install and then run.sh. 
Follow the Readme file  for details.

It is not perfect and still needs some improvements (in handling service filters for instance) but as such it is already quite useful. 

I hope you'll like the idea. 
Regards,
Arjun",A Service Diagnostics service and graphical WebConsole plugin,2,,,apanday,True,cziegeler,apanday
felix,FELIX-3124,2011-09-20T18:40:55.000+0000,2011-09-20T20:15:02.000+0000,2011-09-20T20:15:02.000+0000,,Fixed,New Feature,Minor,['framework-4.0.0'],['framework-3.2.2'],,,,,,,"['Framework', 'Specification compliance']","['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']","The OSGi R4.3 spec defines the org.osgi.framework.bsnversion framework configuration property that controls whether installing duplicate BSN/version pairs is an error or not. It supports the values ""single"" (which is the default and current behavior) or ""multiple"" (which enables duplicates).",[Framework] Implement OSGi R4.3 property to allow installing bundles with the same BSN and version,,,,rickhall,True,rickhall,rickhall
felix,FELIX-3122,2011-09-20T17:19:50.000+0000,2011-09-20T18:37:28.000+0000,2011-09-20T18:37:28.000+0000,,Fixed,New Feature,Minor,['framework-4.0.0'],['framework-3.2.2'],,,,,,,"['Framework', 'Specification compliance']","['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']",The OSGi R4.3 spec requires that a framework generates a UUID each time it is init'ed. The string value of the UUID is placed in the org.osgi.framework.uuid framework property.,[Framework] Implement OSGi R4.3 framework UUID,,,,rickhall,True,rickhall,rickhall
felix,FELIX-3069,2011-08-05T15:42:41.000+0000,2013-02-26T13:48:42.000+0000,2013-02-26T13:48:42.000+0000,,Won't Fix,New Feature,Minor,,,,,,,,,['iPOJO'],['iPOJO service-oriented component framework'],"iPOJO already supports some basic bind/unbind methods signature (notification only, gives the service, with ServiceRef or not, with a Map or not).

I would like to be able to have the service + any additional service property I'm interested in.
Something like the following code:
{noformat}
public void bindAdapter(XmlAdapter adapter,
                        @Extract(""namespace-uri"")) uri) {
}
{norformat}

With @Extract (better name to be found) extracting the ""namespace-uri"" service property from the ServiceReference and giving it to this method.",Support adapted bind/unbind method signature,,,,sauthieg,True,,sauthieg
felix,FELIX-3066,2011-08-04T09:58:14.000+0000,2011-08-11T09:27:57.000+0000,2011-11-15T16:20:41.000+0000,,Fixed,New Feature,Minor,['maven-scr-plugin-1.7.4'],['maven-scr-plugin-1.7.2'],,,,,,,['SCR Tooling'],"['Tooling for Maven, Ant, and Bnd including SCR annotations']","Next to the configuration option excludeSource, includeSource will be handy to only take a subset of code for generating the serviceComponents.xml.

This will allow you to have a master project that contains all sources and resources, but only creating bundles with DS of a subset of the source tree.",Add support for source directory include filter via plugin configuration,,,['maven'],janvolck,True,cziegeler,janvolck
felix,FELIX-3056,2011-07-26T18:24:21.000+0000,2011-08-02T16:11:08.000+0000,2011-08-02T16:11:08.000+0000,,Fixed,New Feature,Minor,['framework-4.0.0'],['framework-3.2.2'],,,,,,,"['Framework', 'Specification compliance']","['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']","The OSGi R4.3 spec introduces a new service registry hook, called EventListenerHook, which replaces the EventHook. Essentially, it is a more fine-grained version of the EventHook giving you control over precise listeners rather than operating on all listeners from a given bundle.",[Framework] Implement OSGi R4.3 event listener service registry hook,,,,rickhall,True,rickhall,rickhall
felix,FELIX-3052,2011-07-22T14:31:25.000+0000,2011-07-22T14:33:44.000+0000,2011-07-22T14:33:44.000+0000,,Fixed,New Feature,Minor,['framework-4.0.0'],['framework-3.2.2'],,,,,,,"['Framework', 'Specification compliance']","['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']",OSGi R4.3 requires that the system bundle provide generic capabilities for the execution environment (deprecating the old approach) as well arbitrary user-defined generic capabilities using the org.osgi.framework.system.capabilities and org.osgi.framework.system.capabilities.extra properties (similar to the org.osgi.framework.system.packages and org.osgi.framework.system.packages.extra properties). We need to implement this and provide default EE values.,[Framework] Implement OSGi R4.3 system bundle generic capabilities,,,,rickhall,True,rickhall,rickhall
felix,FELIX-3045,2011-07-18T18:09:56.000+0000,2011-10-21T19:03:33.000+0000,2013-05-02T02:30:52.000+0000,,Fixed,New Feature,Major,['webconsole-packageadmin-plugin 1.0.0'],,,,,,,,['Web Console'],['A web based management console'],I recently put together a small web console plugin which allows a user to enter a list of package or class names and get back the list of bundles which export those packages (deduping the list first) as well as a dependency block to insert into a Maven pom.,new webconsole plugin - dependency finder,,,,justinedelson,True,fmeschbe,justinedelson
felix,FELIX-3032,2011-07-11T15:40:17.000+0000,2011-08-02T14:31:13.000+0000,2011-08-02T14:31:13.000+0000,,Fixed,New Feature,Major,['framework-4.0.0'],['framework-3.2.2'],,,,,,,"['Framework', 'Specification compliance']","['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']","The OSGi R4.3 spec introduces a hook mechanism, called bundle hooks, to control which bundles can see other bundles via BundleContext.getBundles() and bundle events. We need to implement this functionality.",[Framework] Implement OSGi R4.3 bundle hooks,,,,rickhall,True,rickhall,rickhall
felix,FELIX-3018,2011-06-30T16:50:18.000+0000,,2013-06-07T14:25:18.000+0000,,,New Feature,Major,['maven-bundle-plugin-future'],['maven-bundle-plugin-2.3.4'],,,,,,,['Maven Bundle Plugin'],[''],"Please take a look at the discussion in FELIX-1571 that triggered this RFE to be filed. 

There are at least two predominant approaches to generating OSGi bundles using maven-bundle-plugin:
a) Use packaging type like war, jar, ejb, etc., configure bundle plugin's manifest goal to be run in an appropriate phase like process-classes and configure the maven-archiver to use bundle plugin generated MANIFEST.MF in the final artifact. 

b) Use packaging type bundle so that bundle plugin is responsible for making the final jar as well.

Each have their pros and cons. Contrary to approach #b, which is an OSGi-first approach, approach #a is where OSGi metadata generation is an additional step in the build process. User sets up the their project following maven conventions as per their packaging type and then they additionally configure bundle plugin to help them generate a valid OSGi bundle. It is but natural that many enterprise Java developers who are used to developing wars, ejb jars, etc. prefer approach #a. 

With all the recent fixes to maven-bundle-plugin, things have improved quite a lot. Approach #a is an optimal way to generate proper OSGi bundle except when there are dependencies embedded in the final jar. e.g., user may like to embed some jars in their WEB-INF/lib of the WAB. In such a case, maven archiver knows what all jars to be embedded; after all it is making the final war file. Yet, one has to repeat some of this in Embed-Dependency instruction of bundle plugin in order for bundle plugin to generate proper Bundle-ClassPath and Import-Package header. If Embed-Dependency has extra jars, then unnecessary Import-Package and Bundle-ClassPath will appear in the OSGi metadata. If Embed-Dependency has less jars, then the reverse will happen. I agree to the following comment made by Stuart in FELIX-1571:

""I think the proper solution may be to create a new feature that lets you update the manifest in the generated project artifact. That way you have the WAR artifact available, so bnd can produce the right manifest (and verify it) - although one outstanding issue is this might affect signing... ""

I don't know if there is someway to intercept maven-archiver processing, then bundle plugin could generate the manifest as the penultimate step in the packaging process. Anyway, I am sure with all the maven experts around, someone will suggest a way to do this.",Allow updation of an artifact's manifest at the final stage,,,,sahoo,True,,sahoo
felix,FELIX-3011,2011-06-23T23:22:20.000+0000,2011-06-28T23:35:51.000+0000,2011-10-11T17:09:18.000+0000,,Fixed,New Feature,Major,['maven-bundle-plugin-2.3.5'],,,,,,,,['Maven Bundle Plugin'],[''],,"Add a new goal to save the bnd instructions that would be passed onto bnd, to allow people to use other toolchains such as bndtools",,,,mcculls,True,mcculls,mcculls
felix,FELIX-3007,2011-06-21T12:41:46.000+0000,2011-06-21T12:43:33.000+0000,2011-06-22T06:41:51.000+0000,,Fixed,New Feature,Major,['webconsole-packageadmin-plugin 1.0.0'],,,,,,,,['Web Console'],['A web based management console'],"To chase down a resolution problem, a simple Package Admin Service plugin for the web console was extremely useful. This plugin currently analyzes the space of exported packages and dumps all packages along with their version, exporting and importing bundles.

This helped me find a solution for a class space inconsistency leading to FELIX-3003",Simple Package Admin plugin,,,,fmeschbe,True,fmeschbe,fmeschbe
felix,FELIX-2999,2011-06-15T21:02:53.000+0000,2011-06-15T21:04:37.000+0000,2011-06-15T21:04:37.000+0000,,Fixed,New Feature,Minor,['framework-4.0.0'],['framework-3.2.2'],,,,,,,"['Framework', 'Specification compliance']","['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']","Previous specs were not clear how strict or lenient framework implementations should be when coercing types when evaluating filter strings. The Felix framework was strict and did not massage parsed data when coercing types (e.g., trimming string values before coercing them to a number). Now the spec says that the string representation of number types should be trimmed before attempting coercion.",[Framework] OSGi R4.3 now specifies that number types should be trimmed when evaluating filters,,,,rickhall,True,rickhall,rickhall
felix,FELIX-2991,2011-06-10T17:28:00.000+0000,,2016-01-11T17:55:16.000+0000,,,New Feature,Major,['maven-bundle-plugin-future'],,,,,,,,['Maven Bundle Plugin'],[''],"When using the maven-bundle-plugin to create fragments which add export-packages to a host bundle, the maven-bundle-plugin considers all of the export-packages as superfluous, does not add them to the fragment's MANIFEST.MF file and fails to compile.  

The specific case is when attempting to add exports to a bundle where bnd-wrapping doesn't pick up all of the necessary packages. For example, when executing this line in Karaf:
osgi:install -s mvn:tibco/tibco-crypt/5.1.2

The wrap command skips over a number of packages. So creation of a fragment is necessary. However, the build fails for the above noted reason. The work-around for this issue is to use the maven-jar-plugin to create the fragment.",Allow the creating of fragments that populate Export-Package ,2,1,,karafman,True,,karafman
felix,FELIX-2986,2011-06-07T18:46:07.000+0000,2011-09-09T17:52:34.000+0000,2011-09-09T17:52:34.000+0000,,Fixed,New Feature,Major,['framework-4.0.0'],['framework-3.2.2'],,,,,,,"['Framework', 'Specification compliance']","['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']",The OSGi R4.3 specification introduces resolver hooks to impact how the resolver resolves bundle dependencies.,[Framework] Implement OSGi R4.3 resolver hooks,,,,rickhall,True,rickhall,rickhall
felix,FELIX-2975,2011-05-26T14:40:40.000+0000,2011-08-12T16:51:43.000+0000,2011-08-12T16:51:44.000+0000,,Fixed,New Feature,Major,['framework-4.0.0'],['framework-3.2.2'],,,,,,,"['Framework', 'Specification compliance']","['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']","The OSGi R4.3 spec deprecates the StartLevel service. It is replaced by two new interfaces, FrameworkStartLevel and BundleStartLevel, which are available from the new Bundle.adapt() method. The functionality is equivalent to the StartLevel service, this is just a new way to access it.",[Framework] Implement OSGi R4.3 framework start level object,,,,rickhall,True,rickhall,rickhall
felix,FELIX-2973,2011-05-25T15:33:49.000+0000,2011-09-12T21:51:14.000+0000,2011-09-12T21:51:14.000+0000,,Fixed,New Feature,Major,['framework-4.0.0'],['framework-3.2.2'],,,,,,,"['Framework', 'Specification compliance']","['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']","The OSGi R4.3 spec introduces a generic mechanism for a bundle to provide some capability (Provide-Capability) and other bundles to require such capabilities (Require-Capability). We need to implement this feature. Since it is based on the existing model used by the Felix framework, this should be reasonably straightforward. The biggest changes are the support of specifying types in the manifest header for capabilities and that generic capabilities can express ""uses"" constraints on packages.",[Framework] Implement OSGi R4.3 generic capabilities and requirements,,,,rickhall,True,rickhall,rickhall
felix,FELIX-2969,2011-05-23T15:36:54.000+0000,2011-08-12T16:52:09.000+0000,2011-08-12T16:52:09.000+0000,,Fixed,New Feature,Major,['framework-4.0.0'],['framework-3.2.2'],,,,,,,"['Framework', 'Specification compliance']","['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']",The OSGi R4.3 spec introduces a new FrameworkWiring API available from the system bundle adapt() method to replace the PackageAdmin service. We need to implement it.,[Framework] Implement OSGi R4.3 framework wiring object,,,,rickhall,True,rickhall,rickhall
felix,FELIX-2967,2011-05-23T02:59:48.000+0000,2018-07-03T19:40:01.000+0000,2018-07-03T19:40:01.000+0000,,Won't Fix,New Feature,Major,,,,,,,,,['Log Service'],['The Log Service from the OSGi R4 specification (Section 101).'],"Log information is critical during debugging and maintenance.
Now, the log service sub-project only provides memory-based implementation.
And, the pax or other log implementations are quite difficult to be installed and used.

Do people here have a plan to improve the current log service to support file-logging?
Really looking forward.",file-based log service wanted,,,"['log', 'logging']",drhades,True,,drhades
felix,FELIX-2961,2011-05-20T17:58:03.000+0000,2016-05-11T07:00:41.000+0000,2016-07-12T04:53:04.000+0000,,Won't Fix,New Feature,Major,,['scr annotations 1.5.0'],,,,,,,['SCR Tooling'],"['Tooling for Maven, Ant, and Bnd including SCR annotations']","hello!

when I use config admin to control instaniation of scr component services, I use this pattern:


// 1) in config source bundle:
			Configuration config = configAdmin.getConfiguration(""ZZZ"", null);
			Dictionary<String, String> props = config.getProperties();
			config.update(props);


//  2) in config target bundle:
@Service
@Component(name = ""AAA"", policy = ConfigurationPolicy.REQUIRE, immediate = true)
public class BucketPlugin implements PluginSpaceService {

	@Property(name = ""service.pid"")
	protected static final String PID = ""ZZZ"";


// 3) despite the fact service.pid ""looks good"" in xml for the tagret compenent:
    <scr:component enabled=""true"" immediate=""true"" name=""AAA"" configuration-policy=""require"">
        <implementation class=""com.ddfplus.core.space.BucketPlugin""/>
        <service servicefactory=""false"">
            <provide interface=""com.ddfplus.api.plugin.PluginSpaceService""/>
        </service>
        <property name=""service.pid"" type=""String"" value=""ZZZ""/>


// 4) the scr fails to initialize the component; intitialzation works only when 
(scr.component.name == config.service.pid) and NOT when (config.service.pid == scr.component.property.pid)


// 5) if I look on the the config target in console (when I do manage to inititialize it),
it shows that again, actual service.pid comes from scr.component.name and not from scr.component.property.service.pid


thank you.
",@Component annotation should support defining the name with a compile time constant (instead of just a constant value),2,,,andrei.pozolotin,True,,andrei.pozolotin
felix,FELIX-2959,2011-05-19T16:06:18.000+0000,2011-05-25T15:11:10.000+0000,2011-05-25T15:11:10.000+0000,,Fixed,New Feature,Major,['framework-4.0.0'],['framework-3.2.2'],,,,,,,"['Framework', 'Specification compliance']","['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']",Implement the OSGi R4.3 specification's new class loading hook to enable byte code weaving.,[Framework] Implement OSGi R4.3 class loader byte-code weaving hook,1,,,rickhall,True,rickhall,rickhall
felix,FELIX-2932,2011-04-25T09:05:48.000+0000,2011-04-25T09:08:20.000+0000,2012-05-14T14:58:28.000+0000,,Fixed,New Feature,Major,['ipojo-core-1.8.2'],['iPOJO-1.8.0'],,,,,,,['iPOJO'],['iPOJO service-oriented component framework'],"iPOJO analyzes bundles asynchronously using a separated thread. This makes iPOJO not usable on the Google App Engine. So, an option should be provided disabling the 'creator' thread and processing bundles synchronously.",Allows disabling the asynchronous processing in the iPOJO Extender,,,,clement.escoffier,True,clement.escoffier,clement.escoffier
felix,FELIX-2920,2011-04-18T08:00:04.000+0000,,2013-06-07T13:05:02.000+0000,,,New Feature,Major,['maven-bundle-plugin-future'],,,,2592000,2592000,2592000,,['Maven Bundle Plugin'],[''],"It's helpfull to add another goal checkConfig to check a configurations of bundles with the bundle 0 to see if all bundles can be resovled and reports some warning or errors like a package exported by two bundles, a package exported by a bundle less version and by another with a version ....

It's a test to see if these bundles can be started on a osgi configuration.

You can use felix implemention to do it.",Check a list of bundles to see if all bundles can be resolved when its will deploy on osgi framework.,1,1,,chomats,True,,chomats
felix,FELIX-2919,2011-04-18T07:54:13.000+0000,,2013-06-07T14:54:17.000+0000,,,New Feature,Major,['maven-bundle-plugin-future'],,,,2592000,2592000,2592000,,['Maven Bundle Plugin'],[''],"maven bundle plugin allows to aggregate binaries but it doesn't aggregate source when it's possible.
Add this action on goal source:jar and a configuration to activate this feature. ",Aggregate sources when aggregate binaries,,,,chomats,True,,chomats
felix,FELIX-2897,2011-03-31T00:57:49.000+0000,2011-09-23T13:01:14.000+0000,2013-09-28T17:40:02.000+0000,,Fixed,New Feature,Major,['http-2.2.1'],,,,,,,,['HTTP Service'],['The HTTP Service and the Http Whiteboard Service from the OSGi R7 specification (Section 102 and 140).'],"Jetty has a JMX module which can be enabled. See http://docs.codehaus.org/display/JETTY/JMX
",Add support for Jetty JMX MBeans,1,,,justinedelson,True,fmeschbe,justinedelson
felix,FELIX-2896,2011-03-30T07:57:04.000+0000,2012-08-15T12:51:15.000+0000,2013-06-06T15:34:55.000+0000,,Fixed,New Feature,Major,['webconsole-4.2.0'],['webconsole-3.1.8'],,,,,,,['Web Console'],['A web based management console'],"Currently all information displayed about a bundle is coded into the bundle list plugin.
We could improve this by defining a BundleInfoProvider interface which might return a map for a bundle.
The bundle list plugin will query all providers and display all key/value pairs as additional information about the bundle.

Open questions:
- how to we handle localization?
- how can a bundle provide this service without having a dependency to the web console api?",Add support for bundle info providers,2,,,cziegeler,True,v_valchev,cziegeler
felix,FELIX-2853,2011-02-22T17:18:57.000+0000,2011-03-04T14:22:12.000+0000,2011-03-13T12:55:52.000+0000,,Fixed,New Feature,Minor,"['maven-scr-plugin-1.7.0', 'scr ant task 1.1.0', 'scr generator 1.1.0']","[' maven-scr-plugin-1.6.0', 'scr generator 1.0.0']",,,,,,,['SCR Tooling'],"['Tooling for Maven, Ant, and Bnd including SCR annotations']","It's time to deprecate the old javadoc tags - they will still be supported of course, but users should rather use the scr annotations instead.
If javadoc tags are used a meaningful warning (like Maven does) should be printed out.",Deprecate javadoc tags,,,,cziegeler,True,cziegeler,cziegeler
felix,FELIX-2790,2011-01-19T09:45:49.000+0000,,2011-01-19T09:45:49.000+0000,,,New Feature,Minor,,,,,,,,,['Framework'],"['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).']",,Add a configurable timeout when trying to lock a bundle for modification (start / stop / update / etc....),,,,gnt,True,,gnt
felix,FELIX-2767,2011-01-07T17:12:04.000+0000,2011-01-11T08:48:33.000+0000,2011-01-11T16:26:53.000+0000,,Fixed,New Feature,Minor,['gogo.shell-0.8.0'],['gogo-0.6.1'],,,,,,,['Gogo Shell'],['scripting support and non-OSGi built-in commands'],"I would need to specify the IP address on which gogo telnet is listening (in addition to the port)
Please find attached a proposed patch. 

/arjun",gogo telnet IP address,,,,apanday,True,,apanday
felix,FELIX-2764,2011-01-05T01:14:46.000+0000,2011-01-05T01:19:46.000+0000,2015-03-17T07:06:58.000+0000,,Fixed,New Feature,Major,['gogo.runtime-0.8.0'],,,,,,,,['Gogo Runtime'],['RFC-147 shell runtime'],,Publish OSGi EventAdmin events when executing a command line,,,,gnt,True,gnt,gnt
felix,FELIX-2761,2011-01-04T16:34:09.000+0000,2011-01-05T01:19:40.000+0000,2015-03-17T07:07:06.000+0000,,Fixed,New Feature,Major,['gogo.runtime-0.8.0'],,,,,,,,['Gogo Runtime'],['RFC-147 shell runtime'],,Add a way to listen to execution of command lines,,,,gnt,True,gnt,gnt
felix,FELIX-2743,2010-12-23T16:20:47.000+0000,2010-12-23T16:34:10.000+0000,2013-02-04T12:33:22.000+0000,,Fixed,New Feature,Major,['iPOJO-1.8.0'],,,,,,,,['iPOJO'],['iPOJO service-oriented component framework'],"The Pojoization class is the core class managing the bundle manipulation.
However, extending this mechanism is quite hard so far and definitely require to ease the integration of the manipulation into environment such as Eclipse or IntelliJ.",Modify Pojoization to be more easily customizable,,,,clement.escoffier,True,clement.escoffier,clement.escoffier
felix,FELIX-2742,2010-12-23T16:16:44.000+0000,2010-12-23T16:18:31.000+0000,2013-02-04T12:33:24.000+0000,,Fixed,New Feature,Major,['iPOJO-1.8.0'],,,,,,,,['iPOJO'],['iPOJO service-oriented component framework'],Allow services to be injected into constructor parameters,Constructor injection of service dependencies,,,,clement.escoffier,True,clement.escoffier,clement.escoffier
felix,FELIX-2733,2010-12-17T10:22:53.000+0000,2010-12-17T10:27:00.000+0000,2013-03-25T14:40:05.000+0000,,Fixed,New Feature,Major,['iPOJO-1.8.0'],,,,,,,,['iPOJO'],['iPOJO service-oriented component framework'],The maven-ipojo-plugin does not yet support project using JAR and WAR as packaging (only BUNDLE) is supported. Those types should be supported.,The maven-ipojo-plugin should support JAR and WAR as packaging type,2,,,clement.escoffier,True,clement.escoffier,clement.escoffier
felix,FELIX-2730,2010-12-14T08:40:01.000+0000,,2010-12-17T10:50:17.000+0000,,,New Feature,Minor,,,,,,,,,['UPnP Subproject'],"['An effort for releasing the UPnP Device Service from the OSGi R4 specification (Section 111), tools, and examples using such specification.']","One of the larger areas of usage with UPnP is to expose a DMS. There are also many consumers of DMS devices, e.g. TVs. Many of the new TVs coming to the market are branded ""DLNA compatible"". This means they will require DMS devices to have the XML tag shown below in the device description:

<dlna:X_DLNADOC xmlns:dlna=""urn:schemas-dlna-org:device-1-0"">DMS-1.00</dlna:X_DLNADOC>

This is described in paragraph 7.2.10 in the document named ""DLNA Networked Device Interoperability Guidelines"". This is just one of many DLNA specific tags. However, it seems that a great number of DMR devices (consumers of DMS devices, e.g. TVs) expects this tag and only shows the DMS as a valid DMS if it is included.
I know this has nothing to do with UPnP since it is a DLNA specific tag, but it increases the usability of the UPnP stack and the number of usecases supported a great deal! Moreover, there is no way to set this XML tag in the device description using the OSGi UPnP API as it is currently specified.

What is needed is that the developer needs some way of controlling the content of the device description to include this tag.

So when creating the device the programmer supplies properties to control various things, such as name and description. It would be good to supply a property called ""X_DLNADOC"" with the value of the tag, e.g. DMS-1.00. So the code to create a DLNA supported device would look like:

public class DmsDevice implements UPnPDevice {
    private Dictionary dictionary = new Hashtable();

    public DmsDevice() {
        dictionary.put(UPnPDevice.FRIENDLY_NAME, ""My Name"");
        dictionary.put(""X_DLNADOC"", ""DMS-1.00"");
        ....
    }

    public Dictionary getDescriptions(String locale) {
        return this.dictionary;
    }
    ....
}

This should result in an XML like:

<?xml version=""1.0"" encoding=""utf-8""?>
<root xmlns=""urn:schemas-upnp-org:device-1-0"">
   <specVersion>
      <major>1</major>
      <minor>0</minor>
   </specVersion>
   <URLBase>http://<myip>:<myport></URLBase>
   <device>
      <deviceType>urn:schemas-upnp-org:device:MediaServer:1</deviceType>
      <dlna:X_DLNADOC xmlns:dlna=""urn:schemas-dlna-org:device-1-0"">DMS-1.00</dlna:X_DLNADOC>
      <friendlyName>My Name</friendlyName>
      ........
</root>",UPnP exposed to/used by DLNA devices,1,1,,alzear,True,,alzear
felix,FELIX-2720,2010-12-06T16:44:54.000+0000,2011-06-27T00:43:47.000+0000,2011-10-11T17:09:22.000+0000,,Duplicate,New Feature,Major,,['maven-bundle-plugin-2.0.1'],,,,,,,['Maven Bundle Plugin'],[''],"I think it's a important feature if you can manage the maven tags in maven-bundle-plugin. The dependencies tagged as optional must not be seen as mandatory dependencies when we install a bundle. Second, you don't need to configure twice the pom, one for the dependency and one for the bundle-plugin",implementartion for optional tags in maven-bundle-plugin,,,,luca.stancaqpiano,True,,luca.stancaqpiano
felix,FELIX-2719,2010-12-06T13:27:23.000+0000,2011-10-22T21:24:53.000+0000,2012-11-20T11:00:51.000+0000,,Fixed,New Feature,Major,[' metatype-1.0.6'],['metatype-1.0.4'],,,,,,,['Metatype Service'],['The Metatype Service from the OSGi R6 specification (Section 105).'],"The current reader / parser does not support name spaces. Add support for it, plus a unit test to validate the behavior.",Add name space support to the meta type implementation,1,,,marrs,True,fmeschbe,marrs
felix,FELIX-2712,2010-11-30T16:44:14.000+0000,2010-12-23T18:18:49.000+0000,2012-11-20T10:58:17.000+0000,,Fixed,New Feature,Major,['scr-1.6.2'],[' scr-1.6.0'],,,,,,,['Declarative Services (SCR)'],['The Declarative Services from the OSGi R7 specification (Section 112).'],"Currently, SCR only provides an ""scr"" command for the old shell. It should also include a Gogo command. A very simple approach would be to factor out the command implementation from the Command interface, to eliminate the dependency on the Shell package. This object would simply have a single method like public void scr(String[] args) that would do the current processing. For Gogo you'd just register this object directly as the command with some service properites, for Shell you'd wrap it in a Command.

A better approach would be to look at the OBR command for Gogo. In it, all OBR subcommands (e.g., obr list) just become methods on the service object and accept the needed parameters. The ""obr"" command becomes the command scope (in the service properties), so you can do ""obr:list"" at the Gogo prompt or just ""list"" if there is no ambiguity. You could still wrap this object in a Command to be compatible with Shell.

I'd recommend the second approach, since it allows you to leverage the Gogo annotations to provide decent help for the command.",[SCR] Add Gogo command support,3,1,,rickhall,True,fmeschbe,rickhall
felix,FELIX-2709,2010-11-23T21:26:25.000+0000,2011-12-19T09:21:54.000+0000,2012-11-23T06:54:45.000+0000,,Fixed,New Feature,Major,['webconsole-4.0.0'],['webconsole-3.1.6'],,,,,,,['Web Console'],['A web based management console'],,Allow webconsole context root be obtained from framework properties,1,,,tdiesler,True,fmeschbe,tdiesler
felix,FELIX-2692,2010-11-09T19:48:02.000+0000,2011-06-24T13:49:12.000+0000,2011-06-24T13:49:17.000+0000,,Fixed,New Feature,Major,['maven-bundle-plugin-2.3.4'],['maven-bundle-plugin-2.0.1'],,,,,,,['Maven Bundle Plugin'],[''],"Given the standardisation on WABs as part of the official specification, and that WARs currently don't have web data associated with them, would it make sense to support a new packaging type 'wab' for Maven projects that does the same as the WAR plugin, but adding metadata as well? It might need to determine the difference between 'compile' and 'provided' for inclusion in the 'WEB-INF/lib' subdirectory, so that the dependency information is recorded but not packaged into the WAR file.
",Support maven type 'wab' for web bundles,1,,,alex.blewitt,True,mcculls,alex.blewitt
felix,FELIX-2679,2010-11-01T19:26:34.000+0000,2010-11-01T20:03:00.000+0000,2014-03-12T08:55:24.000+0000,,Fixed,New Feature,Minor,"[' maven-scr-plugin-1.6.0', 'scr ant task 1.0.0', 'scr generator 1.0.0', 'scr annotations 1.4.0']",,,,,,,,['SCR Tooling'],"['Tooling for Maven, Ant, and Bnd including SCR annotations']","Currently annotation tag providers must be listed/configured by the user, e.g. in the maven plugin configuration, in order to be ""found"".

We could use the META-INF/services mechanism to find them.

The annotations module lists their providers in the META-INF/services and the scr generator searches them.

In addition, we can remove the dependency from the ant task and the maven scr plugin to the scr annotations",Detect AnnotationTagProvider's through META-INF/services,,,,cziegeler,True,cziegeler,cziegeler
felix,FELIX-2678,2010-10-31T05:59:27.000+0000,2010-11-01T15:38:13.000+0000,2010-11-03T20:40:25.000+0000,,Fixed,New Feature,Minor,['framework-3.0.6'],['framework-3.0.5'],,,,,,,['Main'],['The launcher for the framework'],"Right now, there is no way to specify what the start level is for bundles that are found in the auto deploy directory as specified by ""felix.auto.deploy.dir"" in the felix configuration. By default, it seems that these bundles are always loaded first as compared to other bundles defined in config.properties using the ""felix.auto.start.<level>"" properties.

Since the shell bundles are in the bundle directory, it would be nice if one could specify that the shell bundles are loaded *after* the application bundles had been loaded, so the shell prompt would show up only then as well, indicating that system startup is complete.

Thus being able to specify a <level> which is related to the <level>s on the ""felix.auto.start"" property probably makes sense.",Startlevel for bundles in auto.deploy.dir,1,,,twunden,True,rickhall,twunden
felix,FELIX-2667,2010-10-18T13:13:27.000+0000,2012-09-19T13:46:30.000+0000,2012-09-19T13:46:30.000+0000,,Fixed,New Feature,Major,,['fileinstall-3.0.2'],,,,,,,['File Install'],"['A bundle that watches a directory set by the felix.fileinstall.dir property; the default is ""load"". JAR files in this directory are installed and uninstalled when they are no longer there. Updating the JAR file in the directory will update the bundle. The bundle allows you to use downloads from a browser, drag and drop, or the command line copy function, for OSGi management.']","In big projects (more than 30 bundles) deploying all the bundles to watching directory leads to bump of memory and CPU usage (sometimes till OutOfMemory). We should limit files to be deployed at a time.
The patch introduces new integer property ""felix.fileinstall.files.batch.size"" to enable processing watching directory by chunks. Default value - 0, which disables limit.",FileInstall ability to install from watching directory by portions,2,1,,vbuell,True,gnt,vbuell
felix,FELIX-2649,2010-10-11T08:14:10.000+0000,2010-10-11T14:19:46.000+0000,2010-11-09T09:46:55.000+0000,,Fixed,New Feature,Major,['webconsole-3.1.6'],['webconsole-3.1.2'],,,,,,,['Web Console'],['A web based management console'],"Currently if one wants to develop a configuration printer, the developed component has to import/implement the configuration printer interface. This ties the bundle to the web console bundle - which makes it more complicated to implement such extensions than it should be.
For plugins we already have the dynamic registration without requiring to implement a web console specific interface by searching for servlet services with specific properties.

We could implement something similar for configuration printers:
We search for all services having at least these properties:
- felix.webconsole.label
- felix.webconsole.title
- felix.webconsole.configprinter.modes

If any service has these three properties *and* has a method printConfiguration(PrintWriter) or printConfiguration(PrintWriter, String (=mode)), this service is added as a configuration printer. This method is searched by reflection.",Support for configuration printers without requiring them to implement the interface,,,,cziegeler,True,cziegeler,cziegeler
felix,FELIX-2647,2010-10-10T20:21:43.000+0000,2013-12-30T10:39:50.000+0000,2014-01-18T19:46:13.000+0000,,Fixed,New Feature,Major,['coordinator-1.0.0'],,,,,,,,"['Coordinator Service', 'Specification compliance']","['The Coordinator Service from the OSGi R6 specification (Section 130).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']","The OSGi R 4.3 draft 2 foresees the specification of a new Coordinator Service supporting a light-weight transaction-like functionality. Apache Felix should have an implementation of this service to be able to support the extension of the Configuration Admin service which depends on the Coordinator service.

The Apache Felix implementation will obey the Provisional OSGi API policy [1] by renaming the exports as follows:

   org.osgi.jmx.service.coordination ==> org.apache.felix.jmx.service.coordination
   org.osgi.service.coordination ==> org.apache.felix.service.coordination

and marking the exports with the mandatory attribute status=""provisional"" as well as marking all interfaces/classes deprecated.

For full details see the OSGi R 4.3 draft 2 available from http://www.osgi.org/Download/File?url=/download/osgi-4.3-early-draft2.pdf

[1] http://felix.apache.org/site/provisional-osgi-api-policy.html",Implement Coordinator Service,1,,,fmeschbe,True,cziegeler,fmeschbe
felix,FELIX-2646,2010-10-08T20:46:22.000+0000,2010-11-03T20:41:48.000+0000,2010-11-03T20:41:47.000+0000,,Fixed,New Feature,Minor,['framework-3.0.6'],['framework-3.0.4'],,,,,,,['Framework'],"['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).']","Currently, the framework completely ignores concurrent access to the same bundle cache and leaves it as a configuration issue. The downside of this approach is that a mistaken configuration could corrupt your bundle cache. We could avoid this by introducing some sort of bundle cache locking protocol using java.nio.channels.FileLock. I think it would be sufficient to simply implement a fail-fast locking protocol.",[Framework] Locking could be used to prevent concurrent access to a single bundle cache,,,,rickhall,True,rickhall,rickhall
felix,FELIX-2642,2010-10-07T13:31:48.000+0000,2010-10-21T11:20:32.000+0000,2010-11-09T09:44:36.000+0000,,Fixed,New Feature,Major,['scr ant task 1.0.0'],,,,,,,,['SCR Tooling'],"['Tooling for Maven, Ant, and Bnd including SCR annotations']","There have already been requests for an ANT task providing the same functionality to ANT build systems as the Maven SCR Plugin provides to Maven. The plugin code has already been refactored to isolate the Maven specifics from the actual SCR descriptor generation stuff.

This issue is about implementing the ANT task glue.",Create an ANT task for the SCR plugin functionality,,,,fmeschbe,True,fmeschbe,fmeschbe
felix,FELIX-2638,2010-10-06T13:36:09.000+0000,2010-10-06T15:12:24.000+0000,2010-11-09T09:46:53.000+0000,,Fixed,New Feature,Major,['webconsole-3.1.6'],['webconsole-3.1.2'],,,,,,,['Web Console'],['A web based management console'],"I would be really handy for getting up to date information about the instance if the output of a single configuration printer is accessible via http.

For example, something like /system/console/config.txt?printer=XYZ gives me the text output of that printer.

Now, while I guess implementing this should be really easy, the problematic part is the XYZ - for now we just have the title of the printer - which might not be unique.",Make a single configuration printer output available via http,,,,cziegeler,True,cziegeler,cziegeler
felix,FELIX-2637,2010-10-06T11:02:00.000+0000,2012-10-03T13:07:19.000+0000,2012-10-09T09:06:00.000+0000,,Incomplete,New Feature,Minor,,,,,,,,,,,"Attached is the initial contribution of a Monitor Admin Service implementation.

The zip file contains 3 projects:

uk.co.card.osgi.monitor             - The main implementation project.
uk.co.card.osgi.monitor.build   - The parent pom.
uk.co.card.osgi.monitor.test     - Pax-exam based integration tests.

MD5: CF023CF49909B2E318D834073C0C4A97
",Monitor Admin Service contribution,3,,,nickw,True,,nickw
felix,FELIX-2625,2010-09-24T06:33:30.000+0000,2012-01-31T13:05:53.000+0000,2012-01-31T13:05:53.000+0000,,Fixed,New Feature,Major,['webconsole-gogo-plugin-1.0.0'],,,,,,,,['Web Console'],['A web based management console'],"The Gogo Shell has replaced the simple Felix Shell as the command line tool for OSGi Management. We should provide support for this shell from within the Web Console, too.",Provide a plugin for the Gogo Shell,,,,fmeschbe,True,fmeschbe,fmeschbe
felix,FELIX-2544,2010-08-17T14:00:11.000+0000,2010-08-17T14:55:16.000+0000,2010-08-17T14:55:16.000+0000,,Fixed,New Feature,Major,['shell.remote-1.1.0'],['shell.remote-1.0.4'],,,,,,,['Remote Shell'],['Simple remote access for the Felix shell'],"Currently, shell.remote only supports the Felix shell. It would be nice if it also supported the Gogo shell.",Modify shell.remote to work with Gogo too,,,,rickhall,True,rickhall,rickhall
felix,FELIX-2537,2010-08-11T14:23:44.000+0000,2010-08-11T14:28:52.000+0000,2010-08-11T14:28:52.000+0000,,Fixed,New Feature,Major,['sigil-1.0.0'],,,,,,,,['Sigil'],['OSGi tooling'],,Add support for scripting junit test scenarios with sigil junit,,,,davemssavage,True,davemssavage,davemssavage
felix,FELIX-2530,2010-08-08T16:38:15.000+0000,2014-04-30T16:55:29.000+0000,2014-04-30T16:55:29.000+0000,,Fixed,New Feature,Major,,,,,,,,,['Declarative Services (SCR)'],['The Declarative Services from the OSGi R7 specification (Section 112).'],"The current Declarative Services administrative API has one major issue in that it does not differentiate between the declaration of a component and zero, one, or more actual instances (Component Configurations in Spec lingo). This leads to not-so-nice and inappropriate separation of concern in the implementation of the API.

Therefore a new API should be devised which differentiates between declaration and instance (configuration).",New Declarative Services administrative API,1,,,fmeschbe,True,fmeschbe,fmeschbe
felix,FELIX-2514,2010-08-03T09:10:27.000+0000,2010-08-06T14:10:52.000+0000,2015-03-17T07:03:03.000+0000,,Fixed,New Feature,Major,['fileinstall-3.1.0'],,,,,,,,['File Install'],"['A bundle that watches a directory set by the felix.fileinstall.dir property; the default is ""load"". JAR files in this directory are installed and uninstalled when they are no longer there. Updating the JAR file in the directory will update the bundle. The bundle allows you to use downloads from a browser, drag and drop, or the command line copy function, for OSGi management.']","For easier integration with other environments it would be nice if the felix.fileinstall.dir property could point to more than a single directory, like several dirs separated by a "":""

This would make the use/integration of File Install in Apache Sling much easier as  Sling by default has its own initial watch directory, but users might already use this property to configure their additional location.",felix.fileinstall.dir should support more than one directory,,,,cziegeler,True,cziegeler,cziegeler
felix,FELIX-2513,2010-08-03T09:06:27.000+0000,2010-08-06T14:35:53.000+0000,2015-03-17T07:03:03.000+0000,,Fixed,New Feature,Major,['fileinstall-3.1.0'],,,,,,,,['File Install'],"['A bundle that watches a directory set by the felix.fileinstall.dir property; the default is ""load"". JAR files in this directory are installed and uninstalled when they are no longer there. Updating the JAR file in the directory will update the bundle. The bundle allows you to use downloads from a browser, drag and drop, or the command line copy function, for OSGi management.']","The current format read by file install for configurations uses the java.util.Properties - this format does not support multi values and objects other than strings.
As configurations usually use other types than string, it would be nice to support the richer format like for example the config admin implementation uses.

To be compatible, I suggest we leave the old behaviour for "".cfg"" files and use a new extension "".config"" for the other format.",Support richer format for configurations,,,,cziegeler,True,cziegeler,cziegeler
felix,FELIX-2461,2010-07-03T15:08:02.000+0000,2010-07-03T15:15:12.000+0000,2010-09-03T11:39:47.000+0000,,Fixed,New Feature,Major,['iPOJO-1.8.0'],,,,,,,,['iPOJO'],['iPOJO service-oriented component framework'],"The @serviceController annotation does not allow to set the targeted specification. By adding a 'specification' attribute, the controller can be assigned to control one interface only and then allowing fine tuning like:

@Component
@Provides
public class MyClass implements Service1, Service2 {

  @ServiceController(specification=Service1.class)
  boolean m_controller1;

  @ServiceController(specification=Service2.class)
  boolean m_controller2;
}

If no 'specification' set, all not-already-targeted interfaces are targeted.
@Component
@Provides
public class MyClass implements Service1, Service2, Service3 {

  @ServiceController(specification=Service1.class)
  boolean m_controller1;

  @ServiceController // Control Service2 and Service 3
  boolean m_controller2;
}

",Allow specifying the targeted service interface in the @ServiceController,,,,clement.escoffier,True,clement.escoffier,clement.escoffier
felix,FELIX-2410,2010-06-15T08:40:54.000+0000,,2010-06-15T08:43:06.000+0000,,,New Feature,Major,,['webconsole-3.0.0'],,,,,,,['Web Console'],['A web based management console'],As a follow-up to FELIX-2246 a simple service factory to implement web console plugins may be provided.,Provide simple template service factory console plugin,,,,fmeschbe,True,,fmeschbe
felix,FELIX-2363,2010-05-27T15:59:39.000+0000,2010-05-27T18:23:31.000+0000,2012-02-26T15:42:26.000+0000,,Fixed,New Feature,Major,['gogo-0.6.0'],['gogo-0.4.0'],,,,,,,['Gogo Runtime'],['RFC-147 shell runtime'],"Gogo should have annotations for command implementations that allow for optional and out-of-order arguments as well as descriptive information. This will be beneficial for creating a common ""help"" system too.

[Technically, this work is basically done. I created this issue so the work would be documented in the Gogo change log and also because I am going to try to do a few changes to the existing implementation to align with some potential future spec changes.]",[Gogo] Add annotations for creating commands with optional and out-of-order arguments,1,,,rickhall,True,rickhall,rickhall
felix,FELIX-2322,2010-05-04T02:15:36.000+0000,,2017-08-12T15:00:34.000+0000,,,New Feature,Major,['maven-bundle-plugin-future'],,,,,,,,['Maven Bundle Plugin'],[''],"It seems like there is no way to specify the compression level of the bundle JAR file generated by maven-bundle-plugin.  To reduce the bandwidth, I'd like to compress the JAR as much as possible.  It would be nice if there is a configurable property in the plugin's <configuration/> section in the pom.xml.",Configurable JAR compression level,2,1,,trustin,True,,trustin
felix,FELIX-2303,2010-04-27T06:55:23.000+0000,2010-04-27T07:07:59.000+0000,2014-03-12T08:55:24.000+0000,,Fixed,New Feature,Major,"[' maven-scr-plugin-1.4.4', ' scr annotations 1.3.0']","['maven-scr-plugin-1.4.2', 'scr annotations 1.2.0']",,,,,,,['SCR Tooling'],"['Tooling for Maven, Ant, and Bnd including SCR annotations']","A new SlingFilter annotation which creates internally the required component, service, and property tags:
@SlingFilter with properties
int order - required
FilterScope scope : defaults to request scope
bool generateComponent defaults to true
bool generateService defaults to true
String name - defaults to default name
boolean metatype defaults to false (like the component annotation)
label and description : defaults to default values",Add annotation for a Sling servlet filter,,,,cziegeler,True,cziegeler,cziegeler
felix,FELIX-2279,2010-04-13T12:37:04.000+0000,2010-05-04T15:40:49.000+0000,2010-05-27T06:02:50.000+0000,,Fixed,New Feature,Major,['iPOJO-1.8.0'],['iPOJO-1.4.0'],,,,,,,['iPOJO'],['iPOJO service-oriented component framework'],"As a convenience, it would be very useful for an iPOJO service to have access to its ServiceReference object. E g for logging purposes this is essential information. Assuming that the service has been registered prior to the @Validate method being called, it makes sense to provide the ServiceReference as an argument to this method.

If the service is not registered until the @Valid method returns, then another callback (e g @Registered) is needed.",Access to ServiceReference in iPOJO service,,,,rodehav,True,clement.escoffier,rodehav
felix,FELIX-2209,2010-03-17T07:13:05.000+0000,2011-06-22T06:40:38.000+0000,2011-06-22T06:41:02.000+0000,,Duplicate,New Feature,Major,['webconsole-packageadmin-plugin 1.0.0'],,,,,,,,['Web Console'],['A web based management console'],"To allow for close introspection into the impoer/export package management and resolution, a plugin should be devised interfacing with the PackageAdmin service.

Such a plugin could then also be used to implement the original request of FELIX-1441: allowing to search for imported and exported packages

In addition, duplicates etc. could be shown.",Implement a PackageAdmin plugin,,,,fmeschbe,True,fmeschbe,fmeschbe
felix,FELIX-2197,2010-03-12T09:57:54.000+0000,,2012-02-29T16:19:42.000+0000,,,New Feature,Major,,['framework-2.0.4'],,,,,,,['Framework'],"['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).']","Please implement something like Equinox's org.eclipse.osgi.baseadaptor.hooks.ClassLoadingHook. We need it for running Felix under Terracotta (there is HOWTO for Equinox: http://www.terracotta.org/confluence/display/wiki/Run+with+Eclipse+Equinox). 

We patched the Felix Framework for the workaround. The patch is attached. We use it like this:

...
import org.apache.felix.framework.ClassLoaderHook;
import org.apache.felix.framework.Felix;
import com.tc.object.bytecode.hook.impl.ClassProcessorHelper;
import com.tc.object.loaders.NamedClassLoader;
...
	final Properties properties = new Properties();
...
	properties.put(""classLoaderHook"", new ClassLoaderHook() {
		@Override
		public void classLoaderCreared(final ClassLoader classLoader) {
			if (classLoader instanceof NamedClassLoader) {
				((NamedClassLoader) classLoader).__tc_setClassLoaderName(classLoader.toString());
				ClassProcessorHelper.registerGlobalLoader((NamedClassLoader) classLoader, null);
			}
		}
	});
		
	return new Felix(properties);
...

Thanks in advance.",Please implement something like Equinox's org.eclipse.osgi.baseadaptor.hooks.ClassLoadingHook,,,,zdila,True,,zdila
felix,FELIX-2185,2010-03-10T10:30:31.000+0000,,2012-08-15T16:04:58.000+0000,,,New Feature,Minor,,,,,,,,,['Web Console'],['A web based management console'],"With the current web console, after being updated to JQuery + JQuery UI the plugin structure becomes quite clear and simple:

1. There is a Servlet (AbstractWebConsolePlugin), which main role is to generate the required data for the plugin, typically a JSON and execute actions.
2. Although the actual rendering is in the Servlet, it use a template file, that contains the HTML markup
3. The main logic, that makes the interface interactive is JavaScript code, that uses the JSON from the Servlet and modifies the HTML markup.

This separation makes it quite easy to develop applications in a static way replacing the upper components with:
1. a sample JSON data, statically generated
2. a HTML file that is equals to : header + template + footer
3. almost the same JavaScript code, that instead of using AJAX request to obtain the data, uses the sample data in 1.

I must say, that personally have developer several plugins exactly using static HTML file. 

Although easy it consumes time to create that empty HTML file and start developing the plugin. It would be much easier, if we have an option to automatically generate a zip file - containing the HTML updated with the latest header footer, the JavaScript libraries, included by default (e.g. res/lib/) folder, common images and webconsole.css file.

This task can be easily achieved with modern build tools. Because my minimal knowledge of the Maven build system, I've implemented that task in a simple ANT build file, which must be placed in the webconsole root folder. When ant is invoked with that file, it will generate a file named 'static-test.zip' that contains a template, which is ready for modification and includes up-to-date sources.


",easy plugin prototyping - tools for developers,1,,,v_valchev,True,,v_valchev
felix,FELIX-2174,2010-03-05T14:44:35.000+0000,,2010-03-05T14:44:35.000+0000,,,New Feature,Minor,,,,,,,,,['Bundle Repository (OBR)'],[''],"Currently the repository.xml is read once then cached for the lifetime of the OBR bundle. If the bundles available on the OBR server change the repository.xml is never re-read, so the client doesn't see the changes.

It would be nice if the OBR bundle could check the server to see if there have been any changes, either by polling or by a specific request from the client code to RepositoryAdmin.

Perhaps the repository.xml could include a ""lastChanged"" timestamp or, as the repository.xml file may become large, perhaps there should be two requests; one to check if anything has changed, the second to re-read the repository.xml.",Detect changes to repository.xml and update cache,,,,nickw,True,,nickw
felix,FELIX-2145,2010-03-01T10:50:29.000+0000,2010-05-18T16:33:38.000+0000,2010-05-18T16:33:38.000+0000,,Won't Fix,New Feature,Major,,,,,,,,,['Karaf'],[''],,Add a new feature for woodstox stax parser (and make the obr feature depend on it for correct speed),,,,gnt,True,gnt,gnt
felix,FELIX-2144,2010-03-01T09:33:16.000+0000,2010-03-01T09:37:14.000+0000,2010-04-01T11:48:34.000+0000,,Fixed,New Feature,Major,['bundlerepository-1.6.0'],,,,,,,,['Bundle Repository (OBR)'],[''],"Global constraints on resources would be useful to be able to add constraints to the resolution like:
  * do not use a resource
  * add version range to used resources ...

",Add global requirements and capabilities,,,,gnt,True,gnt,gnt
felix,FELIX-2132,2010-02-24T17:48:56.000+0000,2010-02-24T18:31:19.000+0000,2010-04-25T14:40:35.000+0000,,Fixed,New Feature,Major,['iPOJO-1.6.0'],,,,,,,,['iPOJO'],['iPOJO service-oriented component framework'],"Provides something like:
@Component
@Provides
public class MyImpl implements Service {
    @ServiceController
    private boolean controller;
}

To provide a way to control the service exposition, a boolean has to be intercepted to reflect the changes on the provided service.
If the boolean is set to false, the service is unpublished, if it is set to true the service is published.

The controller also accepts an initial value to avoid publishing the service at startup.",Provides a way to control service exposition from the implementation class,,,,clement.escoffier,True,clement.escoffier,clement.escoffier
felix,FELIX-2103,2010-02-18T20:55:33.000+0000,2010-02-18T22:16:42.000+0000,2010-04-01T11:48:33.000+0000,,Fixed,New Feature,Major,['bundlerepository-1.6.0'],,,,,,,,['Bundle Repository (OBR)'],[''],"The current obr url handler has been designed to support updating bundles.
It should be possible to update it to allow installation of bundles, for example:

   install obr:symbolicname
or
   install obr:symbolicname/version

",Improve the OBR url handler to be able to access external bundles,,,,gnt,True,gnt,gnt
felix,FELIX-2100,2010-02-18T19:29:32.000+0000,2016-09-08T17:40:29.000+0000,2016-12-29T10:52:50.000+0000,,Implemented,New Feature,Major,,,,,,,,,"['Bundle Repository (OBR)', 'Configuration Admin']","['', 'The Configuration Admin from the OSGi R7 specification (Section 104).']","I've a request to be able to make customer specific configuration bundles - bundles that includes Configurations for other bundles. If a bundle is deployed, extracts the configuration files and register it with configadmin. If bundle is removed, removes configurations. If you have ideas, please share it! (I'm new in this Felix world, I've used it only with Sling as a user)

I will make a short proporsal and a whiteboard implementation.

The name of impmelemtation is configloader
The configloader service will implement the SynchronousBundleListener, and registering itself in activation and unregistering in deactivation.

When a new bundle is registering, checking the Bundle-InitialConfigurations in META-INF/MANIFEST.MF file. If the entry is presented, checking the given folders for *.xml files describes the configurations entry. (To handle the Factory services also).

Is it correct?

",Initial Config Loader,2,,,robson@semmi.se,True,,robson@semmi.se
felix,FELIX-2051,2010-02-08T08:53:44.000+0000,,2016-10-25T23:40:05.000+0000,,,New Feature,Major,,['iPOJO-1.4.0'],,,,,,,['iPOJO'],['iPOJO service-oriented component framework'],"I would really appreciate if iPOJO could support the automatic generation of metatype.xml files for the MetaType Service.
DS does this with the attribute ""metatype=true"" on the component annotation, after which the maven plugin generates the MetaType Service file.",Support the MetaType Service,1,1,,cathodion,True,,cathodion
felix,FELIX-2047,2010-02-05T08:31:40.000+0000,2010-02-05T09:41:04.000+0000,2010-02-05T09:41:04.000+0000,,Fixed,New Feature,Major,['karaf-1.4.0'],,,,,,,,['Karaf'],[''],,Support for WAR files,,,,gnt,True,gnt,gnt
felix,FELIX-2033,2010-02-03T12:21:21.000+0000,2010-02-03T15:19:21.000+0000,2010-02-03T15:19:21.000+0000,,Fixed,New Feature,Major,['karaf-1.4.0'],,,,,,,,['Karaf'],[''],,Provide an easy to use layer for writing pax-exam test for Karaf,,,,gnt,True,gnt,gnt
felix,FELIX-2020,2010-01-29T15:51:43.000+0000,2010-02-03T15:19:12.000+0000,2010-02-22T06:59:25.000+0000,,Fixed,New Feature,Major,['eventadmin-1.2.2'],['eventadmin 1.0.0'],,,,,,,['Event Admin'],['The Event Admin from the OSGi R6 specification (Section 113).'],"If the configuration admin is available it should be possible to configure the event admin through configuration (and not just through framework properties)
If the metatype service is available, a provider should be registered for the event admin as well - this makes the event admin configurable through the web console",Make event admin configurable through configuration admin,,,,cziegeler,True,cziegeler,cziegeler
felix,FELIX-1991,2010-01-18T19:52:14.000+0000,2010-01-19T21:13:16.000+0000,2010-01-20T14:55:44.000+0000,,Fixed,New Feature,Major,['framework-2.0.3'],['framework-2.0.2'],,,,,,,['Framework'],"['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).']","Currently, the same boot delegation class loader is used for all bundles. In cases where you are integrating with other module systems, making this configurable per bundle could allow you to bridge to other module systems.",Allow boot delegation class loader to be configurable per bundle,,,,rickhall,True,rickhall,rickhall
felix,FELIX-1975,2010-01-11T17:45:30.000+0000,2010-01-25T15:26:51.000+0000,2010-01-25T15:26:51.000+0000,,Fixed,New Feature,Major,['karaf-1.4.0'],,,,,,,,['Karaf'],[''],"Add oracle database support to Karaf jdbc locking feature.

Current default JDBC class does not work well with Oracle databases, please add support for functionality with Oracle.",[Karaf] Add oracle database support to Karaf jdbc locking feature.,,,,jgoodyear,True,gertvanthienen,jgoodyear
felix,FELIX-1974,2010-01-10T22:20:42.000+0000,2017-10-24T10:22:30.000+0000,2018-04-13T21:54:35.000+0000,,Workaround,New Feature,Minor,['framework-5.6.10'],"['framework-0.8.0', 'framework-1.0.0', 'framework-1.0.1', 'framework-1.0.3', 'framework-1.0.4', 'framework-1.2.0', 'framework-1.2.1', 'framework-1.2.2', 'framework-1.4.0', 'framework-1.4.1', 'framework-1.6.0', 'framework-1.6.1', 'framework-1.8.0', 'framework-1.8.1', 'framework-2.0.0', 'framework-2.0.1', 'framework-2.0.2']",,,,,,,['Framework'],"['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).']","This issue is described in section 3.14 of the OSGi R4 specification. The main jist is to provide a standard mechanism to place classes on the JVM boot class path. This mechanisms is modeled as bundle fragments, but it not really completely related to it. The boot class path aspect is likely to be dependent on specific JVMs. Please note that we have support for ""normal"" extension bundles already.",Implement boot class path extension bundle support,2,,,karlpauls,True,karlpauls,karlpauls
felix,FELIX-1963,2010-01-02T19:23:41.000+0000,2015-10-02T07:11:00.000+0000,2015-10-02T07:11:00.000+0000,,Fixed,New Feature,Major,,['http-2.0.4'],,,,,,,['HTTP Service'],['The HTTP Service and the Http Whiteboard Service from the OSGi R7 specification (Section 102 and 140).'],Check out the possibility to share ServletContext between bundles. If ServletContextManager is global instead of local to each bundle then it chould be done. If you want to share ServletContext each HttpContext must implement equals method that ensures equality.,Add possibility to share ServletContext between bundles,2,1,,srs,True,srs,srs
felix,FELIX-1962,2009-12-31T11:01:37.000+0000,2011-01-10T07:30:23.000+0000,2011-02-08T09:46:44.000+0000,,Fixed,New Feature,Major,['http-2.2.0'],['http-2.0.4'],,,,,,,['HTTP Service'],['The HTTP Service and the Http Whiteboard Service from the OSGi R7 specification (Section 102 and 140).'],"The new Http Service implementation currently does not support any Servlet API listeners at all. Support for some listeners can easily be implemented in a transparent way: ServletContextAttributeListener, ServletRequestListener, ServletRequestAttributeListener.

The HttpSession listeners can probably not easily be implemented in such a transparent way.

The ServletContextListener is probably not worth it supporting. Most (if not all) use cases for ServletContextListeners in traditional web applications can be solved in better ways in an OSGi framework.",Add support for (select) Servlet API listeners,1,1,,fmeschbe,True,fmeschbe,fmeschbe
felix,FELIX-1959,2009-12-22T09:18:08.000+0000,2010-02-22T14:21:14.000+0000,2010-04-01T11:50:36.000+0000,,Fixed,New Feature,Minor,['webconsole-3.0.0'],,,,,,,,['Web Console'],['A web based management console'],"Currently the Web Console uses heavily on the JQuery framework. Using a unified JavaScript framework simplifies development of all plugins and unifies the used approach.

However, when talking about visual styling, there are number of differences because each plugin developer uses own styles.

My suggestion is to adopt the JQuery UI . The benefits of using it as unified widget/css framework are:
- no time to spend on writing widgets already in the library
- clean CSS visual styling
- easy way to change the L&F by changing the theme (extended branding support!)
- improved cross-browser support (JQuery UI takes care of CSS differences)

Using the JQuery UI framework the developer shouldn't care about color but only for layout - components position; and for data being displayed.

To illustrate the benefits I've saved the ""Log Service"" page, modified it to use JQuery UI, took screen-shot, modified the theme CSS only, and again took screen-shot, and finally added the original L&F for reference, so you can easily compare the result.

The attached image contains the combined screen-shots.
""",Move towards unified L&F and extended branding support,1,,,v_valchev,True,fmeschbe,v_valchev
felix,FELIX-1957,2009-12-21T14:07:14.000+0000,2010-02-18T21:24:33.000+0000,2010-02-18T21:29:08.000+0000,,Fixed,New Feature,Major,['webconsole-2.0.6'],['webconsole-2.0.4'],,,,,,,['Web Console'],['A web based management console'],"Currently the Web Console requires some methods that are JDK 1.4+ specific.
In some embedded JVM the methods are not available. 
However, it's not needed to sync with each embedded JVM but make sure to run at least at OSGi/Minimum-1.0 Execution Environment.

The problem methods are:
String.matches(pattern);   (LicenseServlet & BundlesServlet)
String.replaceAll();              (ConfigurationRender)
String.split();                        (BundleRepositoryRender)

The class LinkedHashMap is missing in OSGi/Minimum-1.0 EE but used in the ConfigurationListener.",Make Web Console compatible with OSGi/Minimum-1.0 EE,1,,,v_valchev,True,fmeschbe,v_valchev
felix,FELIX-1893,2009-11-26T07:26:44.000+0000,2009-12-07T11:02:40.000+0000,2010-07-30T11:42:06.000+0000,,Fixed,New Feature,Major,[' scr-1.4.0'],['scr-1.2.0'],,,,,,,['Declarative Services (SCR)'],['The Declarative Services from the OSGi R7 specification (Section 112).'],While working on FELIX-1841 it became apparent that the current Declarative Services specificaiton 1.1 (as contained in OSGi Compendium 4.2) is missing functionality to inform a component about modified service reference properties of a bound service. The goal of this issue is to define an extension to DS specificaiton allowing passing this information.,"Add ""update"" callback support to inform components of modified service properties",,,,fmeschbe,True,fmeschbe,fmeschbe
felix,FELIX-1891,2009-11-24T00:10:55.000+0000,2009-12-14T00:47:58.000+0000,2009-12-14T00:47:58.000+0000,,Fixed,New Feature,Minor,,,,,,,,,['IO Connector Service'],['The IO Connector Service from the OSGi R4 specification (Section 109).'],,IO Connector Service contribution,,,,adamwojtuniak,True,adamwojtuniak,adamwojtuniak
felix,FELIX-1884,2009-11-20T18:17:19.000+0000,2009-11-26T12:30:30.000+0000,2009-12-21T17:13:34.000+0000,,Fixed,New Feature,Major,['webconsole-2.0.4'],['webconsole-2.0.2'],,,,,,,['Web Console'],['A web based management console'],,WebConsole should have a Services plugin,,,,justinedelson,True,fmeschbe,justinedelson
felix,FELIX-1875,2009-11-17T17:40:46.000+0000,2010-01-10T19:28:14.000+0000,2010-02-22T06:59:25.000+0000,,Fixed,New Feature,Minor,['eventadmin-1.2.2'],,,,,,,,"['Event Admin', 'Specification compliance']","['The Event Admin from the OSGi R6 specification (Section 113).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']","The Event Admin spec has seen some minor modifications for the R4.2 release cycle, so we should add support for and/or verify we handle the changes.",Add R4.2 support for Event Admin,,,,rickhall,True,karlpauls,rickhall
felix,FELIX-1853,2009-11-06T18:49:52.000+0000,2009-12-07T19:33:26.000+0000,2009-12-07T19:33:26.000+0000,,Fixed,New Feature,Minor,,,,,,,,,['User Admin'],['The User Admin from the OSGi R4 specification (Section 107).'],Implements UserAdmin,User Admin contribution,,,,adamwojtuniak,True,rickhall,adamwojtuniak
felix,FELIX-1837,2009-11-01T14:20:43.000+0000,,2013-06-07T13:06:53.000+0000,,,New Feature,Minor,['maven-bundle-plugin-future'],,,,,,,,['Maven Bundle Plugin'],[''],"It would be nice, if custom capabilities etc. could be specified in the POM instead of the obr.xml, so that project inheritance and other maven features can be used.",Specify custom capabilities etc. in POM,1,1,,hampelratte,True,,hampelratte
felix,FELIX-1815,2009-10-27T17:23:22.000+0000,2010-02-09T21:19:08.000+0000,2010-02-09T21:19:08.000+0000,,Fixed,New Feature,Minor,['karaf-1.4.0'],,,,,,,,['Karaf'],[''],"[Karaf] Introduce 'admin:disconnect' command to allow ssh client to disconnect from karaf instances.

When a user connects to a running Karaf instance they generally will want to disconnect from that instance at some time. If they execure 'osgi:shutdown' they will terminate the instance runtime. Issuing a '^D' to disconnect is not consistent with other karaf commands. I would like to be able to type 'admin:disconnect' while logged into an instance and be returned to parent instance (where ever the original connect command was issued from).",Introduce 'shell:logout' command to close the current shell,,,,jgoodyear,True,gnt,jgoodyear
felix,FELIX-1808,2009-10-26T13:05:26.000+0000,2009-10-26T13:40:29.000+0000,2009-12-21T17:13:34.000+0000,,Fixed,New Feature,Major,['webconsole-2.0.4'],['webconsole-2.0.2'],,,,,,,['Web Console'],['A web based management console'],"There are times where configurations exist in the system which are bound to bundles which do not exist any more. This may for example happen if a bundle is being uninstalled while the Configuration Admin service is not running. Another cause might be explicit configuration binding as per a call to Configuration.setBundleLocation() as the trunk version of Apache Felix SCR does. A Bundle location set in this way is not removed even if the bundle is uninstalled.

To fix these issues, it would be helpfull if the web console would allow explicit unbinding of configurations by calling Configuration.setBundleLocation() with a null location. This does not cause configurations to actually be removed from any ManagedService[Factory] services or DS components. In fact the configuration will be bound again once it assigned. But in case of wrong bundle bindings, the configuration may afterwards be used again by components now located in bundles with a different bundle location.",Support unbinding configurations through the Web Console,,,,fmeschbe,True,fmeschbe,fmeschbe
felix,FELIX-1796,2009-10-22T15:10:56.000+0000,2011-03-07T19:41:38.000+0000,2013-09-28T17:40:01.000+0000,,Fixed,New Feature,Major,['http-2.2.1'],,,,,,,,['HTTP Service'],['The HTTP Service and the Http Whiteboard Service from the OSGi R7 specification (Section 102 and 140).'],,Add cometd/bayeux support to HTTP Service,1,,,tmoloney,True,,tmoloney
felix,FELIX-1764,2009-10-16T06:19:17.000+0000,2010-06-09T15:33:19.000+0000,2016-12-19T17:23:45.000+0000,,Fixed,New Feature,Major,['webconsole-3.1.0'],['webconsole-2.0.0'],,,,,,,['Web Console'],['A web based management console'],"Currently the web console only support HTTP BASIC authentication with its own ""user management"". There is no way of supporting multiple users with varying access rights.

Some applications already have infrastructure to authenticate users and/or to define access control, such as JAAS, OSGi User Admin or other ...

Guillaume Nodet in [1] proposed a service interface to plug such access control. The Web Console should be enhance to support this service API and fall back to the current setup if no service is available.

[1] http://markmail.org/message/5gwqlt7b3gfz7427",Add support for pluggable access control,1,,,fmeschbe,True,gnt,fmeschbe
felix,FELIX-1696,2009-10-03T21:44:37.000+0000,,2009-10-03T21:44:37.000+0000,,,New Feature,Major,,,,,,,,,['Sigil'],['OSGi tooling'],Nice to add a hello world example project which we can refer to in the online docs. Suggest a gogo command line?,Create hello world sigil project ,,,,davemssavage,True,,davemssavage
felix,FELIX-1672,2009-10-01T08:31:18.000+0000,,2009-10-01T08:31:18.000+0000,,,New Feature,Minor,,,,,,,,,['Sigil'],['OSGi tooling'],,Create new help cheat sheets to demo apache sigil usage,,,,davemssavage,True,,davemssavage
felix,FELIX-1656,2009-09-29T14:56:54.000+0000,2009-10-05T16:29:45.000+0000,2009-11-25T12:20:04.000+0000,,Fixed,New Feature,Major,['karaf-1.2.0'],['karaf-1.0.0'],,,,,,,['Karaf'],[''],I 've prepared a clear command for karaf shell to clear the screen. Please review and apply.,new Shell command: shell:clear,,,,lhein,True,gnt,lhein
felix,FELIX-1644,2009-09-25T20:03:57.000+0000,2009-09-25T20:27:17.000+0000,2016-12-19T17:24:48.000+0000,,Fixed,New Feature,Major,['webconsole-2.0.0'],['webconsole-1.2.10'],,,,,,,['Web Console'],['A web based management console'],"Earlier Web Console versions provided an update button on the bundles page to update bundles from the OSGi Bundle Repository. This button was enabled or disabled dynamically by looking the OBR for a potential bundle update. Since this lookup took a long time (particularly the first time and particularly if one of the configured OBRs was not responsive) we removed that feature.

This should be reconfigured and the update button should be reinserted, altough implemented differently:

  * The button is always enabled, such that rendering is not affected 
  * Update tries bundle location URL first and OBR second

The idea of trying the bundle location URL first is, that this URL is a special URL provided by a deployment system, which may act correctly when trying to connect to it. If the bundle location cannot be converted to an URL or the URL cannot be read from, the update should be tried form the OSGi bundle repository.",Reintroduce button to update a single bundle,,,,fmeschbe,True,fmeschbe,fmeschbe
felix,FELIX-1568,2009-09-08T09:22:31.000+0000,2010-03-05T14:49:19.000+0000,2011-06-24T12:36:55.000+0000,,Fixed,New Feature,Major,['maven-bundle-plugin-2.1.0'],,,,,,,,['Maven Bundle Plugin'],[''],"I found a workaround which is the following xml snippet, but a built-in goal would be way easier ;-)

            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-antrun-plugin</artifactId>
                <version>1.2</version>
                <executions>
                    <execution>
                        <id>create-prop</id>
                        <phase>generate-resources</phase>
                        <configuration>
                            <tasks>
                                <taskdef resource=""net/sf/antcontrib/antcontrib.properties"" classpathref=""maven.plugin.classpath""/>
                                <property name=""ant.regexp.regexpimpl"" value=""org.apache.tools.ant.util.regexp.Jdk14RegexpRegexp""/>
                                <property name=""mv"" value=""${project.version}""/>
                                <echo message=""Maven version:   ${mv}"" />
                                <propertyregex property=""ov.p1"" input=""${mv}"" regexp=""(\d+)(?:\.(\d+)(?:\.(\d+))?)?(?:[^a-zA-Z0-9](.*))?"" replace=""\1"" defaultValue=""0""/>
                                <propertyregex property=""ov.p2"" input=""${mv}"" regexp=""(\d+)(?:\.(\d+)(?:\.(\d+))?)?(?:[^a-zA-Z0-9](.*))?"" replace="".\2"" defaultValue="".0""/>
                                <propertyregex property=""ov.p3"" input=""${mv}"" regexp=""(\d+)(?:\.(\d+)(?:\.(\d+))?)?(?:[^a-zA-Z0-9](.*))?"" replace="".\3"" defaultValue="".0""/>
                                <propertyregex property=""ov.p4"" input=""${mv}"" regexp=""(\d+)(?:\.(\d+)(?:\.(\d+))?)?(?:[^a-zA-Z0-9](.*))?"" replace="".\4"" defaultValue=""""/>
                                <propertyregex property=""ov.p1a"" input=""${ov.p1}"" regexp=""(.+)"" replace=""\1"" defaultValue=""0""/>
                                <propertyregex property=""ov.p2a"" input=""${ov.p2}"" regexp=""(\..+)"" replace=""\1"" defaultValue="".0""/>
                                <propertyregex property=""ov.p3a"" input=""${ov.p3}"" regexp=""(\..+)"" replace=""\1"" defaultValue="".0""/>
                                <propertyregex property=""ov.p4a"" input=""${ov.p4}"" regexp=""(\..+)"" replace=""\1"" defaultValue=""""/>
                                <property name=""ov"" value=""${ov.p1a}${ov.p2a}${ov.p3a}${ov.p4a}""/>
                                <echo message=""OSGi version:    ${ov}"" />
                                <mkdir dir=""target""/>
                                <echo message=""karaf.osgi.version = ${ov}"" file=""target/filter.txt""/>
                            </tasks>
                        </configuration>
                        <goals>
                            <goal>run</goal>
                        </goals>
                    </execution>
                </executions>
                <dependencies>
                    <dependency>
                        <groupId>ant-contrib</groupId>
                        <artifactId>ant-contrib</artifactId>
                        <version>1.0b3</version>
                    </dependency>
                    <dependency>
                        <groupId>ant</groupId>
                        <artifactId>ant-optional</artifactId>
                        <version>1.5.3-1</version>
                    </dependency>
               </dependencies>
            </plugin>
",Goal to transform a maven version into an OSGi version,,,,gnt,True,gnt,gnt
felix,FELIX-1547,2009-09-01T14:25:43.000+0000,2009-09-08T18:41:24.000+0000,2009-09-18T15:20:08.000+0000,,Fixed,New Feature,Major,['karaf-1.0.0'],,,,,,,,['Karaf'],[''],"Karaf has admin commands to create new instances from within its shell. Examples are
  admin:create
  admin:list
  admin:start
etc...

It would be good if (some of) these commands were available from the OS-level command line - outside of the Karaf container.",OS shell level admin commands for Karaf,1,,,bosschaert,True,gnt,bosschaert
felix,FELIX-1546,2009-08-31T22:20:41.000+0000,2010-05-11T09:20:10.000+0000,2011-05-10T08:34:46.000+0000,,Fixed,New Feature,Minor,,,,,,,,,['Dependency Manager'],['Provides dynamic service and component dependency management.'],"This issue proposes some new features for DM:

Feature 1): dealing with service dynamics: Temporal dependencies
----------------------------------------------------------------
   
So far, with DM, when a service looses one of its required dependencies, the service is
deactivated. For stateless required dependencies, it would then be nice to support a temporal
required dependency, which blocks the caller thread while the dependency is being updated.

In the attached DM.tgz patch, you will find a new ""TemporalServiceDependency.java"" class, which extends the
""ServiceDependency"" class. In TemporalServiceDependency.java, you'll find a setTimeout method
which takes a timeout parameter, in millis, specifying  the max time to wait (30 seconds by
default). After timeout, an unchecked ""ServiceUnavailableException"" is fired to the caller
thread. 

Here is a more specific description of what the new ""TemporalServiceDependency"" class is doing
exactly: 
The class actually uses a Dynamic Proxy which wraps the actual service dependency
instance. When the service is lost, we check if there is an available replacement service from the  
tracker. If true, then we just update our internal service with the new one. If false, then we set
a flag, which blocks any thread that invokes the service through the Dynamic Proxy. On timeout,
we raise an unchecked ServiceUnavailableException, telling that the required service is unavailable (but we
don't deactivate the service). The next time the dependency is invoked and if the service has come
up, then the method will invoke the new service transparently.

Using setTimeout(0) is supported: The thread won't be blocked, but will catch the unchecked
exception immediately, in case of service loss.

Of course, I don't pretend here to propose a feature as powerful as iPOJO ""temporal dependencies""
or Blueprint specification.  However, you will see that the proposed patch is somewhat
lightweight and is just fine for Dependency Manager. 

For example, Here is an example ->

dm.add(createService()
       .setImplementation(Client.class)
       .add(createTemporalServiceDependency()
	    .setService(MyService.class) // AutoConfig
	    .setTimeout(5000)));

public class Client implements Runnable {
  private Thread _thread;
  private MyService _service; // Dynamic Proxy auto injected (AUTO CONFIG MODE)

  void start() {
    _thread = new Thread(this);
    _thread.start();
  }

  public void run() {
    while (true) {
	try { 
	  _service.doService(); // will block while myService is updated
	} catch (Throwable t) {
	  t.printStackTrace(); // ServiceUnavailableException if the service is not here for more than 5 seconds
	}
    }
  }
}

One last note: this feature is only meaningful for stateless services, not for sateful
services. But for stateless services, I think that it is cool to not re-activate the whole
service while the dependency is updated. For stateful services, I don't think that temporal
dependencies applies because we would then loose service internal states between updates ...

One last note: in TemporalServiceDependency, I override returned type which is only supported in
jdk 1.5.

For instance, the TemporalServiceDependency.setTimeout is defined as this:

  public TemporalServiceDependency setTimeout( long timeout ) { ...}

So, I am wondering if that would be possible to impose jdk 1.5 for the next version of DM ?

Feature 2): dealing with Bound Service Replacement:
---------------------------------------------------

We would like DM to be able to replace a lost required dependency, by another existing one (if
available from the registry), but without deactivating the service. Currently, the service is
deactivated when loosing one of its required dependencies. If, however, a replacement is
available from tracker, we just would like invoke the ""changed"" service callback (instead of
deactivating it).

I know that in DeclarativeService, the ""added"", then the ""removed"" callback would be invoked, but
I think It's much more intuitive to invoke the ""changed"" callback.

For example, when a required service dependency is lost, DM could behave like the following: 

1- lookup another service (which matches the lost service dependency filter).
2- If found, update the service (in AutoConfig mode), or invoke a ""changed"" callbacks.
3- Otherwise, if AutoConfig is turned off and there is no ""changed"" callback, then deactivate the
   service (like it is the case currently). 

The fix in located in the ServiceDependency.java (see removedService method).
Notice that I also changed the makeUnavailable method.

Feature 3): let the DM shell display service dependencies filters.
-----------------------------------------------------------------------------

Currently, the DM shell displays the service dependencies without the service filter. So that is
problematic when a service have multiple dependencies on the same service, but with different
filters.

For example: when typing the command ""dm"", the patch is now displaying the following:

[7] test.dm
    class test.Client registered
      test.Printer(color=true) service required available
      test.Printer(color=false) service required available

The fix is located in ServiceDependency.java, in the getName() method, which appends the service
filter after the service name. Notice that we don't display the 'objectClass' parameter.

Auto Configuration vs Callbacks
-------------------------------

Well this is not a feature, but rather an improvement regarding the issue 
https://issues.apache.org/jira/browse/FELIX-1278

Recently, you fixed the issue FELIX-1278, where we asked to turn off autoconf when setCallbacks is invoked 
(thank you for the fix !).
However, I think we forgot to ask you to not turn off autoconfig if both setCallback /
setAutoConfig(true) are invoked. Indeed, in some times, it may make sense to use both methods, 
like this: ->

   dm.add(createServiceDependency()
	  .setService(MyService.class)
	  .setRequired(false)
	  .setAutoConfig(""_field"") 
 	  .setCallbacks(""add"", ""remove""));

Here, we inject an optional configuration in AutoConfig mode (field injection), but we also
provide a callback method to be able to handle new services (after activation).

You could tell me that with the current svn fix, we could invoke setCallbacks BEFORE
setAutoConfig. But I just added a flag when
setAutoConfig method is called explicitly (in order to avoid turning off auto config from
setCallbacks method). The fix in located in ServideDependency.java

Regards;
/pierre
",DM/Temporal Dependency/Bound Service Replacement features,,,,pderop,True,marrs,pderop
felix,FELIX-1537,2009-08-28T08:09:40.000+0000,2009-10-13T16:15:56.000+0000,2009-10-13T16:15:56.000+0000,,Fixed,New Feature,Major,['fileinstall-2.0.4'],['fileinstall-1.2.0'],,,,,,,['File Install'],"['A bundle that watches a directory set by the felix.fileinstall.dir property; the default is ""load"". JAR files in this directory are installed and uninstalled when they are no longer there. Updating the JAR file in the directory will update the bundle. The bundle allows you to use downloads from a browser, drag and drop, or the command line copy function, for OSGi management.']","File Install already supports .cfg file. It would be interristing to also support XML property files, as described in http://java.sun.com/j2se/1.5.0/docs/api/java/util/Properties.html.",File Install should support XML property files,,,,clement.escoffier,True,gnt,clement.escoffier
felix,FELIX-1524,2009-08-25T08:50:14.000+0000,2009-08-25T09:48:19.000+0000,2009-08-25T09:48:19.000+0000,,Fixed,New Feature,Major,['karaf-1.0.0'],,,,,,,,['Karaf'],[''],,Implement a better tac command,,,,gnt,True,gnt,gnt
felix,FELIX-1522,2009-08-25T08:21:29.000+0000,,2010-06-21T15:22:47.000+0000,,,New Feature,Major,,,,,,,,,['Gogo Runtime'],['RFC-147 shell runtime'],,Add an eval command / keyword,,,,gnt,True,,gnt
felix,FELIX-1521,2009-08-24T20:36:36.000+0000,,2010-06-21T15:22:49.000+0000,,,New Feature,Major,,,,,,,,,['Gogo Runtime'],['RFC-147 shell runtime'],,[gogo] ANTLR grammar for gogo,1,,,gnt,True,,gnt
felix,FELIX-1510,2009-08-20T16:34:11.000+0000,2009-08-20T16:37:06.000+0000,2009-08-20T16:37:06.000+0000,,Fixed,New Feature,Major,['shell-1.4.0'],['shell-1.2.0'],,,,,,,['Shell'],['Shell service for OSGi.'],"It might be convenient if there was a shell command to set the initial bundle start level. Currently, the only way to set it is via a configuration property.",There is no command to set the initial bundle start level,,,,rickhall,True,rickhall,rickhall
felix,FELIX-1485,2009-08-14T23:56:39.000+0000,2009-08-17T06:29:08.000+0000,2009-09-08T13:16:29.000+0000,,Fixed,New Feature,Major,['karaf-1.0.0'],,,,,,,,['Karaf'],[''],We need a new tab in the console for the admin commands (create/destroy/start/stop).,Admin commands support in Karaf webconsole,,,,mwilkos,True,gertvanthienen,mwilkos
felix,FELIX-1483,2009-08-13T08:56:21.000+0000,2009-08-31T06:42:25.000+0000,2009-09-08T13:19:11.000+0000,,Fixed,New Feature,Major,['fileinstall-2.0.0'],,,,,,,,['File Install'],"['A bundle that watches a directory set by the felix.fileinstall.dir property; the default is ""load"". JAR files in this directory are installed and uninstalled when they are no longer there. Updating the JAR file in the directory will update the bundle. The bundle allows you to use downloads from a browser, drag and drop, or the command line copy function, for OSGi management.']","At development time, one would create a folder inside the watched directory with the content of the jar inside.
Any changes to one file would make fileinstall to recreate the bundle and update it.",Fileinstall should support exploded artifacts,,,,gnt,True,gnt,gnt
felix,FELIX-1482,2009-08-13T07:32:28.000+0000,2012-03-19T13:47:08.000+0000,2012-03-19T13:47:08.000+0000,,Won't Fix,New Feature,Major,,,,,,,,,['File Install'],"['A bundle that watches a directory set by the felix.fileinstall.dir property; the default is ""load"". JAR files in this directory are installed and uninstalled when they are no longer there. Updating the JAR file in the directory will update the bundle. The bundle allows you to use downloads from a browser, drag and drop, or the command line copy function, for OSGi management.']",,"Support for ""stopping"" watched bundles",,,,gnt,True,,gnt
felix,FELIX-1478,2009-08-12T18:25:37.000+0000,2009-08-26T19:47:23.000+0000,2009-08-26T19:47:23.000+0000,,Fixed,New Feature,Minor,['framework-2.0.0'],['framework-1.8.1'],,,,,,,['Main'],['The launcher for the framework'],"As it stands currently (and dating back to the Oscar days), if someone does a control-C, then the framework process just dies and there is no attempt to shutdown cleanly. If we add a shutdown hook, then we can cleanly shutdown the framework.

I am not sure if this would necessarily impact anyone who directly uses our launcher to launch the framework. I imagine there could possibly be an issue if someone uses our launcher, but registers their own shutdown hook. We also have the potential issue of a deadlock or some situation where we cannot exit, which could prevent the VM from exiting.

Any one have any thoughts?",Add shutdown hook to launcher to cleanly shutdown the framework if the process is killed,,,,rickhall,True,rickhall,rickhall
felix,FELIX-1446,2009-08-03T18:21:19.000+0000,2009-08-03T18:25:58.000+0000,2009-08-06T15:57:34.000+0000,,Fixed,New Feature,Major,['framework-2.0.0'],['framework-1.8.1'],,,,,,,['Main'],['The launcher for the framework'],"The approach for managing which bundles are automatically deployed when the framework starts is somewhat configuration intensive; i.e., you must always edit the configuration properties. It seems we could simplify this process by treating the 'bundle' directory as an auto-deploy directory, where its contained bundles are installed and started when the framework is started. With this approach, the user does not need to edit the configuration file to add or remove bundles from framework startup, he or she simply needs to add or remove bundles from the bundle directory.

This is similar to how File Install works, but it is not a run-time mechanism like File Install, it is only at startup time. Originally, I had thought about actually using File Install, but I think it is too complicated for the launcher, especially with respect to threading.",Framework launcher should automatically deploy bundles in bundle directory,,,,rickhall,True,rickhall,rickhall
felix,FELIX-1441,2009-08-03T07:46:48.000+0000,2010-03-19T07:55:44.000+0000,2010-04-01T11:50:35.000+0000,,Fixed,New Feature,Major,['webconsole-3.0.0'],,,,,,,,['Web Console'],['A web based management console'],It will be good have an ability to have a search facility which searches bundle manifest entries of all installed bundles.,Search manifest entries of bundles,,,,sahoo,True,v_valchev,sahoo
felix,FELIX-1409,2009-07-24T11:30:41.000+0000,,2009-07-24T11:30:41.000+0000,,,New Feature,Major,,,,,,,,,['Sigil'],['OSGi tooling'],Would be very useful if the sigil eclipse plugin interoperated with the scala eclipse plugin http://www.scala-lang.org/node/94,Support for sigil to work with scala in eclipse,,1,,davemssavage,True,,davemssavage
felix,FELIX-1385,2009-07-19T14:00:30.000+0000,,2009-07-19T14:00:30.000+0000,,,New Feature,Trivial,,,,,,,,,['Sigil'],['OSGi tooling'],"Currently the eclipse ui adds a hard coded comment at the top of the sigil.properties file ""# sigil project file, saved by plugin.""

Would be useful to allow developers to set workspace/project specific settings to set their own comment, e.g. the apache license header...",Add preference to eclipse ui to set comment on sigil.properties file,,,,davemssavage,True,,davemssavage
felix,FELIX-1372,2009-07-16T15:50:44.000+0000,2009-09-17T17:33:41.000+0000,2009-09-17T17:33:41.000+0000,,Fixed,New Feature,Minor,,,,,,,,,['Sigil'],['OSGi tooling'],sigil should support HTTP authentication when retrieving the remote OBR index. ,sigil OBR repository should support HTTP authentication,,,,db82407,True,db82407,db82407
felix,FELIX-1358,2009-07-13T20:59:26.000+0000,,2009-07-13T20:59:26.000+0000,,,New Feature,Minor,,,,,,,,,['Sigil'],['OSGi tooling'],,Create gogo cli for sigil junit integration,,,,davemssavage,True,,davemssavage
felix,FELIX-1350,2009-07-13T18:27:07.000+0000,,2009-07-13T18:27:07.000+0000,,,New Feature,Trivial,,,,,,,,,['Sigil'],['OSGi tooling'],,Allow sigil to be used just to create jars - i.e. like bndwrap - does not need ivy resolution just generate the jar from already built classes,,,,davemssavage,True,,davemssavage
felix,FELIX-1339,2009-07-13T17:57:06.000+0000,,2009-07-13T17:57:06.000+0000,,,New Feature,Minor,,,,,,,,,['Sigil'],['OSGi tooling'],The repository view could be improved in many ways via extensible tools to manipulate the bundles found in it. This is just one option.,Create query tools to inspect a bundle in a repository and find which bundles may import or require it,,,,davemssavage,True,,davemssavage
felix,FELIX-1338,2009-07-13T17:54:03.000+0000,,2009-07-13T17:54:03.000+0000,,,New Feature,Major,,,,,,,,,['Sigil'],['OSGi tooling'],"Creating ""new sigil project"" will overwrite the existing sigil.properties files. Import existing fails as there are no .eclipse and .classpath entries.

Potential to create a sigil eclipse:eclipse type function like maven? Also possibility to integrate with eclipse import wizards?",Provide mechanism to import sigil projects that do not have corresponding eclipse .project and .classpath entries - i.e. those created on command line outside of eclipse,,,,davemssavage,True,,davemssavage
felix,FELIX-1331,2009-07-13T17:26:58.000+0000,,2009-07-13T17:26:58.000+0000,,,New Feature,Major,,,,,,,,,['Sigil'],['OSGi tooling'],Possibly related to FELIX-1324 which would allow us to bring up a container in a specific configuration and run unit test with reports fed back to IDE,Support for junit testing in IDE integration,,,,davemssavage,True,,davemssavage
felix,FELIX-1324,2009-07-13T13:43:00.000+0000,,2010-09-03T14:30:31.000+0000,,,New Feature,Major,,,,,,,,,['Sigil'],['OSGi tooling'],"Sigil previously had the ability to launch and debug a Newton OSGi container. This has been removed in the port to apache as it brought in dependencies on a product that was not compatible with the apache licence.

Should be possible to rebuild this using existing felix tools",Add support for runtime debug/launch of felix via eclipse,,,,davemssavage,True,davemssavage,davemssavage
felix,FELIX-1323,2009-07-13T13:39:04.000+0000,2009-09-27T15:01:38.000+0000,2009-09-27T15:01:38.000+0000,,Fixed,New Feature,Major,['sigil-1.0.0'],,,,,,,,['Sigil'],['OSGi tooling'],"Currently sigil is built using a PDE server build. However this is difficult to manage as it requires you to download binary dependencies locally.

Sigil on the other hand has the ability to download dependencies as maven or ivy do. Would be useful to eat our own dog food in this respect.",Use sigil to build sigil,,,,davemssavage,True,davemssavage,davemssavage
felix,FELIX-1317,2009-07-10T15:12:52.000+0000,2009-07-10T16:25:54.000+0000,2009-07-10T16:25:54.000+0000,,Duplicate,New Feature,Major,,['framework-1.8.1'],,,,,,,['Framework'],"['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).']","It seems that Bundle.start(int) with START_ACTIVATION_POLICY is not supported

http://www.osgi.org/javadoc/r4v41/org/osgi/framework/Bundle.html#start(int)

In Felix-1.8.1 I see 

    public void start(int options) throws BundleException
    {
        if ((options & Bundle.START_ACTIVATION_POLICY) > 0)
        {
            throw new UnsupportedOperationException(
                ""The activation policy feature has not yet been implemented."");
        }

   ....
}


Is there a way to still work with Bundle-ActivationPolicy: lazy ?",Added support for Bundle.START_ACTIVATION_POLICY  on start,,,,tdiesler,True,,tdiesler
felix,FELIX-1298,2009-07-02T22:24:03.000+0000,2009-07-03T16:31:29.000+0000,2009-09-11T13:39:02.000+0000,,Fixed,New Feature,Major,['framework-2.0.0'],['framework-1.8.1'],,,,,,,"['Framework', 'Specification compliance']","['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']","The new standard launching and embedding API in R4.2 introduced a new configuration property, org.osgi.framework.library.extensions, that allows you to specify a comma-delimited set of extensions to try when searching for native code libraries in addition to the file extension returned by System.mapLibraryName(), which is necessary on some operating systems. We need to add support for this.",Implement support for new org.osgi.framework.library.extensions configuration property,1,,,rickhall,True,rickhall,rickhall
felix,FELIX-1297,2009-07-02T22:21:59.000+0000,2009-09-01T09:56:03.000+0000,2009-09-01T14:17:58.000+0000,,Fixed,New Feature,Minor,['framework-2.0.0'],['framework-1.8.1'],,,,,,,"['Framework', 'Specification compliance']","['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']","The new standard launching and embedding API in R4.2 introduced a new configuration property, org.osgi.framework.command.execpermission, that allows you to specify a command to execute on an extracted native library file to change its file permissions, which is necessary on some operating systems. We need to add support for this.",Implement support for new org.osgi.framework.command.execpermission configuration property,,,,rickhall,True,karlpauls,rickhall
felix,FELIX-1289,2009-06-30T20:56:14.000+0000,2009-07-01T13:42:16.000+0000,2009-07-26T00:44:39.000+0000,,Fixed,New Feature,Major,['framework-2.0.0'],['framework-1.8.1'],,,,,,,"['Framework', 'Specification compliance']","['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']","java.lang.NoSuchMethodError: org/osgi/framework/FrameworkUtil.getBundle(Ljava/lang/Class;)Lorg/osgi/framework/Bundle;

The new FrameworkUtil.getBundle method from the OSGi 4.2 core spec is not yet supported in Felix ...",Support for FrameworkUtil.getBundle(),,,,ebullient,True,rickhall,ebullient
felix,FELIX-1284,2009-06-29T15:45:56.000+0000,2009-07-28T14:04:29.000+0000,2009-12-08T15:49:26.000+0000,,Fixed,New Feature,Major,['scr-1.2.0'],,,,,,,,"['Declarative Services (SCR)', 'Specification compliance']","['The Declarative Services from the OSGi R7 specification (Section 112).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']","112.5.11-12, 112.7.1 from 4.2 Compendium spec (numbers may be slightly different-- these are from the May 6 draft).

Modifying a component configuration can occur if 
* the component description specifies the modified attribute and 
* the component properties of the component configuration use a Configuration object from the Configuration Admin service and 
* that Configuration object is modified without causing the component configuration to become unsatisfied. 

If this occurs, the component instance will be _notified of the change in the component properties_.

If the modified attribute is not specified, then the component configuration will become unsatisfied if its component properties use a Configuration object and that Configuration object is modified in any way.

--

Basically: you can specify a 'modified' attribute/method that should be called when ConfigAdmin pushes a changed configuration for a component, instead of deactivating and then re-activating the component on a configuration change.
",Support for the 'modified' operation (DS in OSGi 4.2 compendium),,,,ebullient,True,fmeschbe,ebullient
felix,FELIX-1261,2009-06-21T01:54:41.000+0000,2009-07-03T14:12:17.000+0000,2009-09-08T13:16:08.000+0000,,Fixed,New Feature,Major,['karaf-1.0.0'],,,,,,,,['Karaf'],[''],"Currently we can't Install/Uninstall Karaf Features from Felix WebConsole. In my Google Summer of Code project I created an Extension Plugin for web console, which lists Karaf Features and gives admin ability to manage them.",Install/Uninstall Karaf Features from Felix WebConsole,,,,mwilkos,True,gertvanthienen,mwilkos
felix,FELIX-1250,2009-06-17T20:26:47.000+0000,2009-06-17T20:27:49.000+0000,2009-06-17T20:27:49.000+0000,,Fixed,New Feature,Major,['framework-2.0.0'],['framework-1.8.0'],,,,,,,"['Framework', 'Specification compliance']","['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']","The R4.2 specification introduces a new runtime exception, called ServiceException. This is largely intended to be used by service implementations and middleware, but the framework must also use it to throw exceptions when ServiceFactory objects misbehave. If a ServiceFactory returns null, throws an exception, or returns an object not castable to objectClass interfaces, then the framework must fire a framework event with a service exception as the cause.",Support service exceptions for service factories,,,,rickhall,True,rickhall,rickhall
felix,FELIX-1244,2009-06-16T18:43:59.000+0000,2009-06-26T15:07:57.000+0000,2009-06-26T15:07:57.000+0000,,Fixed,New Feature,Major,['framework-2.0.0'],['framework-1.8.0'],,,,,,,"['Framework', 'Specification compliance']","['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']","The OSGi R4.2 specification introduces a new ServiceEvent type MODIFIED_ENDMATCH. It allows service listeners to be notified about when a previously matching service stops matching the listener's service filter due to the registering bundle changing the service properties. This is necessary for situations, like the ServiceTracker, where you want to stop tracking a service when it no longer matches a listener's filter, but in the current situation the listener would not be notified with a MODIFIED event if the new properties no longer matched the listener's filter.",Add support for ServiceEvent.MODIFIED_ENDMATCH,,,,rickhall,True,rickhall,rickhall
felix,FELIX-1234,2009-06-12T14:10:41.000+0000,2009-08-14T19:27:09.000+0000,2009-08-25T08:09:16.000+0000,,Fixed,New Feature,Major,['configadmin-1.2.0'],['configadmin-1.0.10'],,,,,,,"['Configuration Admin', 'Specification compliance']","['The Configuration Admin from the OSGi R7 specification (Section 104).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']","When calling plugins, only the plugins whose cm.target service property matches the PID of the updated configuration is called. If the service receiving the configuration is a ManagedServiceFactory, the factory PID must be used to match the cm.target property instead.

The current implementation in the ConfigurationManager.callPlugins method always considers the PID.",Configuration Plugins should be called for all configuration instances of factory targets,,,,fmeschbe,True,fmeschbe,fmeschbe
felix,FELIX-1232,2009-06-12T14:05:04.000+0000,2009-07-31T14:45:52.000+0000,2009-12-08T15:49:26.000+0000,,Fixed,New Feature,Major,['scr-1.2.0'],['scr-1.0.8'],,,,,,,"['Declarative Services (SCR)', 'Specification compliance']","['The Declarative Services from the OSGi R7 specification (Section 112).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']","R4.2 of the OSGi compendium spec will introduce the notion of private properties. Private properties are properties whose names have leading dots. Such private properties should not be propagated to service properties.

See also 104.4.3, Property Propagation, in the draft R4.2 Compendium Specification (dated 2009/03/10).",Do not use private configuration properties as service properties,,1,,fmeschbe,True,fmeschbe,fmeschbe
felix,FELIX-1231,2009-06-12T13:56:24.000+0000,2009-08-20T11:37:03.000+0000,2009-08-20T11:37:03.000+0000,,Duplicate,New Feature,Major,['configadmin-1.2.0'],['configadmin-1.0.10'],,,,,,,['Configuration Admin'],['The Configuration Admin from the OSGi R7 specification (Section 104).'],"The upcoming OSGi Core Specification 4.2 will allow services to register with multiple PIDs. That is the service.pid service property may be a string, a String[] or a Collection of Strings. Currently the configadmin implementation only supports single value service.pid properties.

For specification compliance and support of such services the configadmin must be fixed.",Support multi-value service.pid service properties,,,,fmeschbe,True,,fmeschbe
felix,FELIX-1229,2009-06-12T11:39:13.000+0000,2009-09-08T07:03:27.000+0000,2014-03-12T08:55:24.000+0000,,Fixed,New Feature,Major,"['maven-scr-plugin-1.4.0', 'scr annotations 1.0.0']","['maven-scr-plugin-1.2.0', 'scr annotations 0.9.0']",,,,,,,['SCR Tooling'],"['Tooling for Maven, Ant, and Bnd including SCR annotations']",R4.2 will contain some new features for DS. This bug is about implementing new java doc tags and annotations supporting this new features,Support new features from the R4.2 release,1,,,cziegeler,True,cziegeler,cziegeler
felix,FELIX-1206,2009-06-08T21:17:23.000+0000,2009-06-11T09:37:23.000+0000,2009-06-11T09:37:23.000+0000,,Fixed,New Feature,Major,['karaf-1.0.0'],['karaf-1.0.0'],,,,,,,['Karaf'],[''],propappend is a convenience command that appends a given value to an existing property ,[PATCH] config/propappend,,,,robertburrelldonkin,True,gnt,robertburrelldonkin
felix,FELIX-1205,2009-06-08T19:37:43.000+0000,2009-06-08T19:38:09.000+0000,2009-07-26T20:25:58.000+0000,,Fixed,New Feature,Major,['framework-2.0.0'],,,,,,,,"['Framework', 'Specification compliance']","['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']","We need to update to the latest OSGi API, which is now frozen for the R4.2 release.",Update to the latest OSGi R4.2 API,,,,rickhall,True,rickhall,rickhall
felix,FELIX-1193,2009-06-01T17:06:57.000+0000,2009-07-31T02:33:56.000+0000,2009-07-31T02:33:56.000+0000,,Fixed,New Feature,Major,['framework-2.0.0'],['framework-1.8.0'],,,,,,,"['Framework', 'Specification compliance']","['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']",The new standard framework launching and embedding API introduced a property (org.osgi.framework.bundle.parent) to configure the class loader used for boot delegation. We should modify Felix to support this property.,Implement org.osgi.framework.bundle.parent from RFC 132,,,,rickhall,True,rickhall,rickhall
felix,FELIX-1181,2009-05-27T01:38:30.000+0000,2009-05-28T20:02:12.000+0000,2009-05-28T20:02:12.000+0000,,Fixed,New Feature,Major,['shell-1.4.0'],['shell-1.2.0'],,,,,,,['Shell'],['Shell service for OSGi.'],"To support activation policies, a new flag should be added to the shell ""start"" command when starting bundles, which will indicate whether or not to use the bundle's declared activation policy.","Add support for activation policy in shell ""start"" command",,,,rickhall,True,rickhall,rickhall
felix,FELIX-1142,2009-05-11T14:16:00.000+0000,2010-02-19T04:41:15.000+0000,2010-02-19T10:25:27.000+0000,,Fixed,New Feature,Trivial,,,,,,,,,,,,Donation of sigil project,3,,,davemssavage,True,rickhall,davemssavage
felix,FELIX-1111,2009-04-30T09:22:40.000+0000,2009-05-06T06:59:16.000+0000,2009-05-06T06:59:16.000+0000,,Fixed,New Feature,Major,['karaf-1.0.0'],,,,,,,,['Karaf'],[''],"Not sure how to do that, but we need to be able to run on equinox.
A possibility would be to include both felix and equinox in two different main jars and ship both.
Another option would be to have two different distributions.

In both cases, we need to first investigate possible problems with equinox and solve them.",Ability to run Karaf on Equinox,,,,gnt,True,gnt,gnt
felix,FELIX-1109,2009-04-30T08:15:56.000+0000,2009-04-30T09:11:18.000+0000,2009-05-03T21:17:37.000+0000,,Fixed,New Feature,Major,['karaf-1.0.0'],,,,,,,,['Karaf'],[''],,Deployer for blueprint applications,,,,gnt,True,gnt,gnt
felix,FELIX-1056,2009-04-23T20:15:29.000+0000,2009-05-19T06:25:12.000+0000,2009-05-19T22:32:23.000+0000,,Fixed,New Feature,Minor,['maven-scr-plugin-1.2.0'],,,,,,,,['SCR Tooling'],"['Tooling for Maven, Ant, and Bnd including SCR annotations']","this is an enhancements based on FELIX-1010 and takes over the concept of SLING-902 to support convenience annotations for SCR components for other projects as well.
my first thought was to submit this to the sling project (see SLING-902), but this would require two new projects in sling (one for the annotation and another for the annotation tag provider implementation), and requiring referencing this two new projects in all poms using it. quite complicated for a simple convenience annotation.

to make life easier it would be best to add such project-specific annotations the the scr-annotations/scrplugin projects.
attached is a patch for this. feel free to change package names.

if accepted in felix, the SLING-902 becomes obsolete.",java annotations: add slingservlet annotation,,,,sseifert,True,,sseifert
felix,FELIX-1052,2009-04-22T20:39:17.000+0000,2009-05-11T16:45:48.000+0000,2009-05-11T17:40:17.000+0000,,Fixed,New Feature,Minor,['shell-1.4.0'],['shell-1.2.0'],,,,,,,"['Log Service', 'Shell']","['The Log Service from the OSGi R4 specification (Section 101).', 'Shell service for OSGi.']","It is currently not possible to display recent logs (created with the Log Service) using the Apache Felix Shell.
The attached bundle define a shell Command to display log entries.",Add log shell command,,,,fdiotalevi,True,rickhall,fdiotalevi
felix,FELIX-1012,2009-03-30T14:57:57.000+0000,2009-04-08T16:18:25.000+0000,2009-04-08T16:18:25.000+0000,,Fixed,New Feature,Major,,,,,,,,,,,,Transactions in OSGi (RFC 98),,,,gnt,True,,gnt
felix,FELIX-1011,2009-03-30T07:31:07.000+0000,,2009-05-12T21:00:38.000+0000,,,New Feature,Major,,,,,,,,,,,,JNDI / OSGi integation (RFC 0142),1,,,gnt,True,rickhall,gnt
felix,FELIX-1010,2009-03-29T18:22:18.000+0000,2009-05-20T06:07:11.000+0000,2009-05-20T06:07:11.000+0000,,Fixed,New Feature,Major,['maven-scr-plugin-1.2.0'],['maven-scr-plugin-1.0.10'],,,,,,,['SCR Tooling'],"['Tooling for Maven, Ant, and Bnd including SCR annotations']","goals of this proposal:
- allow definition of SCR components with java annotations instead of QDox tags
- advantages: strong typing, auto-completion and jump to source documentation in modern IDEs
- support built-in annotations with 1:1 matching the old scr.* tags, and allow definition of custom annotations for other felix/scr-based projects to minimalize syntax overhead
- the QDox tags are still supported, but cannot be mixed with annotations whithing the same source file

attached to this ticket is a full implemented and tested patch, that supports all feates supported by the scr.* QDox tags today. some of the more ""exotic"" features are not tested in detail, only the generated descriptors where compared.

i created a new project ""scrplugin-annotations"", that contains only the annotations for easy referencing without unwanted transitive dependencies. i'm not sure if the package and artifact name are well chosen.


Example 1
---------

QDox version:

/**
 * Service class with QDox annotations.
 * 
 * @scr.component
 * @scr.property name=""testProperty"" value=""testValue""
 * @scr.service
 */
public class MinimalServiceQDox implements {
...

Annotation version:

/**
 * Service class with java annotations.
 */
@Component
@Property(name = ""testProperty"", value = ""testValue"")
@Service
public class MinimalServiceAnnotations {
...


Example 2
---------

QDox version:

/**
 * Service class with QDox annotations.
 * 
 * @scr.component name=""QDoxName"" label=""theLabel"" description=""theDescription""
 *                immediate=""false"" enabled=""false"" factory=""xx.yy.zz""
 * @scr.service interface=""org.osgi.service.component.ComponentInstance""
 *              servicefactory=""true""
 * @scr.service interface=""java.lang.Readable""
 * @scr.property name=""stringProp"" value=""theValue"" label=""thePropLabel""
 *               description=""thePropDesc"" options 0=""option0"" 1=""option1""
 *               2=""option2""
 * @scr.property name=""intProp"" value=""5"" type=""Integer""
 * @scr.property name=""multiProp"" values.0=""multiValue1"" values.1=""multiValue2""
 */
public class ServiceQDox implements ComponentInstance, Readable {

    /**
     * @scr.reference cardinality=0..1, dynamic=true
     */
    MinimalServiceQDox reference;
...


Annotation version:

/**
 * Service class with java annotations.
 */
@Component(name = ""AnnotName"", label = ""theLabel"", description = ""theDescription"", immediate = false, enabled = false, factory = ""xx.yy.zz"")
@Services( { @Service(value = ComponentInstance.class, serviceFactory = true), @Service(Readable.class) })
@Properties( {
        @Property(name = ""stringProp"", value = ""theValue"", label = ""thePropLabel"", description = ""thePropDesc"", options = {
                @PropertyOption(name = ""0"", value = ""option0""), @PropertyOption(name = ""1"", value = ""option1""),
                @PropertyOption(name = ""2"", value = ""option2"") }),
        @Property(name = ""intProp"", value = ""5"", type = Integer.class),
        @Property(name = ""multiProp"", value = { ""multiValue1"", ""multiValue2"" }) })
public class ServiceAnnotations implements ComponentInstance, Readable {

    @Reference(cardinality = ReferenceCardinality.ZERO_TO_ONE, policy = ReferencePolicy.DYNAMIC)
    MinimalServiceAnnotations reference;
...


Example 3 - using Custom Annotation from other project
------------------------------------------------------

QDox version:

/**
 * Sample servlet with sling mappings.
 * 
 * @scr.component immediate=""true""
 * @scr.service interface=""javax.servlet.Servlet""
 * @scr.property name=""sling.servlet.methods"" value=""GET""
 * @scr.property name=""sling.servlet.resourceTypes""
 *               value=""/apps/test/components/samplecomponent""
 * @scr.property name=""sling.servlet.extensions"" values.0=""html"" values.1=""json""
 */
public class SlingServletQDox implements Servlet {


Annotation version:

/**
 * Sample servlet with sling mappings.
 */
@SlingServlet(methods = ""GET"", resourceTypes = ""/apps/test/components/samplecomponent"", extensions = { ""html"", ""json"" })
public class SlingServletAnnotation implements Servlet {


Custom annotation mappings can be integrated by defining a class implementing ""org.apache.felix.scrplugin.tags.annotation.AnnotationTagProvider"" for the new plugin property ""annotationTagProviders"" in the pom.
",add java annotation support to felix-scr-plugin,1,1,,sseifert,True,cziegeler,sseifert
felix,FELIX-1009,2009-03-28T05:16:37.000+0000,2009-03-28T05:19:39.000+0000,2009-03-31T14:51:36.000+0000,,Fixed,New Feature,Minor,['shell-1.2.0'],,,,,,,,['Shell'],['Shell service for OSGi.'],"We should add commands for displaying required and requiring bundles, similar to the imports and exports commands for packages.",Add requires/requirers commands to shell,,,,rickhall,True,rickhall,rickhall
felix,FELIX-1006,2009-03-27T07:21:50.000+0000,2009-03-27T07:27:21.000+0000,2009-03-31T14:51:36.000+0000,,Fixed,New Feature,Minor,['shell-1.2.0'],,,,,,,,['Shell'],['Shell service for OSGi.'],We should add a command to the shell to list the imports of a bundle.,Add command to shell to list a bundles imported packages,,,,rickhall,True,rickhall,rickhall
felix,FELIX-976,2009-03-06T13:50:43.000+0000,2009-03-29T15:46:11.000+0000,2009-03-30T06:05:29.000+0000,,Fixed,New Feature,Major,['bundlerepository-1.4.0'],,,,,,,,['Bundle Repository (OBR)'],[''],"Hi

the obr shell command doesn't directly support an repository update. The attached patch adds the convenience command 'update-url' to the command shell. So you don't have to remove and add an url by hand.

Kristian",OBR update-url shell command,,,,kkoehler,True,rickhall,kkoehler
felix,FELIX-974,2009-03-05T11:56:43.000+0000,,2014-06-20T06:04:13.000+0000,,,New Feature,Major,['maven-bundle-plugin-future'],,,,,,,,['Maven Bundle Plugin'],[''],"Hi

currently there is no option to add the project dependencies to the local obr repository file. This prevents me to use the local obr at all because some dependencies are not available via 'obr update'. If the plugin would also add the dependencies to the local obr file I could install my bundles with all dependencies from there.

Kristian",Possibility to install dependencies to obr repository when installing maven artifact,1,3,['patch'],kkoehler,True,,kkoehler
felix,FELIX-954,2009-02-22T19:52:37.000+0000,2011-06-29T00:59:47.000+0000,2011-10-11T17:09:18.000+0000,,Fixed,New Feature,Major,['maven-bundle-plugin-2.3.4'],['maven-bundle-plugin-1.4.3'],,,,,,,['Maven Bundle Plugin'],[''],"It would be great if maven-bundle-plugin used resolution=optional for packages that are coming from _optional_ Maven dependencies.

The current behavior causes problems e.g. when embedding libraries with lots of optional dependencies. As described by Detelin Yordanov: ""[I]t seems that BND finds the
references to [the optional packages] and assumes them to be mandatory adding a bunch of imports in the manifest."" This can easily result in hundreds of import-package declarations importing packages that are actually not required by the application. In these cases it is not practical to handle this manually in the plug-in / BND configuration.

This issue was already discussed on felix-users list in Jan 2009 -- see thread ""Maven bundle plugin: Is it possible to mark optional Maven dependencies to be imported with resolition=optional""
http://mail-archives.apache.org/mod_mbox/felix-users/200901.mbox/%3C81f0d9c0901280604x6097dc19r8aa460673bfdad6@mail.gmail.com%3E",Import-Package should include packages from optional Maven dependencies with resolution=optional,7,4,,pjuhos,True,,pjuhos
felix,FELIX-946,2009-02-18T08:24:16.000+0000,2009-06-22T15:24:19.000+0000,2009-09-30T21:39:44.000+0000,,Fixed,New Feature,Trivial,['gogo-0.2.0'],,,,,,,,['Gogo Runtime'],['RFC-147 shell runtime'],Source code donation for the OSGi Shell proposal,Code for new OSGi TSL proposal,,,,pkriens,True,marrs,pkriens
felix,FELIX-930,2009-02-07T11:14:24.000+0000,2009-06-15T09:08:33.000+0000,2009-07-31T15:07:37.000+0000,,Fixed,New Feature,Major,['scr-1.2.0'],['scr-1.0.6'],,,,,,,"['Declarative Services (SCR)', 'Specification compliance']","['The Declarative Services from the OSGi R7 specification (Section 112).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']","The schema has been updated and thus has a new namespace is updated to

                 http://www.osgi.org/xmlns/scr/v1.1.0

This is due to changes to support backwards and forwards computability in the future (after this namespace change). Hopefully we can avoid further namespace changes by introducing extra requirements on the SCR parser at this time.
",XML schema namespace change,,,,fmeschbe,True,fmeschbe,fmeschbe
felix,FELIX-929,2009-02-07T11:13:12.000+0000,2009-06-15T09:06:56.000+0000,2009-07-31T15:07:37.000+0000,,Fixed,New Feature,Major,['scr-1.2.0'],['scr-1.0.6'],,,,,,,"['Declarative Services (SCR)', 'Specification compliance']","['The Declarative Services from the OSGi R7 specification (Section 112).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']","To reduce the amount of XML that must be written for a component description, the name attributes of the component and reference elements will be changed from required to optional. This change is only effective for documents in the new namespace.

The default value of the name attribute of the component element is the value of the class attribute of the nested implementation element.

The default value of the name attribute of the reference element is the value of the interface attribute of the reference element.
",Making name attributes optional,,,,fmeschbe,True,fmeschbe,fmeschbe
felix,FELIX-928,2009-02-07T11:12:19.000+0000,2009-08-19T13:35:08.000+0000,2009-12-08T15:49:23.000+0000,,Fixed,New Feature,Major,['scr-1.2.0'],['scr-1.0.6'],,,,,,,"['Declarative Services (SCR)', 'Specification compliance']","['The Declarative Services from the OSGi R7 specification (Section 112).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']","A way is needed to allow wild specification of the XML documents containing the component descriptions.

To support this, section 112.4.1 will be updated to state that the path element of the Service-Component header grammar may include wildcards in the last component of the path. For example:

     Service-Component: OSGI-INF/*.xml

Only the last component of the path may use wildcards so that Bundle.findEntries can be used to locate the XML document within the bundle and its fragments.
",Allow use of wildcards in Service-Component header,,,,fmeschbe,True,fmeschbe,fmeschbe
felix,FELIX-927,2009-02-07T11:11:25.000+0000,2009-08-03T08:03:29.000+0000,2009-12-08T15:49:23.000+0000,,Fixed,New Feature,Major,['scr-1.2.0'],['scr-1.0.6'],,,,,,,"['Declarative Services (SCR)', 'Specification compliance']","['The Declarative Services from the OSGi R7 specification (Section 112).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']","A bind or unbind method must have one of the following prototypes:

    protected void <method-name>(ServiceReference);
    protected void <method-name>(<parameter-type>);
    protected void <method-name>(<parameter-type>, Map);

If the bind or unbind method has the third prototype, then the service object of the bound service is passed to the method as the first argument and a Map containing the service properties of the bound service is passed as the second argument. The method's first parameter type must be assignable from the type specified by the reference's interface attribute. That is, the service object of the bound service must be castable to the method's first parameter type.

When searching for the bind or unbind method to call, SCR must look through the component implementation class hierarchy. The declared methods of each class are searched for a method with the specified name that takes one or two parameters. The method is searched for using the following priority:

1.The method takes a single parameter and the type of the parameter is org.osgi.framework.ServiceReference.

2.The method takes a single parameter and the type of the parameter is the type specified by the reference's interface attribute.

3.The method takes a single parameter and the type of the parameter is assignable from the type specified by the reference's interface attribute. If multiple methods match this rule, this implies the method name is overloaded and SCR may choose any of the methods to call.

4.The method takes two parameters and the type of the first parameter is the type specified by the reference's interface attribute and the type of the second parameter is java.util.Map

5.The method takes two parameters and the type of the first parameter is assignable from the type specified by the reference's interface attribute and the type of the second parameter is java.util.Map. If multiple methods match this rule, this implies the method name is overloaded and SCR may choose any of the methods to call.
",Allow bind and unbind methods to receive the service properties,,,,fmeschbe,True,fmeschbe,fmeschbe
felix,FELIX-925,2009-02-07T07:52:51.000+0000,2009-07-28T13:53:52.000+0000,2009-12-08T15:49:23.000+0000,,Fixed,New Feature,Major,['scr-1.2.0'],['scr-1.0.6'],,,,,,,"['Declarative Services (SCR)', 'Specification compliance']","['The Declarative Services from the OSGi R7 specification (Section 112).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']","A way is needed to avoid using DS API at all in components with SCR. This means the activate and deactivate methods should not require the ComponentContext parameter. We should also allow the names of the activate and deactivate methods to be specified to avoid requiring specific method names.

To support this, the follow attributes will be added to the component element:

    <attribute name=""activate"" type=""token"" use=""optional"" default=""activate"" />
    <attribute name=""deactivate"" type=""token"" use=""optional default=""deactivate"" />

The activate attribute will specify the name of the activate method and the deactivate attribute will specify the name of the deactivate method.

The signature for the activate and deactivate methods is:

                    p r o t e c t ed vo i d <me thod - name> (< a r gumen t s > ) ;

<arguments> can be zero or more arguments.

For the activate method each argument must be of one of the following types:
     ●   ComponentContext - the Component Context for the component
     ●   BundleContext - the Bundle Context of the component's bundle
     ●   Map - the Component Properties from ComponentContext.getProperties.

If any argument of the activate method is not one of the above types, SCR must log an error message with the Log Service, if present, and the component configuration is not activated.

For the deactivate method each argument must be of one of the following types:
     ●   int/Integer - the deactivation reason
     ●   ComponentContext - the Component Context for the component
     ●   BundleContext - the Bundle Context of the component's bundle
     ●   Map - the Component Properties from ComponentContext.getProperties.

If any argument of the deactivate method is not one of the above types, SCR must log an error message with the Log Service, if present, and the deactivation of the component configuration will continue.

The methods may also be declared public. The same rules as specified in 112.5.8 will be used to locate the activate and deactivate methods in the implementation class hierarchy.

3.2.1 Component deactivation reasons

When a component is deactivated, the reason for the deactivation can be passed to the deactivate method. The following deactivation reasons are specified in ComponentConstants.

                 /**
                   * The reason the component instance was deactivated is unspecified.
                   *
                   * @since 1.1
                   */
                 public static final int DEACTIVATION_REASON_UNSPECIFIED = 0;
                 /**
                   * The component instance was deactivated because the component was disabled.
                   *
                   * @since 1.1
                   */
                 public static final int DEACTIVATION_REASON_DISABLED = 1;
                 /**
                   * The component instance was deactivated because a reference became unsatisfied.
                   *
                   * @since 1.1
                   */
                  public static final int DEACTIVATION_REASON_REFERENCE = 2;
                  /**
                   * The component instance was deactivated because its configuration was changed.
                   *
                   * @since 1.1
                   */
                  public static final int DEACTIVATION_REASON_CONFIGURATION_MODIFIED = 3;
                  /**
                   * The component instance was deactivated because its configuration was deleted.
                   *
                   * @since 1.1
                   */
                  public static final int DEACTIVATION_REASON_CONFIGURATION_DELETED = 4;
                  /**
                   * The component instance was deactivated because the component was disposed.
                   *
                   * @since 1.1
                   */
                  public static final int DEACTIVATION_REASON_DISPOSED = 5;
                  /**
                   * The component instance was deactivated because the bundle was stopped.
                   *
                   * @since 1.1
                   */
                  public static final int DEACTIVATION_REASON_BUNDLE_STOPPED = 6;
",Extend SCR to allow alternate activate and deactivate method signatures,,1,,fmeschbe,True,fmeschbe,fmeschbe
felix,FELIX-924,2009-02-07T06:48:39.000+0000,2009-07-28T14:25:27.000+0000,2009-12-08T15:49:23.000+0000,,Fixed,New Feature,Major,['scr-1.2.0'],['scr-1.0.6'],,,,,,,"['Declarative Services (SCR)', 'Specification compliance']","['The Declarative Services from the OSGi R7 specification (Section 112).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']","A way is needed for the component declaration to say only create a component configuration IF there is a Configuration( or Configurations).

To support this, the follow attribute is added to the component element:

   <attribute name=""configuration-policy"" type=""scr:Tconfiguration-policy""
                 default=""optional"" use=""optional"" />
   <simpleType name=""Tconfiguration-policy"">
                 <restriction base=""string"">
                                <enumeration value=""optional"" />
                                <enumeration value=""require"" />
                                <enumeration value=""ignore"" />
                 </restriction>
   </simpleType>

If the attribute is present and set to require, then a component cannot be satisfied (section 112.5.2) unless there is a Configuration in ConfigurationAdmin for the component. In this situation, the No Configuration case in 112.7 does not apply.If the component is a Factory Component and the component is not satisfied because there is no Configuration present, then the ComponentFactory service will not be registered.

If the attribute is present and set to ignore, then ConfigurationAdmin will not be consulted for the component. In this situation, only the No Configuration case in 112.7 applies.

If the attribute is not present or present and set to optional, then SCR will act as it did prior to this RFC. That is, a Configuration will be used if present in ConfigurationAdmin.",No component instance if no Configuration,,1,,fmeschbe,True,fmeschbe,fmeschbe
felix,FELIX-922,2009-02-06T09:02:26.000+0000,2009-08-31T06:41:54.000+0000,2009-09-08T13:19:10.000+0000,,Fixed,New Feature,Major,['fileinstall-2.0.0'],,,,,,,,['File Install'],"['A bundle that watches a directory set by the felix.fileinstall.dir property; the default is ""load"". JAR files in this directory are installed and uninstalled when they are no longer there. Updating the JAR file in the directory will update the bundle. The bundle allows you to use downloads from a browser, drag and drop, or the command line copy function, for OSGi management.']","Currently File Install recognizes only jar files and cfg files. It tries to install jar files as OSGi bundles and cfg files as configuration entries. It should be possible to extend it to add support for new artifact types, e.g., deployment packages.",File Install bundle should be extensible to support new artifact type,,1,,sahoo,True,gnt,sahoo
felix,FELIX-912,2009-01-30T16:53:47.000+0000,2009-02-16T09:45:46.000+0000,2009-03-12T08:05:00.000+0000,,Fixed,New Feature,Minor,['maven-bundle-plugin-2.0.0'],,,,,,,,['Maven Bundle Plugin'],[''],"The current bundleplugin default for Export-Package is based on the bundle symbolic name, which in turn is based on the Maven groupId and artifactId. While this works for a lot of projects that use the primary package as the bundle symbolic name (or for the Maven metadata) it isn't perfect. I'm thinking of enhancing the bundleplugin to look at the project source to find the main packages in the project, which could be used as the default Export-Package (or possibly Private-Package too).

The overall goal is to allow people to take a Maven project, change the packaging from ""jar"" to ""bundle"" and they'll get the same content. At the moment, because the bundle content is derived from the Export-Package / Private-Package / Include-Resource instructions this may not be the case. We already do this sort of thing for Maven resources -> Include-Resource, so there is a precedent...",Improve default Export-Package / Private-Package settings by scanning the project source,,,,mcculls,True,mcculls,mcculls
felix,FELIX-878,2009-01-14T19:09:31.000+0000,2009-01-14T20:22:27.000+0000,2009-01-14T20:22:27.000+0000,,Fixed,New Feature,Major,['webconsole-1.2.8'],['webconsole-1.2.2'],,,,,,,['Web Console'],['A web based management console'],The sling maven plugin adds configurations through the webconsole; in order to remove or update configs it would be great to get configurations as json.,Allow to get configurations directly in json format,,,,cziegeler,True,cziegeler,cziegeler
felix,FELIX-833,2008-11-23T06:50:26.000+0000,2010-02-19T20:21:34.000+0000,2010-02-19T20:21:34.000+0000,,Won't Fix,New Feature,Minor,,,,,,,,,['Bundle Repository (OBR)'],[''],"Apologies for a vague entry: but I am hoping this is an ok way to introduce the thing.

So: version 4.0.0 of Woodstox xml processor (http://woodstox.codehaus.org) has OSGi-enabled versions of its deliverables (jars). These are built using Ant and bnd task. This is done for the core Woodstox jar as well as supporting pieces.
After doing this, it was suggested that I should check out if others might be interested in using these artifacts, whatever the method might be (i.e. getting copies of jars or something).

Right now Woodstox project does publish these jars via Maven repos, even though build process is not done using Maven; they should be visible via Codehaus maven repo and replicated to other usual open Maven repositories.
Anyway, I don't know if there is any work involved. I am excited about adding OSGi support, and am hoping it might make using Woodstox even easier.
",Woodstox xml processor bundle inclusion?,1,,,cowtowncoder,True,,cowtowncoder
felix,FELIX-826,2008-11-20T00:19:17.000+0000,2009-01-30T23:00:09.000+0000,2009-01-30T23:03:48.000+0000,,Fixed,New Feature,Major,['shell.remote-1.0.4'],['shell.remote-1.0.2'],,,,,,,['Remote Shell'],['Simple remote access for the Felix shell'],,Enable the remote shell to bind to an IP and make 127.0.0.1 the default,2,,,ptriller,True,fmeschbe,ptriller
felix,FELIX-814,2008-11-12T16:05:40.000+0000,2013-11-17T13:59:33.000+0000,2013-11-17T13:59:33.000+0000,,Fixed,New Feature,Minor,,,,,,,,,['Documentation'],['Documentation issues on the web site or for various sup-projects.'],"I'm learning OSGi at the moment and missed the example with Declarative Services. I've written a slightly abbreviated (compared to the other examples) proposal for the missing example 10. Since I don't seem to have write access to that part of the Wiki, I'm attaching the source text here (Not sure if the syntax is correct). I hope it's useful.

The page goes here: http://cwiki.apache.org/confluence/display/FELIX/Apache+Felix+OSGi+Tutorial",[PATCH] Tutorials: addition of missing Example 10 (Declarative Services),2,,,jeremias@apache.org,True,chetanm,jeremias@apache.org
felix,FELIX-804,2008-11-04T18:04:04.000+0000,2008-11-14T17:27:54.000+0000,2008-11-15T15:52:40.000+0000,,Fixed,New Feature,Major,['framework-1.4.1'],['framework-1.4.0'],,,,,,,['Framework'],"['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).']","This bug relates to https://issues.apache.org/jira/browse/CXF-1897
The CXF DOSGi project contains an implementation of the ListenerHook of RFC 126. This could should move into Felix.

See also: http://www.mail-archive.com/dev@felix.apache.org/msg06475.html",Incorporate the Service Registry ListenerHook (RFC 126) work done in the CXF DOSGi project,,,,bosschaert,True,rickhall,bosschaert
felix,FELIX-791,2008-10-24T01:20:06.000+0000,2009-02-16T10:01:11.000+0000,2009-03-12T08:10:20.000+0000,,Cannot Reproduce,New Feature,Minor,,['maven-bundle-plugin-1.4.3'],,,,,,,['Maven Bundle Plugin'],[''],"The bundle plugin currently ignores the filtered resources generated by the maven filter and instead incorporates the unfiltered files from the src directory. It would be good to induce a change to allow the filtered resources to be included in the final jar.

Currently a way around the issue would involve generating the manifest file only and use the maven-jar-plugin to create the final jar. ",To include filtered resouces in the final jar,,,,huankev,True,,huankev
felix,FELIX-790,2008-10-23T16:39:44.000+0000,2008-12-22T09:50:05.000+0000,2008-12-22T09:50:05.000+0000,,Fixed,New Feature,Major,['webconsole-1.2.2'],['webconsole-1.2.0'],,,,,,,['Web Console'],['A web based management console'],A new console plugin to display the last events (the number of events displayed should be configurable),Add console plugin to display OSGi events,,,,cziegeler,True,cziegeler,cziegeler
felix,FELIX-781,2008-10-16T12:04:28.000+0000,2008-10-16T12:06:29.000+0000,2008-10-16T12:06:29.000+0000,,Fixed,New Feature,Major,['webconsole-1.2.2'],['webconsole-1.2.0'],,,,,,,['Web Console'],['A web based management console'],"To analyze issues in a running system, having basic information about the active threads would be very helpfull. For example we just encountered an issue in Sling, where presumably a thread terminated. It would have been interesting to view upfront whether the thread was there or not.",Add basic thread dump to Configuration Status page,,,,fmeschbe,True,fmeschbe,fmeschbe
felix,FELIX-755,2008-10-09T13:16:00.000+0000,2008-10-10T15:41:47.000+0000,2008-10-14T16:19:44.000+0000,,Fixed,New Feature,Major,['framework-1.4.0'],['framework-1.2.1'],,,,,,,['Framework'],"['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).']","In some situations, it is convenient to start the framework with a clean bundle cache. To address this, we should consider adding some sort of support to clean the cache between framework executions.",Add support for cleaning the bundle cache,,,,rickhall,True,rickhall,rickhall
felix,FELIX-753,2008-10-08T18:48:13.000+0000,2008-10-16T20:29:12.000+0000,2008-10-23T16:25:24.000+0000,,Fixed,New Feature,Major,['framework-1.4.0'],['framework-1.2.1'],,,,,,,['Framework'],"['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).']","It is possible that the next OSGi specification will define a standard embedding API for frameworks. Felix should be modified to align with current discussions in this area in an effort to prototype a new design. The new API is based on Felix' existing approach, so the changes are not significant, but it will break backwards compatibility.",Improve Felix' embedding API to align with newly proposed standard framework API,,,,rickhall,True,rickhall,rickhall
felix,FELIX-749,2008-09-29T17:27:52.000+0000,2009-06-12T20:22:59.000+0000,2009-06-12T20:22:59.000+0000,,Fixed,New Feature,Major,['framework-2.0.0'],['framework-1.2.1'],,,,,,,"['Framework', 'Specification compliance']","['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']",Support lazy activation from the R4.1 specification,Add support for lazy activation of bundles,,,,rickhall,True,rickhall,rickhall
felix,FELIX-741,2008-09-26T23:16:05.000+0000,2008-09-29T17:42:02.000+0000,2013-05-02T02:29:17.000+0000,,Fixed,New Feature,Minor,['shell-1.2.0'],,,,,,,,['Shell'],['Shell service for OSGi.'],"Once we modify Felix to support transient starting/stopping of bundles, we should modify the shell's start/stop commands to support it (e.g., perhaps accept a ""-t"" option).",Modify shell start/stop commands to support transient starting/stopping of bundles,,,,rickhall,True,rickhall,rickhall
felix,FELIX-713,2008-09-08T15:18:28.000+0000,2008-09-29T17:40:15.000+0000,2013-05-02T02:29:17.000+0000,,Fixed,New Feature,Major,['framework-1.4.0'],['framework-1.2.1'],,,,,,,"['Framework', 'Specification compliance']","['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']","I would like Felix to support for Bundle.start(int), which is introduced in OSGi R4, version 4.1. This issue tracks transiently starting/stopping bundles.",Add support for transiently starting/stopping bundles,1,,,sahoo,True,rickhall,sahoo
felix,FELIX-691,2008-08-20T12:50:57.000+0000,2008-09-29T10:19:17.000+0000,2008-09-29T10:19:17.000+0000,,Fixed,New Feature,Major,['webconsole-1.2.0'],['webconsole-1.0.0'],,,,,,,['Web Console'],['A web based management console'],"Add a new tab to the console to inspect installed deployment packages, install/update new packages and uninstall them.",Add support for the deployment admin,,,,cziegeler,True,cziegeler,cziegeler
felix,FELIX-663,2008-08-07T04:39:47.000+0000,2008-09-04T14:12:38.000+0000,2008-09-06T04:17:09.000+0000,,Fixed,New Feature,Major,,,,,,,,,['Documentation'],['Documentation issues on the web site or for various sup-projects.'],A How To document explaining how to integrate Felix into Netbeans. ,How To integrate Felix into Netbeans,,,,rdjackson,True,rickhall,rdjackson
felix,FELIX-658,2008-08-04T12:10:52.000+0000,2008-08-04T13:23:08.000+0000,2014-04-01T12:48:24.000+0000,,Fixed,New Feature,Major,,,,,,,,,['Deployment Admin'],"['The Deployment Admin service (OSGi 4.1 compendium chapter 114) standardizes the access to some of the responsibilities of the management agent: that is, the lifecycle management of interlinked resources on an OSGi Service Platform.']","Deployment Admin supports the atomic installation, update and removal of a group of bundles called a package. Once installed, you can create fix packages to upgrade from one version to the next. Often, those updates only contain a subset of the bundles. However (and this is very un-OSGi-like) the spec dictates that even if you update only one bundle in a package, you have to first stop them all, then do the update, and finally start them all again. I would like to have a switch that allows you to change this behavior to only update what is actually changed.

Background: To maintain the software and updates to a gateway, we often define a gateway to contain one deployment package containing all bundles, because that's the only way to truly create atomic updates (once you start having more deployment packages, you run the risk of wanting to update things in more than one package, and then run into the issue that there is no overarching mechanism to do those updates in one transaction).",Extend deployment admin to optionally support not stopping the world on updates.,,,,marrs,True,,marrs
felix,FELIX-648,2008-07-27T21:28:28.000+0000,2008-09-08T16:15:20.000+0000,2008-09-08T16:15:20.000+0000,,Won't Fix,New Feature,Trivial,,,,,,,,,['Framework'],"['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).']","As org.osgi.service.permissionadmin and condpermadmin package is part of core OSGi spec I'm thinking that the system bundle of Felix should also export those packages even if there is no actual service implementation of those services.

As an use case I have some bundles that import those packages (non optional) but those bundles could function if actual services are not present.

In order to this on the current code base is just a matter of changing the maven bundle plugin <Export-Package> to include this packages while building the framework bundle and add them to the property ""org.osgi.framework.system.packages"" in main/conf.properties. I can provide a patch if required.

As comparison, Equinox/Knopflerfish use the same approach. Export the packages but do not actually implement the service and I guess that Felix does not do that as there are no implementations of this services built in into framework. I guess that the other implementations provide those packages as they export all the packages from osgi core spec.",Make system bundle export org.osgi.service.permissionadmin/condpermadmin,,,,adreghiciu,True,,adreghiciu
felix,FELIX-615,2008-06-27T20:40:52.000+0000,2008-08-19T13:51:23.000+0000,2008-09-10T20:18:35.000+0000,,Fixed,New Feature,Minor,['shell.remote-1.0.2'],,,,,,,,['Remote Shell'],['Simple remote access for the Felix shell'],Simple and low profile remote access to the felix shell using a telnet client.,Simple remote shell service access (telnet),,,,wimpi,True,fmeschbe,wimpi
felix,FELIX-610,2008-06-17T08:56:02.000+0000,,2008-06-17T08:56:02.000+0000,,,New Feature,Minor,,,,,,,,,['MOSGi'],['Managed OSGi using JMX'],The console can connect to jmx remote environment through various connection protocols,Multi-protocol jmxconsole support,,,,sfrenot,True,sfrenot,sfrenot
felix,FELIX-604,2008-06-12T22:27:14.000+0000,2008-06-17T11:45:02.000+0000,2008-06-17T11:45:02.000+0000,,Fixed,New Feature,Major,['webconsole-1.2.0'],,,,,,,,['Web Console'],['A web based management console'],"Add a page to the web console which extracts LICENSE and NOTICE files from bundles (and JAR files embedded in bundles) for easy display à-la ""About..."" dialog box.",Add License/Notice page,,,,fmeschbe,True,fmeschbe,fmeschbe
felix,FELIX-576,2008-05-28T08:53:34.000+0000,2008-06-03T07:26:17.000+0000,2008-06-03T07:50:28.000+0000,,Fixed,New Feature,Major,['maven-scr-plugin-1.0.5'],['maven-scr-plugin-1.0.4'],,,,,,,['SCR Tooling'],"['Tooling for Maven, Ant, and Bnd including SCR annotations']","It would be nice to have something like a nameRef attribute for properties to references constants (in the same way we reference values), so something like:

@scr.property nameRef=""Constants.PROPERTY_A"" valueRef=""DEFAULT_A""
",Add support for referencing name constants for property names.,,,,cziegeler,True,cziegeler,cziegeler
felix,FELIX-556,2008-05-14T06:05:05.000+0000,2008-07-09T08:40:39.000+0000,2008-08-27T10:50:15.000+0000,,Fixed,New Feature,Major,['maven-bundle-plugin-1.4.2'],['maven-bundle-plugin-1.4.1'],,,,,,,['Maven Bundle Plugin'],[''],"There are situations where one needed to get resources from files other than jar files (zip, mar, aar etc. )in the classpath. maven-bunlde-plugin provides <Embed-Dependency/> to embed dependencies. But this includes whole of resources without any restriction, which could be cumbersome. If a simple filtering mechanism can be introduced in this section, it would be really helpful. ",Filtering mechanims to <Embed-Dependency/>,,,,saminda,True,mcculls,saminda
felix,FELIX-554,2008-05-13T13:13:52.000+0000,2009-01-29T18:08:43.000+0000,2009-03-12T08:07:44.000+0000,,Fixed,New Feature,Minor,['maven-bundle-plugin-1.4.3'],"['maven-bundle-plugin-1.4.0', 'maven-bundle-plugin-1.4.1']",,,,,,,['Maven Bundle Plugin'],[''],"With version 1.4.1 of maven-bundle-plugin , it is possible to incrementally build a remote obr repository, by adding new bundles using ""mvn deploy -DremoteOBR"", but the removal of obsolete bundles from the repository remains manual, thus error prone.

it would be nice to have a way to automatically remove a bundle from a remote OBR, e.g. to replace a version of a bundle with a new version . 
Removing a bundle would mean suppress the reference to this bundle from the repository.xml file .
Optionaly, we could also ""undeploy"" the artifact itself, but then we have to suppress the folder and edit the maven metadata.


",allow removing a bundle from a remote OBR,,,,agerodol,True,mcculls,agerodol
felix,FELIX-553,2008-05-13T13:04:36.000+0000,2008-08-02T16:53:55.000+0000,2008-08-27T10:50:15.000+0000,,Fixed,New Feature,Minor,['maven-bundle-plugin-1.4.2'],"['maven-bundle-plugin-1.4.0', 'maven-bundle-plugin-1.4.1']",,,,,,,['Maven Bundle Plugin'],[''],"there is a ""clean"" goal on the local repository, which suppresses the references to non-existent files.
It would be nice to have a similar feature for a remote repository (managed with ""mvn deploy -DremoteOBR"") .
",allow cleaning a remote OBR,,,,agerodol,True,mcculls,agerodol
felix,FELIX-568,2008-04-29T14:17:33.000+0000,2008-06-17T11:47:18.000+0000,2008-06-17T11:47:18.000+0000,,Won't Fix,New Feature,Major,,,,,,,,,['Web Console'],['A web based management console'],"This patch adds a separate HTML page showing bundle details to the Sling web console. It displays the same details as those displayed by the ajax-based ""popup"".

Use case: I found myself configuring a bundle with Export-Package etc. and continuously installing it to Sling to see if all package dependencies could be resolved. It is quite tedious to reload the bundle list page, scroll down to the bundle in question and click on the link to get the ajax-based details popup. A simpler (and more restful) way is to have a separate URL and HTML page for a single bundle's details.",[Patch] Separate bundle details page in sling web console,,,,alexander.klimetschek,True,fmeschbe,alexander.klimetschek
felix,FELIX-543,2008-04-25T07:59:02.000+0000,2008-04-30T05:43:52.000+0000,2008-04-30T12:52:17.000+0000,,Fixed,New Feature,Minor,['maven-bundle-plugin-1.4.1'],['maven-bundle-plugin-1.4.0'],,,,,,,['Maven Bundle Plugin'],[''],"I have a use-case for using classifier to create multiple bundles with different symbolic names but the same Bundle-Version.

This is not currently possible, as the maven-bundle-plugin automatically appends the classifier to the Bundle-Version.

Could some switch be added, to disable the appending of classifier to Bundle-Version?

Thanks,

Derek

stuart.mcculloch wrote:

the use-case for adding this feature was for people who
wanted to produce JDK5 and JDK1.4 versions of the _same_ bundle, so
because the symbolic name would be the same, the classifier must then
be added to the version to distinguish them.

it is the intended behaviour for the above use-case, but we could add a switch
to turn it off - it's difficult to detect whether the classifier should be added to the
version automatically, because each bundling process is run in isolation

there's also no way to distinguish where the bundle version came from in Maven
(ie. general config or specific to the classifier) once it reaches the bundleplugin,
so a new switch is the only safe way to detect the different use-case...

",add switch to prevent classifier being appended to Bundle-Version,,,,db82407,True,mcculls,db82407
felix,FELIX-538,2008-04-15T15:27:17.000+0000,2011-10-13T13:57:38.000+0000,2011-10-13T13:57:38.000+0000,,Fixed,New Feature,Minor,,,,,,,,,['HTTP Service'],['The HTTP Service and the Http Whiteboard Service from the OSGi R7 specification (Section 102 and 140).'],"In my sandbox I have committed a very simple, multi-threaded, file-based web server. The web server was designed so that it could easily be used in a bundle (e.g., it can be started, stopped, and restarted) and already has an Activator. It would be interesting if we could extend this with servlet support and tailor it specifically for the OSGi HTTP Service specification.

If we could do this, then we could have one ""heavyweight"" OSGi HTTP Service implementation based on Jetty and this really lightweight version for small device requirements",Create a really lightweight version of the HTTP Service,1,,,rickhall,True,kgilmer,rickhall
felix,FELIX-534,2008-04-12T10:50:52.000+0000,,2010-05-27T20:51:57.000+0000,,,New Feature,Minor,,,,,,,,,['Shell'],['Shell service for OSGi.'],"If you have many bundles it would be nice if you could filter certain status when executing the ps command like so:
ps active resolved --> shows active and resolved bundles
ps --> shows all bundles (just like its currently)

Simple solution is attached.

btw.: there is no ""shell component"" in Felix Jira..",Allow filtering by status in ps command (shell),1,,,tonit,True,,tonit
felix,FELIX-531,2008-04-07T20:30:25.000+0000,2008-04-23T11:06:00.000+0000,2008-05-08T16:32:00.000+0000,2008-04-18,Fixed,New Feature,Minor,['maven-bundle-plugin-1.4.1'],['maven-bundle-plugin-1.4.0'],,,7200,7200,7200,,['Maven Bundle Plugin'],[''],"It would be nice if the wrap and bundleall goals of the maven-bundle-plugin would allow for Import-Packages to allow for optional resolution.  

As such, I've created a patch which allows this with a new configuration parameter: importResolution.  This now tells bnd to make all imports optional:

                <configuration>
                    <importResolution>*;resolution:=optional</importResolution>
                </configuration>

I'll attach the patch as soon as I figure out how.  ",Allow the import package scope to be set for the bundleall goal in the maven-bundle-plugin,1,,,msmoot,True,mcculls,msmoot
felix,FELIX-530,2008-04-07T19:01:32.000+0000,2008-08-25T13:09:23.000+0000,2008-08-25T13:09:23.000+0000,,Won't Fix,New Feature,Major,,,,,,,,,['File Install'],"['A bundle that watches a directory set by the felix.fileinstall.dir property; the default is ""load"". JAR files in this directory are installed and uninstalled when they are no longer there. Updating the JAR file in the directory will update the bundle. The bundle allows you to use downloads from a browser, drag and drop, or the command line copy function, for OSGi management.']","I would like to pull the actions out of the DirectoryWatcher class.
Instead, this class just detect/tracks changes like it currently does and calls an appropriate action contributed via extender pattern.
This way, other bundles can 
1. contribute functionality like handling other stuff than final bundles and .cfg files.
2. replace the current behavior
3. DirectoryWatcher gets slicker / easier to maintain

The current two behaviors (install/update/remove on .jar Bundles and .cfg) could be installed in the activator by default (just like felix.shell does with its commands)",Separate DirectoryWatch function from action,,,,tonit,True,,tonit
felix,FELIX-528,2008-04-06T17:44:42.000+0000,2008-08-25T13:08:32.000+0000,2008-08-25T13:08:32.000+0000,,Won't Fix,New Feature,Minor,,,,,,,,,['File Install'],"['A bundle that watches a directory set by the felix.fileinstall.dir property; the default is ""load"". JAR files in this directory are installed and uninstalled when they are no longer there. Updating the JAR file in the directory will update the bundle. The bundle allows you to use downloads from a browser, drag and drop, or the command line copy function, for OSGi management.']","Currently FileInstall just supports ""flat"" jar-Files as bundles in its load folder.
How about supporting (nested) folders?
This way, you could:
1. just drop an arbitrary folder structure and fileinstall would install all bundles inside.
2. you could enable/disable entire ""bundle-sets"" with:
2a. in/move it out of the folder
2b. rename the folder with prefix that means ""unrecognized folder""

Just a thought..
Toni",Support nested folders in fileinstall,,,,tonit,True,,tonit
felix,FELIX-524,2008-03-28T10:15:42.000+0000,2008-08-25T13:10:38.000+0000,2008-08-25T13:10:38.000+0000,,Fixed,New Feature,Major,,,,,,,,,['File Install'],"['A bundle that watches a directory set by the felix.fileinstall.dir property; the default is ""load"". JAR files in this directory are installed and uninstalled when they are no longer there. Updating the JAR file in the directory will update the bundle. The bundle allows you to use downloads from a browser, drag and drop, or the command line copy function, for OSGi management.']","Currently, File Install only looks in the System properties for the name of the the load directory. I want to use it inside Eclipse (getting tired of update manager) but then it would be nice if file install could be get the load directory from an environment variable. This way, when I install a new Eclipse I only have to install file install and load my extra bundles from one location. If it was a property, I would have to edit configuration files.

I added this in my local aQute copy, if nobody takes up the maintenance of this unit then I can do it if I get commit access",Support Environment variables for getting the load directory for FileInstall,,,,pkriens,True,,pkriens
felix,FELIX-509,2008-03-04T18:34:41.000+0000,2008-03-11T07:02:15.000+0000,2008-03-11T07:08:06.000+0000,,Fixed,New Feature,Minor,['maven-scr-plugin-1.0.4'],"['maven-scr-plugin-1.0.1', 'maven-scr-plugin-1.0.2', 'maven-scr-plugin-1.0.3']",,,,,,,['SCR Tooling'],"['Tooling for Maven, Ant, and Bnd including SCR annotations']","As discussed on the Felix Users mailing list regarding a problem with a third-party component (non relevant but QDox in that specific case), there was a clear indication that having the ability to control which directories are processed by the Maven SCR Plugin would be helpful.

This feature would allow things like:

	<configuration>
		<exclude>target/generated-sources</exclude>
	</configuration>

Current Implementation:

The JavaClassDescriptorManager constructor gets all source directories from MavenProject.getCompileSourceRoots(). The exclusion should be coded here consulting the plugin configuration block.

Since the plugin is currently ""include all exclude explicit"" there may be no purpose in actually having an include directive, unless it would support sub-tree checking, which would complicate things quite a bit (as in exclude target/foo but include target/foo/bar).
",Add support for explicit source directory include/exclude via plugin configuration,,,,rodrigo.madera,True,cziegeler,rodrigo.madera
felix,FELIX-498,2008-02-19T19:21:29.000+0000,2008-02-20T07:25:13.000+0000,2008-02-20T22:24:18.000+0000,,Fixed,New Feature,Trivial,['maven-bundle-plugin-1.4.0'],,,,,,,,"['Bundle Repository (OBR)', 'Maven Bundle Plugin', 'Maven OBR Plugin (Deprecated)']","['', '', '']","I think that it would be nice if the command:
mvn javadoc:jar <install|deploy> 
will add to the OBR repository the javadoc artifact.

I think that we can use either a javadocURL tag pointing the URL containing the Javadoc or the documentation tag. IMHO the best policy would be:
if documentation info is extracted from the bundle or POM that we set javadocURL  tag otherwise we set either documentation and javadocURL to the javadocArtifact.
",Enable javadoc deployment on OBR repository,,,,stefano.lenzi,True,mcculls,stefano.lenzi
felix,FELIX-492,2008-02-15T16:11:20.000+0000,2008-02-18T17:45:11.000+0000,2008-02-20T22:17:04.000+0000,,Fixed,New Feature,Minor,['maven-bundle-plugin-1.4.0'],,,,,,,,['Maven Bundle Plugin'],[''],"AFAIK at the moment it's not artifact generated by the plugin can't contains a classifier. 
I think that adding support for classifier to maven-bundle-plugin will be useful.",Add support to classifier: enable bunlde:bundle goal to generate classified artifact,,,,stefano.lenzi,True,mcculls,stefano.lenzi
felix,FELIX-470,2008-01-23T16:01:33.000+0000,2008-03-05T20:03:37.000+0000,2008-03-05T20:03:37.000+0000,,Fixed,New Feature,Minor,,,,,,,,,,,"A simple bundle that tracks changes in a directory and mirrors these in a framework, both bundles and configuration files.",FileInstall,,,,pkriens,True,rickhall,pkriens
felix,FELIX-465,2008-01-17T14:39:59.000+0000,,2009-05-21T19:59:59.000+0000,,,New Feature,Minor,,['framework-1.0.1'],,,,,,,['Framework'],"['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).']","The issue is discussed here in the forum mail thread:
http://www.nabble.com/Turn-off-bundle-cache-td14843520.html",Support a memory based bundle cache,4,1,,sahoo,True,,sahoo
felix,FELIX-459,2008-01-12T17:21:34.000+0000,2008-02-01T17:27:46.000+0000,2008-02-18T17:46:46.000+0000,,Fixed,New Feature,Minor,['maven-bundle-plugin-1.2.1'],['framework-1.0.0'],,,,,,,['Maven Bundle Plugin'],[''],"Currently there is no way to exclude or customize the ""uses"" clause generated by BND for Export-Package entries.

There are a number of scenarios where this would be useful:

  1) when the generated ""uses"" clause is very large, this can cause problems with Eclipse/PDE (possibly due to a bug in the PDE logic, or because PDE does some extra processing based on this clause).
  2) there may be a package that should be in the ""uses"" list but for some reason it's not been detected by BND.
  3) the ""uses"" clause can expose internal implementation details, that you might not want to in the manifest.

this still needs to be investigated and designed, so this work will be done after the 1.2.0 release of the bundleplugin.","BND: allow customization of the ""uses"" clause in the generated Export-Package",,,,mcculls,True,mcculls,mcculls
felix,FELIX-454,2008-01-10T08:54:51.000+0000,2013-04-24T14:32:50.000+0000,2013-04-24T14:32:50.000+0000,,Fixed,New Feature,Major,,,,,,,,,['Deployment Admin'],"['The Deployment Admin service (OSGi 4.1 compendium chapter 114) standardizes the access to some of the responsibilities of the management agent: that is, the lifecycle management of interlinked resources on an OSGi Service Platform.']","Now that Deployment Admin is on its way into Felix (see FELIX-452), we should also provide an implementation of the AutoConf Service.",Deployment Admin: Provide implementation of the R4.1 Autoconf Service,2,,,fmeschbe,True,cvs,fmeschbe
felix,FELIX-452,2008-01-09T17:27:20.000+0000,2008-01-30T16:54:36.000+0000,2008-01-30T17:00:14.000+0000,,Fixed,New Feature,Major,,,,,,,,,['Deployment Admin'],"['The Deployment Admin service (OSGi 4.1 compendium chapter 114) standardizes the access to some of the responsibilities of the management agent: that is, the lifecycle management of interlinked resources on an OSGi Service Platform.']","Attached is the Deployment Admin contribution, as found here:
https://opensource.luminis.net/confluence/x/CYAG
",Deployment Admin contribution,,,,marrs,True,marrs,marrs
felix,FELIX-440,2007-12-18T23:03:51.000+0000,2007-12-18T23:07:21.000+0000,2007-12-18T23:07:21.000+0000,,Fixed,New Feature,Minor,,,,,,,,,['Framework'],"['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).']","Google recently announced a mobile platform called Android. An important part of that platform is their Dalvik VM, which can run ""modified"" Java bytecode. It is possible to make the framework load bundles on dalvik with some small modifications (as described in this article: http://blog.luminis.nl/luminis/entry/osgi_on_google_android_using).  The changes are very small, the overhead is low, and we will not create a dependency on anything so I will go ahead and commit the support.",Google Android Support,,,,karlpauls,True,karlpauls,karlpauls
felix,FELIX-433,2007-12-06T15:44:06.000+0000,2007-12-07T08:37:21.000+0000,2008-01-30T10:39:28.000+0000,,Fixed,New Feature,Major,['framework-1.0.0'],['framework-0.8.0'],,,,,,,['Maven Bundle Plugin'],[''],"When using Maven 2.0.7 in a reactor based project, modules that depend on maven-bundle-plugin modules will be compiled with the classpath set to the project's target/classes directory.  And since the classes may not be there the module that depends on the bundle fails to compile.",Bundle Plugin should optionally extract the bundel jar to target/classes,,,,chirino,True,mcculls,chirino
felix,FELIX-420,2007-11-16T11:05:23.000+0000,,2009-07-14T11:22:41.000+0000,,,New Feature,Major,,,,,,,,,,,"Eclipse allows other osgi frameworks like Felix to plug in into eclipse via the extension point org.eclipse.pde.ui.osgiFrameworks.
Such an plugin would be very valuable to debug and develope OSGI bundles WITH Felix in Eclipse.",Launcher for Eclipse: org.eclipse.pde.ui.osgiFrameworks,1,,,landon,True,,landon
felix,FELIX-397,2007-10-18T04:45:27.000+0000,2007-10-18T06:00:28.000+0000,2008-01-30T10:39:27.000+0000,,Duplicate,New Feature,Major,,,,,,,,,['Maven Bundle Plugin'],[''],"Maven bundle plugin can nicely collect all Maven dependencies, generate bundle manifest  and create jar package. However it is not very useful if you need to run that bundle in self-hosted mode for debugging. Eclipse provides nice development and debugging environment for OSGi bundles, but bundles created by this plugin can't be easily used there.

One possible option would be to provide a separate goal that would create folder structure compatible with Eclipse PDE. Basically place generated manifest file into the ${basedir}/META-INF/MANIFEST.MF and copy jars according to the Bundle-ClassPath attribute. Then such project could be imported into Eclipse and used with PDE.",Interoperability with Eclipse Equinox and PDE,1,,,eu,True,,eu
felix,FELIX-376,2007-09-21T07:58:38.000+0000,2007-09-21T11:21:37.000+0000,2007-09-21T16:58:56.000+0000,,Fixed,New Feature,Minor,,['framework-1.0.0'],,,,,,,['Maven Bundle Plugin'],[''],"Currently the bundle goal just creates the bundle - it would be nice if it also could write the manifest to a customizable location, to help with Eclipse integration. This would stop people from having to use the manifest goal (which recomputes the manifest) or other plugins just to get the manifest out from the final bundle.",Support writing of manifest to the file system when using bundle goal,,,,mcculls,True,mcculls,mcculls
felix,FELIX-336,2007-08-10T02:00:45.000+0000,2011-06-27T00:54:30.000+0000,2011-06-27T00:54:30.000+0000,,Won't Fix,New Feature,Minor,,"['maven-bundle-plugin-1.2.1', 'maven-bundle-plugin-1.2.0', 'maven-bundle-plugin-1.4.0']",,,,,,,['Maven Bundle Plugin'],[''],"Suggestion from felix mailing list: it would be useful if the bundle-plugin could optimize the contents of the bundle (ie. discard unnecessary internal contents) and possibly perform some obfuscation. Ideally we wouldn't want to discard classes from exported packages as that would lead to split-packages...

Currently, it's possible to use the minijar plugin to optimize the bundle, but this could discard exported classes as it has no OSGi knowledge.

This issue will be used to collect together designs and opinions on baking such support into the bundle-plugin compared to keeping it separate.",Add support for optimization and obfuscation of bundle contents,3,2,,mcculls,True,,mcculls
felix,FELIX-323,2007-07-09T13:35:37.000+0000,2009-02-18T13:17:23.000+0000,2014-06-13T09:27:13.000+0000,,Fixed,New Feature,Minor,,,,,,,,,['HTTP Service'],['The HTTP Service and the Http Whiteboard Service from the OSGi R7 specification (Section 102 and 140).'],The OSGi spec states that the HTTP service should use environment properties to make its (secure) port configurable. This does not support run-time reconfiguration. Adding support for ConfigAdmin based configuration would make it possible to easily (re)configure this service. The fall-back could still be the current behaviour.,Add ConfigAdmin support to the HTTP service,,,,marrs,True,,marrs
felix,FELIX-320,2007-07-04T15:32:56.000+0000,2007-07-04T17:08:46.000+0000,2007-07-04T17:08:46.000+0000,,Fixed,New Feature,Major,['framework-1.0.0'],['framework-1.0.0'],,,,,,,['Felix Commons'],['Initiative to offer bundled version of common open source libaries.'],"The attached patch creates a patch for the xml-api (JAXP, sax, dom, xml-resolver). This allows the use of the bundled xml apis and implementation without the need of putting them into the lib/endorsed directory.",Bundle for the whole xml api,,,,cziegeler,True,cziegeler,cziegeler
felix,FELIX-311,2007-06-22T12:52:20.000+0000,2007-12-02T13:30:54.000+0000,2007-12-02T13:30:54.000+0000,,Fixed,New Feature,Major,,,,,,,,,['iPOJO'],['iPOJO service-oriented component framework'],"New iPOJO Version add some features : 
- Externalized Manipulator Process
- Method interception
- Performance improvement
- Clarify metadata
- Manage instance name unicity
...",New iPOJO version,,,,clement.escoffier,True,,clement.escoffier
felix,FELIX-310,2007-06-20T18:51:14.000+0000,,2009-06-21T19:39:30.000+0000,,,New Feature,Major,,,,,,,,,"['Bundle Repository (OBR)', 'Installer']","['', 'Anything related to the Felix installers or Felix installation.']","If you've ever used the 'mvn jetty:run' plugin, you'll know how damn useful it is to boot up a web app quickly via the maven plugin for jetty.

It would be great if you could run...

{code}
mvn felix:run
{code}

which would create the bundle in the current project (using the bundle plugin), then boot up an embedded Felix container and deploy the bundle inside the container to check things work fine. You could maybe also add dependencies of other bundles to be deployed with it for system tests?",provide a maven 'felix:run' goal for running a maven created bundle inside an embedded Felix server,4,2,,jstrachan,True,,jstrachan
felix,FELIX-308,2007-06-20T12:42:10.000+0000,2007-08-23T09:02:45.000+0000,2007-12-16T17:30:25.000+0000,,Fixed,New Feature,Major,,,,,,,,,['Maven Bundle Plugin'],[''],"The following is a proposal for how to support embedding dependency JAR files in the bundle plugin. The general approach is a slightly modified version of a proposal by Peter Kriens. The idea is to add a mechanism to deal with embedding JARs that is very similar to how the old maven plugin worked, but doing it in a slightly more generic way than the old plugin by adding the following instruction:

    embed-dependency        ::= clause ( ',' clause ) *
    clause                  ::= MATCH ( ';' attr '=' MATCH )
    attr                    ::= 'groupId' | 'artifactId' | 'version' | 'scope'
    MATCH                   ::= <globbed regular expressions>

This instruction would be used to match the specified Maven dependencies for embedding. Any matching dependency would have its JAR file embedded onto the resulting bundle JAR file and it would be appended to the Bundle-ClassPath header after ""."".

This would allow people to easily achieve the same behavior as the old plugin by simply doing:

    <embed-dependency>*;scope=compile|runtime</embed-dependency>

Thus, this instruction would automatically embed any maven dependencies that were of scope ""compile"" or ""runtime"" and append them to the bundle class path.",Add support for embedding dependency JAR files to bundle plugin,1,1,,rickhall,True,mcculls,rickhall
felix,FELIX-303,2007-06-09T09:50:53.000+0000,2009-11-16T07:38:01.000+0000,2011-05-10T08:30:26.000+0000,,Fixed,New Feature,Major,,['framework-0.8.0'],,,,,,,['Dependency Manager'],['Provides dynamic service and component dependency management.'],"When creating service implementations out of POJO's, the more complex implementations will often be composed out of several POJO's, using composition instead of inheritance. This means instead of one implementation, you have N implementations. The consequences of this are that for both the callbacks and the injected services, you want to be able to explicitly specify one of more instances, instead of assuming there's only one. The API needs to be extended to support this (basically adding a parameter of type Object[] for some of the methods).",Support for compositions,2,,,marrs,True,marrs,marrs
felix,FELIX-299,2007-05-24T13:11:19.000+0000,2008-03-12T18:21:04.000+0000,2008-03-12T18:21:04.000+0000,,Fixed,New Feature,Major,,,,,,,,,['Felix Commons'],['Initiative to offer bundled version of common open source libaries.'],Create an OSGi bundle for the backport-util-concurrent 3.0 jar.,backport-util-concurrent 3.0 wrapping,1,,,adreghiciu,True,stefano.lenzi,adreghiciu
felix,FELIX-298,2007-05-24T12:57:38.000+0000,2008-03-12T18:42:10.000+0000,2008-03-12T18:42:10.000+0000,,Fixed,New Feature,Minor,,,,,,,,,['Felix Commons'],['Initiative to offer bundled version of common open source libaries.'],Create an OSGi bundle for the c3p0 0.0.1.1 jar.,c3p0 0.9.1.1 wrapping,1,,,adreghiciu,True,stefano.lenzi,adreghiciu
felix,FELIX-297,2007-05-24T12:28:56.000+0000,2007-07-05T06:38:54.000+0000,2007-07-05T06:38:54.000+0000,,Fixed,New Feature,Major,['framework-1.0.0'],,,,,,,,['Felix Commons'],['Initiative to offer bundled version of common open source libaries.'],Create an OSGi bundle for the portlet-api 1.0 jar.,portlet-api 1.0 wrapping,,,,adreghiciu,True,cziegeler,adreghiciu
felix,FELIX-296,2007-05-24T12:22:46.000+0000,2007-09-24T13:37:34.000+0000,2007-09-24T13:37:34.000+0000,,Fixed,New Feature,Minor,,,,,,,,,['Felix Commons'],['Initiative to offer bundled version of common open source libaries.'],Create an OSGi bundle for the jsp-api 2.0 jar.,jsp-api 2.0 wrapping,,,,adreghiciu,True,fmeschbe,adreghiciu
felix,FELIX-295,2007-05-24T10:18:55.000+0000,2007-09-24T13:32:53.000+0000,2007-09-24T13:32:53.000+0000,,Fixed,New Feature,Minor,,,,,,,,,['Felix Commons'],['Initiative to offer bundled version of common open source libaries.'],Create an OSGi bundle for the servlet-api 2.4 jar.,servlet-api 2.4 wrapping ,,,,adreghiciu,True,fmeschbe,adreghiciu
felix,FELIX-294,2007-05-24T08:23:29.000+0000,2007-07-06T14:22:51.000+0000,2007-07-06T14:22:51.000+0000,,Fixed,New Feature,Minor,['framework-1.0.0'],,,,,,,,['Felix Commons'],['Initiative to offer bundled version of common open source libaries.'],Create an OSGi bundle for the commons-validator 1.3.1 jar.,commons-validator 1.3.1 wrapping ,,,,adreghiciu,True,cziegeler,adreghiciu
felix,FELIX-284,2007-05-15T10:55:25.000+0000,2007-11-23T13:06:09.000+0000,2008-05-30T18:15:57.000+0000,,Fixed,New Feature,Major,['scr-1.0.0'],,,,,,,,['Declarative Services (SCR)'],['The Declarative Services from the OSGi R7 specification (Section 112).'],"Currently, it is not directly visible, what state a component has and what references are satisified.

The intent is to define a simple API - along the lines of the ServiceBinder architecure interfaces.

These interfaces may then be used to implement integrations into management applications such as the Felix shell or JMX.",Add Management API,,,,fmeschbe,True,fmeschbe,fmeschbe
felix,FELIX-274,2007-04-24T14:21:07.000+0000,2007-04-24T14:29:13.000+0000,2007-04-24T14:29:24.000+0000,,Fixed,New Feature,Major,['framework-1.0.0'],,,,,,,,['Felix Commons'],['Initiative to offer bundled version of common open source libaries.'],Add wrapping for Commons Email 1.0,commons-email 1.0 wrapping,,,,fmeschbe,True,fmeschbe,fmeschbe
felix,FELIX-266,2007-04-05T15:27:06.000+0000,2007-04-10T15:26:35.000+0000,2007-04-24T12:55:39.000+0000,,Fixed,New Feature,Major,,,,,,,,,['iPOJO'],['iPOJO service-oriented component framework'],Provides the iPOJO Composition,iPOJO Composition,,,,clement.escoffier,True,rickhall,clement.escoffier
felix,FELIX-257,2007-03-15T00:12:18.000+0000,2007-04-13T10:31:40.000+0000,2007-04-13T10:31:40.000+0000,,Fixed,New Feature,Major,,,,,,,,,['Maven Bundle Plugin'],[''],"I (and as I see some other more) would like to be able to get the manifest generated by maven-bundle-plugin also outside the generated bundle. 
A use case is the development using the PDE support form Eclipse. This requires the manifest in a specific location and I would love to have it generated by maven-bundle-plugin/bnd. In this way we can eliminate the necesity of maintaining 2 manifests (one manifest and the settings from pom).and will be for sure less error prone as you will be able to use for development the same manifest you would have in runtime.

There is already done by carlos sanchez on FELIX-199.",maven-bundle-plugin and manifest generation outside the bundle (jar),1,1,,adreghiciu,True,,adreghiciu
felix,FELIX-255,2007-03-13T16:45:46.000+0000,2007-05-17T06:38:47.000+0000,2007-05-17T06:38:47.000+0000,,Fixed,New Feature,Major,,,,,,,,,['Maven Bundle Plugin'],[''],"Posted also on the list at http://www.mail-archive.com/felix-dev@incubator.apache.org/msg04299.html.

Most of the time (at least in the cases I had) you would like to wrap only
the content of the jar you are targeting and not also the transitive
dependencies that the targeted jar has. You can do this in at least two
ways:
1. add <exclusions> to the dependency. This is verbose for the case that the
targeted jar has extensive dependencies and irrelevant to the process of
wrapping
2. set the Export-package directive  to export only those packages that you
want. here you have to know the internals of the package and if the jar has
some resources as licences, xmls' outside the main package you have to add
them one by one.

So, here I am proposing a new configuration option: excludeTransitive that
is suppose to exclude the transitive artifacts if set to true.

An example implementation can be found at the following location:
http://maven.apache.org/plugins/maven-dependency-plugin/xref/org/apache/maven/plugin/dependency/utils/filters/TransitivityFilter.html

Alin Dreghiciu

PS. Maven does not support a property of the articat as isTransitive()","maven-bundle-plugin, wrapping and excludeTransitive",1,,,adreghiciu,True,,adreghiciu
felix,FELIX-251,2007-03-12T12:24:01.000+0000,2007-03-16T15:36:58.000+0000,2007-04-24T12:58:51.000+0000,,Fixed,New Feature,Major,,,,,,,,,['iPOJO'],['iPOJO service-oriented component framework'],"Manipulated classed should allow to get the component instance to do a kind of  ""reflection"".
Manipulation data contains only information about implemented interfaces and field, it should contain information about methods too.",Add manipulation of  Add POJO interface ,,,,clement.escoffier,True,rickhall,clement.escoffier
felix,FELIX-247,2007-03-07T08:12:29.000+0000,2008-02-19T09:54:23.000+0000,2008-03-03T02:26:48.000+0000,,Fixed,New Feature,Minor,['maven-bundle-plugin-1.4.0'],,,,,,,,['Maven Bundle Plugin'],[''],"Modify maven-bundle-plugin so that runing

  mvn org.apache.felix:maven-bundle-plugin:ant

creates the build.xml and MANIFEST.MF files for the current source code.  This allows a working bundle to be created from a source distribution using ant (without using maven).

The goal of this issue is not to allow development using ant but to provide a way of distributing source code that uses ant to build a working bundle.  Any changes to the source code that affect the bundle headers would not be updated in the manifest, since the manifest file is created at the same time as the build file and is not changed by any ant task.",Add ant goal to maven-bundle-plugin which creates a build.xml and MANIFEST.MF with bundle headers,1,,,tmoloney,True,mcculls,tmoloney
felix,FELIX-246,2007-03-06T21:58:31.000+0000,2007-03-14T04:31:57.000+0000,2007-03-16T17:08:14.000+0000,,Fixed,New Feature,Minor,,,,,,,,,['Felix Commons'],['Initiative to offer bundled version of common open source libaries.'],Create an OSGi bundle for commons-jxpath.,Create an OSGi bundle for commons-jxpath,,,,jconlon,True,enriquer9,jconlon
felix,FELIX-245,2007-03-06T13:05:38.000+0000,,2007-03-14T22:10:10.000+0000,,,New Feature,Minor,,,,,,,,,['Felix Commons'],['Initiative to offer bundled version of common open source libaries.'],Create an OSGi bundle for the hsqldb 1.8.0.7  jar.,hsqldb wrapped as OSGi bundle,,,,adreghiciu,True,enriquer9,adreghiciu
felix,FELIX-241,2007-03-05T18:53:33.000+0000,2007-03-14T04:29:14.000+0000,2007-03-15T23:07:08.000+0000,,Fixed,New Feature,Minor,,,,,,,,,['Felix Commons'],['Initiative to offer bundled version of common open source libaries.'],Create an OSGi bundle for the commons-attributes-compiler 2.2 jar.,commons-attributes-compiler wrapping as OSGi bundle,,,,adreghiciu,True,enriquer9,adreghiciu
felix,FELIX-240,2007-03-05T18:21:27.000+0000,2007-03-14T04:27:30.000+0000,2007-03-15T23:07:08.000+0000,,Fixed,New Feature,Minor,,,,,,,,,['Felix Commons'],['Initiative to offer bundled version of common open source libaries.'],Create an OSGi bundle for the commons-attributes-api 2.2 jar.,commons-attributes-api wrapping as OSGi bundle,,,,adreghiciu,True,enriquer9,adreghiciu
felix,FELIX-239,2007-03-05T17:47:48.000+0000,2007-03-14T04:40:46.000+0000,2007-03-15T23:07:08.000+0000,,Fixed,New Feature,Minor,,,,,,,,,['Felix Commons'],['Initiative to offer bundled version of common open source libaries.'],Create an OSGi bundle for jstl.,jstl wrapping as OSGI bundle,,,,adreghiciu,True,enriquer9,adreghiciu
felix,FELIX-238,2007-03-05T17:12:13.000+0000,2007-03-14T04:22:44.000+0000,2007-03-15T23:07:07.000+0000,,Fixed,New Feature,Minor,,,,,,,,,['Felix Commons'],['Initiative to offer bundled version of common open source libaries.'],Create an OSGi bundle for the aopalliance.,aopalliance wrapping as OSGi bundle,,,,adreghiciu,True,enriquer9,adreghiciu
felix,FELIX-237,2007-03-05T11:30:39.000+0000,,2007-03-14T22:21:30.000+0000,,,New Feature,Minor,,,,,,,,,['Felix Commons'],['Initiative to offer bundled version of common open source libaries.'],,Create an OSGi bundle for jmxtools,,,,tmoloney,True,enriquer9,tmoloney
felix,FELIX-236,2007-03-05T11:28:38.000+0000,,2007-03-23T13:08:06.000+0000,,,New Feature,Minor,,,,,,,,,['Felix Commons'],['Initiative to offer bundled version of common open source libaries.'],,Create an OSGi bundle for jms,,,,tmoloney,True,enriquer9,tmoloney
felix,FELIX-235,2007-03-05T11:27:08.000+0000,2007-03-14T04:49:48.000+0000,2007-03-16T15:40:25.000+0000,,Fixed,New Feature,Minor,,,,,,,,,['Felix Commons'],['Initiative to offer bundled version of common open source libaries.'],,Create an OSGi bundle for xml-resolver,,,,tmoloney,True,enriquer9,tmoloney
felix,FELIX-234,2007-03-05T11:25:56.000+0000,2007-03-14T04:48:07.000+0000,2007-03-16T15:39:58.000+0000,,Fixed,New Feature,Minor,,,,,,,,,['Felix Commons'],['Initiative to offer bundled version of common open source libaries.'],,Create an OSGi bundle for xercesImpl,,,,tmoloney,True,enriquer9,tmoloney
felix,FELIX-233,2007-03-05T11:24:46.000+0000,2007-03-14T04:59:09.000+0000,2007-03-16T15:39:30.000+0000,,Fixed,New Feature,Minor,,,,,,,,,['Felix Commons'],['Initiative to offer bundled version of common open source libaries.'],,Create an OSGi bundle for servlet-api,,,,tmoloney,True,enriquer9,tmoloney
felix,FELIX-232,2007-03-05T11:23:33.000+0000,2007-03-14T04:45:41.000+0000,2007-03-16T15:38:56.000+0000,,Fixed,New Feature,Minor,,,,,,,,,['Felix Commons'],['Initiative to offer bundled version of common open source libaries.'],,Create an OSGi bundle for mail,,,,tmoloney,True,enriquer9,tmoloney
felix,FELIX-231,2007-03-05T11:22:13.000+0000,2007-03-14T04:44:15.000+0000,2007-03-16T15:38:29.000+0000,,Fixed,New Feature,Minor,,,,,,,,,['Felix Commons'],['Initiative to offer bundled version of common open source libaries.'],,Create an OSGi bundle for logkit,,,,tmoloney,True,enriquer9,tmoloney
felix,FELIX-230,2007-03-05T11:20:48.000+0000,2007-03-15T19:01:27.000+0000,2007-03-16T15:37:59.000+0000,,Fixed,New Feature,Minor,,,,,,,,,['Felix Commons'],['Initiative to offer bundled version of common open source libaries.'],,Create an OSGi bundle for log4j,,,,tmoloney,True,enriquer9,tmoloney
felix,FELIX-229,2007-03-05T11:15:52.000+0000,2007-03-14T04:25:01.000+0000,2007-03-16T15:37:19.000+0000,,Fixed,New Feature,Minor,,,,,,,,,['Felix Commons'],['Initiative to offer bundled version of common open source libaries.'],,Create an OSGi bundle for avalon-framework,,,,tmoloney,True,enriquer9,tmoloney
felix,FELIX-228,2007-03-05T11:12:11.000+0000,2007-03-14T04:17:06.000+0000,2007-03-16T15:36:54.000+0000,,Fixed,New Feature,Minor,,,,,,,,,['Felix Commons'],['Initiative to offer bundled version of common open source libaries.'],,Create an OSGi bundle for activation,,,,tmoloney,True,enriquer9,tmoloney
felix,FELIX-226,2007-03-04T21:01:32.000+0000,,2007-03-29T20:26:11.000+0000,,,New Feature,Minor,,,,,,,,,['Felix Commons'],['Initiative to offer bundled version of common open source libaries.'],Create an OSGi bundle for the jta.,jta wrapping,,,,adreghiciu,True,enriquer9,adreghiciu
felix,FELIX-225,2007-03-04T20:59:48.000+0000,2007-03-14T04:54:44.000+0000,2007-03-15T23:07:07.000+0000,,Fixed,New Feature,Minor,,,,,,,,,['Felix Commons'],['Initiative to offer bundled version of common open source libaries.'],Create an OSGi bundle for the hibernate3.,hibernate3 wrapping,,,,adreghiciu,True,enriquer9,adreghiciu
felix,FELIX-224,2007-03-04T20:57:57.000+0000,2007-03-14T04:12:56.000+0000,2007-03-15T23:07:07.000+0000,,Fixed,New Feature,Minor,,,,,,,,,['Felix Commons'],['Initiative to offer bundled version of common open source libaries.'],Create an OSGi bundle for the ehcache.,ehcache wrapping,,,,adreghiciu,True,enriquer9,adreghiciu
felix,FELIX-223,2007-03-04T20:54:52.000+0000,2007-03-14T04:36:14.000+0000,2007-03-15T23:07:07.000+0000,,Fixed,New Feature,Minor,,,,,,,,,['Felix Commons'],['Initiative to offer bundled version of common open source libaries.'],Create an OSGi bundle for the dom4j.,dom4j wrapping,,,,adreghiciu,True,enriquer9,adreghiciu
felix,FELIX-219,2007-02-27T13:20:39.000+0000,2007-10-01T04:42:14.000+0000,2008-01-30T10:39:25.000+0000,,Fixed,New Feature,Minor,,,,,,,,,['Maven Bundle Plugin'],[''],Add the install and install-file goals to the install phase.  These goals will add the bundle metadata to an OBR repository file.  The objective is to allow the local Maven repository (~/.m2/repository) to also be an OSGi Bundle Repository.,Update maven-bundle-plugin to install bundles to a local OBR,1,2,,tmoloney,True,stefano.lenzi,tmoloney
felix,FELIX-217,2007-02-21T07:29:34.000+0000,2007-09-09T17:15:39.000+0000,2007-09-09T17:15:39.000+0000,,Won't Fix,New Feature,Trivial,,,,,,,,,,,"There are different threads on the dev list, which discuss around installing bundles of bundles ([1], [2]). In this discussion, I presented the ideas behind a bundle we built to manage the installation/update of a new system we are building. To show more, how it is being done, I attach the code for this bundle here. I put it under ASL 2 and would also agree to add it to Felix as a contribution.

The goals behind this bundle are:

(1) In our system, we have potentially more than one source of installation/upgrade order. So we had to find a solution of making these processes thread safe in a sense, that another thread should not install/update/remove bundles while another thread is for example trying to deploy bundles through OBR. This is solved by having an InstallerService, which should be used instead of directly doing BundleContext.installBundle, etc.

(2) Likewise we had to ""serialize"" uses of the OBR, which is also solved by the InstallerService mentioned above.

(3) We defined the Assembly Bundle, which defines a set of bundles to be installed together.

Any comments here or on the list are very welcome.


[1] http://www.mail-archive.com/felix-dev@incubator.apache.org/msg03809.html
[2] http://www.mail-archive.com/felix-dev@incubator.apache.org/msg04005.html",Assembly Bundle,2,,,fmeschbe,True,fmeschbe,fmeschbe
felix,FELIX-216,2007-02-16T15:49:21.000+0000,2010-08-20T16:45:15.000+0000,2010-08-20T16:45:15.000+0000,,Invalid,New Feature,Minor,,,,,,,,,['Framework'],"['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).']","The attached JAR contains prototype code for an interoperation framework which is intended to connect OSGi frameworks such as Felix to the module system of JSR 277. The prototype is proposed to the Felix community to consider as a contribution to Felix.

The code is licensed under the Apache v2 license and is covered by a software grant and CCLA which has been faxed to the Apache Software Foundation.

The framework is intended as a strawman for discussion by the Felix community as well as by the Expert Groups of JSR 277 and JSR 291.

Felix was chosen as a recipient because of: the author's prior involvement in Apache, the relative simplicity of contributing code to an Apache project, and Richard Hall's involvement in these JSRs.

See the README in the docs directory of the JAR for technical details of the framework.",Strawman module system interoperation framework,1,,,glyn,True,,glyn
felix,FELIX-213,2007-02-12T08:30:55.000+0000,2007-02-22T00:09:08.000+0000,2007-03-16T06:46:30.000+0000,,Fixed,New Feature,Minor,,,,,,,,,['Felix Commons'],['Initiative to offer bundled version of common open source libaries.'],Create an OSGi bundle for the commons-io 1.3 jar.,commons-io 1.3 wrapping,,,,fmeschbe,True,enriquer9,fmeschbe
felix,FELIX-212,2007-02-12T08:29:00.000+0000,2007-02-22T00:08:52.000+0000,2007-03-16T06:48:45.000+0000,,Fixed,New Feature,Minor,,,,,,,,,['Felix Commons'],['Initiative to offer bundled version of common open source libaries.'],Create an OSGi bundle for the commons-httpclient 3.0.1 jar.,commons-httpclient 3.0.1 wrapping,,,,fmeschbe,True,enriquer9,fmeschbe
felix,FELIX-211,2007-02-12T08:27:02.000+0000,2007-02-22T00:08:39.000+0000,2007-03-16T06:48:56.000+0000,,Fixed,New Feature,Minor,,,,,,,,,['Felix Commons'],['Initiative to offer bundled version of common open source libaries.'],Create an OSGi bundle for the commons-fileupload 1.1.1 jar.,commons-fileupload 1.1.1 wrapping,,,,fmeschbe,True,enriquer9,fmeschbe
felix,FELIX-210,2007-02-12T08:25:53.000+0000,2007-02-22T00:08:27.000+0000,2007-03-16T06:49:37.000+0000,,Fixed,New Feature,Minor,,,,,,,,,['Felix Commons'],['Initiative to offer bundled version of common open source libaries.'],Create an OSGi bundle for the commons-el 1.0 jar.,commons-el 1.0 wrapping,,,,fmeschbe,True,enriquer9,fmeschbe
felix,FELIX-209,2007-02-12T08:23:22.000+0000,2007-02-22T00:08:14.000+0000,2007-03-16T06:49:47.000+0000,,Fixed,New Feature,Minor,,,,,,,,,['Felix Commons'],['Initiative to offer bundled version of common open source libaries.'],Create an OSGi bundle for the commons-digester 1.8 jar.,commons-digester 1.8 wrapping,,,,fmeschbe,True,enriquer9,fmeschbe
felix,FELIX-208,2007-02-12T08:21:07.000+0000,2007-02-22T00:08:00.000+0000,2007-03-16T06:49:24.000+0000,,Fixed,New Feature,Minor,,,,,,,,,['Felix Commons'],['Initiative to offer bundled version of common open source libaries.'],Create an OSGi bundle for the commons-configuration 1.3 jar.,commons-configuration 1.3 wrapping,,,,fmeschbe,True,enriquer9,fmeschbe
felix,FELIX-207,2007-02-12T08:19:56.000+0000,2007-02-22T00:07:46.000+0000,2007-03-16T06:50:11.000+0000,,Fixed,New Feature,Minor,,,,,,,,,['Felix Commons'],['Initiative to offer bundled version of common open source libaries.'],Create an OSGi bundle for the commons-codec 1.2 jar.,commons-codec 1.2 wrapping,1,,,fmeschbe,True,enriquer9,fmeschbe
felix,FELIX-206,2007-02-12T08:18:56.000+0000,2007-02-22T00:07:13.000+0000,2007-03-16T06:50:39.000+0000,,Fixed,New Feature,Minor,,,,,,,,,['Felix Commons'],['Initiative to offer bundled version of common open source libaries.'],Create an OSGi bundle for the commons-beanutils 1.7.0 jar.,commons-beanutils 1.7.0 wrapping,,,,fmeschbe,True,enriquer9,fmeschbe
felix,FELIX-205,2007-02-12T08:14:31.000+0000,2007-02-22T00:06:26.000+0000,2007-03-16T06:51:17.000+0000,,Fixed,New Feature,Minor,,,,,,,,,['Felix Commons'],['Initiative to offer bundled version of common open source libaries.'],Create an OSGi bundle for the cglib-full 2.0.2 jar.,cblib 2.0.2 wrapping,,,,fmeschbe,True,enriquer9,fmeschbe
felix,FELIX-204,2007-02-11T04:35:51.000+0000,2007-03-14T04:33:59.000+0000,2007-03-16T15:36:08.000+0000,,Fixed,New Feature,Minor,,,,,,,,,['Felix Commons'],['Initiative to offer bundled version of common open source libaries.'],Create the bundle(s) necessary to use commons-logging with Felix.,commons-logging wrapping,,,,tmoloney,True,enriquer9,tmoloney
felix,FELIX-200,2007-02-02T10:07:20.000+0000,2007-02-05T13:29:45.000+0000,2007-02-05T13:29:45.000+0000,,Fixed,New Feature,Major,,,,,,,,,['iPOJO'],['iPOJO service-oriented component framework'],"Two new features : 

Add the possibility to configure instances with complex property directly from the XML form. For example : 
<property name=""p1"">
    <property name=""p1.1"" value=""a""/>
    <property name=""p1.2"" value=""b""/>
</property>
will create a Dictionary p1 containing two entry ; p1.1 and p1.2

Allow to declare several provided interfaces by using the string array form  :
<provide interface=""{xxx.Foo, xxx.Bar}""/>
will publish inside the same service registration (consequently the same properties) the xxx.Foo service and xxx.Bar.",Support of complex properties and several provided interface declaration,,,,clement.escoffier,True,rickhall,clement.escoffier
felix,FELIX-199,2007-02-01T01:19:41.000+0000,2007-04-10T16:51:42.000+0000,2007-04-17T03:44:59.000+0000,,Fixed,New Feature,Major,,,,,,,,,['Maven Bundle Plugin'],[''],"Running

mvn org.apache.felix:maven-bundle-plugin:recursivebundle

in a project will convert each dependency, including transitive ones, to an OSGi bundle in the target dir. It does not include dependent libraries in the bundle, so each bundle matches the corresponding jar.
",Add a recursive bundle goal to maven plugin to generate osgi bundles for all dependencies,2,5,,carlos,True,rickhall,carlos
felix,FELIX-191,2007-01-03T15:58:59.000+0000,2007-01-24T13:59:44.000+0000,2007-01-24T13:59:44.000+0000,,Fixed,New Feature,Minor,,,,,,,,,['Maven Bundle Plugin'],[''],A new version for bnd is available: 0.0.107,Bnd version 107 available,,,,pkriens,True,rickhall,pkriens
felix,FELIX-183,2006-12-01T22:49:40.000+0000,2008-03-12T18:38:22.000+0000,2008-03-12T18:38:22.000+0000,,Fixed,New Feature,Minor,,,,,,,,,['Felix Commons'],['Initiative to offer bundled version of common open source libaries.'],Create an OSGi bundle for the jzlib 1.0.7 jar.,jzlib 1.0.7 wrapping,1,,,jconlon,True,enriquer9,jconlon
felix,FELIX-182,2006-12-01T22:46:11.000+0000,2007-03-16T17:07:14.000+0000,2007-03-16T17:07:14.000+0000,,Fixed,New Feature,Minor,,,,,,,,,['Felix Commons'],['Initiative to offer bundled version of common open source libaries.'],Create an OSGi bundle for the jdbm 1.0 jar.,jdbm 1.0 wrapping,,,,jconlon,True,enriquer9,jconlon
felix,FELIX-181,2006-12-01T22:43:33.000+0000,2007-02-22T00:37:31.000+0000,2007-03-16T17:11:09.000+0000,,Fixed,New Feature,Minor,,,,,,,,,['Felix Commons'],['Initiative to offer bundled version of common open source libaries.'],Create an OSGi bundle for the commons-lang 2.1 jar.,commons-lang 2.1 wrapping,,,,jconlon,True,enriquer9,jconlon
felix,FELIX-180,2006-12-01T22:40:17.000+0000,2007-07-04T15:34:33.000+0000,2007-07-04T15:34:33.000+0000,,Fixed,New Feature,Minor,['framework-1.0.0'],,,,,,,,['Felix Commons'],['Initiative to offer bundled version of common open source libaries.'],Create an OSGi bundle for the commons-collections 3.2 jar.,commons-collections 3.2 wrapping,,,,jconlon,True,enriquer9,jconlon
felix,FELIX-179,2006-12-01T15:42:29.000+0000,2007-03-14T04:19:29.000+0000,2007-03-16T17:08:47.000+0000,,Fixed,New Feature,Major,,['framework-0.8.0'],,,,,,,['Felix Commons'],['Initiative to offer bundled version of common open source libaries.'],"Create an OSGi bundle for the antlr 2.7.6 jar.
",antlr 2.7.6 wrapping,,,,jconlon,True,enriquer9,jconlon
felix,FELIX-178,2006-12-01T14:24:19.000+0000,2010-04-26T11:40:25.000+0000,2010-04-26T11:40:25.000+0000,,Fixed,New Feature,Minor,['bundlerepository-1.6.0'],,,,,,,,['Bundle Repository (OBR)'],[''],"In order to use OBR to resolve dependencies of locally installed bundles, it would be nice if the functionality of converting a MANIFEST to a Resource was publicly exposed somehow. Then it would be possible to simply convert a local bundle to a Resource, add it to a resolver, call resolve(), and then deploy(). This is possible now, but the developer must implement the conversion by hand when OBR already does this.",OBR should expose some way to convert a locally installed bundle to a Resource,1,1,,rickhall,True,gnt,rickhall
felix,FELIX-167,2006-10-30T14:31:55.000+0000,2006-10-30T16:36:13.000+0000,2007-02-10T02:52:48.000+0000,,Fixed,New Feature,Major,,,,,,,,,['iPOJO'],['iPOJO service-oriented component framework'],"The old arch command does no more work with the new version of iPOJO : 
- the old version does not create an instance of the component
- the architecture model managed by the old version has changes to be more consistent with the iPOJO extensible model",Arch command for iPOJO 0.7,,,,clement.escoffier,True,,clement.escoffier
felix,FELIX-166,2006-10-30T14:25:47.000+0000,2006-10-30T16:34:17.000+0000,2007-02-10T02:52:48.000+0000,,Fixed,New Feature,Major,,,,,,,,,['iPOJO'],['iPOJO service-oriented component framework'],"The new version of iPOJO contains several changes : 
 - Differentiation between component and component instances  
Now, the declaration of a component does not create a component instance, you need to add an instance tag with your component instance configuration:

- Factory
iPOJO 0.7.0-SNAPSHOT provides a factory systems. Indeed, for each declared component type, iPOJO could expose an iPOJO factory service and a
ManagedServiceFactory service. These two services allow the creation of component instance dynamically. It is possible to configure these component instances. Moreover the given configuration can contain ""complex"" object like Map....

- Configuration Handler
The configuration handler was re-implemented to be more consistent with the OSGi specification. Moreover, the metadata, to configure the handler, changes. Henceforth, the metadata looks like:
<properties configurable=""true"">
<property name=""int"" field=""foo""/>
<property name=""int"" field=""bar""/>
</properties>
The configurable attribute allows the publication of the ""ManagedService"" service (dynamic reconfiguration of the component instance)

- Performance improvement :
The iPOJO core was patched to avoid useless handler indirection.

- Filtered service dependency :
The dependency handler had bugs when managing dependency with a filter. This bug was corrected. Now, the dynamism of filtered dependency works.

- Import / Export of the ""org.osgi.service.cm"" package :
The previous version of iPOJO does not export correctly the package (bad version number).",iPOJO New Version,,,,clement.escoffier,True,rickhall,clement.escoffier
felix,FELIX-158,2006-10-19T15:17:20.000+0000,2008-09-08T16:12:18.000+0000,2008-09-08T16:12:18.000+0000,,Fixed,New Feature,Major,,,,,,,,,"['Declarative Services (SCR)', 'Framework']","['The Declarative Services from the OSGi R7 specification (Section 112).', 'The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).']","It looks likely that OSGi R4.1 will include this method for accessing bundle contexts in order to simplify creating cross-framework bundles that require such functionality, such as Declarative Services. This is a easy method to implement. It also needs to be guarded by security checks.",Add Bundle.getBundleContext() method,,1,,rickhall,True,rickhall,rickhall
felix,FELIX-153,2006-09-22T16:17:33.000+0000,2006-09-26T16:50:50.000+0000,2006-09-28T06:25:51.000+0000,,Fixed,New Feature,Major,,,,,,,,,"['Framework', 'Specification compliance']","['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']","This is about the missing implementation of Bundle.getResources(String name).

This method is similar to Bundle.getResource(String name) but returns the list of all matching resources to the provided fully qualified resource name. 

It follows the same algorigthm as class or resource loading that is actually in place:
If the bundle is not resolved and cannot be resolved, the local class path is only used to lookup for resources.
If the bundle is resolved:

   - looks in the parent class loader (boot delegation)
   - looks at imports
   - look at the local class path
   - try to bind dynamic imports and check if it created a matching import.

The visibility rule is like the one for classes. 
Since Felix does not support split packages and does not bundle-require implemented, the visibility is only related to imports. if the resource comes from an import, this method only returns only what that import has a resources with that name.
If the resource comes from the local class path, urls to all resources with that name are returned.
",Implementing Bundle.getResources(String),,,,ogruber,True,rickhall,ogruber
felix,FELIX-139,2006-09-05T08:34:05.000+0000,,2006-09-05T08:34:05.000+0000,,,New Feature,Major,,,,,,,,,['MOSGi'],['Managed OSGi using JMX'],"This MBean enables the remote console to get data from the lowest part of the remote gateways.
",Create an MBean and a JMXConsole tab for low level operating system data,1,,,sfrenot,True,,sfrenot
felix,FELIX-136,2006-08-30T14:30:29.000+0000,,2009-02-16T16:34:12.000+0000,,,New Feature,Minor,,,,,,,,,['Framework'],"['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).']","Eclipse PDE uses ""framework launchers"" for lauching arbitrary OSGi frameworks. Felix in combination with its ""reference:"" protocol is successfully able to be launched by Eclipse PDE, but it is not ideal since it requires that the project be structured with everything in the root directory. Since projects are typically organized around bin/ and classes/ directories, this is less than perfect.

Equinox supports a special property to modify the bundle's class path at development time to alleviate this situation. Such a property could also be added to Felix to improve integration with Eclipse PDE.

For example, DirectoryRevision could be modified to search for a configuration property named ${bundle-symbolic-name}.classpath and could prepend this value to the existing manifest header. To my understanding, this is similar to the approach used by Equinox.

If we implement this, then we should probably add another property to enable/disable development-time features, so that people cannot use this property unless the framework is being used in development mode.",Add property to modify development-time bundle class path for Eclipse PDE,1,,,rickhall,True,rickhall,rickhall
felix,FELIX-135,2006-08-30T11:44:23.000+0000,2006-08-30T12:42:08.000+0000,2007-02-10T02:52:47.000+0000,,Fixed,New Feature,Major,,,,,,,,,['MOSGi'],['Managed OSGi using JMX'],This MBean / Graphical couple is able to remotely install a bundle though remote obr manipulation,A  graphical tab to remotely  interact with obr,1,,,sfrenot,True,,sfrenot
felix,FELIX-134,2006-08-30T11:31:21.000+0000,,2006-08-30T11:37:00.000+0000,,,New Feature,Major,,,,,,,,,['MOSGi'],['Managed OSGi using JMX'],A tab for the jmx console that grabs information from platform MBean about memory activity on a remote gateway. ,A  graphical tab to remotely  get memory activity,1,,,sfrenot,True,,sfrenot
felix,FELIX-133,2006-08-30T11:05:38.000+0000,2006-08-30T11:15:58.000+0000,2007-02-10T02:52:47.000+0000,,Fixed,New Feature,Major,,,,,,,,,['MOSGi'],['Managed OSGi using JMX'],Creating a new tab in the jmx console and a new Mbean in the agent in order to manage bundles lifecycle,Management Bundle Tab,1,,,sfrenot,True,,sfrenot
felix,FELIX-116,2006-08-24T13:50:29.000+0000,2008-03-07T00:46:00.000+0000,2008-03-07T00:46:00.000+0000,,Fixed,New Feature,Major,,,,,,,,,"['Conditional Permission Admin', 'Specification compliance']","['The Conditional Permission Admin from the OSGi R4 specification (Section 9).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']",See section 9 in the OSGi R4 core specification for a full description of the task.,Implement Conditional Permission Admin,,,,rickhall,True,karlpauls,rickhall
felix,FELIX-115,2006-08-24T13:49:16.000+0000,2008-03-07T00:46:50.000+0000,2008-03-07T00:46:50.000+0000,,Fixed,New Feature,Major,,,,,,,,,"['Permission Admin', 'Specification compliance']","['The Permission Admin from the OSGi R4 specification (Section 10).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']",See section 10 in the OSGi R4 Service Core for a full description of the task.,Implement Permission Admin,,,,rickhall,True,karlpauls,rickhall
felix,FELIX-110,2006-08-10T21:56:06.000+0000,2007-10-01T14:09:54.000+0000,2008-05-30T18:15:56.000+0000,,Fixed,New Feature,Major,['scr-1.0.0'],['framework-0.8.0'],,,,,,,['Declarative Services (SCR)'],['The Declarative Services from the OSGi R7 specification (Section 112).'],"I completed  the current Felix SCR implementation to take into
account <properties> elements.

The modified files are in attachement and the modified source sections
are bracketed by // CHANGED lines
",completion of the current Felix SCR implementation to take into account components <properties> elements,,,,donsez,True,,donsez
felix,FELIX-102,2006-07-26T08:12:03.000+0000,2008-08-28T20:48:00.000+0000,2013-05-02T02:28:59.000+0000,,Fixed,New Feature,Minor,['framework-1.2.0'],,,,,,,,"['Framework', 'Specification compliance']","['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']",Section 3.5.2 of the OSGi R4 core specification describes singleton bundles. Felix does not currently support this.,Implement singleton bundle support,1,1,,rickhall,True,rickhall,rickhall
felix,FELIX-101,2006-07-26T08:09:13.000+0000,2007-09-16T19:59:07.000+0000,2007-09-16T19:59:07.000+0000,,Fixed,New Feature,Major,,,,,,,,,"['Framework', 'Specification compliance']","['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']","Section 3.5.5 of the OSGi R4 core specification specifically allows the same package to be exported more than once if they have different attributes or directives, Felix does not currently support this. Implementing this may have some hidden, tricky issues, but it should not be that difficult.",Implement support for exporting the same package more than once,1,,,rickhall,True,karlpauls,rickhall
felix,FELIX-92,2006-07-14T04:31:09.000+0000,2006-11-21T18:40:25.000+0000,2007-02-10T02:52:45.000+0000,,Fixed,New Feature,Minor,,['framework-0.8.0'],,,,,,,['Maven Bundle Plugin'],[''],Provide fine grain support for inlined artifacts where users may designate which files from a dependency jar should be unrolled into the bundle.,Support file-level desigation in inlinedArtifacts,,,,cwall,True,rickhall,cwall
felix,FELIX-86,2006-06-24T21:19:13.000+0000,2006-08-24T12:02:49.000+0000,2007-02-10T02:52:45.000+0000,,Fixed,New Feature,Major,['framework-0.8.0'],['framework-0.8.0'],,,,,,,"['Configuration Admin', 'Event Admin', 'User Admin', 'Wire Admin']","['The Configuration Admin from the OSGi R7 specification (Section 104).', 'The Event Admin from the OSGi R6 specification (Section 113).', 'The User Admin from the OSGi R4 specification (Section 107).', 'The Wire Admin from the OSGi R4 specification (Section 108).']","We need to implement mappings from ConfigurationEvents, UserAdminEvents, and WireAdminEvents to EventAdminEvents similar to what we've done with the UPnPEvents.",Implement the missing event mappings to the EventAdmin,,,,karlpauls,True,karlpauls,karlpauls
felix,FELIX-74,2006-05-24T14:22:34.000+0000,2006-05-24T14:45:00.000+0000,2007-02-10T02:52:42.000+0000,,Fixed,New Feature,Minor,,,,,,,,,['Maven Bundle Plugin'],[''],"The manual maintenance of OSGi import package headers is painful. There is a need to automatically generate these imports from the Java code maintained in the JAR. The ZIP file enclosed contains changes to the OSGi maven plugin that provide this facility, and some more.",Provide automatic generation of import package headers,,,,pkriens,True,rickhall,pkriens
felix,FELIX-64,2006-04-21T16:57:27.000+0000,2009-09-24T20:52:38.000+0000,2015-08-01T09:25:11.000+0000,,Fixed,New Feature,Minor,['http-2.0.2'],['http-2.0.2'],,,,,,,['HTTP Service'],['The HTTP Service and the Http Whiteboard Service from the OSGi R7 specification (Section 102 and 140).'],"The HTTP Service was defined before the whiteboard approach for services became common, so it would be nice to implement a standard adaptor for the HTTP Service that implemented this pattern. To be more precise, the spec'ed behavior of the HTTP Service requires that servlet instances be registered with it explicitly. The whiteboard adaptor would allow servlet instances to be registered as services in the service registry and have the HTTP Service automatically register them by listening for service events.",Create a whiteboard adaptor for the HTTP Service,1,,,rickhall,True,srs,rickhall
felix,FELIX-58,2006-04-20T14:33:41.000+0000,2014-02-12T07:49:08.000+0000,2014-02-12T09:18:09.000+0000,,Won't Fix,New Feature,Minor,,,,,,,,,['HTTP Service'],['The HTTP Service and the Http Whiteboard Service from the OSGi R7 specification (Section 102 and 140).'],"Since our HTTP Service uses Jetty underneath, it would be interesting to extend it to support deploying WARs in bundles. Marcel Offermans has done some work in this area, so I am assigning this issue to him. :-) I am not sure if this issue can be done independently of a move to Jetty 5/6 (see FELIX-55) or if it should be coordinated or merged somehow.",Extend HTTP Service to support deploying WARs,3,,,rickhall,True,marrs,rickhall
felix,FELIX-56,2006-04-19T20:19:32.000+0000,2006-08-24T15:26:09.000+0000,2006-08-24T15:26:09.000+0000,,Fixed,New Feature,Major,,,,,,,,,['Manifest Generator (mangen)'],['OSGi bundle manifest generator'],"Move current codebase from SF location and modify package naming and build model for use within Felix.

Migrate current Ascert held Twiki documents to Apache

Raise JIRA reports for current ""enhancement / TODO"" items",Initial migration of mangen from current SF project,,,,walkerr,True,walkerr,walkerr
felix,FELIX-49,2006-03-27T02:29:33.000+0000,2006-03-30T06:08:38.000+0000,2006-03-30T06:08:31.000+0000,,Fixed,New Feature,Major,,,,,,,,,,,"We currently have OSGi R4 Core sources ASL'd in the Felix repo.  The goal of this issue is to obtain, commit, and package OSGi R4 Compendium sources with ASL headers.

Preparing these sources for M2 deployment and general ASL availability will help us create comprehensive builds for other OSGi R4 service implementations using M2.
",Create new M2 module for R4 Compendium ASL'd sources,,,,enriquer9,True,enriquer9,enriquer9
felix,FELIX-42,2005-12-28T06:41:37.000+0000,,2011-06-25T18:33:34.000+0000,,,New Feature,Minor,,,,,,,,,['Framework'],"['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).']","Many third-party libraries use the context class loader associated with a thread as a means to implement plugins or as a way to gain access to classes in a context outside of the library itself, such as from another component or module. This mechanism does not play well with the OSGi framework, which does its own fair share of class loader magic. Further, the context class loader provides access to potentially arbitrary classes, which (from the perspective of the OSGi framework) breaks modularity. Regardless, in the interest of making Felix play well with these third-party libraries, it is worthwhile to investigate whether a strategy for dealing with the context class loader can be devised, such as setting the context class loader at the callback entry points for each bundle.",Define a strategy for dealing with context class loader usage,,,,rickhall,True,,rickhall
felix,FELIX-35,2005-10-12T23:16:23.000+0000,2008-10-15T20:42:28.000+0000,2008-10-15T20:42:28.000+0000,,Fixed,New Feature,Major,['framework-1.4.0'],['framework-1.2.1'],,,,,,,"['Framework', 'Specification compliance']","['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']","This issue is described in section 7 of the OSGi R4 specification. PackageAdmin has seen several new methods added to it for R4, which include:

    PackageAdmin.getExportedPackages(String)
    PackageAdmin.resolveBundles(Bundle[])
    PackageAdmin.getRequiredBundles(String)
    PackageAdmin.getBundles(String, String)
    PackageAdmin.getFragments(Bundle)
    PackageAdmin.getHosts(Bundle)
    PackageAdmin.getBundle(Class)
    PackageAdmin.getBundleType(Bundle)

Many of these methods can only be implemented when other corresponding issues have been resolved (e.g., bundle fragments). However, several of them can already be implemented now.",Implement new PackageAdmin methods,,,,rickhall,True,rickhall,rickhall
felix,FELIX-34,2005-10-12T23:08:10.000+0000,2006-07-25T10:16:58.000+0000,2006-07-25T10:17:12.000+0000,,Fixed,New Feature,Major,,,,,,,,,"['Framework', 'Specification compliance']","['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']",This issue is described in section 4.6.1 in the OSGi R4 specification. There are new bundle events of type RESOLVED/UNRESOLVED and STARTING/STOPPING that need to be fired.,Implement new bundle event types,,,,rickhall,True,rickhall,rickhall
felix,FELIX-33,2005-10-12T23:06:24.000+0000,2009-06-30T20:43:17.000+0000,2009-08-28T16:05:08.000+0000,,Fixed,New Feature,Major,['framework-2.0.0'],,,,,,,,"['Framework', 'Specification compliance']","['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']","This issue is described in section 4.5 of the OSGi R4 specification. When the system bundle is updated, the framework is supposed to shutdown and restart.",Implement system bundle update,,,,rickhall,True,rickhall,rickhall
felix,FELIX-32,2005-10-12T22:18:17.000+0000,2007-01-29T17:39:39.000+0000,2007-01-29T17:39:39.000+0000,,Fixed,New Feature,Major,,,,,,,,,"['Framework', 'Specification compliance']","['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']","This issue is described in section 5.9 of the OSGi R4 specification. The main jist of this issue is that R4 allows multiple versions of the same package to be shared in the framework at the same time. This means that the framework must dynamically partition the service registry based on package versions (e.g., a bundle using log v1, should not see a log v2 service). To support bundles that want to inspect all services regardless of package version, new API was defined in R4 for such purposes. This API includes BundleContext.getAllServiceReferences() as well as a new AllServiceListener.

Most of the plumbing for all of this works correctly in Felix now, but these last two API aspects still need to be implemented. Their implementation should be pretty straightforward.","Implement mechanisms for accessing ""all"" service references",,,,rickhall,True,,rickhall
felix,FELIX-31,2005-10-12T20:55:43.000+0000,2006-10-20T19:30:51.000+0000,2006-10-20T19:30:51.000+0000,,Fixed,New Feature,Major,,,,,,,,,"['Framework', 'Specification compliance']","['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']","This issue is described in section 4.3 of the OSGi R4 specification. There are various methods in the Bundle interface that are not covered by other issues, such as:

    Bundle.getSymbolicName()
    Bundle.loadClass()
    Bundle.getResources(String)
    Bundle.getEntryPaths(String)
    Bundle.getEntry(String)
    Bundle.findEntries(String, String, boolean)
    Bundle.getLastModified()

Most of these methods are reasonable simplistic and will involve localized changes.",Implement new Bundle interface methods,,,,rickhall,True,rickhall,rickhall
felix,FELIX-30,2005-10-12T20:45:21.000+0000,2010-01-10T22:17:36.000+0000,2010-01-10T22:17:56.000+0000,,Fixed,New Feature,Minor,['framework-2.0.3'],,,,,,,,"['Framework', 'Specification compliance']","['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']","This issue is described in section 3.15 of the OSGi R4 specification. The main jist is to provide a standard mechanism to place classes on the JVM boot class path and to deliver framework extensions and/or updates. This mechanisms is modeled as bundle fragments, but it not really completely related to it. The boot class path aspect is likely to be dependent on specific JVMs.",Implement extension bundles,,,,rickhall,True,karlpauls,rickhall
felix,FELIX-29,2005-10-12T18:37:11.000+0000,2009-05-08T14:47:14.000+0000,2009-05-08T14:47:14.000+0000,,Fixed,New Feature,Major,['framework-1.8.0'],['framework-0.8.0'],,,,,,,"['Framework', 'Specification compliance']","['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']",This issue is described in section 3.14 of the OSGi R4 specification. The main gist is that a logical bundles can now be defined in more than one physical bundle. This means that fragment bundles logically merge with their host bundle at run time. This issue impacts the resolver algorithm and will likely have some impacts on the run-time class loading search order.,Implement bundle fragments,13,12,,rickhall,True,rickhall,rickhall
felix,FELIX-28,2005-10-12T18:15:29.000+0000,2009-06-16T20:42:38.000+0000,2009-06-16T20:42:38.000+0000,,Fixed,New Feature,Major,,['framework-0.8.0'],,,,,,,"['Framework', 'Specification compliance']","['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']",This issue is described in section 3.13 of the OSGi R4 specification. The main jist is that bundles can now express explicit dependencies on other named bundles. This means that the depending bundle imports everything that the target bundle exports. It is also possible to re-export imported packages. This issue impacts the resolver algorithm and will likely have some impacts on the run-time class loading search order.,Implement require-bundle functionality,4,4,,rickhall,True,rickhall,rickhall
felix,FELIX-27,2005-10-12T18:03:57.000+0000,2007-01-18T01:39:02.000+0000,2007-01-18T01:39:02.000+0000,,Fixed,New Feature,Major,,,,,,,,,"['Framework', 'Specification compliance']","['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']",This issue is described in section 3.10 of the OSGi R4 specification. The main jist is to add localization support to bundle manifest data. This is a new feature that includes new API.,Implement bundle localization,,,,rickhall,True,rickhall,rickhall
felix,FELIX-25,2005-10-12T17:09:40.000+0000,2006-04-21T14:54:10.000+0000,2006-04-21T23:51:24.000+0000,,Fixed,New Feature,Minor,,,,,,,,,"['Framework', 'Specification compliance']","['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']","This issue is described in section 3.8.3 of the OSGi R4 specification. The general jist is that this property is used to short circuit the ""strict"" parent class loading behavior of the framework; normally, only ""java.*"" packages are delegated to the parent, but packages listed in this property will also be delegated to the parent. This is likely a pretty localized change.",Implement boot delagation property,,,,rickhall,True,rickhall,rickhall
felix,FELIX-24,2005-10-12T17:06:19.000+0000,2006-02-07T23:56:21.000+0000,2006-02-07T23:56:21.000+0000,,Fixed,New Feature,Major,,,,,,,,,"['Framework', 'Specification compliance']","['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']","This issue is described in section 3.8.1 of the OSGi R4 specification. The main jist is that bundles can now specify directories inside their bundle JAR file to be on their class path, in addition to the bundle JAR itself and embedded JAR files. This should be a reasonably localized change, although it may impact the current bundle cache interface, but I don't think so.",Add support for directories in bundle class path,,,,rickhall,True,rickhall,rickhall
felix,FELIX-23,2005-10-12T17:02:07.000+0000,2007-01-25T15:06:13.000+0000,2007-01-25T15:06:13.000+0000,,Fixed,New Feature,Major,,,,,,,,,"['Framework', 'Specification compliance']","['The core framework of the OSGi R6 specification (Sections 2, 3, 4, 5, 6, 7, 8, and 11).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']","This issue is documented in sections 3.3 and 999 in the OSGi R4 specification and compendium. This should not be a significant amount of work, we just need to implement some code to verify a bundle's required execution environment.",Implement execution environment checking,1,,,rickhall,True,rickhall,rickhall
felix,FELIX-22,2005-10-12T16:59:07.000+0000,2010-01-10T22:14:31.000+0000,2010-01-10T22:14:31.000+0000,,Fixed,New Feature,Major,['framework.security-1.0.0'],,,,,,,,['Framework Security'],['Implementation of the security part of the OSGi specification'],"This issue relates to section 2 of the OSGi R4 specification. This issue is somewhat difficult in general, but even more difficult if we hope to create a solution that works easily on J2ME and constrained devices.",Implement support for digitally signed bundles,,,,rickhall,True,karlpauls,rickhall
felix,FELIX-20,2005-10-08T22:26:39.000+0000,2007-04-11T18:28:26.000+0000,2008-03-27T15:05:30.000+0000,,Fixed,New Feature,Major,['metatype-1.0.0'],,,,,,,,"['Metatype Service', 'Specification compliance']","['The Metatype Service from the OSGi R6 specification (Section 105).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']",See section 105 in the OSGi R4 Service Compendium for a full description of the task.,Implement Metatype Service,,,,rickhall,True,fmeschbe,rickhall
felix,FELIX-19,2005-10-08T22:24:03.000+0000,2006-04-28T18:29:34.000+0000,2006-07-04T22:24:49.000+0000,,Fixed,New Feature,Major,['framework-0.8.0'],,,,,,,,"['Event Admin', 'Specification compliance']","['The Event Admin from the OSGi R6 specification (Section 113).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']",See section 113 in the OSGi R4 Service Compendium for a full description of the task.,Implement Event Admin,,,,rickhall,True,karlpauls,rickhall
felix,FELIX-18,2005-10-08T22:23:13.000+0000,2006-04-21T23:46:18.000+0000,2008-05-30T18:15:55.000+0000,,Fixed,New Feature,Major,['scr-1.0.0'],,,,,,,,"['Declarative Services (SCR)', 'Specification compliance']","['The Declarative Services from the OSGi R7 specification (Section 112).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']",See section 112 in the OSGi R4 Service Compendium for a full description of the task.,Implement Declarative Services,,,,rickhall,True,,rickhall
felix,FELIX-17,2005-10-08T22:22:28.000+0000,,2006-03-29T07:35:32.000+0000,,,New Feature,Major,,,,,,,,,"['Specification compliance', 'UPnP Subproject']","['The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.', 'An effort for releasing the UPnP Device Service from the OSGi R4 specification (Section 111), tools, and examples using such specification.']","See section 111 in the OSGi R4 Service Compendium for a full description of the task. Since implementations of this service exist, perhaps a donation would be possible.",Implement UPnP Device Service,,,,rickhall,True,,rickhall
felix,FELIX-16,2005-10-08T22:21:54.000+0000,,2006-01-27T17:27:54.000+0000,,,New Feature,Major,,,,,,,,,"['Initial Provisioning', 'Specification compliance']","['The Initial Provisioning from the OSGi R4 specification (Section 110).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']",See section 110 in the OSGi R4 Service Compendium for a full description of the task.,Implement Initial Provisioning,,,,rickhall,True,,rickhall
felix,FELIX-15,2005-10-08T22:20:18.000+0000,,2005-10-08T22:20:43.000+0000,,,New Feature,Major,,,,,,,,,"['IO Connector Service', 'Specification compliance']","['The IO Connector Service from the OSGi R4 specification (Section 109).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']",See section 109 in the OSGi R4 Service Compendium for a full description of the task.,Implement IO Connector Service,,,,rickhall,True,,rickhall
felix,FELIX-14,2005-10-08T22:19:40.000+0000,2012-11-16T10:33:16.000+0000,2012-11-16T10:33:16.000+0000,,Fixed,New Feature,Major,,,,,,,,,"['Specification compliance', 'User Admin']","['The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.', 'The User Admin from the OSGi R4 specification (Section 107).']","See section 107 in the OSGi R4 Service Compendium for a full description of the task. Since implementations of this service exist, perhaps a donation would be possible.",Implement User Admin,1,,,rickhall,True,,rickhall
felix,FELIX-13,2005-10-08T22:14:46.000+0000,2006-05-25T22:38:28.000+0000,2006-07-04T22:25:46.000+0000,,Fixed,New Feature,Major,['framework-0.8.0'],,,,,,,,"['Specification compliance', 'Wire Admin']","['The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.', 'The Wire Admin from the OSGi R4 specification (Section 108).']","See section 108 in the OSGi R4 Service Compendium for a full description of the task. Since implementations of this service exist, perhaps a donation would be possible.",Implement Wire Admin,,,,rickhall,True,humberto,rickhall
felix,FELIX-12,2005-10-08T22:13:39.000+0000,2007-08-01T09:09:41.000+0000,2009-06-22T15:38:10.000+0000,,Fixed,New Feature,Major,['prefs-1.0.2'],,,,,,,,"['Preferences Service', 'Specification compliance']","['The Preferences Service from the OSGi R4 specification (Section 106).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']","See section 106 in the OSGi R4 Service Compendium for a full description of the task. Since implementations of this service exist, perhaps a donation would be possible.",Implement Preferences Service,,,,rickhall,True,cziegeler,rickhall
felix,FELIX-11,2005-10-08T22:11:39.000+0000,2007-04-11T18:15:56.000+0000,2007-04-11T18:16:37.000+0000,,Fixed,New Feature,Major,['framework-1.0.0'],,,,,,,,"['Configuration Admin', 'Specification compliance']","['The Configuration Admin from the OSGi R7 specification (Section 104).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']","See section 104 in the OSGi R4 Service Compendium for a full description of the task. Since implementations of this service exist, perhaps a donation would be possible.",Implement Configuration Admin,1,,,rickhall,True,fmeschbe,rickhall
felix,FELIX-10,2005-10-08T22:11:12.000+0000,2009-03-11T22:40:41.000+0000,2009-03-11T22:40:41.000+0000,,Fixed,New Feature,Major,,,,,,,,,"['Device Access', 'Specification compliance']","['The Device Access from the OSGi R4 specification (Section 103).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']","See section 103 in the OSGi R4 Service Compendium for a full description of the task. Since implementations of this service exist, perhaps a donation would be possible.",Implement Device Access,,,,rickhall,True,marrs,rickhall
felix,FELIX-9,2005-10-08T22:10:22.000+0000,2006-04-19T22:25:09.000+0000,2006-04-19T22:25:09.000+0000,,Fixed,New Feature,Major,,,,,,,,,"['HTTP Service', 'Specification compliance']","['The HTTP Service and the Http Whiteboard Service from the OSGi R7 specification (Section 102 and 140).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']","See section 102 in the OSGi R4 Service Compendium for a full description of the task. Since implementations of this service exist, perhaps a donation would be possible.",Implement HTTP Service,,,,rickhall,True,rickhall,rickhall
felix,FELIX-8,2005-10-08T22:09:23.000+0000,2008-02-15T16:53:37.000+0000,2008-02-15T16:53:37.000+0000,,Fixed,New Feature,Major,,,,,,,,,"['Log Service', 'Specification compliance']","['The Log Service from the OSGi R4 specification (Section 101).', 'The component is used to capture tasks that correspond to specification compliance issues. It does not refer to an actual logical component of the project. Tasks associated with this component will most likely also be assigned to an actual logical component.']","See section 101 in the OSGi R4 Service Compendium for a full description of the task. Since implementations of this service exist, perhaps a donation would be possible.",Implement Log Service,,,,rickhall,True,,rickhall
commons-fileupload,FILEUPLOAD-148,2007-10-01T20:54:36.000+0000,2007-10-02T11:02:01.000+0000,2007-10-02T11:23:10.000+0000,,Fixed,New Feature,Major,,['1.2'],,,,,,,,,"Need method

FileItemFactory.setMaxStringLength(int limitInBytes)

When this parameter is set, calling of FileItem.getString() when getSize() exceeds limitInBytes should throw Exception. This is required to avoid OOME in case of wrongly submitted forms (i. e. when bad guy puts big file into the form field ""fileDescription"").

Or even better sizeThreshold should be used for this value.",FileItemFactory.setMaxStringLength(),1,,,yozh,True,,yozh
commons-fileupload,FILEUPLOAD-119,2006-11-09T04:06:14.000+0000,2006-11-12T07:16:22.000+0000,2018-05-16T19:48:26.000+0000,,Won't Fix,New Feature,Critical,,,,,,,,,,,"Hi,
Can we upload the file in the encypted format and later decrypt it to show the file so that noone can directly open the file from the disk space (hard disk) where i have uploaded the file usinf fileUpload functionality. This is the major concern for our project ?

Is there any external library which can do this for us even just after we upload the file through this API ?

Thanks & Regards
Inderjeet",The encryption & decryption of uploaded file,3,,,ikalra,True,,ikalra
commons-fileupload,FILEUPLOAD-112,2006-05-22T19:55:27.000+0000,2006-08-04T21:24:46.000+0000,2007-03-09T20:32:11.000+0000,,Fixed,New Feature,Major,['1.2'],,,,,,,,,,"Currently parsing is all or nothing. This is unfortunate in a streaming environment. For example, if you *know* that the big parameter is the latest, then you might setup the environment, depending on other parameters, which have already been read, before reading the last parameter.
",Support for iterative parsing,1,,,jochen@apache.org,True,jochen@apache.org,jochen@apache.org
httpcomponents-client,HTTPCLIENT-1951,2018-11-26T20:04:40.000+0000,2018-11-27T01:06:35.000+0000,2018-11-27T01:06:35.000+0000,,Won't Fix,New Feature,Major,,['5.0'],,,,,,,['HttpClient (async)'],['HttpClient async (non-blocking) code'],"I would like to add support for SOCKS proxies to the async HttpClient.  SOCKS proxies are already supported for the classic client via the standard JVM support, however the JVM does not support SOCKS for nio Sockets and so the async HttpClient cannot currently use a SOCKS proxy.",SOCKS proxy support for async HttpClient,1,,,dmaplesden,True,,dmaplesden
httpcomponents-client,HTTPCLIENT-1937,2018-08-05T03:48:00.000+0000,2018-08-07T09:23:33.000+0000,2018-08-07T09:23:33.000+0000,,Fixed,New Feature,Major,,"['4.5.7', '5.0']",,,3600,3600,3600,,['HttpClient (async)'],['HttpClient async (non-blocking) code']," Please add OWASP Dependency Check to the build (pom.xml).  OWASP DC makes an outbound REST call to MITRE Common Vulnerabilities & Exposures (CVE) to perform a lookup for each dependant .jar to list any/all known vulnerabilities for each jar.  This step is needed because a manual MITRE CVE lookup/check on the main component does not include checking for vulnerabilities in components or in dependant libraries.

OWASP Dependency check : https://www.owasp.org/index.php/OWASP_Dependency_Check has plug-ins for most Java build/make types (ant, maven, ivy, gradle).   

Also, add the appropriate command to the nightly build to generate a report of all known vulnerabilities in any/all third party libraries/dependencies that get pulled in. example : mvn -Powasp -Dtest=false -DfailIfNoTests=false clean aggregate

Generating this report nightly/weekly will help inform the project's development team if any dependant libraries have a reported known vulnerailities.  Project teams that keep up with removing vulnerabilities on a weekly basis will help protect businesses that rely on these open source componets.",Please add OWASP Dependency Check to the build (pom.xml),1,,"['build', 'easy-fix', 'security']",ABakerIII,True,,ABakerIII
httpcomponents-client,HTTPCLIENT-1916,2018-04-20T02:58:10.000+0000,2019-02-27T18:28:40.000+0000,2019-02-27T18:28:40.000+0000,,Won't Fix,New Feature,Trivial,,"['4.5.5', '4.5.6', '5.0 Beta1', '5.0 Beta2']",,,1800,1800,1800,,['HttpClient (classic)'],['HttpClient classic (blocking) code'],"It would be convenient to have a *removeParameter* method added to class *URIBuilder*.

One rationale for this proposal is the use case when of reusing an *URIBuilder* to create multiple similar HMAC signed requests. In that scenario it is not sufficient to invoke *setParameter* with the values of the parameters that have changed. One also needs to remove the _signature_ parameter from the previous request before the current request can be signed and a new _signature_ parameter added to it.

This would be the implementation of the proposed method *removeParameter*:
{code:java}
    /**
     * Remove parameter of URI query if set. The parameter name is expected to be unescaped and 
     * may contain non ASCII characters.
     * <p>
     * Please note query parameters and custom query component are mutually exclusive. This method
     * will remove custom query if present.
     * </p>
     */
    public URIBuilder removeParameter(final String param) {
        if (this.queryParams == null) {
            return this;
        }
        if (!this.queryParams.isEmpty()) {
            for (final Iterator<NameValuePair> it = this.queryParams.iterator(); it.hasNext(); ) {
                final NameValuePair nvp = it.next();
                if (nvp.getName().equals(param)) {
                    it.remove();
                }
            }
        }
        this.encodedQuery = null;
        this.encodedSchemeSpecificPart = null;
        this.query = null;
        return this;
   }
{code}
Since the proposed implementation above is the current implementation of *setParameter* minus the one line that actually adds the new parameter, then the implementation of *setParameter* could be simplified greatly to:

 
{code:java}
    /**
     * Sets parameter of URI query overriding existing value if set. The parameter name and value
     * are expected to be unescaped and may contain non ASCII characters.
     * <p>
     * Please note query parameters and custom query component are mutually exclusive. This method
     * will remove custom query if present.
     * </p>
     */
    public URIBuilder setParameter(final String param, final String value) {
        removeParameter(param);
        if (this.queryParams == null) {
            this.queryParams = new ArrayList<NameValuePair>();
        }
        this.queryParams.add(new BasicNameValuePair(param, value));
        return this;
    }
{code}
 ",Add method removeParameter to URIBuilder,2,,"['features', 'stuck', 'volunteers-wanted']",jmoraleda,True,,jmoraleda
httpcomponents-client,HTTPCLIENT-1874,2017-10-07T23:49:20.000+0000,,2017-12-28T09:22:47.000+0000,,,New Feature,Major,['Stuck'],,,,,,,,,,"I believe it would be useful to have an Exponential Back-off algorithm implementation, as an HttpRequestRetryHandler or under a new interface, named HttpRequestRetryPolicy.

The implementation could provide both synchronous (similar to the existing retry mechanism) and asynchronous retries, using a provided ExecutorService/ScheduledExecutorService.

Any feedback would be much appreciated. If you agree with the value of such a feature, I could proceed to the implementation.",Exponential back-off retry mechanism,2,,"['stuck', 'volunteers-wanted']",sermojohn,True,,sermojohn
httpcomponents-client,HTTPCLIENT-1872,2017-09-26T19:14:21.000+0000,2017-10-08T11:03:41.000+0000,2017-10-08T11:03:41.000+0000,,Won't Fix,New Feature,Major,['5.0 Alpha3'],['4.5.3'],,,,,,,['HttpClient (classic)'],['HttpClient classic (blocking) code'],"The Cookie interface does not support the HttpOnly attribute as specified in RFC-6265 section 4.1.2.6 (""The HttpOnly Attribute""). Presently in a project where the HttpClient is being used to implement a reverse proxy, we are losing the HttpOnly attribute set from the back-end server. I'd be happy to create a patch for 4.5.4 if it helps
","Add HttpOnly boolean to Cookie, ClientCookie etc.",2,,,rsand,True,,rsand
httpcomponents-client,HTTPCLIENT-1852,2017-05-19T00:01:47.000+0000,2017-05-20T02:48:11.000+0000,2018-08-15T17:55:24.000+0000,,Fixed,New Feature,Major,,,,,,,,,['HttpClient (classic)'],['HttpClient classic (blocking) code'],Add APIs URIBuilder.localhost() and setHost(InetAddress),Add APIs URIBuilder.localhost() and setHost(InetAddress),2,,,ggregory,True,ggregory,ggregory
httpcomponents-client,HTTPCLIENT-1846,2017-05-08T17:17:10.000+0000,,2017-05-08T17:57:18.000+0000,,,New Feature,Major,,,,,,,,,,,"Get content decoders using the Java Service Loader mechanism.

Current decoders are hard-coded for gzip and deflate.

The goal is to allow the client to find other decoders through the Java Service Loader mechanism. 

- The client will find its own gzip and deflate decoders through this mechanism.
- The client will find the decoders supported by Apache Commons Compress if the upcoming {{httpclient5-compress}} module is present.",Get content decoders using the Java Service Loader mechanism,1,,,ggregory,True,ggregory,ggregory
httpcomponents-client,HTTPCLIENT-1843,2017-05-07T17:29:08.000+0000,,2017-05-08T17:17:40.000+0000,,,New Feature,Major,['Future'],,,,,,,,,,"Create module httpclient5-compress to use Apache Commons Compress.

Allow users to benefit from the support of various algorithms supported through the API of Apache Commons Compress.

[Apache Commons Compress|https://commons.apache.org/proper/commons-compress/] 1.14 will support Brotli when released.

HttpClient should be recoded to find codec implementaions based on a Java {{ServiceLoader}}. Then, {{httpclient5-compress}} can add to that registry if it is on the classpath, the user will not have to do anything extra.",Create module httpclient5-compress to use Apache Commons Compress,3,,,ggregory,True,,ggregory
httpcomponents-client,HTTPCLIENT-1817,2017-02-20T22:13:51.000+0000,2017-02-20T22:41:27.000+0000,2017-02-20T22:58:57.000+0000,,Fixed,New Feature,Major,"['4.5.4', '5.0 Alpha2']",,,,,,,,['HttpClient (classic)'],['HttpClient classic (blocking) code'],"Add a ""Trust All"" TrustStrategy implementation. Like this:

{code:java}
/*
 * ====================================================================
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * ""License""); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 * ====================================================================
 *
 * This software consists of voluntary contributions made by many
 * individuals on behalf of the Apache Software Foundation.  For more
 * information on the Apache Software Foundation, please see
 * <http://www.apache.org/>.
 *
 */
package org.apache.http.conn.ssl;

import java.security.cert.CertificateException;
import java.security.cert.X509Certificate;

/**
 * A trust strategy that accepts all certificates as trusted. Verification of
 * all other certificates is done by the trust manager configured in the SSL
 * context.
 *
 * @since 4.5.4
 */
public class TrustAllStrategy implements TrustStrategy {

	public static final TrustAllStrategy INSTANCE = new TrustAllStrategy();

	@Override
	public boolean isTrusted(final X509Certificate[] chain, final String authType) throws CertificateException {
		return true;
	}

}
{code}","Add a ""Trust All"" TrustStrategy implementation",1,,,ggregory,True,ggregory,ggregory
httpcomponents-client,HTTPCLIENT-1820,2016-09-20T15:56:10.000+0000,2017-05-02T13:33:01.000+0000,2017-05-02T13:33:01.000+0000,,Fixed,New Feature,Minor,['5.0 Alpha2'],,,,,,,,,,Currently in our application we are retrying requests manually in the 'failed' method within our HttpAsyncResponseConsumer. This generates some collateral problems (that have been solved) but it would be great to have a retry handler in the PoolingNHttpClientConnectionManager to avoid coding retries in callbacks.,Add support for retrying requests,2,,,joan.balaguero@ventusproxy.com,True,,joan.balaguero@ventusproxy.com
httpcomponents-client,HTTPCLIENT-1818,2016-08-29T09:43:46.000+0000,2019-02-27T18:29:40.000+0000,2019-02-27T18:29:40.000+0000,,Won't Fix,New Feature,Major,,,,,,,,,,,"I have a requirement to attach all logs with a transaction id. I set this ID in Log4j MDC, but I need a way to intercept the request in a way I can set this MDC key in the internal thread pools used by the async client.
The current interceptors are run before the IO thread pool starts to run,",Add support for interceptors to the internal Async IO Threas,1,2,"['stuck', 'volunteers-wanted']",yairogen,True,,yairogen
httpcomponents-client,HTTPCLIENT-1752,2016-06-22T12:30:25.000+0000,2016-08-10T13:52:50.000+0000,2017-05-17T13:19:35.000+0000,,Fixed,New Feature,Major,['5.0 Alpha2'],['4.5.2'],,,,,,,['HttpClient (classic)'],['HttpClient classic (blocking) code'],"In deployments other than production (e.g. dev, qa, integration testing, etc.) it is often useful to deploy self-signed certificates instead of certificates signed by a trusted CA for cost and simplicity reasons.

By default, the http client does not validate a self signed certificate because it is not signed by a trusted CA root. 

One way to have the http client to validate the self signed certificate is to add the self-signed certificate (or the detached CA root that signed it) in the java trustore.
This operation is a configuration only change (no need to change code) however it typically requires accessing the FS and the scope of trust can't be easily modified at runtime.

Another way to have the http client to validate the self signed certificate is to use the TrustSelfSignedStrategy [0] strategy when building the http client.
This requires modifying the code.

In order to use the second approach without modifying code, it would be interesting to allow configuring a set of URIs for which the relaxed SSL mode should be used.

The configuration could be implemented similarly to the implementation of the central prox configuration (OSGI) in HTTPCLIENT-1238. In addition to allowing sel-signed certificates, the configuration could as well allow to skip FQDN check using the NoopHostnameVerifier [1].
Of course, this feature *must not* be deployed in production environment as it is totally insecure.

[0] https://hc.apache.org/httpcomponents-client-ga/httpclient/apidocs/org/apache/http/conn/ssl/TrustSelfSignedStrategy.html
[1] https://hc.apache.org/httpcomponents-client-ga/httpclient/apidocs/org/apache/http/conn/ssl/NoopHostnameVerifier.html",Allow to configure the OSGI clients with relaxed SSL checks,4,,,marett,True,simone.tripodi,marett
httpcomponents-client,HTTPCLIENT-1750,2016-06-08T19:35:07.000+0000,2016-06-11T15:56:47.000+0000,2016-06-11T15:56:47.000+0000,,Fixed,New Feature,Major,"['4.5.3', '5.0 Alpha2']",,,,,,,,['HttpClient (classic)'],['HttpClient classic (blocking) code'],"It is fairly difficult (maybe impossible) to use the CachingHttpClientBuilder effectively with the OSGi proxy support originally added in HTTPCLIENT-1238.  In order to support this, a corresponding CachingHttpClientBuilderFactory interface should be created with an implementation.",Add support in OSGi support for CachingHttpClientBuilder,2,,,justinedelson,True,,justinedelson
httpcomponents-client,HTTPCLIENT-1714,2016-01-26T05:10:58.000+0000,2016-01-26T05:23:58.000+0000,2017-05-04T09:49:52.000+0000,,Fixed,New Feature,Major,"['4.5.2', '5.0 Alpha2']",,,,,,,,['HttpClient (classic)'],['HttpClient classic (blocking) code'],"Add HttpClientBuilder.setDnsResolver(DnsResolver).

See https://github.com/apache/httpclient/pull/42/files",Add HttpClientBuilder.setDnsResolver(DnsResolver),2,,,ggregory,True,ggregory,ggregory
httpcomponents-client,HTTPCLIENT-1592,2014-12-30T01:34:05.000+0000,2014-12-30T09:57:35.000+0000,2017-05-04T09:49:08.000+0000,,Not A Problem,New Feature,Minor,,"['4.3.5', '4.3.6', '4.4 Beta1']",,,,,,,['HttpClient (classic)'],['HttpClient classic (blocking) code'],"It looks like 4.3.x versions of HttpClient no longer supports the configuration of the time to wait for a 100 Continue response. Is there any particular reason why this was removed and can it be added back?

Thanks,
Kevin",Add ability to configure the wait time for 100 Continue,1,,,kfongson,True,,kfongson
httpcomponents-client,HTTPCLIENT-1505,2014-05-08T14:03:54.000+0000,2014-05-12T11:33:10.000+0000,2017-05-04T09:48:55.000+0000,,Won't Fix,New Feature,Major,,['4.3.3'],,,,,,,['HttpClient (classic)'],['HttpClient classic (blocking) code'],"Currently, HttpMime allows to build multipart entities but one cannot parse them from a response. One has to rely on a third-party lib. It would be handy if such components would be avaiable in this package.",Add a MultipartEntityReader,1,,,michael-o,True,,michael-o
httpcomponents-client,HTTPCLIENT-1493,2014-03-31T21:36:26.000+0000,2014-06-23T12:22:53.000+0000,2017-05-04T09:49:03.000+0000,,Fixed,New Feature,Major,['4.4 Alpha1'],,,,,,,,,,"Stale connection checking is expensive. At present if it is enabled, it is performed for every connection. This is quite expensive and generates additional requests.

One reason for a server to drop a connection is when the connection has been idle for a while. Some servers send Keep-Alive headers to inform the client of this; if they don't, then the client will see an unexpected disconnection. Stale connection checking avoids this problem, but at the cost of additional requests. When a connection is not busy, this is acceptable, but for a busy connection the checks are mostly unnecessary.

Another reason for dropping a connection is after it has been reused many times. The server should send a connection close as part of the final response, but if it does not, a stale check would help.

For these reasons, it would be useful to be able to implement conditional stale connection checking, based on connection idle time and/or connection reuse count. These conditions should be optionally selectable.",Conditional stale connection checking,,,,sebb,True,,sebb
httpcomponents-client,HTTPCLIENT-1480,2014-03-12T20:21:34.000+0000,2014-06-23T12:23:42.000+0000,2017-05-04T09:49:55.000+0000,,Fixed,New Feature,Minor,['4.4 Alpha1'],['4.3.3'],,,,,,,['Fluent HC'],['HttpClient Fluent API'],"This issue relates to HTTPCORE-365 (which was the original issue about this topic). What I actually want to achieve is to have a convenience method to allow us to call the viaProxy() method with a String as parameter. E.g.:
Request.Get(uri).viaProxy(""localhost:8080"").execute();

Notes about the attached patch:
- I did not know where to put the new method ""protected static HttpHost parseProxy(String proxy)"" so I put it inside Request.java
- the parseProxy(String) is static because it is a utility class, so you can move to a more ""appropriate place"" and anyone can use it for other use-cases.
- following the suggestion of Gary Gregory, I made it compatible with IPv6 (following links below):
-- http://en.wikipedia.org/wiki/IPv6_address#Literal_IPv6_addresses_in_network_resource_identifiers
-- http://stackoverflow.com/questions/186829/ipv6-and-ports
-- http://serverfault.com/questions/205793/how-can-one-distinguish-the-host-and-the-port-in-an-ipv6-url

Hope to see it in 4.3.4 :-).",Provide a convenience method to parse a proxy with format localhost:port,1,,,remisbaima,True,,remisbaima
httpcomponents-client,HTTPCLIENT-1475,2014-03-05T12:53:14.000+0000,2014-03-05T14:46:35.000+0000,2014-03-05T14:48:00.000+0000,,Not A Problem,New Feature,Minor,,['4.3.3'],,,,,,,['HttpClient (classic)'],['HttpClient classic (blocking) code'],"Hi,

Coding a use case involving massive querying of a web API, but having requests count restrictions, I'm trying to count all requests sent by HttpClient, including configured default retries and HTTP501-specific retries.

The restriction is an access key, provided as a GET parameter, and being restricted to n daily usages, being revoked by the service maintainer if this rule is not respected (this is about the Steam gaming platform web API, nothing warez).

In a nutshell, the needed feature is an overall requests-sent counter for each processed HttpGet.

I found no way of doing this, and the source of 'org.apache.http.impl.execchain.RetryExec' (keyword: 'execCount') let me think this is not possible to do so. 'execCount' is a local variable not transmitted to other objects, so it doesn't seem like the info is provided in some other convenience stat object of the API. This counter could be in the deeps of 'requestExecutor.execute(route, request, context, execAware)' by the way.

Also, I thought of extending the retry-related handlers, to count manually in some overriden 'retryRequest(ex, execCount, context)' methods. But the 'request' is not available in here, making it impossible to know what HttpGet/URL one is counting for.

Issue is best described on Stackoverflow, also mentionning the ' https://issues.apache.org/jira/browse/HTTPCORE-21 ', which may be related (a ' git diff --name-only 4.0-alpha4 4.0-beta1 ' was too big, so I could not find any relevant usage tip. '4.0-alpha6' was not tagged):
http://stackoverflow.com/questions/22195471/how-to-count-all-http-requests-sent-retries-in

It may be a my-fault ""RTFM"" or an API documentation issue also. I really hope a feature or workaround exists to achieve so :)

Thanks in advance
",Add a requests count readable stat for each HttpGet to be processed,1,,"['HttpGet', 'count', 'retry']",pascalav,True,,pascalav
httpcomponents-client,HTTPCLIENT-1469,2014-02-21T23:56:02.000+0000,,2014-06-23T07:34:27.000+0000,,,New Feature,Minor,['5.0'],['4.3.2'],,,,,,,['HttpCache'],['HttpClient Caching components'],"The draft of the revised HTTP/1.1 (should be RFC'd shortly) says other response header can be included in 304 responses if they exists for the purpose of guiding cache updates.

http://tools.ietf.org/html/draft-ietf-httpbis-p4-conditional-26#section-4.1

   The server generating a 304 response MUST generate any of the
   following header fields that would have been sent in a 200 (OK)
   response to the same request: Cache-Control, Content-Location, Date,
   ETag, Expires, and Vary.

   Since the goal of a 304 response is to minimize information transfer
   when the recipient already has one or more cached representations, a
   sender SHOULD NOT generate representation metadata other than the
   above listed fields unless said metadata exists for the purpose of
   guiding cache updates (e.g., Last-Modified might be useful if the
   response does not have an ETag field).

This issue is to make it possible to include Last-Modified in a cached 304 response.",Option to Include Last-Modified in 304 Not Modified Response,1,,['patch'],jamesrdf,True,,jamesrdf
httpcomponents-client,HTTPCLIENT-1404,2013-09-26T18:41:38.000+0000,2017-05-02T13:34:35.000+0000,2017-05-02T13:34:51.000+0000,,Won't Fix,New Feature,Minor,,,,,,,,,['HttpClient (classic)'],['HttpClient classic (blocking) code'],"For clients that have high request rate to their corresponding services, it is critical that the client has an efficient connection pool and reuse free connections as often as possible, and is free from bugs like not releasing resources from http response that causes low reuse of connections and timed out waiting for connection. To meet that requirement, the developers need to have insight of current state of the connection pool.

Connection pool, like any object pool or cache, needs to have statistics for fine tuning, like Guava CacheBuilder that provides statics for cache hit ratio and many others.

It will be very beneficial if the PoolingClientConnectionManager provides following statistics:

- count of occurrences that a free entry is obtained when requesting a connection 
- count of occurrences that a new entry is created
- count of occurrences that an entry is released
- count of occurrences that an entry is deleted

I was able to provide such statistics for the deprecated ThreadSafeClientConnManager in Ribbon:

https://github.com/Netflix/ribbon/pull/47

But it seems that PoolingClientConnectionManager does not provide such hooks.

",Provide connection pool usage statistics for PoolingClientConnectionManager,1,,,allenxwang,True,,allenxwang
httpcomponents-client,HTTPCLIENT-1401,2013-09-20T22:41:50.000+0000,2013-09-25T07:46:19.000+0000,2017-05-04T09:49:46.000+0000,,Not A Problem,New Feature,Major,,['4.3 Beta2'],,,,,,,['Documentation'],['HttpClient Documentation'],"The RequestConfig.Builder provides no hints as to whether, for example, setConnectionRequestTimeout(int) is expecting seconds or milliseconds (and confusion seems likely given that milliseconds are usually stored in a long variable). Other methods on the config builders seem similarly ambiguous. A simple Javadoc comment -- or at least a link to documentation/tutorial pages explaining use of the Builder -- would be helpful to avoid having to either Google for the answer or set up time-consuming experiments.",Add Javadoc or docs link to RequestConfig.Builder,2,,,abissell,True,,abissell
httpcomponents-client,HTTPCLIENT-1394,2013-08-27T21:27:54.000+0000,2014-01-24T17:27:32.000+0000,2017-05-04T09:49:53.000+0000,,Fixed,New Feature,Major,['4.4 Alpha1'],,,,,,,,,,"HttpClient has made great progress in supporting NTLM and Negotiate, but the setup often requires considerable configuration.  Using native calls, we can authenticate the current logged in user directly via windows with no configuration.",Native windows Negotiate/NTLM via JNA,1,,,ryantxu,True,,ryantxu
httpcomponents-client,HTTPCLIENT-1261,2012-11-15T14:33:47.000+0000,2012-12-11T13:35:53.000+0000,2013-10-05T19:41:07.000+0000,,Fixed,New Feature,Major,['4.3 Alpha1'],,,,,,,,['HttpClient (classic)'],['HttpClient classic (blocking) code'],"http.agent can be set as a system property to override a user agent string but is not currently noticed by SystemDefaultHttpClient, it just continues to use its default.

The native java http implementation (URLConnection) does honor this system property.

Can SystemDefaultHttpClient be enhanced to honor this?

Thanks.",Make SystemDefaultHttpClient honor http.agent system property,2,,,mboyers,True,,mboyers
httpcomponents-client,HTTPCLIENT-1246,2012-10-06T17:10:47.000+0000,2015-07-27T16:19:05.000+0000,2018-02-19T22:00:27.000+0000,,Won't Fix,New Feature,Major,,,,,,,,,['HttpClient (classic)'],['HttpClient classic (blocking) code'],"Oleg asked me to submit this patch via JIRA.  This is an initial version of code to enable JMX for requests/timeouts/etc.

I've uploaded the patch as an attachment.

Here are my comments about what I have and have not done.  This is from an email earlier in the week:

What I have at this point is a version of a JmxEnabledHttpClient that implements the HttpClient interface.  You can instantiate a JmxEnabledHttpClient given an HttpClient.  I have decoupled it from the work I had done previously (Spring-dependent), so it works without any additional dependencies.  I probably just need a couple more hours to clean this area up.  I did work on it a bit yesterday.

But I have not made progress in integrating the stats more closely with the HttpClient codebase.  So it doesn't currently have visibility into some of the underlying behavior of HttpClient and some of the mechanisms for retaining and managing statistics could probably be done more cheaply if they were integrated more closely.  This is the area I just don't think I'm going to be able to find the time to do, sadly.

So what I'm thinking at this point: I'll hand off what I've done (which is functional) and you guys can decide on whether or not to use it as the basis for anything going forward.  Even if you decide not to ultimately check it in with the rest of the HttpClient code, it could still be useful to you in the short-term, as it could easily be turned into its own jar and used with the existing HttpClient.  I'm pretty sure it records everything we talked about previous except the current pool size.",Patch for JmxEnabledHttpClient,6,3,,mboyers,True,,mboyers
httpcomponents-client,HTTPCLIENT-1239,2012-10-03T10:06:01.000+0000,2012-10-03T15:29:34.000+0000,2013-10-05T19:41:52.000+0000,,Fixed,New Feature,Major,['4.2.2'],['4.2.1'],,,,,,,['HttpClient (classic)'],['HttpClient classic (blocking) code'],The org.apache.http.localserver.LocalTestServer uses a ListenerThread that standard (not configurable / overridable) instantiates a DefaultHttpServerConnection. We have the need to support more then the set of supported methods (RFC2616 does not include the PATCH method) and therefore we would like to override createHttpRequestFactory defined in the AbstractHttpServerConnection (as this is already supported). ,Support for overridable DefaultHttpServerConnection in LocalTestServer,2,,,oharsta,True,,oharsta
httpcomponents-client,HTTPCLIENT-1819,2012-08-14T18:09:24.000+0000,2017-05-02T13:50:16.000+0000,2017-05-02T13:50:16.000+0000,,Duplicate,New Feature,Major,,,,,,,,,,,"org.apache.http.impl.client.DecompressingHttpClient should have an nio counterpart: DecompressingHttpAsyncClient. 

Since I needed the functionality for my own project, I went ahead and made the appropriate changes to DecompressingHttpClient to allow it to decorate an HttpAsyncClient. 
I'm attaching the files here, for consideration for a future release.",org.apache.http.impl.client.DecompressingHttpClient should have an nio counterpart: DecompressingHttpAsyncClient,3,1,,tinclon,True,,tinclon
httpcomponents-client,HTTPCLIENT-1222,2012-08-08T08:28:17.000+0000,2012-10-06T21:16:16.000+0000,2013-10-05T19:42:25.000+0000,,Duplicate,New Feature,Major,,['4.2.1'],,,1209600,1209600,1209600,,['HttpClient (classic)'],['HttpClient classic (blocking) code'],"Implement HttpClient which will monitor metrics via JMX

Register MBean in constructor of JmxMonitoredHttpClient

Implement - HttpRequestInterceptor, HttpResponseInterceptor (Also allow custom interceptors and custom MBean)

Implement execute method and update below metrics for request.

What metrics to measure ?
TotalBytes read
Total number of requests
Total time
Maximum request time
Minimum request time
Total failures 


",Monitoring httpclient metrics via JMX,2,,['patch'],jaikit,True,,jaikit
httpcomponents-client,HTTPCLIENT-1191,2012-05-15T19:38:15.000+0000,2012-05-16T12:21:54.000+0000,2013-10-05T19:41:41.000+0000,,Fixed,New Feature,Major,['4.2 Final'],['4.1.3'],,,,,,,['HttpClient (classic)'],['HttpClient classic (blocking) code'],"RFC 5789 defines the PATCH method for HTTP for partial updates, http://tools.ietf.org/html/rfc5789. Although not very commonly used yet it's definitely present in some APIs, for example GitHub's API v3 http://developer.github.com/v3/#http-verbs or YouTube's API v2 https://developers.google.com/youtube/2.0/developers_guide_protocol_partial_updates. ",Add support for the PATCH method for HTTP,4,1,,rstoya05,True,,rstoya05
httpcomponents-client,HTTPCLIENT-1182,2012-04-09T20:34:29.000+0000,2012-04-10T19:24:28.000+0000,2013-10-05T19:41:50.000+0000,,Fixed,New Feature,Major,['4.2 Final'],"['4.1.3', '4.2 Beta1']",,,,,,,['HttpClient (classic)'],['HttpClient classic (blocking) code'],"Our application use Java Webstart for deployment.  Amoung other things, Webstart gives us the ability to access the system's (in our case, Windows) certificate system.  For instance, one of our client is using certificate based authentication to their webserver.  This is done through a hardware device they attach to their system.  Window's already has a way to interface with this device, and Webstart has a way to interface with the Windows API.

I don't think we can get by with using any SocketFactory that we create.  (We would have to check with Oracle to be sure.)  I think we need to use the one that is set as the default in HttpsURLConnection.

What I am suggesting is that another constructor be added to allow for just wrapping this one.  I was not planning on putting a dependancy on HttpsURLConnection, but rather just add the ability to wrap any javax.net.ssl.SSLSocketFactory.

This will not be a big change to the API.  I will get a patch ready soon.",Add a constructor to org.apache.http.conn.ssl.SSLSocketFactory to allow for directly wrapping a javax.net.ssl.SSLSocketFactory socketfactory,,,"['SSLSocketFactory', 'Webstart', 'patch']",mmclaassen,True,,mmclaassen
httpcomponents-client,HTTPCLIENT-1132,2011-10-03T20:06:50.000+0000,2011-10-06T20:19:35.000+0000,2013-10-05T19:42:17.000+0000,,Fixed,New Feature,Major,['4.2 Alpha1'],,,,,,,,,,"With 3.1 version of HttpClient it was possible to establish a tunneled connection to a generic non http server through an authenticated proxy, but since 3.1 version does not support NTLMv2 and Kerberos authentication, that are supported in 4.x version, it is very useful to port the features provided by ProxyClient to 4.x API.",porting of ProxyClient from 3.1 to 4.x API,,,,dindondero0,True,,dindondero0
httpcomponents-client,HTTPCLIENT-1123,2011-09-02T19:13:13.000+0000,2011-10-10T20:19:05.000+0000,2013-10-05T19:42:29.000+0000,,Fixed,New Feature,Minor,['4.2 Alpha1'],,,,,,,,['HttpClient (classic)'],['HttpClient classic (blocking) code'],"When working with HttpClient in restrictive environments, where the user doesn't have the permissisions to edit the local /etc/hosts file or the DNS configuration, can be eased with an DNS Overrider capability. 

This can be useful with JMeter which can follow redirects automatically and resolve some of the redirected hosts against its configuration. Another example is a custom forward proxy, written in Java and based on httpclient, which can be deployed is such a restricted environment that would ease the development of various web solutions for some developers. ",Implement a way to override or resolve DNS entries defined in the OS,,,,avasile,True,,avasile
httpcomponents-client,HTTPCLIENT-1109,2011-07-19T20:33:23.000+0000,2017-05-02T13:25:51.000+0000,2017-05-02T13:25:51.000+0000,,Fixed,New Feature,Major,['5.0 Alpha2'],['4.1.1'],,,,,,,['HttpClient (classic)'],['HttpClient classic (blocking) code'],"c.f. HTTPCLIENT-1108

A suggestion was made for a different connection re-use approach - one based on https://bugzilla.mozilla.org/show_bug.cgi?id=624739

I was asked to raise a new issue to cover the possibility of having different connection re-use implementations available.",Parameterise connection reuse strategy,1,,,jabley,True,,jabley
httpcomponents-client,HTTPCLIENT-1101,2011-06-15T16:48:32.000+0000,2017-05-02T13:17:09.000+0000,2017-10-08T09:41:16.000+0000,,Fixed,New Feature,Major,['4.5.4'],,,,,,,,['HttpClient (classic)'],['HttpClient classic (blocking) code'],"I'm currently working on a patch (wrote most of it on a cross-country flight) that will adapt the size of a per-route connection pool based on the interactions we see from that particular host. There's a sample implementation that does TCP-style additive increase/multiplicative decrease (AIMD) adaptation of the per-route pool where successful requests allow probing for more connections, but socket timeouts, connection timeouts, and 503s all result in backoffs.

I'm hoping to hook this up for a demo to show multiple clients hitting a server with a fixed capacity where we can kill one client and the others then increase their pool sizes to take advantage of the unused server capacity. We can then restart the client and see things rebalance again. This would enable folks to use HttpClient e.g. in an application server cluster setting, where we wouldn't have to precompute or adjust the connection pool sizes as we add/remove nodes from the cluster (whether intentionally or via failures).

Once I get that proof of concept working I'll post a patch for review. Roughly the patch hooks into AbstractHttpClient to look either for an HttpResponse or to catch an Exception, then hands those events off to another object to decide whether to backoff or not. In turn, we dynamically manage a ConnPerRouteBean to adjust the maxPerRoute to allow for the pool to grow or shrink naturally with TSSCM. Default implementations are all backwards compatible and don't change behavior.

Thoughts?",adaptive connection pool sizing,1,,,jonm,True,jonm,jonm
httpcomponents-client,HTTPCLIENT-1076,2011-04-06T18:36:04.000+0000,2011-08-22T19:44:06.000+0000,2013-10-05T19:41:49.000+0000,,Fixed,New Feature,Major,['4.2 Alpha1'],,,,,,,,,,"Develop fluent API / facade to HttpClient based on code currently maintained by Apache Stanbol and Apache Sling projects. 

For details see 

http://markmail.org/message/mmyljtgjp3za6kyz

or contact Apache HttpComponents committers at dev@hc.apache.org",[GSoC 2011] Fluent API to HttpClient,1,1,"['gsoc2011', 'mentor']",olegk,True,,olegk
httpcomponents-client,HTTPCLIENT-1074,2011-03-26T20:13:14.000+0000,,2019-05-22T03:53:09.000+0000,,,New Feature,Minor,['Future'],['4.0.3'],,,,,,,,,"HttpClient has connect timeout and read timeout. I also need a ""query timeout"" parameter.

Have a situation:

We are some server ""B"". Client ""A"" calls us with timeout 10s. To serve requests we call some internal HTTP service ""C"":

A -> B -> C

HttpClient is used to call C from B.

Our call to that service ""C"" should be completed within 9s or should be interrupted, because:

* client ""A"" should get meaningful error message instead of just socket timeout
* we should not consume server resources (threads, connection) after client ""A"" stopped a request

This behavior cannot be implemented with just connect timeout + socket timeout. For example, if socket timeout is 5 seconds, and client sends 1 byte portions, socket timeout won't happen for a long time.

So I'm requesting ""query timeout"" parameter. It's value is a system time (milliseconds since epoch). HttpClient should execute request with respect to connect timeout and socket timeout as usual, but if ""query timeout"" time has come, HttpClient throws an exception.

AFAIU, this behavior can be now achieved by using a extra thread and abort() method.",Query timeout parameter / request deadline,2,2,,yozh,True,,yozh
httpcomponents-client,HTTPCLIENT-1059,2011-02-18T14:38:32.000+0000,2011-03-14T19:03:22.000+0000,2013-10-05T19:42:17.000+0000,,Won't Fix,New Feature,Major,,"['3.1 (end of life)', '4.0.3', '4.1 Final']",,,86400,86400,86400,,['HttpClient (classic)'],['HttpClient classic (blocking) code'],"I have been using HttpClient for a while now with success.  However, I have recently encountering some problems which I think have an easy solution, although it is not accessible to me through the current API.

Situation (my test enviroment):

ServerA:
* Runs Tomcat 6
* Contains jar files that are downloaded via webstart
* Only accepts non-SSL connections

ServerB:
* Runs Tomcat 6
* Contains jar files that are downloaded via webstart
* Contains a servlet
* Only accepts SSL connections

If I connect to serverB (via webstart) to download the jars, my app connects to the servlet on ServerB and everything is fine.
However, if I connect to ServerA (via webstart) and then the app tries to connect to ServerB, the SSL connection cannot be established.

I believe this has to do with the underlying socket factory that is being used.  This is crucial, since I need to use the default socket factory managed by webstart that automatically prompts for certificate issues and allows for automatic use of client certificates.

Theory
----------------
Terminology:
javax.SSLSocketFactory = javax.net.ssl.SSLSocketFactory 
apache.SSLSocketFactory = org.apache.http.conn.ssl.SSLSocketFactory

I am not exactly sure what is happening, but I have a theory.  In my theory, the crux of the problem is that the DEFAULT apache.SSLSocketFactory is set in the static initializer of the apache.SSLSocketFactory class.  In the first scenario above, webstart creates a new javax.SSLSocketFactory for the jar download, and makes this the default.  This happens before the apache.SSLSocketFactory class is loaded, and, therefore, before the static initializer is executed.  In the second scenario, this happens in the other order and the javax.SSLSocketFactory wrapped by the apache.SSLSocketFactory is not the one used by webstart

I need to ensure that the javax.SSLSocketFactory is initialized (and replaced) first, and then create an apache.SSLSocketFactory to wrap this.  The apache.SSLSocketFactory does have a wrapping construtor, but it is the private no-argument constructor.  If this were public in some manner, I could create my apache.SSLSocketFactory after I was sure that the HttpsURLConnection default socket factory was the one I needed.

Theory Corroborated
--------------------
I downloaded the httpclient 4.01 source (the version we are currently using), changed the no-arg constructor to be public, and used it to create a new apache.SSLSocketFactory after I initialized the HttpsURLConnection.  This did indeed solve the problem. Perhaps there is more going on here than I realize, but it was not complicated and it worked.

Another far easier work-around would be to do something like this:

Constructor<SSLSocketFactory> con = SSLSocketFactory.class.getConstructor((Class<?>[]) null);
con.setAccessible(true);
fact = con.newInstance((Object[])null);

RFE
-------------

Make the no-arg constructor public or provide access to it though another method

I don't really see a down-side to making the no-arg constructor public.  However, if this needs to be private to prevent certain types of subclassing, a static method could be added to create an apache.SSLSocketFactory that wraps the default javax.SSLSocketFactory.  If the constructor is private just for historical reasons, than it may be easier to just make it public.

If making the constructor public is undesirable for some reason, here is a recommendation for a change with minimal impact on the API and existing clients:

Add method createSocketFactory(), as:

	public static SSLSocketFactory createSocketFactory() {
		return new SSLSocketFactory();
	}

This will:
1) Solve the problem
2) Preserve backwards compatibility
3) Encourage the new semantics
4) Ensure that any protections afforded by the private SSLSocketFactory constructor are still in place

In any case, after this change I think it is a good idea to deprecate the getSocketFactory() method.  I would suspect that this method is not accessed frequently, so there is no real reason to optimize it in this way.  I am also not sure if encouraging the use of a mutable* global shared socket factory is a good idea.  If people want to use a particular socket factory, they should just create a new one, configure it how they want, and then keep it around to reuse as appropriate...without being in danger of another part of the application configuring it differently.

* It seems to be mutable only through a single deprecated method, however.",Allow for the on-demand creation of a new SSLSocketFactory that wraps the HttpsURLConnection.getDefaultSSLSocketFactory(),,,"['HttpsURLConnection', 'SSLSocketFactory', 'ssl', 'webstart']",mmclaassen,True,,mmclaassen
httpcomponents-client,HTTPCLIENT-1030,2010-12-03T21:24:44.000+0000,2010-12-04T21:18:22.000+0000,2011-01-25T11:00:36.000+0000,,Fixed,New Feature,Major,,['4.1 Beta1'],,,,,,,['HttpClient (classic)'],['HttpClient classic (blocking) code'],"It would be useful to Implement an ""ignoreCookies"" CookieSpec, as was done in Commons HC 3.1

This should be registered by DefaultHttpClient.createCookieSpecRegistry().

Patch to follow.","Implement ""ignoreCookies"" CookieSpec",,,,sebb,True,,sebb
httpcomponents-client,HTTPCLIENT-1025,2010-11-30T15:13:58.000+0000,2010-11-30T16:41:17.000+0000,2016-09-20T19:31:43.000+0000,,Not A Problem,New Feature,Major,,,,,,,,,['HttpClient (classic)'],['HttpClient classic (blocking) code'],"Hi,
I have a service that gives xml data. But it didn't connect, I reseach all of documents on internet  and you site, I develop my code on you site example
I give the example of my code :

protected InputStream getInputStream() {
		try {
			SchemeRegistry schemeRegistry = new SchemeRegistry();

    		// http scheme
    		schemeRegistry.register(new Scheme(""http"", PlainSocketFactory.getSocketFactory(), 80));
    		// https scheme
    		schemeRegistry.register(new Scheme(""https"", new EasySSLSocketFactory(), 443));

    		params = new BasicHttpParams();
    		params.setParameter(ConnManagerPNames.MAX_TOTAL_CONNECTIONS, 1);
    		params.setParameter(ConnManagerPNames.MAX_CONNECTIONS_PER_ROUTE, new ConnPerRouteBean(1));
    		params.setParameter(HttpProtocolParams.USE_EXPECT_CONTINUE, false);
    		HttpProtocolParams.setVersion(params, HttpVersion.HTTP_1_1);
    		HttpProtocolParams.setContentCharset(params, ""utf8"");

    		// ignore that the ssl cert is self signed
    		CredentialsProvider credentialsProvider = new BasicCredentialsProvider();
    		credentialsProvider.setCredentials(new AuthScope(""www.yemeksepeti.com"", AuthScope.ANY_PORT),
    				new UsernamePasswordCredentials(""cs_basari"", ""bas0102ari""));
    		clientConnectionManager = new ThreadSafeClientConnManager(params, schemeRegistry);

    		context = new BasicHttpContext();
    		context.setAttribute(""http.auth.credentials-provider"", credentialsProvider);
    		 System.out.println(""----------------yemeksepetiandroid-----------------------http://www.yemeksepeti.com/YemeksepetiCatalogWebService/CatalogExportMobile.asmx/Mobile_GetCities?"");
             
    		DefaultHttpClient client = new DefaultHttpClient(clientConnectionManager, params);
    		HttpGet get = new HttpGet(""http://www.yemeksepeti.com/YemeksepetiCatalogWebService/CatalogExportMobile.asmx/Mobile_GetCities?"");
    		HttpResponse response = client.execute(get, context);
    	    HttpEntity entity = response.getEntity();
            System.out.println(response.getStatusLine());
            if (entity != null) {
                System.out.println(""Response content length: "" + entity.getContentLength());
            }
           return entity.getContent();
		} catch (IOException e) {
			throw new RuntimeException(e);
		}
	}
	static class PreemptiveAuth implements HttpRequestInterceptor {

        public void process(
                final HttpRequest request, 
                final HttpContext context) throws HttpException, IOException {
            
            AuthState authState = (AuthState) context.getAttribute(
                    ClientContext.TARGET_AUTH_STATE);
            
            // If no auth scheme avaialble yet, try to initialize it preemptively
            if (authState.getAuthScheme() == null) {
                AuthScheme authScheme = (AuthScheme) context.getAttribute(
                        ""preemptive-auth"");
                CredentialsProvider credsProvider = (CredentialsProvider) context.getAttribute(
                        ClientContext.CREDS_PROVIDER);
                HttpHost targetHost = (HttpHost) context.getAttribute(
                        ExecutionContext.HTTP_TARGET_HOST);
                if (authScheme != null) {
                    Credentials creds = credsProvider.getCredentials(
                            new AuthScope(
                                    targetHost.getHostName(), 
                                    targetHost.getPort()));
                    if (creds == null) {
                        throw new HttpException(""No credentials for preemptive authentication"");
                    }
                    authState.setAuthScheme(authScheme);
                    authState.setCredentials(creds);
                }
            }
            
        }


    }



And it gives these error log:

11-30 16:41:08.902: WARN/etAuthenticationHandler(629): Authentication scheme ntlm not supported
11-30 16:41:08.932: WARN/DefaultRequestDirector(629): Authentication error: Unable to respond to any of these challenges: {ntlm=WWW-Authenticate: NTLM, negotiate=WWW-Authenticate: Negotiate}
11-30 16:41:08.952: INFO/System.out(629): HTTP/1.1 401 Unauthorized
11-30 16:41:08.952: INFO/System.out(629): Response content length: 1656
11-30 16:42:56.021: WARN/jdwp(629): Debugger is telling the VM to exit with code=1
11-30 16:42:56.021: INFO/dalvikvm(629): GC lifetime allocation: 2845 bytes
11-30 16:42:56.342: DEBUG/Zygote(33): Process 629 exited cleanly (1)
11-30 16:42:56.382: INFO/ActivityManager(69): Process com.basarimobile.android.yemeksepeti (pid 629) has died.
11-30 16:42:56.442: INFO/WindowManager(69): WIN DEATH: Window{4511be98 com.basarimobile.android.yemeksepeti/com.basarimobile.android.yemeksepeti.Login paused=false}
11-30 16:42:56.602: INFO/UsageStats(69): Unexpected resume of com.android.launcher while already resumed in com.basarimobile.android.yemeksepeti
11-30 16:42:56.692: WARN/InputManagerService(69): Got RemoteException sending setActive(false) notification to pid 629 uid 10036

        

How I can solve my problem thanks?",Can't connect HttpUrl wiht Authentication,,,,filizgokce,True,,filizgokce
httpcomponents-client,HTTPCLIENT-1019,2010-10-26T22:00:09.000+0000,2010-10-27T15:44:32.000+0000,2013-10-05T19:42:04.000+0000,,Won't Fix,New Feature,Major,,"['4.0.1', '4.0.2', '4.0.3']",,,,,,,['HttpClient (classic)'],['HttpClient classic (blocking) code'],"If you are trying to run the httpclient-osgi version 4.0.1, there is little opportunity to make it work without hand rolling your own apache commons logging.  Using the Maven descriptor:
        <dependency>
            <groupId>org.apache.httpcomponents</groupId>
            <artifactId>httpclient-osgi</artifactId>
            <version>4.0.1</version>
        </dependency>
You will find that the downloaded bundle manifest has:
Import-Package: javax.crypto,javax.crypto.spec,javax.net.ssl,javax.sec
 urity.auth.x500,org.apache.commons.logging;version=""1.1.1"",org.apache
...
Emphasis on org.apache.commons.logging;version=""1.1.1"".  Basing my understanding of the bug at 
https://issues.apache.org/jira/browse/LOGGING-124
which I have also commented on, there is no such thing as an exported osgi package version 1.1.1 of commons logging due to problems with the OSGi classloader.  

Support needs to be made to make commons logging optional in the osgi world and if the logging mechanism for commons logging isn't there, then move forward to something that doesn't make waves for the OSGi runtime.
","httpclient-osgi Requires commons-logging 1.1.1 exported in OSGi, which they have no plan to move forward with",,1,,zilatica,True,,zilatica
httpcomponents-client,HTTPCLIENT-1018,2010-10-21T13:05:56.000+0000,2010-10-21T22:51:15.000+0000,2016-09-20T19:31:43.000+0000,,Invalid,New Feature,Major,,,,,,,,,['HttpClient (classic)'],['HttpClient classic (blocking) code'],"Many bigger companies use Kerberos Authentication. When used on Webservers this usually means that the server uses the Spnego Protocoll to negotiate with the client which authentication to use. 
So Httpclient should support the Spnego protocoll together with Kerberos authentication to authenticate against this class of webservers.

The spring security project with it´s kerberos extension already supports the spnego and kerberos protocol on the server side. The source code shows how the authentication is done on the server side and will surely help to implement the client part.

",Support Spnego Authentication using Kerberos,1,,,cschneider,True,,cschneider
httpcomponents-client,HTTPCLIENT-1014,2010-10-14T10:46:49.000+0000,2010-10-16T17:27:47.000+0000,2011-01-25T11:00:31.000+0000,,Fixed,New Feature,Major,['4.1 Beta1'],['4.1 Alpha2'],,,,,,,['HttpClient (classic)'],['HttpClient classic (blocking) code'],"InputStreamBody can not determine the content length, which in turn causes requests to be sent with a content length of 0, even though the content is there. .NET Servers have trouble dealing with this.

ByteArrayBody provides an alternative that alliviates this limitation.

Source:
 
import java.io.IOException;
import java.io.OutputStream;

import org.apache.http.entity.mime.MIME;
import org.apache.http.entity.mime.content.AbstractContentBody;

/**
 * Body part that is built using a byte array containing a file.
 * 
 * @author Axel Fontaine
 */
public class ByteArrayBody extends AbstractContentBody {
    /**
     * The contents of the file contained in this part.
     */
    private byte[] data;

    /**
     * The name of the file contained in this part.
     */
    private String filename;
    
    /**
     * Creates a new ByteArrayBody.
     * 
     * @param data The contents of the file contained in this part.
     * @param mimeType The mime type of the file contained in this part.
     * @param filename The name of the file contained in this part.
     */
    public ByteArrayBody(final byte[] data, final String mimeType, final String filename) {
        super(mimeType);
        if (data == null) {
            throw new IllegalArgumentException(""byte[] may not be null"");
        }
        this.data = data;
        this.filename = filename;
    }

    /**
     * Creates a new ByteArrayBody.
     * 
     * @param data The contents of the file contained in this part.
     * @param filename The name of the file contained in this part.
     */
    public ByteArrayBody(final byte[] data, final String filename) {
        this(data, ""application/octet-stream"", filename);
    }

    @Override
    public String getFilename() {
        return filename;
    }

    @Override
    public void writeTo(OutputStream out) throws IOException {
        out.write(data);
    }

    @Override
    public String getCharset() {
        return null;
    }

    @Override
    public String getTransferEncoding() {
        return MIME.ENC_BINARY;
    }

    @Override
    public long getContentLength() {
        return data.length;
    }
}
",ByteArrayBody as an alternative to InputStreamBody,,,,axel.fontaine,True,,axel.fontaine
httpcomponents-client,HTTPCLIENT-1010,2010-10-12T18:21:39.000+0000,2010-10-12T18:38:14.000+0000,2010-10-12T18:38:14.000+0000,,Duplicate,New Feature,Major,,['4.1 Alpha2'],,,,,,,['HttpCache'],['HttpClient Caching components'],"This allows the caller to find out if an actual connection has been made. One possible way of doing this is to add a custom header to resonses generated from the cache. The attached patch adds the header:

X-From-Cache: true

The use case where this is needed is when you want to know whether you are actually connected to the internet or whether contacted server is alive without generating extra out of order requests.

I was also looking if it was possible to detect this by looking at the Via header, but it looks like the same header is added whether the response is served from the backend or the local cache.",add an X-From-Cache header to responses that are served from the cache,,,,fberger,True,,fberger
httpcomponents-client,HTTPCLIENT-982,2010-08-20T12:59:02.000+0000,2010-08-27T19:41:53.000+0000,2011-01-25T11:00:19.000+0000,,Fixed,New Feature,Trivial,['4.1 Beta1'],['4.1 Alpha2'],,,,,,,['HttpCache'],['HttpClient Caching components'],"Is there a way to know if the response has been served from the cache or not ?
That's an information which might be useful for monitoring the activity of the cache.

If there's no current way, maybe a flag could be added in the request context whenever the response comes from the cache ... ?

",Could we get a way to know if the response has been served from the cache or not ?,,,,vcarel,True,,vcarel
httpcomponents-client,HTTPCLIENT-978,2010-08-11T17:32:37.000+0000,2010-08-21T20:32:49.000+0000,2011-01-25T11:00:17.000+0000,,Fixed,New Feature,Major,['4.1 Beta1'],,,,,,,,['HttpCache'],['HttpClient Caching components'],"Provide an implementation of the HttpCache interface that stores cache entries in ehcache.
",provide an ehcache implementation for HttpCache,3,,,cimjmoore,True,,cimjmoore
httpcomponents-client,HTTPCLIENT-977,2010-08-11T17:28:12.000+0000,2010-09-08T21:22:04.000+0000,2011-01-25T11:00:17.000+0000,,Fixed,New Feature,Minor,['4.1 Beta1'],,,,,,,,['HttpCache'],['HttpClient Caching components'],The feature here would be an implementation of the HttpCache interface that stored cache entries in memcached.,provide a memcached implementation for HttpCache,3,,,cimjmoore,True,,cimjmoore
httpcomponents-client,HTTPCLIENT-975,2010-08-04T21:12:04.000+0000,2010-12-20T18:31:18.000+0000,2010-12-20T18:31:33.000+0000,,Fixed,New Feature,Major,['4.1 Final'],['4.1 Beta1'],,,,,,,['HttpCache'],['HttpClient Caching components'],"These are Cache-Control extensions that allow an origin server to specify some additional behavior for stale cache entries. Stale-on-error configurations allow a cache to continue serving stale content for a certain period of time if a revalidation fails, and stale-while-revalidate similarly allows revalidation to occur asynchronously. Some reverse proxies such as Squid can be configured to understand these headers, which means that some origin servers are probably sending them, and that we can likewise take advantage of them.
",add support to caching module for RFC 5861 (stale-on-error and stale-while-revalidate),3,,,cimjmoore,True,jonm,cimjmoore
httpcomponents-client,HTTPCLIENT-973,2010-08-03T13:14:16.000+0000,,2017-05-02T13:50:46.000+0000,,,New Feature,Major,['Future'],,,,,,,,['HttpClient (classic)'],['HttpClient classic (blocking) code'],"Websocket are designed for browser use but I see useful use cases in a java client as well:

- java clients for existing websites where changes are pushed to clients using websockets
- use of websockets to tunnel through firewalls
- put all communication behind single port on servlet engine.

A good example on emerging use is websocket+stomp (http://jmesnil.net/stomp-websocket/doc/, http://www.nighttale.net/activemq/activemq-54-stomp-over-web-sockets.html)

so I believe hc should support websocket clients.",Websocket support,9,9,['volunteers-wanted'],xylifyx,True,,xylifyx
httpcomponents-client,HTTPCLIENT-926,2010-03-14T05:31:43.000+0000,2010-04-14T20:00:38.000+0000,2011-01-25T10:59:31.000+0000,,Fixed,New Feature,Major,['4.1 Alpha2'],,,,,,,,['Contrib'],['HttpClient Contrib (unsupported) components'],Add support for the the Amazon S3 authentication scheme as defined by the online document: http://docs.amazonwebservices.com/AmazonS3/latest/index.html?RESTAuthentication.html,Add Amazon S3 authentication support,,,,caskater47,True,,caskater47
httpcomponents-client,HTTPCLIENT-915,2010-02-19T09:11:41.000+0000,2010-02-22T18:47:53.000+0000,2016-09-20T19:30:38.000+0000,,Fixed,New Feature,Minor,['4.1 Alpha2'],['4.0.1'],,,,,,,['HttpClient (classic)'],['HttpClient classic (blocking) code'],"It would be nice to have a way to attach user defined attributes to a connection.
Ideally it'd be nice if such support could be added to HttpClientConnection, but understandably this may not be possible due to back-compatibility issues.
So, we could have something like HttpConnectionContext perhaps (or similar) with:

HttpConnectionContext#setAttribute(String name, Object value)
Object HttpConnectionContext#getAttribute(String name)

This would be made available in the HttpContext of a request (like the connection is today):

HttpConnectionContext connectionContext = (HttpConnectionContext) httpContext.getAttribute(ExecutionContext.HTTP_CONNECTION_CONTEXT);

This would make a few things much cleaner to implement than they are today: The most obvious being my current use case of wanting connection isolated cookies.

Currently to achieve this goal we need to provide custom client connection + connection operator + connection manager implementations. Then there is no clean way to currently obtain the actual connection instance created by a custom operator in the HttpContext: As it's wrapped by the connection pool and #getWrappedConnection is protected - so we need to resort to reflection in interceptors.

Providing a clean mechanism for attaching user defined attributes to a connection instance as described above would make such implementations far far simpler.
",Provide a clean mechanism to attatch user define attributes to connections,,,,irvingd,True,,irvingd
httpcomponents-client,HTTPCLIENT-851,2009-05-17T15:01:27.000+0000,2011-07-20T15:12:59.000+0000,2013-10-05T19:41:18.000+0000,,Won't Fix,New Feature,Major,,['4.0 Beta 2'],,,,,,,,,"HttpClient API is pretty much low-level, and needs some obscure setup (Registry, ..) to target the common use case (sometimes, you don't care about the low level details, you just
want to fire an http client, and hit a given web resource).

I have extracted some supporting code out of my application in the hope to make it widely available for others. It is available at :
http://www.sirika.com/maven2/releases/com/sirika/httpclienthelpers/com.sirika.httpclienthelpers-assembly/0.1/com.sirika.httpclienthelpers-assembly-0.1-all.zip

It contains :
- HttpClientTemplate that automatically open/close connections in case of exceptions.It also provides a kind of generic http error handling mechanism.
- a spring HttpClientFactoryBean (as well as a helper factory class for httpclient) that allows setting up http client for the common use case 
- an InputStreamSourceBody that can be used to create POST entities that are repeatable, wrapping a spring InputStreamSource
- the code for the GZIP interceptors that are documented as examples on httpclient website. 

The code is still not tested (just extracted that logic from my application), needs some polishing, and a few things like the error handling mechanism still need more thought. But I feel it can be helpful to others.
So, my question is : can we create a kind of -contrib project for httpclient, that allows people to share their helpers with the community ?

the code is currently available under under apache v2 license, but I'm willing to adopt whatever licensing/copyright issues that would help to make this available.

Oh, also, if you want to see the code in action, here's an example of  a REST client that uses it :
http://www.sirika.com/maven2/releases/com/sirika/pymager/pymager-java-client-assembly/0.11/pymager-java-client-assembly-0.11-all.zip
(which is another project born out of extracted code from my application, that I'm trying to push as open source ; I just lack time to handle the community / website / etc issues).

Sami Dalouche","Helper classes for HttpClient : spring-like templates, Gzip interceptor, spring FactoryBean, ...",1,,,samokk,True,,samokk
httpcomponents-client,HTTPCLIENT-836,2009-03-22T16:45:26.000+0000,2009-10-12T15:27:27.000+0000,2016-09-20T19:31:39.000+0000,,Won't Fix,New Feature,Major,,['4.0 Beta 2'],,,,,,,['HttpClient (classic)'],['HttpClient classic (blocking) code'],"Would it be possible to add OAuth AuthScheme?

I have some example code using the http://oauth.googlecode.com/svn/code/java/ oauth library. It might be better to refactor that code into the main code base if anyone has the time.

I'll see if I can attach my implementation.

Paul",Add OAuth AuthScheme,2,1,,p_d_austin,True,,p_d_austin
httpcomponents-client,HTTPCLIENT-834,2009-03-15T21:15:04.000+0000,2009-11-04T19:26:24.000+0000,2018-10-19T15:30:34.000+0000,,Fixed,New Feature,Major,['4.1 Alpha1'],['4.0 Beta 2'],,,,,,,['HttpClient (classic)'],['HttpClient classic (blocking) code'],"I would like to see HttpClient features brought up to parity with other libraries, both in Java and other languages. c.f. Python's httplib2 (not yet in the standard library, but many would like to see it in there). That library transparently handles gzip and compress content codings.

This issue is to capture possible solutions to providing this sort of innate functionality in HttpClient, so that users aren't required to know RFC2616 intimately. The HttpClient library should do the right thing and use the network in the most efficient manner possible.",Transparent Content Coding support,1,,,jabley,True,,jabley
httpcomponents-client,HTTPCLIENT-816,2009-01-20T01:00:53.000+0000,2009-01-24T11:48:21.000+0000,2017-02-24T14:35:09.000+0000,,Won't Fix,New Feature,Minor,,['3.1.1'],,,3600,3600,3600,,['HttpClient (classic)'],['HttpClient classic (blocking) code'],"I think HttpClient really ought to support Accept-Encoding: gzip without requiring any effort from the client developer. Allowing Content-Encoding: gzip responses can be a great bandwidth saver.

But it also needs to cater for developers that have coded their own solution to this issue.",HttClient should support Accept-Encoding: gzip out of the box,2,,,jabley,True,,jabley
httpcomponents-client,HTTPCLIENT-812,2009-01-05T17:31:22.000+0000,2009-01-06T13:41:55.000+0000,2013-10-05T19:41:23.000+0000,,Won't Fix,New Feature,Minor,,['3.1 (end of life)'],,,7200,7200,7200,,['HttpClient (classic)'],['HttpClient classic (blocking) code'],"Would like to record and then retrieve the time, in ms,  when an ""http get"" request is sent and likewise when the first byte of the response is received.

Downloaded the source code for commons-httpclient-3.0.1.  I built a custom commons-httpclient-3.0.1.jar where org.apache.commons.httpclient.HttpMethodBase records the time an ""http get"" request is first sent (execute method) , along with adding getters and setters.  Also modified org.apache.commons.httpclient.HttpParser to record, get, and set the time that the first byte of the response is received.

This non-standard feature allows us in our project to log and audit how long it's taking the targeted server to respond to our requests.
","Would like to record and then retrieve the time, in ms,  when an ""http get"" request is sent and likewise when the first byte of the response is received.",,,,lawbats,True,,lawbats
httpcomponents-client,HTTPCLIENT-807,2008-10-31T14:22:38.000+0000,2008-10-31T17:17:12.000+0000,2016-09-20T19:31:59.000+0000,,Fixed,New Feature,Major,['4.0 Beta 2'],['4.0 Beta 1'],,,,,,,['HttpClient (classic)'],['HttpClient classic (blocking) code'],"ContentBody and therefore FileBody, StringBody and InputStreamBody do not have a setMimeType method so you can't set the Mime Type, it always defaults. 
Current workaround is to subclass and override getMimeType.",ContentBody doesn't currently have a setMimeType method.,,,,xtien,True,,xtien
httpcomponents-client,HTTPCLIENT-788,2008-07-21T15:44:54.000+0000,2008-07-24T09:28:30.000+0000,2016-09-20T19:29:45.000+0000,,Fixed,New Feature,Minor,['4.0 Beta 1'],['4.0 Alpha 4'],,,,,,,['HttpClient (classic)'],['HttpClient classic (blocking) code'],"Hi,

I just found this useful list: http://publicsuffix.org/
and thought it would be nice to validate cookie domains against it, basically serving as a black list of domain for which never to set any cookies. What do you think about the attached patch? The download/parsing of the list is of course not part of the implementation.

Ortwin",Public Suffix List,,,,oglueck,True,oglueck,oglueck
httpcomponents-client,HTTPCLIENT-761,2008-03-31T00:30:30.000+0000,2008-10-15T21:24:59.000+0000,2013-10-05T19:42:00.000+0000,,Won't Fix,New Feature,Trivial,,['4.0 Alpha 3'],,,,,,,,,"The attached sample (unfortunately, no tests) lets you persist between CookieStore and Properties",Class to Let Users Persist Cookies to-from java.util.Properties,,,,aldrinleal,True,,aldrinleal
httpcomponents-client,HTTPCLIENT-751,2008-02-17T17:26:04.000+0000,,2013-05-02T02:29:32.000+0000,,,New Feature,Minor,['Future'],,,,,,,,['HttpClient (classic)'],['HttpClient classic (blocking) code'],"HTTP allows a client to upgrade its connection from an unsecured one to a secured one via the Upgrade header. This protocol component implements a mandatory upgrade. From RFC2817, section 3.2:

>> OPTIONS * HTTP/1.1
>> Host: example.bank.com
>> Upgrade: TLS/1.0
>> Connection: Upgrade
>>

<< HTTP/1.1 101 Switching Protocols
<< Upgrade: TLS/1.0, HTTP/1.1
<< Connection: Upgrade
<< 

-- send and complete the TLS handshake
-- server answers the original OPTIONS request

<< HTTP/1.1 200 OK
<< Date: Sun, 17 Feb 2008 17:23:35 GMT
<< Server: Apache
<< Content-Length: 0
<< Content-Type: text/plain
<<

My reason for developing this protocol socket factory was to support secure IPP over the standard IPP port.",ProtocolSocketFactory to upgrade an unsecured connection to secured via TLS,,,,sbutler,True,,sbutler
httpcomponents-client,HTTPCLIENT-750,2008-02-17T17:10:27.000+0000,,2016-09-20T19:30:30.000+0000,,,New Feature,Major,['Future'],,,,,,,,['HttpClient (classic)'],['HttpClient classic (blocking) code'],"RFC 2817 (http://tools.ietf.org/html/rfc2817) specifies not only tunnelling of TLS/SSL via proxies, but also upgrading an existing HTTP connection to TLS/SSL. The latter is not commonly used for communication with traditional HTTP servers, but part of other protocols like IPP that are based on HTTP.

The current connection management code assumes that a route planner can determine in advance whether a connection will be secure or not. A connection manager will not reuse an existing unsecure connection if a secure connection is requested. It probably also doesn't consider a returned connection secure if it wasn't requested for a secure route.
One way to improve the situation is to give HttpRoute a trinary security flag, with values plain/secure for the current usage scenario and a new value upgradeable for the new scenario. The two scenarios won't mix, but that is probably not required.
We have to make sure that upgrade to security of an existing plain HTTP connection is correctly tracked and either respected or suitably ignored by the connection manager. If the security flag of the route is 'upgradeable', mixing of scenarios is not required, and the actual security level can be obtained from the connection itself, it is probably safe to let the connection manager ignore it.

cheers,
  Roland",manage connections with upgradeable security,,,,rolandw,True,,rolandw
httpcomponents-client,HTTPCLIENT-733,2008-01-25T21:12:39.000+0000,2008-02-04T17:46:05.000+0000,2011-01-25T10:58:46.000+0000,,Fixed,New Feature,Major,['4.0 Alpha 3'],['4.0 Alpha 2'],,,,,,,['HttpClient (classic)'],['HttpClient classic (blocking) code'],"The HTTP request method, Delete, had not been implemented. I needed it and created an HttpDelete class modeled after HttpGet.",Implementation of Delete method,,,,dgallardo,True,,dgallardo
httpcomponents-client,HTTPCLIENT-728,2008-01-07T06:25:11.000+0000,2008-02-04T15:56:50.000+0000,2013-10-05T19:41:53.000+0000,,Won't Fix,New Feature,Trivial,,['4.0 Alpha 2'],,,,,,,['Contrib'],['HttpClient Contrib (unsupported) components'],"A very simple wrapper is attached here. This one offcourse requires the httpclient-3.1.jar in addition to httpclient-4 and httpcore. 

There's an issue passing parameters from httpclient-4 HttpMethod.getProperties() (HttpParams from 4) to httpclient-3.1 and MultipartRequestEntity's constructor which takes HttpMethodParams (from 3.1). I was thinking about doing an iteration of the parameters, but as far as I could understand HttpMethodParams (from 3.1) is a static configuration, thus it's alot of typing work and research to map httpclient-4s dynamic properties to httpclient-3.1 configuration class. Furthermore I didn't find any way of iterating the the parameters (at a glance). So, I gave that one up. :) 

In conclusion, MultipartRequestEntity is stuck with the default parameters. 

It did it's job for me. That was pretty basic stuff, though. :) 
",Wrapper for httpclient-3.1 MultipartRequestEntity,2,,,magge,True,,magge
httpcomponents-client,HTTPCLIENT-723,2007-12-22T12:59:14.000+0000,2008-02-03T18:16:00.000+0000,2016-09-20T19:30:24.000+0000,,Fixed,New Feature,Minor,['4.0 Alpha 3'],,,,,,,,['HttpClient (classic)'],['HttpClient classic (blocking) code'],"Since we now require Java 5, we should have a route planner that uses the standard Java ProxySelector. That would allow us to automatically pick up proxy settings from system properties or the browser running an applet.
",HttpRoutePlanner based on ProxySelector,,,,rolandw,True,rolandw,rolandw
httpcomponents-client,HTTPCLIENT-720,2007-12-18T14:20:27.000+0000,2008-04-16T17:50:26.000+0000,2013-10-05T19:41:49.000+0000,,Won't Fix,New Feature,Minor,,['3.1 (end of life)'],,,,,,,['Contrib'],['HttpClient Contrib (unsupported) components'],"Allow Java Enterprise applications (J2EE/JEE) to use HttpClient, by providing HttpClient as a standard Resource Adapter (JCA 1.5)",HttpClient Resource Adapter (JCA1.5),,,,sagimann,True,,sagimann
httpcomponents-client,HTTPCLIENT-670,2007-07-17T08:02:05.000+0000,2008-05-22T18:43:21.000+0000,2016-09-20T19:30:21.000+0000,,Fixed,New Feature,Major,['4.0 Beta 1'],,,,,,,,['HttpClient (classic)'],['HttpClient classic (blocking) code'],"Currently Httpclient implicitly uses InetAddress.getByName() for DNS resolution.
This has some drawbacks. One is that the DNS cache of Java per default caches entries forever.

So I'd like to be able to replace InetAddress.getByName() with another DNS client implementation.

",add an interface for plugable dns clients,1,,,arminha,True,,arminha
httpcomponents-client,HTTPCLIENT-656,2007-06-05T07:30:40.000+0000,2007-06-05T20:07:13.000+0000,2016-09-20T19:30:20.000+0000,,Fixed,New Feature,Major,['4.0 Alpha 1'],,,,,,,,['HttpClient (classic)'],['HttpClient classic (blocking) code'],"AFAIK it's not possible to get the IP address of the server of a HttpConnection.

I propose to add a getServerAddress() method to the HttpConnection class that returns the IP address of the server, if the connection has been opened.
And either returns null or throws an Exception if the IP address is not available, i.e. the connection is not open.

Below is a workaround for getting the IP address in current versions.

-----------------------
package org.apache.commons.httpclient;

import java.io.IOException;
import java.net.InetAddress;

public class InetAddressFetcher {
	private HttpConnection hc;

	public InetAddressFetcher(HttpConnection hc) {
		this.hc = hc;
	}

	public InetAddress getInetAddress() throws IOException {
		if (!hc.isOpen()) {
			hc.open();
		}
		return hc.getSocket().getInetAddress();
	}
}",IP address of the server of a HttpConnection,2,,,arminha,True,,arminha
httpcomponents-client,HTTPCLIENT-649,2007-04-24T14:01:46.000+0000,2007-09-08T14:27:29.000+0000,2016-09-20T19:30:18.000+0000,,Fixed,New Feature,Major,['4.0 Alpha 2'],"['3.0.1', '3.1 RC1']",,,,,,,['HttpClient (classic)'],['HttpClient classic (blocking) code'],"HttpClient supports one proxy currently.
Our requirement is to suppport more than one proxy. We may need to connect more than one proxies before connects to target resource. 
I found that HttpMethodDirector creates tunnelled socket and there is no easy way to plugin our custom HttpMethodDirector class with HttpClient other than extending HttpClient to override ""public int executeMethod(HostConfiguration hostconfig, final HttpMethod method, final HttpState state"" method.


",Support multiple proxies,,,,suak2004,True,rolandw,suak2004
httpcomponents-client,HTTPCLIENT-601,2006-09-18T16:20:23.000+0000,2006-10-01T18:58:04.000+0000,2007-04-22T07:11:21.000+0000,,Fixed,New Feature,Major,['3.1 Beta 1'],['3.0.1'],,,,,,,['Contrib'],['HttpClient Contrib (unsupported) components'],"As smartcards and SSL are becoming more and more prevelant, Java Web Start has started to become better equiped to handle these situations.  When running an app within webstart, it can access the browser's keystore, which (at least in our case) accessed the users smartcard to make the SSL connection.

I wanted to start using HttpClient, but needed a way to do so while still mainaining access to the browser's keystore.

My initial tests show that getting the default socket factory from the java.net.HttpURLConnection and wrapping it in a class that implements org.apache.commons.httpclient.protocol.SecureProtocolSocketFactory is sufficient.",SecureProtocolFactoryWrapper class for using the socket factory created by Java Web Start,,,,mmclaassen,True,,mmclaassen
httpcomponents-client,HTTPCLIENT-245,2003-07-15T03:30:42.000+0000,2007-05-12T10:55:02.000+0000,2007-07-20T18:36:10.000+0000,,Fixed,New Feature,Major,['4.0 Alpha 1'],['2.0 Final'],,,,,,,['HttpClient (classic)'],['HttpClient classic (blocking) code'],"I tried this with both the beta2 2.0 release, and the nightly build.  The
following code snippet describes what I am trying to do:

httpClient.getHostConfiguration().setHost(sHost, 80, ""http"");
HttpMethod method=null;
if (sMethod.indexOf(""POST"")!=-1) {
     method=new PostMethod(sURLInfo);
} else {
     method=new GetMethod(sURLInfo);
}
method.setFollowRedirects(true);
httpClient.executeMethod(method);

After this code executes, the ""getFollowRedirects"" method still returns false,
and any redirects which are sent by the webserver are not followed.  As a
temporary workaround, since I want all redirects followed, I commented out the
following code in the HttpMethodBase class in the ""processRedirectResponse"" method:

/*if (!getFollowRedirects()) {
     LOG.info(""Redirect requested but followRedirects is ""
     + ""disabled"");
     return false;
}*/

If this bug has already been reported, I apologize...I searched for and found
nothing related to this issue.",customize handling of 302 redirects,,3,,delonad@netdoor.com,True,,delonad@netdoor.com
httpcomponents-core,HTTPCORE-570,2019-02-06T20:47:45.000+0000,2019-02-12T09:23:35.000+0000,2019-02-12T09:23:35.000+0000,,Fixed,New Feature,Major,['5.0-beta7'],,1800,1800,,,,100,,,"Add getters to {{AsyncServerBootstrap}} and refactor {{create()}} implementation into protected methods.

Right now, you can subclass {{AsyncServerBootstrap}} to customize a {{create()}} method but you cannot get to instance variables as these are private.

The goal is to make creating instances of {{HttpAsyncServer}} simpler by adding getters and refactoring create() implementations into protected methods such that subclasses can more simply create custom instances.

That does not seem controversial to me so I am not sure a feature branch is needed but I'll create one anyway: {{HTTPCORE-570}}",Add getters to AsyncServerBootstrap and refactor create() impl into protected methods,3,,,ggregory,True,ggregory,ggregory
httpcomponents-core,HTTPCORE-569,2019-02-06T20:35:39.000+0000,2019-02-12T09:24:07.000+0000,2019-02-12T09:24:07.000+0000,,Fixed,New Feature,Major,['5.0-beta7'],,,,,,,,['HttpCore'],['HttpCore components'],"Create new value in {{org.apache.hc.core5.http.protocol.UriPatternType}} to lookup values without an exact match override: This allows path matches _without_ having an exact match override the order in which the patterns are defined (which is the current behavior.)

This is a request from some of my users at work, they just want to a list of URI patterns in a specific order which is observed no matter what. Very handy for testing.

The implementation would refactor {{UriPatternMatcher}} to allow a boolean to be passed in.

I created branch {{HTTPCORE-569}} with a proposal.

You use this by calling {{org.apache.hc.core5.http.impl.bootstrap.AsyncServerBootstrap.setUriPatternType(UriPatternType)}}.
",Create new value in org.apache.hc.core5.http.protocol.UriPatternType to lookup without an exact match override,3,,,ggregory,True,ggregory,ggregory
httpcomponents-core,HTTPCORE-563,2018-11-27T01:05:00.000+0000,2018-12-09T10:23:16.000+0000,2018-12-09T23:19:33.000+0000,,Fixed,New Feature,Major,['5.0-beta7'],['5.0'],,,,,,,['HttpCore NIO'],['HttpCore NIO extensions'],"I would like to add support for SOCKS proxies to HttpCore NIO classes.  SOCKS proxies are already supported for 'classic' or 'blocking' IO via the standard JVM support, however the JVM does not support SOCKS for nio Sockets and so currently the HttpCore NIO classes cannot support a SOCKS proxy.",Add SOCKS Proxy support to HttpCore NIO,3,,,dmaplesden,True,,dmaplesden
httpcomponents-core,HTTPCORE-549,2018-08-05T03:43:39.000+0000,2018-08-07T09:25:30.000+0000,2018-08-07T09:56:57.000+0000,,Fixed,New Feature,Major,,"['4.4.11', '5.0-beta3', '5.0']",,,,,,,['HttpCore'],['HttpCore components']," Please add OWASP Dependency Check to the build (pom.xml).  OWASP DC makes an outbound REST call to MITRE Common Vulnerabilities & Exposures (CVE) to perform a lookup for each dependant .jar to list any/all known vulnerabilities for each jar.  This step is needed because a manual MITRE CVE lookup/check on the main component does not include checking for vulnerabilities in components or in dependant libraries.

OWASP Dependency check : https://www.owasp.org/index.php/OWASP_Dependency_Check has plug-ins for most Java build/make types (ant, maven, ivy, gradle).   

Also, add the appropriate command to the nightly build to generate a report of all known vulnerabilities in any/all third party libraries/dependencies that get pulled in. example : mvn -Powasp -Dtest=false -DfailIfNoTests=false clean aggregate

Generating this report nightly/weekly will help inform the project's development team if any dependant libraries have a reported known vulnerailities.  Project teams that keep up with removing vulnerabilities on a weekly basis will help protect businesses that rely on these open source componets.",Please add OWASP Dependency Check to the build (pom.xml),2,,"['build', 'easy-fix', 'security']",ABakerIII,True,,ABakerIII
httpcomponents-core,HTTPCORE-548,2018-07-30T18:11:34.000+0000,2018-07-30T18:48:58.000+0000,2018-07-30T21:44:41.000+0000,,Fixed,New Feature,Major,['5.0-beta3'],['5.0-beta2'],,,,,,,['HttpCore NIO'],['HttpCore NIO extensions'],Add missing HttpContext parameter to APIs.,Add missing HttpContext parameter to APIs,2,,,ggregory,True,ggregory,ggregory
httpcomponents-core,HTTPCORE-547,2018-07-26T21:32:09.000+0000,2018-07-26T21:33:14.000+0000,2018-07-27T07:09:11.000+0000,,Fixed,New Feature,Major,['5.0-beta3'],,,,,,,,,,Update {{org.apache.hc.core5.http.message.HeaderGroup.removeHeaders(String)}} to return a boolean: {{true}} if any header was removed as a result of this call.,Update org.apache.hc.core5.http.message.HeaderGroup.removeHeaders(String) to return a boolean,2,,,ggregory,True,ggregory,ggregory
httpcomponents-core,HTTPCORE-546,2018-07-26T21:16:59.000+0000,2018-07-26T21:29:23.000+0000,2018-07-27T07:09:09.000+0000,,Fixed,New Feature,Major,['5.0-beta3'],,,,,,,,['HttpCore'],['HttpCore components'],Update {{org.apache.hc.core5.http.message.HeaderGroup.removeHeader(Header)}} to return a boolean: {{true}} if a header was removed as a result of this call.,Update org.apache.hc.core5.http.message.HeaderGroup.removeHeader(Header) to return a boolean,2,,,ggregory,True,ggregory,ggregory
httpcomponents-core,HTTPCORE-545,2018-07-26T21:11:09.000+0000,2018-07-26T21:29:43.000+0000,2018-07-27T07:09:08.000+0000,,Fixed,New Feature,Major,['5.0-beta3'],,,,,,,,['HttpCore'],['HttpCore components'],Add {{org.apache.hc.core5.http.message.HeaderGroup.removeHeaders(Header)}} to remove ALL headers that match the given header.,Add org.apache.hc.core5.http.message.HeaderGroup.removeHeaders(Header),2,,,ggregory,True,ggregory,ggregory
httpcomponents-core,HTTPCORE-544,2018-07-26T14:58:34.000+0000,2018-07-26T15:11:46.000+0000,2018-07-26T20:32:24.000+0000,,Fixed,New Feature,Major,['5.0-beta3'],,,,,,,,,,Add {{org.apache.hc.core5.http.EndpointDetails.getSocketTimeout()}},Add org.apache.hc.core5.http.EndpointDetails.getSocketTimeout(),3,,,ggregory,True,ggregory,ggregory
httpcomponents-core,HTTPCORE-503,2018-01-06T18:40:18.000+0000,2018-01-06T18:58:03.000+0000,2018-01-06T18:58:03.000+0000,,Fixed,New Feature,Major,['4.4.9'],,,,,,,,['HttpCore NIO'],['HttpCore NIO extensions'],"Add APIs to {{DefaultHttpClientIODispatch}} to get its connection factory and handler:
- {{getConnectionFactory()}}
- {{getHandler()}}",Add APIs to DefaultHttpClientIODispatch to get its connection factory and handler,1,,,ggregory,True,ggregory,ggregory
httpcomponents-core,HTTPCORE-502,2018-01-06T18:28:08.000+0000,2018-01-06T18:35:07.000+0000,2018-01-06T18:40:18.000+0000,,Fixed,New Feature,Major,['4.4.9'],,,,,,,,['HttpCore NIO'],['HttpCore NIO extensions'],"Add APIs to {{DefaultHttpServerIODispatch}} to get its connection factory and handler:
- {{getConnectionFactory()}}
- {{getHandler()}}",Add APIs to DefaultHttpServerIODispatch to get its connection factory and handler,2,,,ggregory,True,ggregory,ggregory
httpcomponents-core,HTTPCORE-497,2017-12-05T22:26:43.000+0000,2017-12-05T22:30:40.000+0000,2017-12-06T08:32:00.000+0000,,Fixed,New Feature,Major,['4.4.9'],,,,,,,,,,Add API org.apache.http.nio.protocol.UriHttpAsyncRequestHandlerMapper.getUriPatternMatcher().,Add API Add API org.apache.http.nio.protocol.UriHttpAsyncRequestHandlerMapper.getUriPatternMatcher(),2,,,ggregory,True,ggregory,ggregory
httpcomponents-core,HTTPCORE-479,2017-08-02T02:21:58.000+0000,,2017-08-02T02:22:12.000+0000,,,New Feature,Major,,,,,,,,,,,"Add a ""Via"" HttpRequestInterceptor to add the ""Via"" HTTP header.","Add ""Via"" HttpRequestInterceptor",1,,,ggregory,True,,ggregory
httpcomponents-core,HTTPCORE-478,2017-08-02T02:21:39.000+0000,,2017-08-02T02:21:59.000+0000,,,New Feature,Major,,,,,,,,,,,"Add a ""Forwarded"" HttpRequestInterceptor to add the ""Forwarded"" HTTP header.","Add ""Forwarded"" HttpRequestInterceptor",1,,,ggregory,True,,ggregory
httpcomponents-core,HTTPCORE-471,2017-05-24T07:47:36.000+0000,2017-05-24T08:29:08.000+0000,2017-05-24T08:29:08.000+0000,,Fixed,New Feature,Major,['5.0-alpha4'],,,,,,,,,,Add APIs URIBuilder.localhost() and setHost(InetAddress),Add APIs URIBuilder.localhost() and setHost(InetAddress),1,,,ggregory,True,ggregory,ggregory
httpcomponents-core,HTTPCORE-454,2017-04-19T21:08:25.000+0000,2017-06-26T20:56:03.000+0000,2017-06-26T20:56:03.000+0000,,Fixed,New Feature,Major,['5.0-alpha4'],,,,,,,,,,Add a {{Timeout}} class that extends {{TimeValue}}. {{Timeout}} will validate input so that you cannot define a negative timeout. A couple of handy constants and methods will also be defined (like {{UNDEFINED}} and {{DISABLED}}).,Add a Timeout class that extends TimeValue,2,,,ggregory,True,ggregory,ggregory
httpcomponents-core,HTTPCORE-452,2017-04-17T19:16:08.000+0000,2017-04-18T07:42:20.000+0000,2017-04-18T07:42:20.000+0000,,Fixed,New Feature,Major,['5.0-alpha3'],['5.0-alpha2'],,,,,,,,,Add a {{UriRegexMatcher}} and let it implement a new interface {{LookupRegistry}} such that the current somewhat misnamed {{UriPatternMatcher}} becomes pluggable.,Add a UriRegexMatcher,1,,,ggregory,True,ggregory,ggregory
httpcomponents-core,HTTPCORE-450,2017-03-29T08:19:07.000+0000,2017-03-31T22:10:56.000+0000,2017-05-04T22:35:15.000+0000,,Fixed,New Feature,Major,"['4.4.7', '5.0-alpha3', '5.0-alpha4']",['4.4.6'],,,,,,,,,"Add a provider parameter in {{SSLContextBuilder}} to support third party provider such as The Legion of the Bouncy Castle (https://www.bouncycastle.org/java.html).

New APIs:
- {{org.apache.http.ssl.SSLContextBuilder.setProvider(Provider)}} in release 5.0-alpha3 (released) and 4.4.7 (upcoming)
- {{org.apache.http.ssl.SSLContextBuilder.setProvider(String)}} in release 5.0-alpha4 (upcoming)",Add a Provider parameter in SSLContextBuilder,3,,['features'],Moonlightos,True,ggregory,Moonlightos
httpcomponents-core,HTTPCORE-424,2016-06-14T09:40:59.000+0000,2016-12-11T15:51:48.000+0000,2016-12-11T15:51:48.000+0000,,Fixed,New Feature,Minor,['5.0-alpha2'],,,,,,,,['HttpCore NIO'],['HttpCore NIO extensions'],"h3. Usecase

I make 500req/s to the same server, whose response times average 5ms. This only needs 3 connections. This 500req/s occasionally bursts to 1000req/s, but as I only have 3 connections in my connection pool, I then have to establish a bunch of connections (let's say 17), at which point the latency suffers the cost of a TCP/SSL handshake.

The rate then drops to the baseline 500req/s, but next time the 1000req/s burst occurs, I'll be ready right?! I have 20 connections in my pool, waiting for me to need them.

No, I won't be ready. By the time the burst occurs, these extra connections have timed out, because they've not been used, and these connections are subject to TCP and HTTP time outs (both beyond my control).

It would be nice to have a way to keep these connections active, so they don't idle out, ready for when I next see a burst of 1000req/s.

h3. A possible implementation:

A simple implementation would be to lease connections on a FIFO basis, rather than a LIFO basis (Apache HttpAsyncClient's current behaviour). With a FIFO policy, all connections would be kept active, so they would never idle out.

Apache HttpAsyncClient uses a LinkedList, in LIFO mode: lines 83 and 129 of https://github.com/apache/httpcore/blob/4.4.x/httpcore-nio/src/main/java/org/apache/http/nio/pool/RouteSpecificPool.java#L83


h3. Other HTTP clients:

RxJava uses a Queue (FIFO): https://github.com/ReactiveX/RxJava/blob/2a8d6c75205b13cd9bef2b7f7d47723e1c86454e/src/main/java/rx/internal/util/ObjectPool.java#L28
Vert.x uses a Queue (FIFO): https://github.com/eclipse/vert.x/blob/dbdcb3c785e9d8ff70dcce6a2e148cd3da6d15f6/src/main/java/io/vertx/core/http/impl/Http1xPool.java#L48
async-http-client allows user to specify LIFO or FIFO: https://github.com/AsyncHttpClient/async-http-client/issues/1184","A means to keep connections alive, even if fewer connections would suffice",1,,,tomfitzhenry,True,olegk,tomfitzhenry
httpcomponents-core,HTTPCORE-412,2015-10-25T19:42:46.000+0000,2015-11-12T09:42:54.000+0000,2017-05-11T07:40:28.000+0000,,Fixed,New Feature,Minor,['5.0-alpha1'],,,,,,,,"['HttpCore', 'HttpCore NIO']","['HttpCore components', 'HttpCore NIO extensions']","Currently HTTP client doesn't support sending trailers to a web server.

The section https://tools.ietf.org/html/rfc7230#section-4.4 is need to be implemented.

",Send HTTP trailers in request,2,,,dyaitskov,True,,dyaitskov
httpcomponents-core,HTTPCORE-407,2015-08-29T23:27:38.000+0000,2015-08-30T10:23:12.000+0000,2017-05-17T15:32:36.000+0000,,Fixed,New Feature,Major,['5.0-alpha1'],['4.4.1'],,,,,,,['HttpCore'],['HttpCore components'],"I've coded one. Patch coming up.
",A ByteBufferEntity would be handy.,2,,,bmargulies,True,,bmargulies
httpcomponents-core,HTTPCORE-432,2015-07-30T13:08:27.000+0000,2016-12-10T10:41:26.000+0000,2016-12-10T10:41:27.000+0000,,Fixed,New Feature,Major,['5.0-alpha2'],,,,,,,,"['HttpCore', 'HttpCore NIO']","['HttpCore components', 'HttpCore NIO extensions']",,HttpClient to support HTTP 2.0,3,1,,yairogen,True,olegk,yairogen
httpcomponents-core,HTTPCORE-380,2014-05-07T07:01:58.000+0000,2014-11-26T20:35:56.000+0000,2016-11-25T13:02:11.000+0000,,Fixed,New Feature,Major,['5.0-alpha1'],['4.4-alpha1'],,,,,,,"['HttpCore', 'HttpCore NIO']","['HttpCore components', 'HttpCore NIO extensions']","Hi All,

I have created a fix to support HTTP DELETEs with entity bodies. In HTTP Core, the HttpRequest object (BasicHttpRequest or BasicHttpEntityEnclosingRequest) is made by looking at the HTTP method. But sometimes an API could accept HTTP Deletes with payloads for example [1]. To support this I have introduced a new type of HTTP requests ""HttpEntityPossibleEnclosingRequest"". By calling the ""getFinalHttpRequest"" method in this class, the proper HTTP request object can be made, i.e. HttpRequest or HttpEntityEnclosingRequest. The logic is if the HttpEntityPossibleEnclosingRequest has the CONTENT_LENGTH header, an HttpEntityEnclosingRequest is made and an HttpRequest is made otherwise.

Note. the spec does not prohibit an HTTP Delete having a payload [2] [3].

[1] http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/docs-delete-by-query.html
[2] http://stackoverflow.com/a/299696/1411653
[3] http://www.w3.org/Protocols/rfc2616/rfc2616-sec9.html#sec9.7

Thanks,
Jeewantha",Support for HTTP Delete with entity body,6,,,jeewamp,True,,jeewamp
httpcomponents-core,HTTPCORE-365,2013-11-25T22:11:33.000+0000,2014-06-02T15:37:20.000+0000,2014-06-02T15:38:04.000+0000,,Fixed,New Feature,Minor,['4.4-alpha1'],['4.3'],,,,,,,['HttpCore'],['HttpCore components'],"When we write e.g.:
HttpHost proxy = new HttpHost(""my-proxy:8080"");

HttpHost interprets it as a hostname=""my-proxy:8080"" and port=-1. I believe it should be able to parse the exemplified ""my-proxy:8080"" correctly.",HttpHost(hostname) should be able to parse format hostname:port,3,,,remisbaima,True,,remisbaima
httpcomponents-core,HTTPCORE-341,2013-05-27T15:40:24.000+0000,2013-06-05T08:57:13.000+0000,2013-10-05T20:09:14.000+0000,,Fixed,New Feature,Critical,['4.3'],"['4.2.2', '4.3']",,,,,,,,,"Currently the AbstractNIOConnPool is capable to handle timeouts for en-queued requests (requests added in leasingRequests connection).
What AbstractNIOConnPool - deal with slow connections, that already are open and being executed. Slow connections are connections that are not completed withing some request processing timeout.

Example. Consider a specific case, when a remote server holds a connection open, but transmits information slowly (e.g. 8 bytes in each 100ms). In application under the high load it means that we would stuck with no free connections to process new requests for a long period of time.

In such cases, as a developer I want to specify time in which we would close current open connection, even if it is already processing the data.",As a developer I want NIO Pool to close 'slow' http requests automatically,1,,['ha'],ignatalexeyenko,True,,ignatalexeyenko
httpcomponents-core,HTTPCORE-340,2013-05-27T14:27:39.000+0000,2013-05-28T08:55:04.000+0000,2013-10-05T20:09:18.000+0000,,Fixed,New Feature,Critical,['4.3'],"['4.2.2', '4.3-beta2']",,,,,,,['HttpCore NIO'],['HttpCore NIO extensions'],"The component httpasyncclient (class PoolingClientAsyncConnectionManager) uses httpcompnents-core (class AbstractNIOConnPool).

In specific cases - such as remote server has slow response time (or just do not respond anything for a while), there could be a scenario, when AbstractNIOConnPool deletes request from leasingRequests which are not yet started.

As a developer I want to have ability to specify leaseTimeout which would control how long request would be kept in leasingRequests unit it would be deleted.
",AbstractNIOConnPool to support lease timeout,2,,,ignatalexeyenko,True,,ignatalexeyenko
httpcomponents-core,HTTPCORE-311,2012-09-14T21:02:23.000+0000,2012-10-25T21:13:11.000+0000,2013-10-05T20:09:17.000+0000,,Won't Fix,New Feature,Major,,['4.2.1'],,,,,,,['HttpCore'],['HttpCore components'],"The org.apache.http.pool.ConnPoolControl interface exposes valuable runtime metrics for a connection pool's individual routes and global values. However, it uses a bean class, org.apache.http.pool.PoolStats, to exposes the global values. This is problematic when trying to monitor these connection pool values at runtime since PoolStats is not Serializable and many jmx monitoring agents only support primitive values well. The value of the PoolStats class seems to be in trying to minimize the amount of locking of the pool required to get stats.

The attached patch includes an interface that extends ConnPoolControl to add four methods to return the primitive value for global pool stats. There's an implementation of this interface that will delegate all of the ConnPoolControl methods to a delegate instance, and support retrieving the primitive global values by periodically retrieving and caching a PoolStats instance internally. This minimizes the amount of locking required while also making it easy to monitor these values.

It would probably be ideal to incorporate this functionality directly into the existing ConnPoolControl interface, but the attached code provides a non-invasive way to achieve the desired effect.",Retrieval of primitive connection pool stat values,2,,,sethweiner,True,,sethweiner
httpcomponents-core,HTTPCORE-232,2010-09-08T11:12:57.000+0000,2010-09-08T20:26:24.000+0000,2011-01-25T11:00:41.000+0000,,Fixed,New Feature,Trivial,['4.1'],"['4.0.1', '4.1-alpha1', '4.1-beta1', '4.1-beta2']",,,,,,,['HttpCore'],['HttpCore components'],"There is a constant file, HttpStatus, that contains all the HTTP status codes.  It would be nice if there was such a file for HTTP headers as well.",Add a constants file that contained HTTP header names,,,,lajoie@itumi.biz,True,,lajoie@itumi.biz
httpcomponents-core,HTTPCORE-230,2010-07-14T04:24:05.000+0000,2010-07-15T20:10:38.000+0000,2010-09-07T12:39:28.000+0000,,Fixed,New Feature,Major,['4.1-beta2'],,,,,,,,['HttpCore'],['HttpCore components'],"I found this issue HTTPCORE-181 raised before.
I got the similar requirement that we want to measure all kinds of time-related data to get the performance report such as:
1. DNS parsing time
2. Connection setup time
2. header parsing time
3. body transfer time
...
I found that there are a lot of ""private"" fields which have no setter method in this component, this makes us hard to extend the functionality.
For example in org.apache.http.impl.io.AbstractSessionInputBuffer#init() method, it would be better to change ""this.metrics = new HttpTransportMetricsImpl();"" to ""this.metrics = createMetrics()"" (a protected method） which subclasses can easily override this if they want to get an extended HttpTransportMetricsImpl instance.

Can some one look at this feature?",Need time related metrics data,,,,colin,True,,colin
httpcomponents-core,HTTPCORE-183,2009-02-03T22:47:50.000+0000,2009-07-11T15:57:28.000+0000,2010-09-07T12:39:14.000+0000,,Fixed,New Feature,Major,['4.1-alpha1'],,,,,,,,['HttpCore'],['HttpCore components'],Add thread-safe implementations of HttpParams and HttpProcessor,Add thread-safe implementations of HttpParams and HttpProcessor,,1,,olegk,True,,olegk
httpcomponents-core,HTTPCORE-150,2008-02-23T21:19:32.000+0000,2008-04-14T14:35:22.000+0000,2009-02-09T21:38:15.000+0000,,Fixed,New Feature,Minor,['4.0-beta2'],,,,,,,,['HttpCore'],['HttpCore components'],"We need an entity that serializes a Java object, at least in contrib if not in proper.
Every few months there's somebody asking for it. Should be a nice programming exercise.
Deserializing would be nice as well, but seems less important.

If we put it in core, it's available for everybody but has to use java.io.Serializable.
If we put it in client, it could use generics. Don't know if that is useful in this case.

cheers,
  Roland
",SerializerEntity,,,,rolandw,True,,rolandw
httpcomponents-core,HTTPCORE-148,2008-02-15T17:49:31.000+0000,2008-02-24T20:42:41.000+0000,2009-02-09T21:38:15.000+0000,,Fixed,New Feature,Major,['4.0-beta2'],['4.0-beta1'],,,,,,,['HttpCore NIO'],['HttpCore NIO extensions'],"Attached is a patch for AsyncNHttpServiceHandler.  It actually works (as tested by running & hitting it with IE.)  :)

To test, run the example 'AsyncNHttpFileServer' in the examples directory or the TestAsyncNHttpHandlers test.",Create AsyncNHttpServiceHandler & AsyncNHttpClientHandler,,,,sberlin,True,olegk,sberlin
httpcomponents-core,HTTPCORE-131,2007-12-12T16:39:38.000+0000,2008-01-07T22:00:13.000+0000,2009-02-09T21:38:13.000+0000,,Won't Fix,New Feature,Minor,['4.0-beta1'],['4.0-alpha6'],,,,,,,['HttpCore'],['HttpCore components'],"Johannes Koch suggested on the dev@ list that there should be a way to access the keys in the HttpParams interface. Due to the nested, linked, or stacked nature of HttpParams implementations, a simple iterator is not feasible. However, collecting keys in a set could be implemented:

HttpParams.collectKeys(Set keySet)

This would affect all implementations in HttpCore and HttpClient.
",give access to HttpParams keys,,,,rolandw,True,,rolandw
httpcomponents-core,HTTPCORE-125,2007-10-26T17:46:09.000+0000,2008-09-08T13:27:12.000+0000,2009-02-09T21:38:13.000+0000,,Fixed,New Feature,Minor,['4.0-beta3'],,,,,,,,"['HttpCore', 'HttpCore NIO']","['HttpCore components', 'HttpCore NIO extensions']","Make the HTTPcore, HTTPcore NIO and HTTPcore NIOSSL libraries available as one (or three) OSGi bundles. See also the wiki:  http://wiki.apache.org/jakarta-httpclient/HttpComponentsAndOSGi",Package HTTP Core as OSGi bundle,1,,,misterb,True,,misterb
httpcomponents-core,HTTPCORE-111,2007-08-27T23:35:50.000+0000,2007-08-29T15:01:47.000+0000,2007-08-29T15:01:47.000+0000,,Invalid,New Feature,Minor,,['4.0-alpha5'],,,,,,,['HttpCore NIO'],['HttpCore NIO extensions'],"Following your example for the basic, non-blocking HTTP server, I am trying to simulate the info that you would get from the standard apache httpd daemon, e.g.  remote IP, URI and return code that is typically found in access.log.

First, I tried getting this information from the EventLogger/EventListener

            if (conn instanceof DefaultNHttpServerConnection)
            {
            	DefaultNHttpServerConnection d = (DefaultNHttpServerConnection) conn;
            	logger.info(""connection from "" + d.getRemoteAddress().getHostAddress());
            	logger.info(""requested: "" + d.getHttpRequest().getRequestLine().getUri());
            	logger.info(""responded: "" + d.getHttpResponse().getStatusLine().getStatusCode());
            }

... but this causes some IO/Exception

Second, I tried logging this from the handle method in your HttpRequestHandler, but I do not see how I can get the remote IP address.

Seems like I can get the remote IP from the eventlistener, but not the URI and status code.

Or I can get the URI and status code from the httprequesthandler, but not the remote IP.

Thanks.","Provide method to get remote_ip, URI and status",,,,siehhube,True,,siehhube
httpcomponents-core,HTTPCORE-68,2007-04-23T22:42:09.000+0000,2007-12-08T17:50:20.000+0000,2009-02-09T21:38:12.000+0000,,Fixed,New Feature,Major,['4.0-beta1'],,,,,,,,,,"Provide an interface for entities that support event based processing.

Related: HTTPCORE-43 
",Provide NIO entity,,,,spingel,True,,spingel
httpcomponents-core,HTTPCORE-34,2007-01-28T12:22:48.000+0000,2007-01-29T16:29:20.000+0000,2007-01-29T16:29:20.000+0000,,Won't Fix,New Feature,Major,,['4.0-alpha3'],,,,,,,['HttpCore'],['HttpCore components'],"For connection management, a callback is needed that indicates when an incoming entity (stream) is closed or has been read to the end.
These events indicate that the connection can be (auto-) released.

A candidate for implementing this callback are the various InputStream classes in org.apache.http.io.
We probably should have a common base interface with methods for registering a callback handler.
",need callback on entity/stream close or EOF,,,,rolandw,True,rolandw,rolandw
httpcomponents-core,HTTPCORE-32,2007-01-27T15:48:28.000+0000,2007-02-04T14:07:35.000+0000,2007-11-11T19:21:49.000+0000,,Fixed,New Feature,Minor,['4.0-alpha4'],['4.0-alpha3'],,,,,,,['HttpCore'],['HttpCore components'],"HttpClient interface should expose the lists of request and response interceptors in an HttpProcessor to allow for programmatic modification. Currently, there is no interface that allows for modification, only the implementation class BasicHttpProcessor. Interfaces should be defined in HttpCore rather than HttpClient so they can be implemented directly by BasicHttpProcessor. Suggested names:
HttpRequestInterceptorList
HttpResponseInterceptorList

cheers,
  Roland
",provide interface to manipulate interceptor lists,,,,rolandw,True,rolandw,rolandw
httpcomponents-core,HTTPCORE-19,2006-11-30T17:25:15.000+0000,2006-12-05T22:01:30.000+0000,2007-11-11T19:26:12.000+0000,,Won't Fix,New Feature,Major,,['4.0-alpha2'],,,,,,,['HttpCore'],['HttpCore components'],PlainSocketFactory class doesn't support creation of socket with timeout! = 0. We defined our own SocketFactory that supports timeout.,PlainSocketFactory class doesn't support creation of socket with timeout! = 0. We defined our own SocketFactory that supports timeout,,,,flash,True,,flash
httpcomponents-core,HTTPCORE-18,2006-11-30T17:21:36.000+0000,2006-11-30T17:36:15.000+0000,2006-11-30T17:36:15.000+0000,,Won't Fix,New Feature,Minor,,['4.0-alpha2'],,,,,,,['HttpCore'],['HttpCore components'],"We extended the ProxyHost class to contain equals method in order to compare between ProxyHost and HttpHost 
instances (for pooling issues).",We extended the ProxyHost class to contain equals method in order to compare between ProxyHost and HttpHost instances (for pooling issues).,,,,flash,True,,flash
httpcomponents-core,HTTPCORE-17,2006-11-30T17:15:43.000+0000,2006-11-30T17:26:50.000+0000,2006-11-30T17:26:50.000+0000,,Won't Fix,New Feature,Minor,,['4.0-alpha2'],,,,,,,['HttpCore'],['HttpCore components'],"In RequestContent class, if one of the headers TRANSFER_ENCODING or CONTENT_LEN already exist 
a ProtocolException is thrown. We removed the exception because if one of these headers exists we want to do
nothing, just forward it as it is","In RequestContent class, if one of the headers TRANSFER_ENCODING or CONTENT_LEN already exist a ProtocolException is thrown. We removed the exception because if one of these headers exists we want to do nothing, just forward it as it is",,,,flash,True,,flash
httpcomponents-core,HTTPCORE-15,2006-11-20T14:00:49.000+0000,2006-11-21T19:58:59.000+0000,2007-11-11T19:22:37.000+0000,,Fixed,New Feature,Minor,['4.0-alpha3'],['4.0-alpha3'],,,,,,,['HttpCore'],['HttpCore components'],Currently the API does not provide a way for implementations to find out the remote IP address of a request,Provide a way to access IP Address of the remote client making a request,,,,asankha,True,olegk,asankha
httpcomponents-core,HTTPCORE-4,2006-05-19T08:31:53.000+0000,2006-06-02T19:03:40.000+0000,2007-11-11T19:23:30.000+0000,,Fixed,New Feature,Major,['4.0-alpha2'],['4.0-alpha1'],,,,,,,['HttpCore'],['HttpCore components'],"It would be desirable to be able to specify limits in the parsing of HTTP messages, so that impractically large content (inadvertently or maliciously) fails in a manageable way, rather than triggering an OutOfMemoryError. 

One possibility would be to set limits on HTTP header line lengths and number of headers; once exceeded, an exception would be thrown.  

Another would be to set a byte-total cap on how much content can be considered to contribute to the headers; past that cap, an exception would be thrown.

A possible wrinkle would be implementing compatible limits at other places mid- or late-message where unbounded numbers of headers could again appear (multipart; chunked; footers). 

See also:

 http://issues.apache.org/jira/browse/HTTPCORE-3?page=all

 http://issues.apache.org/bugzilla/show_bug.cgi?id=25468

",feature request: optional header limits to contain OOME risks,1,,,gojomo,True,olegk,gojomo
commons-io,IO-608,2019-05-27T12:14:12.000+0000,2019-05-27T12:19:46.000+0000,2019-05-27T12:19:46.000+0000,,Fixed,New Feature,Major,['2.7'],,,,,,,,['Streams/Writers'],[''],"We have a handy {{NullOutputStream}}.
 
I'd like to add a convenience {{NullPrintStream}}, which would dead simple:
 
{code:java} 
/**
 * This PrintStream writes all data to the famous <b>/dev/null</b>.
 * <p>
 * This print stream has no destination (file/socket etc.) and all bytes written to it are ignored and lost.
 * </p>
 * @since 2.7
 */
public class NullPrintStream extends PrintStream {

    public static final NullPrintStream NULL_PRINT_STREAM = new NullPrintStream();

    @SuppressWarnings(""resource"")
    public NullPrintStream() {
        super(new NullOutputStream());
    }

}
{code}",Add a convenience NullPrintStream,1,,,ggregory,True,ggregory,ggregory
commons-io,IO-607,2019-05-16T17:58:11.000+0000,,2019-05-17T14:43:14.000+0000,,,New Feature,Major,,,,,,,,,,,Update from Java 7 to Java 8.,Update from Java 7 to Java 8,3,1,,ggregory,True,,ggregory
commons-io,IO-596,2018-12-26T23:16:22.000+0000,,2018-12-27T14:54:43.000+0000,,,New Feature,Major,,,2400,2400,,,,100,['Utilities'],[''],"Particularly for Windows environments where deleting files can turn into a hassle due to the inability to remove open files, I would like to provide a file deletion utility class that has the ability to retry deletions with exponential backoff, force override file attributes to allow for deletion, and deleting as much as possible while accumulating errors.

This functionality was originally inspired by and based on functionality in Jenkins hudson.Util class.",Add DeleteFiles utility class for more robust file deletion strategies,1,,,mattsicker,True,mattsicker,mattsicker
commons-io,IO-595,2018-12-21T17:53:43.000+0000,,2018-12-21T21:37:34.000+0000,,,New Feature,Major,,,,,,,,,['Utilities'],[''],"There are several utility methods in FileUtils that are not present in the Java 7 Files API for Path. These would make up a useful basis for a PathUtils class to complement Java's java.nio.file package. Below is a listing of method ideas adapted from FileUtils for Path that are not already covered by java.nio.file.Files or trivially adapted to something there.
 * touch(Path)
 * contentEquals(Path, Path)
 * copyDirectoryToDirectory(Path, Path, CopyOption...)
 * copyDirectory(Path, Path, CopyOption...)
 * copyDirectory(Path, Path, PathMatcher, CopyOption...)
 * deleteDirectory(Path)
 * deleteQuietly(Path)
 * directoryContains(Path, Path)
 * cleanDirectory(Path)
 * waitFor(Path, long, TimeUnit)
 * forceDelete(Path)
 * forceDeleteOnExit(Path)
 * forceMkdir(Path)
 * forceMkdirParent(Path)
 * sizeOfDirectory(Path) - probably just return BigInteger only",Add similar util class as FileUtils for java.nio.file.Path,2,,,mattsicker,True,,mattsicker
commons-io,IO-583,2018-07-30T00:32:51.000+0000,2018-07-30T15:53:01.000+0000,2018-07-30T15:53:01.000+0000,,Duplicate,New Feature,Major,,"['2.6', '2.7', '3.x']",,,3600,3600,3600,,"['Filters', 'Streams/Writers', 'Utilities']","['', '', '']","Please add OWASP Dependency Check to the build (pom.xml). OWASP DC makes an outbound REST call to MITRE Common Vulnerabilities & Exposures (CVE) to perform a lookup for each dependant .jar to list any/all known vulnerabilities for each jar. This step is needed because a manual MITRE CVE lookup/check on the main component does not include checking for vulnerabilities in components or in dependant libraries.

OWASP Dependency check : https://www.owasp.org/index.php/OWASP_Dependency_Check has plug-ins for most Java build/make types (ant, maven, ivy, gradle).

Also, add the appropriate command to the nightly build to generate a report of all known vulnerabilities in any/all third party libraries/dependencies that get pulled in. example : mvn -Powasp -Dtest=false -DfailIfNoTests=false clean aggregate

Generating this report nightly/weekly will help inform the project's development team if any dependant libraries have a reported known vulnerailities. Project teams that keep up with removing vulnerabilities on a weekly basis will help protect businesses that rely on these open source componets.",Please add OWASP Dependency Check to the build (pom.xml),1,,,ABakerIII,True,,ABakerIII
commons-io,IO-577,2018-05-21T21:09:14.000+0000,2018-05-22T16:08:15.000+0000,2018-06-07T16:24:10.000+0000,,Fixed,New Feature,Major,['2.7'],,,,,,,,['Filters'],[''],"Add readers to filter out given characters,  handy to remove known junk characters from CSV files for example. Please see attached.",Add readers to filter out given characters: CharacterSetFilterReader and CharacterFilterReader.,3,,,ggregory,True,ggregory,ggregory
commons-io,IO-574,2018-03-14T14:41:19.000+0000,,2018-04-11T20:48:00.000+0000,,,New Feature,Major,,['2.6'],,,,,,,['Utilities'],[''],"Add the capability to serialize Java primitives to OutputStream classes.

 
{code:java}
public static void writeInt(OutputStream out, int v) throws IOException {
  out.write((byte) (0xff & (v >> 24)));
  out.write((byte) (0xff & (v >> 16)));
  out.write((byte) (0xff & (v >> 8)));
  out.write((byte) (0xff & v));
}{code}
 

[https://github.com/apache/hbase/blob/c3b4f788b16ac4e0e8cfd319f495308ba4d158f5/hbase-common/src/main/java/org/apache/hadoop/hbase/io/util/StreamUtils.java#L215-L243]

[https://stackoverflow.com/questions/6374915/java-convert-int-to-byte-array-of-4-bytes]

 

Could also create a Serialize object that has an internal buffer that the bytes are written to before calling {{out.write(buf)}} once.",Add Writes of Primitive Values To OutputStream,2,,,belugabehr,True,,belugabehr
commons-io,IO-566,2018-01-01T23:18:17.000+0000,,2018-01-01T23:45:01.000+0000,,,New Feature,Minor,,['2.7'],,,,,,,['Streams/Writers'],[''],"Create a {{ByteArrayOutputStream}} implementation that is not synchronized.

https://github.com/apache/hbase/blob/d276c3275e952f68d0b53953c29297c57c73fa62/hbase-common/src/main/java/org/apache/hadoop/hbase/io/ByteArrayOutputStream.java",Add Non Synchronized ByteArrayOuputStream,1,,,belugabehr,True,,belugabehr
commons-io,IO-548,2017-08-14T16:02:43.000+0000,,2017-08-14T18:07:33.000+0000,,,New Feature,Minor,,['2.5'],,,,,,,['Utilities'],[''],"Please two new versions of {{FileUtils#readFileToByteArray}}:

# Read from an offset in the file, up to a certain length
{{byte[] FileUtils#readFileToByteArray(long fileOffset, int fileLength}}
# Read directly into an existing buffer from a given file offset and buffer length
{{void FileUtils#readFileToByteArray(byte[] buf, long fileOffset, int fileLength, int bufOffset, int bufLength}}","Add FileUtils#readFileToByteArray Offset,Size",2,,,belugabehr,True,,belugabehr
commons-io,IO-539,2017-06-06T19:29:47.000+0000,,2018-01-01T23:18:35.000+0000,,,New Feature,Major,,['2.5'],,,,,,,['Streams/Writers'],[''],Consume code from Apache Hadoop {{org.apache.hadoop.hive.common.io.NonSyncByteArrayInputStream}} and make it more available to a broader audience.,Add NonSyncByteArrayInputStream Implementation,1,,,belugabehr,True,,belugabehr
commons-io,IO-538,2017-06-05T21:58:38.000+0000,,2018-01-01T23:18:58.000+0000,,,New Feature,Minor,,['2.5'],,,,,,,['Streams/Writers'],[''],"There currently exists {{StringBuilderWriter}}. This implementation, as an alternative to java.io.StringWriter, provides an un-synchronized (i.e. for use in a single thread) implementation for better performance.

It would be nice if there way one for CharArrayWriter as well.

https://www.igorkromin.net/index.php/2015/08/20/java-stringwriter-vs-chararraywriter-which-one-to-choose-for-performance/",Add Non Synchronized CharArrayWriter ,2,,,belugabehr,True,,belugabehr
commons-io,IO-533,2017-04-23T14:58:03.000+0000,,2017-04-23T17:23:09.000+0000,,,New Feature,Major,,,,,,,,,,,suggested in https://github.com/apache/commons-io/pull/32,Introduce Tailable interface,2,1,,pascalschumacher,True,,pascalschumacher
commons-io,IO-532,2017-04-23T10:36:34.000+0000,,2017-04-23T10:37:02.000+0000,,,New Feature,Major,,,,,,,,,,,,DirectoryUtils - isEqual to compare directories,1,,,pascalschumacher,True,,pascalschumacher
commons-io,IO-529,2017-02-20T10:03:18.000+0000,,2017-02-20T10:03:18.000+0000,,,New Feature,Major,,,,,86400,86400,86400,,['Streams/Writers'],[''],"I really enjoy using the FileWriterWithEncoding that you offer.

I feel it would be very helpful if there was a simple FileReaderWithEncoding providing similar syntactic sugar.",FileReaderWithEncoding,1,,,jraue,True,,jraue
commons-io,IO-527,2017-01-10T02:45:40.000+0000,,2017-01-10T02:45:54.000+0000,,,New Feature,Major,,,,,,,,,['Streams/Writers'],[''],"Create two Rolling File Output Streams.  They write to a single file and given a certain circumstance, they stop writing to the file, open a new file, and continue writing there.

# Based on date
# Based on max file size",Rolling File Output Stream,1,,,belugabehr,True,,belugabehr
commons-io,IO-521,2016-11-08T20:07:33.000+0000,2016-12-01T21:12:37.000+0000,2016-12-01T21:12:37.000+0000,,Fixed,New Feature,Major,,,,,,,,,,,suggeted in https://github.com/apache/commons-io/pull/8,Add InfiniteCircularInputStream,1,,,pascalschumacher,True,,pascalschumacher
commons-io,IO-519,2016-11-08T18:11:43.000+0000,2016-11-08T18:11:56.000+0000,2017-10-20T06:49:05.000+0000,,Fixed,New Feature,Major,['2.6'],,,,,,,,['Streams/Writers'],[''],added with https://github.com/apache/commons-io/commit/e180578289a5c65e19e430bd3300a72c9ba97a9b,Add MessageDigestCalculatingInputStream,1,,,pascalschumacher,True,jochen@apache.org,pascalschumacher
commons-io,IO-518,2016-11-08T18:09:57.000+0000,2016-11-08T18:10:16.000+0000,2017-10-20T06:49:10.000+0000,,Fixed,New Feature,Major,['2.6'],,,,,,,,['Streams/Writers'],[''],added with https://github.com/apache/commons-io/commit/e180578289a5c65e19e430bd3300a72c9ba97a9b,Add ObservableInputStream,1,,,pascalschumacher,True,jochen@apache.org,pascalschumacher
commons-io,IO-513,2016-08-22T23:43:46.000+0000,2016-11-23T07:00:10.000+0000,2017-10-20T06:49:06.000+0000,,Fixed,New Feature,Minor,['2.6'],,,,,,,,['Utilities'],[''],"Add convenience methods to {{IOUtils}} for reading class path resources and returning them as {{String}}, {{byte[]}}, and {{URL}} respectively.

Github PR: https://github.com/apache/commons-io/pull/17",Add convenience methods for reading class path resources,5,,"['beginner', 'features', 'github-import', 'newbie']",Thorium,True,ggregory,Thorium
commons-io,IO-510,2016-06-20T14:21:06.000+0000,,2018-12-28T22:03:07.000+0000,,,New Feature,Major,,,,,,,,,['Streams/Writers'],[''],"Create a ""ReadAheadInputStream"".  Such a feature would essentially be a BufferedInputStream that spawns a thread for asynchronous reading.  The thread would continuously fill the buffer (if there was space) from the underlying InputStream.  Thread would wait if buffer was full.",ReadAheadInputStream,2,,,belugabehr,True,,belugabehr
commons-io,IO-507,2016-05-07T00:03:36.000+0000,2016-05-07T00:05:23.000+0000,2017-10-14T11:52:16.000+0000,,Fixed,New Feature,Major,['2.6'],,,,,,,,['Utilities'],[''],Add a ByteOrderFactory class to parse Strings and return JRE ByteOrder instances.,Add a ByteOrderUtils class,3,,,ggregory,True,ggregory,ggregory
commons-io,IO-493,2016-01-02T21:59:50.000+0000,2016-12-01T21:45:22.000+0000,2017-10-20T06:49:07.000+0000,,Fixed,New Feature,Trivial,['2.6'],,,,,,,,['Streams/Writers'],[''],"it would be useful to have infinite streams. for example to test how your code behaves with too long input, can it stop reading input before it ends etc. 

Probably in most cases same thing can be done using finite stream that is long enough but it's easier to write and maintain code using an infinite stream rather than calculate after each code change how long input is required

Created [pull request 8|https://github.com/apache/commons-io/pull/8]",Add infinite circular input stream,1,,,piotr.turski,True,pascalschumacher,piotr.turski
commons-io,IO-464,2014-12-08T00:13:09.000+0000,,2014-12-08T23:54:14.000+0000,,,New Feature,Major,,,,,,,,,,,"The iCal format requires long lines to be wrapped.
This is indicated by starting the next line with a single space

Java manifest files use the same syntax

There are probably other instances of this syntax.

This syntax is trivial to generate, but not as easy to read, as there is only indication of the continuation is on the next line. This means that it is necessary to use look-ahead to determine if the current line has a continuation.

It would be useful to provide a Reader to support this.",Reader to support continuation lines,,,,sebb,True,,sebb
commons-io,IO-459,2014-10-27T08:49:29.000+0000,2014-10-27T23:40:33.000+0000,2014-10-27T23:40:33.000+0000,,Fixed,New Feature,Major,['2.5'],,,,,,,,['Streams/Writers'],[''],"The enclosed patch adds these files and test cases. 

There is a use-case for writing these both as InputStream and OutputStream.

Without any concerns of existing use-cases, the OutputStream is probably the most ""correct"" implementation. These two classes originate from apache maven, and existing layering made it much more practical to do this filtering on the ""Input"" side of the stream pipeline, which mandates an inputstream based solution. There should be no technical difference between these, and it should be trivial to adapt these to output streams. This patch only contains InputStream versions

",Add WindowsLineEndingInputStream and UnixLineEndingInputStream,2,,,krosenvold,True,olamy,krosenvold
commons-io,IO-457,2014-10-15T19:07:39.000+0000,2014-10-27T23:10:46.000+0000,2014-10-27T23:10:46.000+0000,,Fixed,New Feature,Major,['2.5'],,,,,,,,['Streams/Writers'],[''],"The enclosed patch contains a BoundedReader, a wrapper that can be used to constrain access to an underlying stream when used with mark/reset - to avoid overflowing the mark limit of the underlying buffer.

This code is originally written by me in the maven project",Add BoundedReader,2,,,krosenvold,True,olamy,krosenvold
commons-io,IO-454,2014-07-31T02:24:54.000+0000,,2015-12-10T18:49:08.000+0000,,,New Feature,Minor,,,,,,,,,['Utilities'],[''],"Hello,

I'd like to see an IOUtils method that calls skip (or skip/read) on a stream until it returns -1.

Thanks.",IOUtils.skipToEnd,4,,,belugabehr,True,,belugabehr
commons-io,IO-446,2014-05-23T21:20:53.000+0000,2015-08-06T13:00:01.000+0000,2015-08-06T13:00:01.000+0000,,Fixed,New Feature,Minor,['2.5'],,,,,,,,['Utilities'],[''],My use case is tailing a log and publishing to a websocket.  I found that using the handleLine method resulted in too many messages being published.  Using a buffer in my listener and only publishing on endOfFileReached was a really simple solution.,adds an endOfFileReached method to the TailerListener,2,,['patch'],barrusj,True,krosenvold,barrusj
commons-io,IO-434,2014-04-09T03:33:46.000+0000,,2014-04-09T03:33:46.000+0000,,,New Feature,Minor,,,,,,,,,['Utilities'],[''],"Add a capability to perform an operation analogous to a linux ""grep"" on files and streams. ",Add Grep Capabilities,1,,,belugabehr,True,,belugabehr
commons-io,IO-432,2014-04-01T03:54:58.000+0000,,2014-04-01T22:05:46.000+0000,,,New Feature,Minor,,,,,,,,,['Streams/Writers'],[''],"Create an InputStream that restricts the underlying stream based on an arbitrary sequence of bytes.

A good start place would be:
https://android.googlesource.com/platform/packages/apps/UnifiedEmail/+/kitkat-release/src/org/apache/james/mime4j/MimeBoundaryInputStream.java

Though change it to take a byte[] boundary and ignore line-feeds",Boundary InputStream,2,,,belugabehr,True,,belugabehr
commons-io,IO-431,2014-04-01T01:16:25.000+0000,,2014-04-01T01:17:07.000+0000,,,New Feature,Minor,,,,,,,,,['Utilities'],[''],"Add a way to check for an arbitrary byte pattern in an array, in a stream, in a file, etc.

http://stackoverflow.com/questions/1507780/searching-for-a-sequence-of-bytes-in-a-binary-file-with-java

Consider a new ByteUtils ??",Finding A Sequence Of Bytes,1,,,belugabehr,True,,belugabehr
commons-io,IO-427,2014-03-05T13:47:37.000+0000,,2015-11-22T07:46:03.000+0000,,,New Feature,Minor,,,,,,,,,['Streams/Writers'],[''],"A new input stream for reading data with fixed length trailers/suffixes. This was briefly discussed on the ML: http://markmail.org/thread/mh4elj7a53mdus2h.

Since I don't often write input streams and haven't contributed to this project before, I offer this up for review to see if it's a welcome addition.",New TrailerInputStream class,2,,['patch'],dmjones500,True,,dmjones500
commons-io,IO-426,2014-02-06T15:49:58.000+0000,2014-02-06T16:02:50.000+0000,2016-11-08T17:58:52.000+0000,,Fixed,New Feature,Major,['2.5'],,,,,,,,['Utilities'],[''],Add API IOUtils.closeQuietly(Closeable...),Add API IOUtils.closeQuietly(Closeable...),1,,,ggregory,True,ggregory,ggregory
commons-io,IO-425,2014-02-04T19:50:39.000+0000,2015-06-23T05:59:32.000+0000,2015-06-23T05:59:32.000+0000,,Fixed,New Feature,Major,['2.5'],,,,,,,,,,"We have a class that uses ThresholdingOutputStream as its super class.  Our class writes to files on the file system and switches to a new file when a size threshold has been reached.  If our system restarts we would like to be able to continue writing to the last file, but for this to work we have to be able to set the private var written in ThresholdingOutputStream.  This patch adds a protected setter method so written can be set to match the file size.",Setter method for ThresholdingOutputStream,2,,,cswank,True,krosenvold,cswank
commons-io,IO-419,2013-12-20T03:31:12.000+0000,,2013-12-20T15:57:40.000+0000,,,New Feature,Minor,,,,,,,,,['Streams/Writers'],[''],"Create a FilterInputStream that removes invalid characters from an XML stream.  Perhaps the InputStream keeps a count of how many bytes it removes.

Invalid XML characters...
http://www.w3.org/TR/xml/#charsets",InputStream That Removes Invalid XML Characters,1,,,belugabehr,True,,belugabehr
commons-io,IO-412,2013-11-29T05:17:51.000+0000,,2013-12-15T04:15:38.000+0000,,,New Feature,Minor,,,,,,,,,['Streams/Writers'],[''],"Feel free to include this new FileOutputstream.  No unit tests yet.  Looking for feedback at this point.

{code}
/**
 * This is a wrapper around a FileOutputStream. It enables a stream to be
 * written to a hidden file in a directory and only then renamed to the final
 * destination if no exceptions occur. This implementation marks the file as
 * hidden by pre-appending the file name with a period. If any exceptions occur
 * during writing, the hidden file will be deleted once close() is called. This
 * class takes advantage of {@link File#renameTo(File)}; therefore, many aspects
 * of the behavior of this class are inherently platform-dependent: The rename
 * operation might not be able to move a file from one filesystem to another, it
 * might not be atomic, and it might not succeed if a file with the destination
 * abstract pathname already exists. This class is most useful when the rename
 * operation is atomic. It is designed for the case when one process is writing
 * to a directory and another process is polling the directory for files to
 * read.
 */
{code}",AllOrNothingOutputStream,2,,,belugabehr,True,,belugabehr
commons-io,IO-394,2013-07-26T01:10:58.000+0000,,2013-07-26T01:11:16.000+0000,,,New Feature,Minor,,,,,,,,,['Streams/Writers'],[''],"Hello,

Attached is a ""ReOpen"" package that I have used with much success.  In particular, I have combined it with the DeferredFile (very handy) class.

These files allow for a user to use an InputStream, close the stream, then re-open it from the start, all very seamlessly.",[IO] ReOpenInputStream,1,,,belugabehr,True,,belugabehr
commons-io,IO-393,2013-07-24T02:24:16.000+0000,,2013-07-24T02:31:53.000+0000,,,New Feature,Minor,,,,,,,,,['Streams/Writers'],[''],"I have included, with this ticket, an implementation of a FileBasedInputStream.  It can be used as a generic wrapper for a FileInputStream and ByteArrayInputStream.  I use it with Spring-Batch as a way to allow an ""Item Reader"" to load files into memory or leave it on disk.

{code}
public static FileBasedInputStream getFileBasedInputStream(File file)
{
InputStream is = null;
DeferredFileOutputStream dfos = new DeferredFileOutputStream(1024, file);

InputStream is = null;
IoUtils.copy(in, dfos);

if (dfos.ifIsInMemory())
{
    is = new ByteInputStream(dfos.getData());
}
else
{
    is = new DeferredFileInputStream(dfos.getFile());
}

IoUtils.closeQuietly(dfos);

return new FileBasedInputStream(file, is);
}

void test1(OutputStream os)
{
FileBasedInputStream fbis = getFileBasedInputStream(new File(""/usr/local/test.txt""));

IoUtils.copy(fbis, os);

fbis.close();
fbis.getFile.delete();

}

void test2(OutputStream os)
{
FileBasedInputStream fbis = getFileBasedInputStream(new File(""/usr/local/test.txt""));


for (int i = 0; i < 10; i++)
{
   IoUtils.copy(fbis, os);
   fbis.reset();
}

fbis.close();
fbis.getFile.delete();

{code}",[IO] FileBasedInputStream,1,,,belugabehr,True,,belugabehr
commons-io,IO-392,2013-07-23T04:01:37.000+0000,,2013-07-26T16:56:42.000+0000,,,New Feature,Minor,,,,,,,,,['Streams/Writers'],[''],I am included a simple SourceStream for reading XML from a string.  Feel free to close issue if this is not suitable for this library.,StringSourceStream,1,,,belugabehr,True,,belugabehr
commons-io,IO-387,2013-06-02T13:49:43.000+0000,,2013-06-03T00:39:27.000+0000,,,New Feature,Major,,,,,,,,,,,"Might be useful to have a method to copy FileStreams using NIO channels.

This should generally be faster than using byte buffers (and may use less memory).",IOUtils method to copy File Streams using NIO Channels,1,,,sebb,True,,sebb
commons-io,IO-385,2013-06-02T12:21:34.000+0000,2013-06-03T21:10:06.000+0000,2016-11-08T17:58:48.000+0000,,Fixed,New Feature,Major,['2.5'],,,,,,,,,,"FileUtils.doCopyFile caches the input file size and only exits the loop once it has read sufficient bytes.

If the input file is truncated after obtaining the file size, the loop might never exit.

One possible way round this might be to check whether anything has been transferred. However, I don't know if it's possible for FileChannel.transferFrom() to temporarily return 0 when there is data available; if so that could cause some existing applications to break.",FileUtils.doCopyFile can potentially loop for ever,,,,sebb,True,,sebb
commons-io,IO-384,2013-06-02T12:07:30.000+0000,,2013-06-02T13:50:54.000+0000,,,New Feature,Major,,,,,,,,,,,"As mentioned in IO-383, FileUtils.doCopyFile caches the file size.

This means that a growing file (e.g. a log file) may cause the method to report failure.

It might be useful to have a method which keeps looping until there is no more input; this should not throw an IOE if the sizes are different.
More data may have been added - or the file truncated - between completing the copy and checking the size.

The method should return the total bytes written; the caller can then check if the count is OK.",New FileUtils method to copy file which might change in size,,,,sebb,True,,sebb
commons-io,IO-400,2013-05-29T23:55:30.000+0000,2013-10-17T00:25:35.000+0000,2013-10-17T00:25:48.000+0000,,Duplicate,New Feature,Major,['2.5'],,,,,,,,,,"Trying to write a large byte array to a FileOutputStream may cause OOME.

This is because such output requires the use of native code, and native code may need to copy the array in order to access it safely, see:

http://docs.oracle.com/javase/7/docs/technotes/guides/jni/spec/design.html#wp1265

It might therefore be useful to have a method which writes from the byte array in chunks. One can create a ByteArrayInputStream from the input, and then use one of the copy() methods, but that creates an unnecessary array buffer (albeit only 4k).

There are already write methods which copy byte[] to OutputStream and char[] to Writer.

Some or all of these could be converted to use chunked output, or there could be new methods to implement the chunking.

Here is a sample implementation of a stand-alone method:

{code}
public static void writeChunked(byte[] data, OutputStream output) throws IOException {
    int bytes = data.length;
    int offset = 0;
    while(bytes > 0) {
        int chunk = Math.min(bytes, DEFAULT_BUFFER_SIZE);
        output.write(data, offset, chunk);
        bytes -= chunk;
        offset += chunk;
    }
}
{code}",IOUtils: add support for copying from large byte buffers,1,,,sebb,True,,sebb
commons-io,IO-382,2013-05-19T23:11:57.000+0000,2013-06-04T00:16:27.000+0000,2016-11-08T17:58:52.000+0000,,Fixed,New Feature,Major,['2.5'],,,,,,,,['Utilities'],[''],"File IO involves native code at some point.
It seems that native code which accesses a Java
array may involve copying the array (depending on the JVM) [1]

This can easily lead to an OutOfMemory condition if the array is large.

It would be useful to provide some utility methods to process the array in chunks of (say) 4K. It's unlikely that applications will use huge read buffers, but they may well need to write large buffers, so only output chunking is likely to be useful.

It might also be useful to create a ChunkedOutputStream/Writer which provides the chunking automatically.

[1] http://docs.oracle.com/javase/7/docs/technotes/guides/jni/spec/design.html#wp1265

",Chunked IO for large arrays,,,,sebb,True,,sebb
commons-io,IO-381,2013-05-08T14:15:34.000+0000,2013-05-08T14:36:58.000+0000,2018-06-09T08:37:02.000+0000,,Fixed,New Feature,Major,['2.5'],,,,,,,,['Utilities'],[''],"Add the API: {{copyInputStreamToFile(final InputStream source, final File destination, boolean closeSource)}}

{code:java}
    /**
     * Copies bytes from an {@link InputStream} <code>source</code> to a file
     * <code>destination</code>. The directories up to <code>destination</code>
     * will be created if they don't already exist. <code>destination</code>
     * will be overwritten if it already exists.
     *
     * @param source  the <code>InputStream</code> to copy bytes from, must not be {@code null}, will be closed
     * @param destination  the non-directory <code>File</code> to write bytes to
     *  (possibly overwriting), must not be {@code null}
     * @param closeSource If true, closes the <code>source</code>
     * @throws IOException if <code>destination</code> is a directory
     * @throws IOException if <code>destination</code> cannot be written
     * @throws IOException if <code>destination</code> needs creating but can't be
     * @throws IOException if an IO error occurs during copying
     * @since 2.5
     */
    public static void copyInputStreamToFile(final InputStream source, final File destination, boolean closeSource) 
            throws IOException {
{code}

Related to [IO-380].",Add FileUtils.copyInputStreamToFile API with option to leave the source open,1,,,ggregory,True,ggregory,ggregory
commons-io,IO-376,2013-04-15T07:51:03.000+0000,,2013-06-02T10:15:26.000+0000,,,New Feature,Minor,,['2.4'],,,,,,,['Streams/Writers'],[''],"The use case:
- log4j writes to the file using RollingFileAppender.
- in parallel I need to read the file contents for later manipulation (storing it's contents elsewhere)

Problem on windows:
- I open FileInputStream for reading from file
- when using IOUtils.copy(InputStream input, OutputStream output) exception is thrown:
{code} 
java.io.IOException: The process cannot access the file because another process has locked a portion of the file
{code} 

It would be great to have API enabling to copy contents of the locked file (on windows) to OutputStream.

The method available in the API currently:
{code:java} 
FileUtils.copyFile(File input, OutputStream output)
{code} 

does the job, except for the files that are locked (in the windows environment) as they're currently written to.

Some of the functionality is already in use in the: Tailer

However (non-bloked) copying full contents is not provided.
I'd need to retrieve a snapshot of the file.

Not sure about the right API, but generally I see 2 options:
- custom (File)InputSteam implementation or maybe
- custom FileUtils.copyFile(File input, OutputStream output)",FileUtils.copyFile for the locked file (windows),2,,,typek_pb,True,,typek_pb
commons-io,IO-367,2013-02-20T22:41:05.000+0000,2017-05-01T15:07:25.000+0000,2017-10-20T06:49:11.000+0000,,Fixed,New Feature,Minor,['2.6'],['2.5'],,,14400,14400,14400,,['Utilities'],[''],"I suggest adding the following convenience methods:

First:
{{void copyToDirectory(final File src, final File destDir)}} which will simply select either
{{copyFileToDirectory}}
or
{{copyDirectoryToDirectory}}.

Second:
{{void copyToDirectory(final Collection<File> srcs, final File destDir)}} which will simply use {{copyToDirectory}} for each file object.

Implementation of these methods should be straight foward as they would only recombine methods that are already existing and tested.",Add convenience methods for copyToDirectory,5,,['features'],cornelius,True,pascalschumacher,cornelius
commons-io,IO-366,2013-02-20T22:15:19.000+0000,,2013-02-20T22:43:11.000+0000,,,New Feature,Minor,,['2.5'],,,172800,172800,172800,,['Utilities'],[''],"While it is easy to copy directory structures with commons-io, it is afaik impossible to provide user feedback about the copy progress - same applies for move.
I would suggest providing alternative interfaces of all public {{copy*}} and {{move*}} methods that include a ""ProgressListener"". The ProgressListener would be a simple Interface like this:

public interface ProgressListener {
    void onProgress(long currentBytes, long totalBytes);
}

So for example:
{{copyToDirectory(final File src, final File destDir)}}
would get its alter ego:
{{copyToDirectory(final File src, final File destDir, final ProgressListener progressListener)}}

To avoid implementation overhead, I suggest that all currently existing {{copy}} & {{move}} methods would then call the new methods with a {{null}} parameter for the Progress Listener.

I did a quick & dirty proof on concept which you can view here:
bq. https://github.com/sne11ius/commons-io/
Please let me know what you think about it. If you like the idea, I will provide a cleaner implementation with comments & tests.",Provide feedback about the progress of time consuming tasks,1,,['features'],cornelius,True,,cornelius
commons-io,IO-360,2012-11-30T15:23:38.000+0000,2012-11-30T15:31:11.000+0000,2016-11-08T17:58:47.000+0000,,Fixed,New Feature,Major,['2.5'],,,,,,,,,,Add the API Charsets.requiredCharsets(). Inspired by Charset.availableCharsets().,Add API Charsets.requiredCharsets(),1,,,ggregory,True,ggregory,ggregory
commons-io,IO-359,2012-11-30T10:06:10.000+0000,2012-11-30T14:19:22.000+0000,2016-11-08T17:58:59.000+0000,,Fixed,New Feature,Major,['2.5'],,,,,,,,,,"I created a patch for IOUtils.skip and skipFully(ReadableByteChannel, long).
Please add it.
","Add IOUtils.skip and skipFully(ReadableByteChannel, long)",2,,,yukoba,True,,yukoba
commons-io,IO-358,2012-11-29T15:04:24.000+0000,2012-11-29T16:01:14.000+0000,2016-11-08T17:59:01.000+0000,,Fixed,New Feature,Major,['2.5'],,,,,,,,,,"Please add IOUtils.readFully(ReadableByteChannel).
I created its patch.
","Add IOUtils.read and readFully(ReadableByteChannel, ByteBuffer buffer)",2,,,yukoba,True,,yukoba
commons-io,IO-340,2012-08-03T19:34:13.000+0000,2013-04-16T20:29:04.000+0000,2016-11-08T18:04:27.000+0000,,Won't Fix,New Feature,Major,,"['2.5', '3.x']",,,86400,86400,86400,,['Utilities'],[''],"The use of file.exists() is failure prone. In many cases the jvm can report that a file does not exist even when it does. This is due to the caching in the operating system.
Since file.exists() is prone to report false values, a second check can and should be made to file.length()>0.

Where file.exists() is checked, it should be replaced with the following dual check:  

A file more reliably exists if:  file.exists() || file.length()>0

Then if the file does not exist, file.length will be 0.","The use of file.exists() is failure prone. Where file.exists() is checked, it should be replaced with the following dual check:  exists = ( file.exists() || file.length()>0 )",1,,"['File', 'exists', 'false']",groovemeister,True,,groovemeister
commons-io,IO-337,2012-06-17T09:27:25.000+0000,,2015-06-01T19:20:05.000+0000,,,New Feature,Minor,,,,,,,,,,,"ByteOrderMark is used primarily for encapsulating the data regarding to Unicode BOMs. Since those are fixed, it would make sense to make it an enum, instead of a public class.
I have attached a patch that only covers ByteOrderMark itself, including an additional getCharset() utility method, and toString() simplification.
This patch does not cover:
1) Its incorrect use (per its perceived purpose) in XmlStreamReader.XML_GUESS_BYTES
2) The now-unnecessary tests",ByteOrderMark can be refactored as an enum,2,,,kunda,True,,kunda
commons-io,IO-330,2012-05-28T15:57:01.000+0000,2013-04-20T23:24:20.000+0000,2016-11-08T17:58:55.000+0000,,Fixed,New Feature,Major,['2.5'],,,,,,,,,,"Might be worth providing output analogues of IOUtils#toBufferedReader().

i.e., if the OutputStream or Writer is not buffered, then wrap it in the corresponding Buffered... class.

Unfortunately, the input method name toBufferedInputStream is already in use, and does not perform the equivalent conversion of an InputStream.

The new methods could use a different prefix, e.g.

asBufferedOutputStream, asBufferedWriter.
Could then create the equivalent input methods and deprecate toBufferedReader in favour of asBufferedReader.",IOUtils#toBufferedOutputStream/toBufferedWriter to conditionally wrap the output,,,,sebb,True,,sebb
commons-io,IO-327,2012-04-18T12:50:04.000+0000,2012-04-18T13:06:34.000+0000,2016-11-08T17:57:26.000+0000,,Fixed,New Feature,Major,['2.4'],,,,,,,,['Utilities'],[''],"Add byteCountToDisplaySize(BigInteger), just like Add byteCountToDisplaySize(long), in fact the long version can call the BI version to avoid duplication.",Add byteCountToDisplaySize(BigInteger),,,,ggregory,True,,ggregory
commons-io,IO-326,2012-04-16T19:39:05.000+0000,2012-06-11T01:12:46.000+0000,2016-11-08T17:57:24.000+0000,,Fixed,New Feature,Major,['2.4'],['2.3'],,,,,,,['Utilities'],[''],"FileUtils.sizeOfDirectory will return a negative number when the size count goes past Long.MAX_VALUE.

Counting with a BigInteger will solve this issue. Options:

- Change the signature of FileUtils.sizeOfDirectory() to return a BigInteger. This will obviously break BC.
- Create a new API to return a BigInteger. What would this new API be called?
-- sizeOfDirectoryAsBigInteger
-- bigIntegerSizeOfDirectory
-- largeSizeOfDirectory
-- ...?",Add new FileUtils.sizeOf[Directory] APIs to return BigInteger,2,,,ggregory,True,,ggregory
commons-io,IO-325,2012-04-16T14:10:18.000+0000,2012-04-16T14:55:18.000+0000,2016-11-08T17:57:22.000+0000,,Fixed,New Feature,Major,['2.4'],,,,,,,,['Utilities'],[''],"Add IOUtils.toByteArray methods to work with URL and URI:
- IOUtils.toByteArray(URI)
- IOUtils.toByteArray(URL)
- IOUtils.toByteArray(URLConnection)
- IOUtils.close(URLConnection)",Add IOUtils.toByteArray methods to work with URL and URI,,,,ggregory,True,,ggregory
commons-io,IO-324,2012-04-16T13:27:39.000+0000,2012-04-16T13:29:03.000+0000,2016-11-08T17:57:23.000+0000,,Fixed,New Feature,Major,['2.4'],,,,,,,,['Utilities'],[''],"Add Charset sister APIs to method that take a String charset name. This is like [IO-318] for 2.3. At least one API was missed:
- org.apache.commons.io.FileUtils.writeStringToFile(File, String, Charset)",Add missing Charset sister APIs to method that take a String charset name.,,,,ggregory,True,,ggregory
commons-io,IO-322,2012-04-10T14:40:15.000+0000,2012-04-10T14:40:26.000+0000,2016-11-08T18:00:55.000+0000,,Fixed,New Feature,Major,['2.3'],,,,,,,,['Utilities'],[''],"Add and use class Charsets, a class that defines constants the required JRE Charsets. Also includes a few util methods.",Add and use class Charsets,,,,ggregory,True,,ggregory
commons-io,IO-318,2012-03-30T12:58:40.000+0000,2012-03-30T15:22:59.000+0000,2016-11-08T18:00:56.000+0000,,Fixed,New Feature,Major,['2.3'],,,,,,,,,,"Add Charset sister APIs to method that take a String charset name (aka encoding). 
For example: foo(..., String charsetName) -> foo(..., Charset charset).
Refactor such that we do not have code duplication of the algorithms.

Known issue: Source compatibility.

Now there are APIs that change only with the last type, String vs. Charset, you will get a compile error if you pass null to the old String API because the target method will be ambiguous: do you want to call the String or Charset version? You must type-cast to one type or the other.

Known issue: checked java.io.UnsupportedEncodingException vs. unchecked java.nio.charset.UnsupportedCharsetException

The JRE API Charset.forName throws the unchecked UnsupportedCharsetException. 
The Commons IO 2.2 String APIs throw the checked UnsupportedEncodingException, a subclass of IOException, when a charset is not available.
The refactored String APIs throw UnsupportedCharsetException from Charset.forName, an unchecked IllegalArgumentException. The String APIs throw IOException, so there is no source compatibility issue.

If you somehow relied on catching the checked UnsupportedEncodingException instead of IOException, its superclass, you should catch the unchecked java.nio.charset.UnsupportedCharsetException to act on the fact that the charset is not available.
",Add Charset sister APIs to method that take a String charset name.,,,,ggregory,True,,ggregory
commons-io,IO-316,2012-03-26T11:57:50.000+0000,,2013-11-30T05:45:22.000+0000,,,New Feature,Minor,,['2.1'],,,,,,,['Streams/Writers'],[''],"Add the new file based I/O class {{BackupFileWriter}} with the following properties:

- Saves the file to a temporary name
- Creates backup of existing file on {{close()}}
- Renames temp file to desired name on {{close()}}

The backup strategy (number of backups, backup file name) should be pluggable.

There should also be a hook to compare the temporary and the existing file and do the rename only when they are different. The default hook should always replace the file.

It should also be possible to override the temporary file name (including the path, so the temp file can be in the same directory or a different one or even on a different disk).",New API: BackupFileWriter,,1,,digulla,True,,digulla
commons-io,IO-315,2012-03-26T09:34:06.000+0000,2013-04-17T14:19:59.000+0000,2016-11-08T18:00:56.000+0000,,Fixed,New Feature,Major,['2.3'],['2.1'],,,,,,,['Streams/Writers'],[''],"Please create an interface ""Encoding"" plus a set of useful defaults (UTF_8, ISO_LATIN_1, CP_1250 and CP_1252).

Use this interface in all places where ""String encoding"" is used now. This would make the API more reliable, improve code reuse and reduce futile catch blocks for {{UnsupportedEncodingException}}.","Replace all ""String encoding"" parameters with a value type",,,,digulla,True,,digulla
commons-io,IO-313,2012-03-23T03:38:08.000+0000,2012-03-23T03:38:33.000+0000,2012-03-29T21:26:58.000+0000,,Fixed,New Feature,Major,['2.2'],,,,,,,,,,Add IOUTils.toBufferedReader(Reader) to return a new BufferedReader unless the given Reader is already a BufferedReader.,Add IOUTils.toBufferedReader(Reader),,,,ggregory,True,ggregory,ggregory
commons-io,IO-310,2012-03-20T01:46:32.000+0000,2012-06-07T12:08:44.000+0000,2016-11-08T18:01:34.000+0000,,Fixed,New Feature,Major,['2.2'],,,,,,,,,,"Add ByteOrderMark constants for UTF-32.

This is useful for XML processing. See http://www.w3.org/TR/2006/REC-xml-20060816/#sec-guessing",Add ByteOrderMark constants for UTF-32,1,,,ggregory,True,,ggregory
commons-io,IO-309,2012-03-17T12:17:33.000+0000,,2012-03-17T12:18:21.000+0000,,,New Feature,Minor,,,,,,,,,,,"The CSV project developed a UnicodeUnescapeReader, but it is no longer needed there.

Perhaps it would be useful in IO?",UnicodeUnescapeReader?,,,,sebb,True,,sebb
commons-io,IO-308,2012-03-16T10:41:54.000+0000,2012-03-22T21:49:55.000+0000,2012-03-29T21:27:12.000+0000,,Fixed,New Feature,Minor,['2.2'],,,,,,,,,,"Unlike the skip buffer, the copyLarge buffers cannot be shared between threads.
The methods allocate their own buffers as local variables (current size 4096)

It might be worth allowing applications to provide their own buffers, and/or specifying the buffer size to be used.
",Allow applications to provide buffer (or size) for copyLarge methods?,,,,sebb,True,,sebb
commons-io,IO-305,2012-03-07T10:14:24.000+0000,2012-03-17T01:22:58.000+0000,2012-03-29T21:27:59.000+0000,,Fixed,New Feature,Minor,['2.2'],,,,,,,,['Utilities'],[''],"    /**
     * Copy from input to output stream
     * @param is : input stream
     * @param os : output stream
     * @param offset : number of bytes to skip from input before copying
     *         -ve values are ignored
     * @param len : number of bytes to copy. -1 means all
     * @param bufferSize : buffer size to use for copying
     * @throws IOException
     */
    public static void copy( InputStream is, OutputStream os, int offset, int len, int bufferSize) throws IOException
       ","New copy() method in IOUtils that takes additional offset, length and buffersize arguments",,,,mmokashi,True,,mmokashi
commons-io,IO-294,2011-12-12T19:03:40.000+0000,,2015-06-17T08:41:03.000+0000,,,New Feature,Major,,['2.1'],,,,,,,['Utilities'],[''],"I have written a little Utility method that might benefit Commons IO:

{code}
public class FileUtils {

    /**
     * Returns a human-readable version of the file size (original is in bytes). The implementation has the following features:
     * <ul>
     * <li>Supports the SI or IEC units.</li>
     * <li>Supports I18n</li>
     * <li>Display a one digit remainder (rounded down if less than 5, rounded up otherwise)</li>
     * <li>Once the main unit is >= 100, drops the remainder which would be over precision.</li>
     * </ul>
     * 
     * @param size The number of bytes.
     * @param useSiUnits if false, uses the IEC (International Electrotechnical Commission) units (powers of 2), else uses SI (International System of Units)
     *            units (powers of 10).
     * @return A human-readable display value (includes units).
     */
    public static String byteCountToDisplaySize(long size, boolean useSiUnits) {
{code}","Adding FileUtils.byteCountToDisplaySize(long size, boolean useSiUnits)",4,,,jnrouvignac,True,,jnrouvignac
commons-io,IO-291,2011-11-05T12:39:56.000+0000,2012-03-29T21:32:42.000+0000,2012-06-12T12:39:50.000+0000,,Fixed,New Feature,Major,['2.2'],['2.1'],,,,,,,['Utilities'],[''],I added a function that determines whether the specified leaf is contains by the specified composite.,Add new function FileUtils.directoryContains,,,['patch'],plcstpierre,True,ggregory,plcstpierre
commons-io,IO-290,2011-10-25T17:13:28.000+0000,2011-11-16T02:48:11.000+0000,2012-03-29T21:32:07.000+0000,,Fixed,New Feature,Major,['2.2'],,,,,,,,,,"When reading from network etc, it's possible for a read to return before it has read as many bytes as would fit in the buffer.
Sometimes multiple reads are needed to fulfil the request.

It would be useful to have read() versions of the skip() methods.

{code}
int actual = read(Reader, byte buffer [,offset, length]) // returns normally on EOF
int actual = read(InputStream, char buffer [,offset, length])

int actual = readFully(Reader, byte buffer [,offset, length]) // throws EOFException
int actual = readFully(InputStream, char buffer [,offset, length])
{code}",Add read/readFully methods to IOUtils,,,,sebb,True,,sebb
commons-io,IO-288,2011-10-18T20:41:31.000+0000,2011-11-16T01:33:56.000+0000,2012-03-29T21:32:23.000+0000,,Fixed,New Feature,Major,['2.2'],,,,,,,,['Utilities'],[''],"I needed to analyse a log file today and I was looking for a ReversedLinesFileReader: A class that behaves exactly like BufferedReader except that it goes from bottom to top when readLine() is called. I didn't find it in IOUtils and the internet didn't help a lot either, e.g. http://www.java2s.com/Tutorial/Java/0180__File/ReversingaFile.htm is a fairly inefficient - the log files I'm analysing are huge and it is not a good idea to load the whole content in the memory. 

So I ended up writing an implementation myself using little memory and the class RandomAccessFile - see attached file. It's used as follows:

int blockSize = 4096; // only that much memory is needed, no matter how big the file is
ReversedLinesFileReader reversedLinesFileReader = new ReversedLinesFileReader (myFile, blockSize, ""UTF-8""); // encoding is supported
String line = null;
while((line=reversedLinesFileReader.readLine())!=null) {
  ... // use the line
  if(enoughLinesSeen) {
     break;  
  }

}
reversedLinesFileReader.close();

I believe this could be useful for other people as well!
",Supply a ReversedLinesFileReader ,,,,henzlerg,True,,henzlerg
commons-io,IO-286,2011-09-27T16:00:21.000+0000,,2012-03-26T18:45:25.000+0000,,,New Feature,Minor,,,,,86400,86400,86400,,['Streams/Writers'],[''],"In CASSANDRA-2820 I reintoduced the FastByteArrayInputStream and FastByteArrayOutputStream to cassandra. These steams are un-synchronized versions of the Apache Harmony ByteArrayInputStream and ByteArrayOutputStream respectively.

During my own testing of the streams I found a big difference in the performance of the standard JDK BA*S steams and the FBA*S streams on most JREs. Then cassandra load testing also showed an up to 10% improvement in cassandra performance using these streams.

Then Thrift has TByteArrayOutputStream which contains a way to get the underlying byte[] buffer without a deep copy that would probably be a good further enhancement.

Patch to follow.",FastByteArray*Stream implementations to replace syncronized JDK ByteArray*Stream,3,3,"['streams', 'synchronized']",keteracel,True,,keteracel
commons-io,IO-284,2011-09-07T15:48:03.000+0000,2011-09-07T15:48:16.000+0000,2011-10-10T15:47:11.000+0000,,Fixed,New Feature,Major,['2.1'],['2.0.1'],,,,,,,['Utilities'],[''],"Add IOUtils API toString for URL and URI to get contents:
- org.apache.commons.io.IOUtils.toString(URI)
- org.apache.commons.io.IOUtils.toString(URI, String)
- org.apache.commons.io.IOUtils.toString(URL)
- org.apache.commons.io.IOUtils.toString(URL, String)

(This is already in SVN, I am adding this ticket to track this in the release notes.)",Add IOUtils API toString for URL and URI to get contents,,,,ggregory,True,,ggregory
commons-io,IO-282,2011-08-30T17:15:16.000+0000,2011-08-30T17:22:56.000+0000,2011-10-10T15:47:20.000+0000,,Fixed,New Feature,Major,['2.1'],['2.0.1'],,,,,,,['Utilities'],[''],"Add API FileUtils.copyFile(File input, OutputStream output)","Add API FileUtils.copyFile(File input, OutputStream output)",,,,ggregory,True,ggregory,ggregory
commons-io,IO-275,2011-06-17T17:58:09.000+0000,2011-11-11T02:15:43.000+0000,2012-03-29T21:32:58.000+0000,,Fixed,New Feature,Major,['2.2'],,,,,,,,['Utilities'],[''],"FileUtils has a nice member function to compare two files.  It would be nice if the client had the option to ignore line endings.  This way files (specifically text files) generated on different platforms (eg windows, unix) would match. ","org.apache.commons.io.FileUtils.contentEquals - Add option to ignore ""line endings""",,,,cjaspromgos,True,,cjaspromgos
commons-io,IO-267,2011-04-01T13:55:49.000+0000,2011-04-01T14:17:04.000+0000,2011-11-10T18:09:35.000+0000,,Won't Fix,New Feature,Major,,,,,,,,,,,It would be useful to be able to use methods such as FileUtils.iterateFiles() in a foreach loop.,Implement Iterable versions of methods that currently provide an Iterator,,,,sebb,True,,sebb
commons-io,IO-256,2010-12-15T08:20:45.000+0000,2010-12-23T00:19:23.000+0000,2011-11-10T18:09:36.000+0000,,Fixed,New Feature,Minor,['2.0.1'],['2.0'],,,,,,,,,There is no easy way how to specify FileAlternationMonitor's monitoring thread name (and other thread parameters). ,Provide thread factory for FileAlternationMonitor,,,,martin.beranek,True,niallp,martin.beranek
commons-io,IO-255,2010-10-31T13:53:59.000+0000,,2012-03-26T11:21:39.000+0000,,,New Feature,Minor,,['2.0'],,,,,,,,,It may be usefule to *capture* the state of a Filesystem and serialize it. I've been playing with a handler that can serialize/de-serialize a FileEntry and its children to/from XML.,XML handler to serialize/de-serialize FileEntry instances to/from XML,,,,niallp,True,niallp,niallp
commons-io,IO-250,2010-09-30T08:35:32.000+0000,,2011-04-11T11:09:53.000+0000,,,New Feature,Minor,['3.x'],,,,10800,10800,10800,,,,"Please consider adding the following method to org.apache.commons.io.FileUtils. I've submitted the method, with test cases below (rather than create a patch file). As a single method, it should prove very simple to integrate. 

The method returns the path to a file, from another file, as described in the Javadoc method header below: 

{code}

    /**
     * Returns the path of a aFile relative to another aFile, for example the location of a file: 
     * <code>resources/language/english/foobar.properties</code> relative to
     * <code>resources/language/japanese/foobar.properties</code> is
     * <code>../../english/foobar.properties</code>
     *
     * @param aFile           the aFile to check relative location
     * @param fromAnotherFile the base location
     * @return the relative location path
     * @throws java.io.IOException on IO error
     */
    public static String pathTo(File aFile, File fromAnotherFile) throws IOException {
        LOGGER.debug(""Find path to file: "" + aFile.toString() + "" from file: "" + fromAnotherFile.toString());
        Stack<File> fileToDirectories = directoriesFor(aFile);
        Stack<File> fileFromDirectories = directoriesFor(fromAnotherFile);

        while (fileToDirectories.peek().equals(fileFromDirectories.peek())) {
            fileToDirectories.pop();
            fileFromDirectories.pop();
            if (fileToDirectories.isEmpty() || fileFromDirectories.isEmpty()) {
                break;
            }
        }

        StringBuilder pathToCommonParentDirectory = new StringBuilder();
        while (!fileFromDirectories.isEmpty()) {
            pathToCommonParentDirectory.append(""../"");
            fileFromDirectories.pop();
        }

        StringBuilder pathToFileFromCommonParentDirectory = new StringBuilder();
        while (!fileToDirectories.isEmpty()) {
            pathToFileFromCommonParentDirectory.append(fileToDirectories.pop().getName());
            if (!fileToDirectories.isEmpty()) {
                pathToFileFromCommonParentDirectory.append(""/"");
            }
        }
        return pathToCommonParentDirectory.toString() + pathToFileFromCommonParentDirectory.toString();
    }

    private static Stack<File> directoriesFor(File file) throws IOException {
        Stack<File> pathElements = new Stack<File>();
        for (File element = file.getCanonicalFile(); element != null; element = element.getParentFile()) {
            pathElements.push(element);
        }
        return pathElements;
    }
{code}

. . . this is useful for batch processing, web applications, etc. 

Test Cases: 

{code}
     @Test
    public void pathTo() throws IOException {
        //Setup
        File file1 = new File(""configs/js/en/a.xml"");
        File file2 = new File(""configs/js/ja/a.xml"");
        Assert.assertNotNull(file1);
        Assert.assertNotNull(file2);

        //Test
        Assert.assertEquals(""../../en/a.xml"", FileUtils.pathTo(file1, file2));
    }

    @Test
    public void pathTo_windowsStyleOnUnixMachine() throws IOException {
        File file1 = new File(""c:/fred/foobar/dude.properties"");
        File file2 = new File(""c:/data/zz.txt"");

        Assert.assertEquals(""../../fred/foobar/dude.properties"", FileUtils.pathTo(file1, file2));
        Assert.assertEquals(""../../../data/zz.txt"", FileUtils.pathTo(file2, file1));
    }

    @Test
    public void pathTo_fromParentDirectory() throws IOException {
        File file1 = new File(""ui-performance-enhancer/out/test/ui-performance-enhancer/configs/css/imported.xml"");
        File file2 = new File(""ui-performance-enhancer/out/test/ui-performance-enhancer/configs/css"");
        Assert.assertEquals(""imported.xml"", FileUtils.pathTo(file1, file2));

    }
{code}","Add FileUtils.pathTo(File aFile, File fromAnotherFile)",,1,,jasperblues,True,,jasperblues
commons-io,IO-244,2010-04-21T14:34:36.000+0000,2010-09-08T17:19:27.000+0000,2011-11-10T18:09:36.000+0000,,Won't Fix,New Feature,Minor,,['1.4'],,,,,,,['Utilities'],[''],"That probably would be useful to have a method like this:
String FilenameUtils.getSystemAccaptableFileName(String name, String substitute)
",Add method to convert string to a form accaptable by system as a filename,,,,sans17,True,,sans17
commons-io,IO-242,2010-03-19T14:23:40.000+0000,2010-04-14T14:15:25.000+0000,2010-07-21T17:00:52.000+0000,,Fixed,New Feature,Minor,['2.0'],,,,,,,,['Streams/Writers'],[''],"In IO-211 we added protected before/after methods for all read and write operations in ProxyInputStream and ProxyOutputStream. I now have a use case where I need similar functionality also for a Writer, so I've implemented the same feature also for ProxyReader and ProxyWriter. I'll attach the patch for review before committing it.",Pre- and post-processing support for ProxyReader/Writer,,,,jukkaz,True,jukkaz,jukkaz
commons-io,IO-222,2009-10-30T01:33:54.000+0000,,2010-09-29T01:44:46.000+0000,,,New Feature,Minor,['3.x'],,,,28800,28800,28800,,['Utilities'],[''],"This is a proposal to add a feature to new releases of Commons IO.

The new functionality is a class that copies files, called FileCopier and some helper classes. This class goes beyond the functionality of the static methods included in the FileUtils class. It has the following functionality:

It uses an extension to an abstract class, FileCopyHelper. FileCopyHelper provides callback methods to FileCopier's copy methods. Extensions to FileCopyHelper provide methods to define behavior for handling destination files that exist, handling exceptions, reporting the status of copy operations, and enables clients to cancel copies after they have begun.
There are several extensions to FileCopyHelper that I have created. FileCopyHelper and all of its extensions are included in a new package, org.apache.commons.io.filecopyhelper to avoid cluttering up org.apache.commons.io. FileCopier uses a default FileCopyHelper extension, SafeFileCopyHelper. This extension skips all destination files that already exist. OverwriteFileCopyHelper overwrites all files that exist. RenameAppendFileCopyHelper appends text to the names of all files that exist. RenameAppendTimestampFileCopyHelper appends the existing file's last modified date to its name. There are a few other FileCopyHelper extensions included, also. All of the FileCopyHelper extensions that I created print the status of copy operations and report exceptions to standard out. Any behavior of the included FileCopyHelper extensions that are not wanted can be avoided by extending FileCopyHelper with custom implementations.

FileCopyHelper also allows clients to send a cancel request to FileCopier to cancel subsequent copy operations. FIleCopier will throw a runtime exception, FileCopyCancelException, when it receives a request to cancel copy operations.

When copying directories, FileCopier gives clients the ability to ""flatten"" directories if they so choose. They can flatten by directory level or by name, using an IOFileFilter. Flattening means the directory itself is not copied; only its contents are copied. It likewise gives clients the ability to ""merge"" directories that exist. They can merge directories by level or by name, using an IOFileFilter. Merge is applicable when FileCopier is using a FileCopyHelper object that renames destination files that already exist. If a directory is to be merged, it means that the directory is not renamed. The contents of the source directory are merged with the contents of the already existing destination directory. Destination directories that are not to be merged are renamed. FileCopier's default behavior is to not merge or flatten any directories.

FileCopier is immutable and uses an inner Builder class to create instances of it.

I made a small change to FileUtils.java. I changed the access for doCopyFile() from private to protected so that FileCopier could make use of its functionality. 

Included is a FileCopierTestCase class and all of its tests pass.

I've attached a gzipped tar file of Commons IO with the changes I made. If you generate javadocs, it should create them for the classes I introduced. Hopefully, they will answer any questions about the functionality that I did not address here.

Also attached is a diff file of the project's source code after I svn added my source code.

The estimate I put for this effort is 8 hours, just for the time that would be spent deciding whether or not to implement the new feature and possibly any changes that should be made to it.

Please let me know what you think of the new functionality and any suggestions for improvement that you have. If there is anything that I added that you would like me to implement differently, please let me know. I hope that you will agree that FileCopier will provide added value to the Commons IO suite of services.

Best Regards,
David Armstrong ",New Functionality: FileCopier object to facilitate copying of files and directories with ability to customize treatment of existing destination files and report copy status,1,1,,davidarmstrong,True,,davidarmstrong
commons-io,IO-221,2009-10-28T08:19:51.000+0000,2010-08-05T00:13:49.000+0000,2011-11-10T18:09:34.000+0000,,Fixed,New Feature,Minor,['2.0'],['1.4'],,,,,,,['Filters'],[''],"The FileFilterUtils class provides many factory methods, but holds back a few needed ones

like FileFilterUtils.suffixFileFilter(String str) (same regarding the PrefixFileFilter)

the SuffixFileFilter has another constructor the provides the case - sensibility 
for the file filter, a much needed one for windows and linux systems.

usage ex : 
FileFilterUtils.SuffixFileFilter(String suffix, IOCase caseSensitivity);


",Addition of new methods to FileFilterUtils,,,,eldadno1,True,niallp,eldadno1
commons-io,IO-211,2009-08-16T00:12:07.000+0000,2009-08-17T21:40:00.000+0000,2010-07-21T17:00:48.000+0000,,Fixed,New Feature,Minor,['2.0'],,,,,,,,['Streams/Writers'],[''],"In many cases a stream decorator needs to add custom pre- or post-processing functionality to all the read and write methods in decorated input and output streams. For example the CountingInputStream needs to override all three read() methods with similar code.

It would be nice if the proxy stream classes provided simple hooks for adding such functionality.",Pre- and post-processing support for proxied streams,,,,jukkaz,True,jukkaz,jukkaz
commons-io,IO-210,2009-08-07T02:42:00.000+0000,2010-03-06T01:27:01.000+0000,2010-07-21T17:00:47.000+0000,,Fixed,New Feature,Minor,['2.0'],['2.0'],,,14400,14400,14400,,['Filters'],[''],"Add a MagicNumberFileFilter implementation of IOFileFilter that is capable of accepting files based on a ""magic number"", which is a set of specific bytes in the file. The filter should be configurable as to use magic numbers supplied as both Strings and byte arrays, and to accept an offset within the file at which to look for the magic number.",Create MagicNumberFileFilter,1,,,mwooten.dev,True,,mwooten.dev
commons-io,IO-203,2009-04-27T11:51:15.000+0000,2010-03-08T20:20:55.000+0000,2010-07-21T17:00:41.000+0000,,Fixed,New Feature,Major,['2.0'],,,,,,,,['Utilities'],[''],"The skip() method is not guaranteed to skip the requested number of bytes, even if there is more data available. This is particularly true of Buffered input streams.

It would be useful to have a skip() method that keeps skipping until the required number of bytes have been read, or EOF was reached, in which case it should throw an Exception.

[I'll add a patch later.]",Add skipFully() method for InputStreams,1,,,sebb,True,,sebb
commons-io,IO-200,2009-03-18T04:51:48.000+0000,2017-04-23T17:49:13.000+0000,2017-04-23T17:49:14.000+0000,,Won't Fix,New Feature,Trivial,,,,,,,,,['Utilities'],[''],"TableBuilder is 'Builder ' that maps the CSV to a matrix and provides interface that allows user to manipulate after it is build by parsing a csv file to it parse() method.(There is only one method implemented and it is for copying a column values to another position, as I could not  think of other operation that may be useful)

Within the TableBuilder, each column of the CSV is represented as byte[] and each becomes a target to be validated against Rule,represented by the interface that you find in the example code below. As TableBuilder ""buildTable"" ,when parse() method is invoked, a byte[]  representation of the value of each CSV cell  is passed to isValid() method of implementations of Rules, which you apply to the TableBuilder instance through the addRule() method. (you you can add as many Rule as you need.)

Rule gets executed until the validation fails or succeeds. If any of the Rule fails, then its replace() is called and the column value being processed gets replaced by the retun value of this method.
Another goodie is that it is possible to refer to the values of preceding cell values of the row within a Rule.
It is useful if you need to see the entries of the preceding cell when validating the value in a Rule. An example would be,

Given a csv,

A,B,C
1,2,3

in order for the value  3 of the column C is to be validated true, the Value of A needs to be less than the value of C.

TableBuilder is RFC 4180 compliant and therefore distinguishes NL exists by itself and NL found in double quotes.
So you can add Rule that practically removes all NL chars found in value enclosed within doublequotes. 
(useful when you need to remove CRLF in double quotes from  CSV exported from Excel)

Currently, TableBuilder implements a method called copyColumn with method signature of,
copyColumn(Rule rule,int from,int to, boolean override) which allows user to manipulate the parsed csv.

What it does is literarly copies column from that is specified  at 'from' and to 'to' position of the matrix.
If override is true, the copying colum is overriden else the column is right shifted and inserted at the specified position.

You can specify some kind of Rule here to rewrite the value being copied from the origin.
An example would be copy column value that all ends with .jpg or .gif and to the position specified prefixing the column value with ""http://some.server.com/imanges."" after checking the image exists, after checking that the named file exists at some location also by an implementation of  another Rule.

TableBuilder is just a ""rough skecth"" idea of CSV parsing.(The code below works fine though) it still needs alot of refactoring and so.
I appreciate any comment on this idea. What do you think? My code style sucks I know! 

Here is simple exampe to use TableBuilder.
{code:title=TableBuilder|borderStyle=solid}
    public static void main(String[] args)throws Exception{
        TableBuilder tableBuilder=new TableBuilder(""UTF-8"",
                new MessageHandler(){
                    public void handleMessage(String message) {
                        System.err.println(message);
                    }
                },0,true);
        tableBuilder.addRule(3,new RemoveNLChars()); //removing NL cahracters found in value.
        tableBuilder.parse(new FileInputStream(""test.txt""),TableBuilder.CSV);
        List<Record> list=tableBuilder.getRowAsListOf(Record.class);
        for(Record record:list)
            System.out.println(record.getA());//TODO not implemented yet!

        tableBuilder.writeTo(new FileOutputStream(""test_mod.txt""),TableBuilder.CSV);
    }

public class RemoveNLChars extends StringValueRuleAdapter {
    protected boolean isValid(String columnValue) {
        return !columnValue.contains(System.getProperty(""line.separator""));
    }

    protected String replace(String columnValue) {
        return columnValue.replaceAll(System.getProperty(""line.separator""),"""");
    }

    public String getMessage() {
        return """";
    }
}

public interface Rule {
    public void setRowReference(List<byte[]> rowReference);
    public void setCharsetName(String charsetName);
    boolean isValid(final byte[] columnValue);
    byte[] replace(final byte[] columnValue);
    String getMessage();
}

//StringValueruleAdapter is an adapter converts the byte[] representation of the cell value.

public  abstract class StringValueRuleAdapter implements Rule{
    private String charsetName;
    private List<byte[]> rowReference;
    
    public void setRowReference(List<byte[]> rowReference) {
        this.rowReference=rowReference;
    }

    public void setCharsetName(String charsetName) {
        this.charsetName=charsetName;
    }

    public final boolean isValid(final byte[] columnValue) {
        String strValue;
        try {
            if(columnValue.length>0)
                strValue=(charsetName!=null) ? new String(columnValue,charsetName) : new String(columnValue);
            else
                strValue="""";
        } catch (UnsupportedEncodingException e) {
            if(columnValue.length>0)
                strValue=new String(columnValue);
            else
                 strValue="""";
        }
        return isValid(strValue);
    }

    public final byte[] replace(final byte[] columnValue) {
        String strValue;
        try {
            if(columnValue.length>0)
                strValue=(charsetName!=null) ? new String(columnValue,charsetName):new String(columnValue);
            else
                strValue="""";
            return (charsetName!=null) ? replace(strValue).getBytes(charsetName):replace(strValue).getBytes();
        } catch (UnsupportedEncodingException e) {
            if(columnValue.length>0)
                strValue=new String(columnValue);
            else
                strValue="""";
            return replace(strValue).getBytes();
        }
    }

    protected String getRowValue(int column) {
        try {
            return (charsetName!=null) ? new String(rowReference.get(column),charsetName) :
                    new String(rowReference.get(column));
        } catch (UnsupportedEncodingException e) {
            return new String(rowReference.get(column));
        } catch(IndexOutOfBoundsException noListFound){
            throw new IllegalArgumentException(""no value exists at the requested column."");
        }
    }

    protected String getPrecedingRowValue(){
        return getRowValue(rowReference.size()-1);
    }

    protected abstract boolean isValid(String columnValue);
    protected abstract String replace(String columnValue);
}


public class TableBuilder {
    public static int CSV=0x2c;
    public static int TSV=0x09;
    private Map<Integer,Set<Rule>> columnRule=new  HashMap<Integer,Set<Rule>>();
    private Table currentTable;
    private byte[] newLineChars;
    private boolean endsWithNL;
    private String charsetName;
    private int rowOffset;
    private boolean useFirstColumnAsRowName;
    private MessageHandler msgHandler=new MessageHandler(){

        public void handleMessage(String message) {
            System.err.println(message);
        }
    };

    public TableBuilder(String charsetName,MessageHandler msgHandler,int rowOffset,boolean useFirstColumnAsRowName){
        this.charsetName=charsetName;
        this.rowOffset=rowOffset;
        this.msgHandler=msgHandler;
        this.useFirstColumnAsRowName=useFirstColumnAsRowName;
    }

    public TableBuilder(String charsetName){
        this.charsetName=charsetName;
    }

    public TableBuilder(){
        
    }

    public void addRule(int column, Rule rule){
        Set<Rule> ruleset;
        if((ruleset=columnRule.get(column))==null){
            ruleset=new LinkedHashSet<Rule>();
            columnRule.put(column,ruleset);
        }
        rule.setCharsetName(charsetName);
        ruleset.add(rule);
    }

    public void parse(InputStream in, int delimiter)throws Exception{
        int bytesRead;
        byte buf[]=new byte[1024];
        ByteArrayOutputStream outbuf=new ByteArrayOutputStream(buf.length);
        while((bytesRead=in.read(buf,0,buf.length))!=-1)
            outbuf.write(buf,0,bytesRead);
        in.close();
        ByteBuffer bytebuffer=ByteBuffer.allocateDirect(outbuf.size()).put(outbuf.toByteArray());
        bytebuffer.flip();
        currentTable=buildTable(bytebuffer,delimiter);
    }

    private class Table {
        private List<byte[]>[] columnMatrix;
        private List<List<byte[]>> rowMatrix;
        
        Table(List<byte[]>[] columnMatrix,List<List<byte[]>> rowMatrix){
            this.columnMatrix=columnMatrix;
            this.rowMatrix=rowMatrix;
        }

        public int getNumOfColumns() {
            return columnMatrix.length;
        }

        public int getNumOfRows(){
            return rowMatrix.size();
        }

        public byte[] getValueAt(int row, int column) {
            return columnMatrix[column].get(row);
        }

        public byte[] getColumnName(int column){
            return columnMatrix[column].get(0);
        }

        public List<byte[]> getColumn(int column){
            return columnMatrix[column];
        }

        public List<byte[]> getRow(int row){
            return rowMatrix.get(row);
        }
        
    }
  //TODO  extract csv row as JavaBean
    public <E> List<E> getRowAsListOf(final Class<E> clazz){
        List<E> list=null;
        Iterator<byte[]> header=currentTable.getRow(0).iterator();
        for(int i=1;i<currentTable.getNumOfRows();i++){
            try {
                E instance=clazz.newInstance();
                for(byte[] value:currentTable.getRow(i)){
                    String name=new String(header.next());
                    //BeanUtils.setProperty(instance,name,value);
                }
                if(list==null)
                    list=new ArrayList<E>();
                list.add(instance);
                header=currentTable.getRow(0).iterator();
            } catch (IllegalAccessException e) {
                e.printStackTrace();
            } catch (InvocationTargetException e) {
                e.printStackTrace();
            } catch (InstantiationException e) {
                e.printStackTrace();
            }
        }
        return list;
    }

    public void writeTo(OutputStream out,int delimiter) throws IOException {

        for(int i=0,j=0;i<currentTable.getNumOfRows();i++,j=0){
            for(byte[] value:currentTable.getRow(i)){
                out.write(value);
                if(++j<currentTable.getNumOfColumns())
                    out.write(delimiter);
            }
            if(i<currentTable.getNumOfRows()-1)
                out.write(newLineChars);
            else{                                             
                if(endsWithNL)
                    out.write(newLineChars);
            }
        }
        out.close();
    }
     
     public void copyColumn(Rule rule,int from,int to, boolean override) {
            int numOfColumns=override ? currentTable.getNumOfColumns():currentTable.getNumOfColumns()+1;
            List<byte[]>[] columnMatrix=(List<byte[]>[])new List[numOfColumns];
            columnMatrix[to]=new ArrayList<byte[]>();
            for(int i=0,j=0;i<columnMatrix.length;i++){
                if(i==to){
                    for(int row=0;row<currentTable.getNumOfRows();row++){
                        byte[] value;
                        if(row>=rowOffset)
                            value=currentTable.getValueAt(row,from);
                        else
                            value=new byte[0];
                        if(rule!=null && row>rowOffset){
                            rule.setCharsetName(charsetName);
                            rule.setRowReference(currentTable.getRow(row));
                            if(!rule.isValid(value)){
                                String columnName;
                                byte[] columnNameByte=currentTable.getColumnName(from);
                                if(columnNameByte.length>0){
                                    try {
                                        if(charsetName!=null)
                                            columnName=""'""+new String(columnNameByte,charsetName).trim()+""'"";
                                        else
                                            columnName=""'""+new String(columnNameByte).trim()+""'"";
                                    } catch (UnsupportedEncodingException e) {
                                        columnName=""'""+new String(columnNameByte).trim()+""'"";
                                    }
                                }else
                                    columnName=""''"";
                                value=rule.replace(value);
                                String msg=rule.getMessage();
                                    if(msg.length()>0)
                                        try {
                                            handleMessage(msg
                                            .replace(""${column_from}"",""""+from)
                                            .replace(""${columnName}"",columnName)
                                            .replace(""${column_to}"",""""+(to+1))
                                            .replace(""${row}"",useFirstColumnAsRowName ? new String(currentTable.getRow(row).get(0),charsetName) : """"+(row+1)));
                                        } catch (UnsupportedEncodingException ignored) {
                                            
                                        }
                            }
                        }
                    columnMatrix[i].add(value);
                    if(override)
                        currentTable.rowMatrix.get(row).remove(i);
                    currentTable.rowMatrix.get(row).add(i,value);
                    }
                    if(override)
                        ++j;
                }else
                    columnMatrix[i]=currentTable.getColumn(j++);
            }
            currentTable=new Table(columnMatrix,currentTable.rowMatrix);
    }
    
    private Table buildTable(ByteBuffer buf,int delimiter) throws ParseException {
        List<byte[]>[] columnMatrix=null;
        List<List<byte[]>> rowMatrix=new ArrayList<List<byte[]>>();
        int i=0,j,currentRow=0,rowIndex=0,column_count=0,column=0;
        endsWithNL=true;
        newLineChars=null;
        int limit=buf.limit();
        int pos=0;

        while(i<limit && ((j=(buf.get(i)&0xff))==0x0d||(j=(buf.get(i)&0xff))==0x0a)){
            if(j==0x0a)
                ++currentRow;
            pos=++i;
        }
        
        int headRow=currentRow;
        while(i<limit){
            int tmp=buf.get(i) & 0xff;
                if(tmp==0x0a){
                    int k=i;
                    while(k>=0 &&((buf.get(k)&0xff)==0x0d||(buf.get(k)&0xff)==0x0a))
                        --k;
                    byte[] prev=new byte[++k-pos];
                    
                    buf.position(pos);
                    buf.get(prev,0,prev.length);
                    List<byte[]> row;
                    try{
                        row=rowMatrix.get(rowIndex);
                    }catch(IndexOutOfBoundsException noListFound){
                        rowMatrix.add(new ArrayList<byte[]>());
                        row=rowMatrix.get(rowIndex);
                    }
                    if(currentRow==headRow){
                        column_count=column;
                        row.add(prev);
                        columnMatrix=(List<byte[]>[])new ArrayList[column+1];
                        Iterator<byte[]> itr;
                        for(j=0,itr=row.iterator();j<columnMatrix.length;j++){
                            columnMatrix[j]=new ArrayList<byte[]>();
                            columnMatrix[j].add(itr.next());
                        }
                    }else if(column_count!=column){
                        throw new ParseException(""column count mismatch on row "",currentRow+1);

                    }else{
                        Set<Rule> ruleset=columnRule.get(column);
                        if(ruleset!=null && currentRow>rowOffset+headRow){
                            byte[] columnNameByte=rowMatrix.get(rowOffset).get(column);
                            Rule rule=validate(ruleset,prev,row);
                            if(rule!=null){
                                String columnName;
                                if(columnNameByte.length>0){
                                    try {
                                        if(charsetName!=null)
                                            columnName=""'""+new String(columnNameByte,charsetName).trim()+""'"";
                                        else
                                            columnName=""'""+new String(columnNameByte).trim()+""'"";
                                    } catch (UnsupportedEncodingException e) {
                                        columnName=""'""+new String(columnNameByte).trim()+""'"";
                                    }
                                }else
                                    columnName=""''"";
                                prev=rule.replace(prev);
                                String msg=rule.getMessage();
                                if(msg.length()>0)
                                    try {
                                        handleMessage(msg
                                            .replace(""${column}"",""""+column)
                                            .replace(""${columnName}"",columnName.trim())
                                            .replace(""${row}"",useFirstColumnAsRowName ? new String(rowMatrix.get(rowIndex).get(0),charsetName) : """"+(currentRow+1)));
                                    } catch (UnsupportedEncodingException ignored) {

                                    }
                            }
                        }
                        columnMatrix[column].add(prev);
                        row.add(prev);
                    }

                    if(newLineChars==null){
                        newLineChars=new byte[++i-k];
                        buf.position(k);
                        buf.get(newLineChars,0,newLineChars.length);
                    }else
                        ++i;
                while(i<limit && ((j=(buf.get(i)&0xff))==0x0d||(j=(buf.get(i)&0xff))==0x0a)){
                    if(j==0x0a)
                        ++currentRow;
                    ++i;
                }
                column=0;
                ++currentRow;

                ++rowIndex;
                pos=i;
            }else if(tmp==delimiter){
                List<byte[]> row;
                try{
                    row=rowMatrix.get(rowIndex);
                }catch(IndexOutOfBoundsException noListFound){
                    rowMatrix.add(new ArrayList<byte[]>());
                    row=rowMatrix.get(rowIndex);
                }
                byte[] prev=new byte[i-pos];
                buf.position(pos);
                buf.get(prev,0,prev.length);
                if(currentRow==headRow)
                    row.add(prev);
                else{
                    Set<Rule> ruleset=columnRule.get(column);
                    if(ruleset!=null && currentRow>rowOffset+headRow){
                        byte[] columnNameByte=rowMatrix.get(rowOffset).get(column);
                            Rule rule=validate(ruleset,prev,row);
                            if(rule!=null){
                                String columnName;
                                if(columnNameByte.length>0){
                                    try {
                                        if(charsetName!=null)
                                            columnName=""'""+new String(columnNameByte,charsetName).trim()+""'"";
                                        else
                                            columnName=""'""+new String(columnNameByte).trim()+""'"";
                                    } catch (UnsupportedEncodingException e) {
                                        columnName=""'""+new String(columnNameByte).trim()+""'"";
                                    }
                                }else
                                    columnName=""''"";
                                prev=rule.replace(prev);
                        String msg=rule.getMessage();
                        if(msg.length()>0)
                            try {
                                handleMessage(msg
                                        .replace(""${column}"",""""+column)
                                        .replace(""${columnName}"",columnName.trim())
                                        .replace(""${row}"",useFirstColumnAsRowName ? new String(rowMatrix.get(rowIndex).get(0),charsetName) : """"+(currentRow+1)));
                            } catch (UnsupportedEncodingException ignored) {

                            }

                            }
                    }
                    columnMatrix[column].add(prev);
                    row.add(prev);
                }
                ++column;
                pos=++i;
            }else
                if((i=_ESCAPED(buf,i))==i)
                    ++i;
        }

        if(pos!=limit){
            endsWithNL=false;
            byte[] remaining=new byte[limit-pos];
            buf.position(pos);
            buf.get(remaining,0,remaining.length);
            
            if(columnMatrix!=null){
                if(column_count!=column)
                    throw new ParseException(""column count mismatch on row "",+1+currentRow);
                List<byte[]> row=rowMatrix.get(rowIndex);
                row.add(remaining);
                Set<Rule> ruleset=columnRule.get(column);
                if(ruleset!=null && currentRow>rowOffset+headRow){
                    byte[] columnNameByte=rowMatrix.get(rowOffset).get(column);
                    Rule rule=validate(ruleset,remaining,row);
                    if(rule!=null){
                        String columnName;
                        if(columnNameByte.length>0){
                            try {
                                if(charsetName!=null)
                                    columnName=""'""+new String(columnNameByte,charsetName).trim()+""'"";
                                else
                                    columnName=""'""+new String(columnNameByte).trim()+""'"";
                            } catch (UnsupportedEncodingException e) {
                                columnName=""'""+new String(columnNameByte).trim()+""'"";
                            }
                        }else
                            columnName=""''"";
                        remaining=rule.replace(remaining);
                        String msg=rule.getMessage();
                        if(msg.length()>0)
                            try {
                                handleMessage(msg
                                .replace(""${column}"",""""+column)
                                .replace(""${columnName}"",columnName.trim())
                                .replace(""${row}"",useFirstColumnAsRowName ? new String(rowMatrix.get(rowIndex).get(0),charsetName) : """"+(currentRow+1)));
                            } catch (UnsupportedEncodingException ignored) {

                            }

                    }
                }
                columnMatrix[column].add(remaining);
            }else{
                columnMatrix=(List<byte[]>[])new List[column+1];
                List<byte[]> row;
                try{
                    row=rowMatrix.get(rowIndex);
                }catch(IndexOutOfBoundsException noListFound){
                    rowMatrix.add(new ArrayList<byte[]>());
                    row=rowMatrix.get(rowIndex);
                }
                row.add(remaining);
                Iterator<byte[]> itr;
                for(j=0,itr=row.iterator();j<columnMatrix.length;j++){
                    columnMatrix[j]=new ArrayList<byte[]>(1);
                    columnMatrix[j].add(itr.next());
                }
            }
        }
        return new Table(columnMatrix,rowMatrix);
    }

    private int _ESCAPED(ByteBuffer src,int i){
        int org=i;
        if(i==src.limit())
            return i;
        int j;
        if((j=_DQUOTE(src,i))==i)
            return i;

        for(i=j;(j=_TEXTDATA(src,i))>i||(j=_COMMA(src,i))>i||(j=_CR(src,i))>i||(j=_LF(src,i))>i||(j=_2DQUOTE(src,i))>i;)
            i=j;

        if(i==_DQUOTE(src,i))
            return org;
        return i;
    }

    private int _TEXTDATA(ByteBuffer src,int i){
        if(i==src.limit())
            return i;
        if(_COMMA(src,i)==i && _CR(src,i)==i && _LF(src,i)==i && _DQUOTE(src,i)==i)
            return ++i;
        return i;
    }

    private int _2DQUOTE(ByteBuffer src,int i) {
        if(i==src.limit())
            return i;
        if(i==_DQUOTE(src,i))
            return i;
        if(i+1==_DQUOTE(src,i+1))
            return i;
        return i+2;
    }

    private int _DQUOTE(ByteBuffer src,int i) {
        return _CHAR(src,i,0x22);
    }

    public int _LF(ByteBuffer src,int i) {
        return _CHAR(src,i,0x0a);
    }

    private int _CR(ByteBuffer src,int i) {
        return _CHAR(src,i,0x0d);
    }

    private int _COMMA(ByteBuffer src,int i) {
        return _CHAR(src,i,0x2c);
    }

    private int _CHAR(ByteBuffer src,int i,int token){
        if(i==src.limit())
            return i;
        if((src.get(i) & 0xff)==token)
            ++i;
        return i;
    }
     
    private void handleMessage(String message) {
        msgHandler.handleMessage(message);
    }
    
    public Rule validate(Set<Rule> ruleset,byte[] value, List<byte[]> rowReference) {
        for(Rule rule:ruleset){
            if(rule!=null){
                rule.setRowReference(rowReference);
                if(!rule.isValid(value))
                    return rule;
            }
        }
        return null;
    }

}

{code} ",CSV component,1,,,hanishi,True,,hanishi
commons-io,IO-199,2009-03-16T11:44:49.000+0000,,2014-09-04T01:41:32.000+0000,,,New Feature,Minor,['3.x'],,,,,,,,['Utilities'],[''],"A method, that can replace strings (keys) in a file would be very useful. 

For example if you have a file with the following text:

{noformat}
The packet must arrive between @dateFrom and @dateTo. 
{noformat}

You can now replace the ""keys"" with a value of your choice..


Perhaps something like this:

{code}	
public static void replaceStrings(File inputFile, File outputFile, List<KeyValuePair<String, String>> replacements) throws IOException{
		BufferedReader in = new BufferedReader(new FileReader(inputFile));
		BufferedWriter out = new BufferedWriter(new FileWriter(outputFile));
		
		String line;
		
		while ((line = in.readLine()) != null) {
			for(KeyValuePair<String, String> kvp : replacements){
				line = line.replace(kvp.getKey(), kvp.getValue());
			}
			out.write(line);
			out.newLine();
		}
		out.flush();
		out.close();
		in.close();
	}
{code}


I think it should also be possible, to replace the strings in the inputfile so you don't have to create a new file (outputfile)

Regards Alex

",Replace strings in file,2,,,a_stroell,True,,a_stroell
commons-io,IO-198,2009-03-15T15:42:22.000+0000,2010-09-29T03:26:25.000+0000,2011-11-10T18:09:32.000+0000,,Fixed,New Feature,Minor,['2.0'],['1.4'],,,28800,28800,28800,,['Filters'],[''],"Add features to FileFilterUtils that allow for filtering collections and maps of files that are not retrieved from directories in the standard manner. This feature could be useful for filtering files that are provided by the user or that do not all reside in the same directory. 

Example:

List<File> files = getUserSelectedFiles();
List<File> xmlFiles = FileFilterUtils.filterList(Arrays.<File>asList(new File(""A.txt""), new File(""B.xml"")), FileFilterUtils.suffixFileFilter("".xml""));

",Add ability in FileFilterUtils to apply file filters to collections and arrays,1,,,mwooten.dev,True,niallp,mwooten.dev
commons-io,IO-197,2009-02-20T17:52:34.000+0000,2010-09-29T01:29:43.000+0000,2013-05-16T14:38:59.000+0000,,Fixed,New Feature,Minor,['2.0'],,,,,,,,['Streams/Writers'],[''],"Apache Jackrabbit has an interesting InputStream implementation that reads up to a specified amount of bytes from an underlying stream, and then acts as if the end of the stream was reached:

http://svn.apache.org/repos/asf/jackrabbit/trunk/jackrabbit-jcr-server/src/main/java/org/apache/jackrabbit/server/io/BoundedInputStream.java

Similar classes are also found in other projects:
http://svn.mucommander.com/cgi-bin/viewvc.cgi/trunk/source/com/mucommander/io/BoundedInputStream.java?content-type=text%2Fplain&view=co
https://www.sunspotworld.com/docs/Purple/javadoc/com/sun/spot/peripheral/BoundedInputStream.html
",BoundedInputStream,,,,ebourg,True,niallp,ebourg
commons-io,IO-193,2009-02-05T16:25:23.000+0000,2009-02-06T11:08:52.000+0000,2010-07-21T17:00:40.000+0000,,Fixed,New Feature,Minor,['2.0'],,,,,,,,['Streams/Writers'],[''],"When testing error handling in code that uses streams one needs a way to simulate an IOException being thrown by a stream. Typically this means using a custom stream class that throws the desired exception. To avoid having to implement such custom classes over and over again for multiple projects, I'd like to introduce such classes in Commons IO.

The proposed BrokenInputStream and BrokenOutputStream always throw a given IOException from all InputStream and OutputStream methods that declare such exceptions.

For example, the following fictional test code:

{code}
Result result = processStream(new InputStream() {
        public int read() throws IOException {
            throw new IOException(""test"");
        }
    });
assertEquals(PROCESSING_FAILED, result);
{code}

could be replaced with:

{code}
Result result = processStream(new BrokenInputStream());
assertEquals(PROCESSING_FAILED, result);
{code}

",Broken input and output streams,,,,jukkaz,True,jukkaz,jukkaz
commons-io,IO-192,2009-01-26T15:37:52.000+0000,2009-08-11T22:18:01.000+0000,2013-05-02T02:29:19.000+0000,,Fixed,New Feature,Minor,['2.0'],,,,,,,,['Streams/Writers'],[''],"I'd like to introduce two new proxy streams, TaggedInputStream and TaggedOutputStream, that tag all exceptions thrown by the proxied streams. The goal is to make it easier to detect the source of an IOException when you're dealing with multiple different streams. For example:

{code}
InputStream input = ...;
OutputStream output = ...;
TaggedOutputStream proxy = new TaggedOutputStream(output);
try {
    IOUtils.copy(input, proxy);
} catch (IOException e) {
    if (proxy.isTagged(e)) {
        // Could not write to the output stream
        // Perhaps we can handle that error somehow (retry, cancel?)
        e.initCause(); // gives the original exception from the proxied stream
    } else {
        // Could not read from the input stream, nothing we can do
        throw e;
    }
}
{code}

I'm working on a patch to implement such a feature.",Tagged input and output streams,,,,jukkaz,True,jukkaz,jukkaz
commons-io,IO-186,2008-11-30T00:00:37.000+0000,2008-11-30T00:02:37.000+0000,2010-07-21T17:00:36.000+0000,,Fixed,New Feature,Minor,['2.0'],['1.4'],,,,,,,,," - New Composite file comparator implementation - delegates to a set of Comparator implementations
 - New Directory file comparator implementation - compares file types (i.e. Directory vs File)",Composite and DIrectory File Comparator implementations,,,,niallp,True,niallp,niallp
commons-io,IO-182,2008-09-27T12:44:08.000+0000,2011-03-12T22:14:12.000+0000,2011-10-10T15:57:29.000+0000,,Fixed,New Feature,Major,['2.1'],['1.4'],,,,,,,['Streams/Writers'],[''],"It would be great to have one additional parameter for writing strins into file - boolean append.
If true, string will be appended to the end of the file.
It is very useful for many purposes, for example logging and so on..",Add new APPEND parameter for writing string into files,3,3,,stefan.simik,True,niallp,stefan.simik
commons-io,IO-178,2008-08-16T13:58:01.000+0000,2008-11-30T01:29:10.000+0000,2010-10-03T23:55:12.000+0000,,Fixed,New Feature,Minor,['2.0'],['1.4'],,,,,,,['Streams/Writers'],[''],"Microsoft tools have the unpleasant habit of writing a byte order mark (the three-byte sequence 0xEF 0xBB 0xBF) at the start of a UTF-8 encoded file.

The CharsetDecoder supplied with the JDK does not simply discard these bytes, but instead returns the BOM character (0xFEFF); see http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6378911 for discussion on this.

This makes life unpleasant for anyone who is processing text data, as the program must look for this character and ignore it.

The BOMExclusionInputStream class is a work-around: it recognizes the BOM at the start of the stream, and skips over it.",BOMInputStream - an InputStream for detected and optionally excludeing an initial Byte Order mark,,,,kdgregory,True,niallp,kdgregory
commons-io,IO-177,2008-08-16T01:06:01.000+0000,2010-09-29T20:59:11.000+0000,2011-11-10T18:09:35.000+0000,,Fixed,New Feature,Minor,['2.0'],,,,7200,7200,7200,,['Utilities'],[''],"Simple implementation of the unix ""tail -f"" functionality.

The classes will need to be updated to Commons IO style and package namespace. No test case is currently written.",New Tailer class,,,,jeffrey.rodriguez,True,niallp,jeffrey.rodriguez
commons-io,IO-172,2008-05-30T10:57:08.000+0000,,2010-04-27T18:51:06.000+0000,,,New Feature,Major,['3.x'],['1.4'],,,,,,,['Utilities'],[''],"While IO offers a rich set of {{IOFileFilters}} for finding files in directories, I feel it's missing a concept similar to Ant's file sets. For example, how would one search for files that match ""**/*.xml"" but that don't match ""bad/**""? The sketched example would require to exclude the directory ""bad"" but only if it is the first component of the path, something like ""foo/bad/bar.xml"" still needs to be included.

Given the increased flexibility of [Ant-like patterns|http://ant.apache.org/manual/dirtasks.html#patterns], it would be cool to have something similar to Ant's [{{DirectoryScanner}}|http://svn.apache.org/viewvc/ant/core/trunk/src/main/org/apache/tools/ant/DirectoryScanner.java?view=markup] available in Commons IO.

Personally, I wouldn't need a full copy of the mentioned class, I believe some method like
{code:java}
Collection<String> scanDirectory(File dir, Collection<String> includes, Collection<String> excludes)
{code}
in {{FileUtils}} would just suffice. Some default excludes like SCM metadata files could be provided as a public static final and unmodifiable string collection. The return value should include path names relative to the base directory instead of absolute paths (it's easy for the caller to resolve the files against the base directory but it's error-prone to relativize absolute paths).
",Support directory scanning based on Ant-like include/exclude patterns,,,,bentmann,True,,bentmann
commons-io,IO-169,2008-05-14T13:28:25.000+0000,,2009-09-07T09:04:17.000+0000,,,New Feature,Trivial,['3.x'],['1.4'],,,,,,,['Utilities'],[''],"FileUtils contains the very useful FileUtils.copyURLToFile. It would makes sense to do it the other around too, or rename them download vs. upload and deprecate the old one. I can provide a quick patch if needed, but this is trivial.",FileUtils.copyFileToURL,,1,,zzrough,True,,zzrough
commons-io,IO-162,2008-04-02T20:18:24.000+0000,2010-10-04T04:40:34.000+0000,2011-11-10T18:09:32.000+0000,,Fixed,New Feature,Major,['2.0'],,,,,,,,['Streams/Writers'],[''],"XmlReader is a class written by Alejandro Abdelnur in the ROME project (http://rome.dev.java.net) to detect encoding from a stream containing an XML document.
It has been integrated into Maven 2.0.8, via XmlStreamReader in plexus-utils, and I added XmlStreamWriter.

commons-io seems the right library to distribute these utilities.",add Xml(Stream)Reader/Writer from ROME,2,2,,hboutemy,True,niallp,hboutemy
commons-io,IO-156,2008-02-17T13:34:25.000+0000,2008-12-04T01:51:02.000+0000,2011-09-07T20:50:44.000+0000,,Fixed,New Feature,Minor,['2.0'],['1.3.2'],,,,,,,,,Those are the same methods as normalize(String filename) and normalizeNoEndSeparator(String filename) but without converting path separator to the platform.,add methods normalizePreserveSepartor and normalizePreserverSepartorNoEndseparator,,,,,,niallp,michael-o
commons-io,IO-153,2008-01-06T11:07:46.000+0000,2008-01-09T08:13:31.000+0000,2008-08-22T07:36:51.000+0000,,Fixed,New Feature,Minor,['1.4'],,,,,,,,['Streams/Writers'],[''],"Amazingly, even in JDK6 there are no constructors on FileWriter that accept an encoding.

Attached is a patch to add FileWriterWithEncoding",FileWriter that accepts an encoding,,,,scolebourne,True,scolebourne,scolebourne
commons-io,IO-152,2008-01-04T10:24:38.000+0000,2008-01-06T22:00:11.000+0000,2008-08-22T07:36:51.000+0000,,Fixed,New Feature,Minor,['1.4'],,,,,,,,['Streams/Writers'],[''],"It would be useful to have a ByteArrayOutputStream.readFrom(InputStream) method to mirror the existing writeTo(OutputStream) method. A call like baos.readFrom(in) would be semantically the same as IOUtils.copy(in, baos), but would avoid an extra in-memory copy of the stream contents, as it could read bytes from the input stream directly into the internal ByteArrayOutputStream buffers.

[update: renamed the method to write(InputStream) as discussed below]",Add ByteArrayOutputStream.write(InputStream),,,,jukkaz,True,jukkaz,jukkaz
commons-io,IO-150,2007-12-27T21:34:13.000+0000,2007-12-28T11:02:58.000+0000,2011-09-07T20:50:46.000+0000,,Duplicate,New Feature,Major,,['1.3.2'],,,,,,,['Filters'],[''],"It'd be nice to have a filter like this:

RegexpFileFilter reff = new RegexpFileFilter(""^di\\d{1,3},+"");

WildcardFilter is nice but insufficent.",Add a RegexpFilenameFilter,,,,,,,michael-o
commons-io,IO-148,2007-12-21T18:26:22.000+0000,2008-01-06T23:24:04.000+0000,2011-03-10T22:27:46.000+0000,,Fixed,New Feature,Minor,['1.4'],,,,,,,,,,"Add an IOException implementation that has constructors which take a cause (see TIKA-104). Constructors which take a cause (Throwable) were not added to IOException until JDK 1.6 but the initCause() method  was added to Throwable in JDK 1.4.

We should copy the Tika implementation and test case here:

http://svn.apache.org/repos/asf/incubator/tika/trunk/src/main/java/org/apache/tika/exception/CauseIOException.java
http://svn.apache.org/repos/asf/incubator/tika/trunk/src/test/java/org/apache/tika/exception/CauseIOExceptionTest.java",IOException with constructors which take a cause,,,,niallp,True,ggregory,niallp
commons-io,IO-145,2007-12-06T18:40:12.000+0000,2008-01-06T00:49:11.000+0000,2008-08-22T07:36:50.000+0000,,Fixed,New Feature,Minor,['1.4'],['1.3.2'],,,,,,,,,Add file comparator implementations - prompted by IO-142,File Comparator implementations,,,,niallp,True,niallp,niallp
commons-io,IO-142,2007-12-04T21:54:25.000+0000,2008-11-29T04:49:26.000+0000,2010-07-21T17:00:31.000+0000,,Fixed,New Feature,Major,['2.0'],['1.4'],,,,,,,['Utilities'],[''],"I searched your current Commons-IO issues/feature requests and did not find the following so I'd like to propose it as a feature request.

Given a filename filter and dir name, the method would return a List<File> of the files that match the filter in last-modified timestamp order.

Sun explicitly does not provide this functionality - from the Sun Java SE 5 API Javadocs, File's listFiles() method descriptions include the following disclaimer:
        ""There is no guarantee that the name strings in the resulting array will appear in any specific order; they are not, in particular, guaranteed to appear in alphabetical order.""

I needed the files in last-modified order so I wrote code to do it and would be glad to share the code with the commons project if you feel it would be useful. 

The signature is:
- public List<File> getFileListInTimestampOrder(FilenameFilter filter, String dirName)

I've already written, tested and used code to do this.

There are additional flavors that might be worthwhile, too.
- public List<File> getFileListInTimestampOrderReversed(FilenameFilter filter, String dirName)
- public List<File> getFileListInNameOrder(FilenameFilter filter, String dirName)
- public List<File> getFileListInNameOrderReversed(FilenameFilter filter, String dirName)

BTW, I originally posted this on commons-lang but was given feedback that it might be a better fit here.
",Add facility to sort file lists/arrays to Comparator implementations,,,,alscherer,True,niallp,alscherer
commons-io,IO-139,2007-11-22T18:13:14.000+0000,2008-02-06T21:28:35.000+0000,2008-08-22T07:36:50.000+0000,,Fixed,New Feature,Minor,['2.0'],,,,,,,,['Streams/Writers'],['']," This implementation, as an alternative to java.io.StringWriter, provides an un-synchronized (i.e. for use in a single thread) implementation for better performance. StringBuilder is a JDK 1.5+ feature",StringBuilder Writer implementation,,,,niallp,True,niallp,niallp
commons-io,IO-138,2007-11-22T17:56:53.000+0000,2008-01-05T15:25:40.000+0000,2013-05-02T02:29:13.000+0000,,Fixed,New Feature,Minor,['1.4'],,,,,,,,['Streams/Writers'],[''],"Reader implementation that handles any CharSequence (String, StringBuffer, StringBuilder or CharBuffer) - an alternative to java.io.StringReader which only caters for String. CharSequence is a JDK 1.4+ feature.",CharSequence Reader implementation,,,,niallp,True,niallp,niallp
commons-io,IO-137,2007-11-21T07:55:54.000+0000,2008-12-05T00:35:46.000+0000,2012-10-04T19:01:15.000+0000,,Fixed,New Feature,Major,['2.0'],['1.3.2'],,,,,,,"['Streams/Writers', 'Utilities']","['', '']","Patch inclues following two methods and test cases for both.

1) New Method: ByteArrayOutputStream.toInputStream
ByteArrayOutputStream exposes its byte buffers by toByteArray(), which creates a fresh buffer and copy existing data to it.
A new method toInputStream() available in patch returns the current contents of baos, as an InputStream, avoiding unnecessary allocation and copying.

2) New Method: IOUtils.toFullyBufferedInputStream
There are situations where the InputStream source is available and it has to be passed to different modules for processing.
It is needed to fetch the full contents of the InputStream in internal buffer(IOUtils.toByteArray() ), convert this buffer to ByteArrayInputStream and use that stream instead. But this is wasteful since toByteArray() requires one fresh allocation and copy operation.
New method copies InputStream to ByteArrayOutputStream and returns baos.toInputStream(), avoiding unnecessary memory allocation and copy.

Testcases are available in respective classes.",Added method for getting InputStream from ByteArrayOutputStream & IOUtils avoiding unnecessary array allocation and copy,1,,,nikunj_trivedi,True,niallp,nikunj_trivedi
commons-io,IO-132,2007-10-20T06:37:53.000+0000,2008-05-30T15:52:58.000+0000,2008-08-22T07:36:50.000+0000,,Fixed,New Feature,Major,['2.0'],,,,,,,,,,"Commons JCI has a ""fam"" (file alteration monitor) module[1] which provides a listener for file and directory create/change/delete events. I have done some work refactoring the JCI implementation and propose adding to to Commons IO. There is already an existing request/propsal to add similar functionality in IO-79 from Jim Harrington (which includes code).

There are IMO two main differences (IMO improvements) in this implementation over exsiting JCI fam module:

1) It uses a matching array technique to compare previous and current directory contents which minimizes object creation
2) FileFilters can be specified to monitor only portions of the file system, ignoring files/directories of no interest. This is more efficient and reduces the amount of noise from un-wanted file/directory events

From a quick look at IO-79, the above also applies to that proposal. Additionally IMO the solution in IO-79 would benefit from been broken out from the two DirectoryPoller and FilePoller artifacts.

There are also some changes to the API specified in JCI's fam:
1) FileObserver (FilesystemAlterationObserver in fam) has additional init() and destroy() methods
2) FileMonitor (FilesystemAlterationMonitor in fam) has addObserver/removeObserver methods rather than addListener/removeListener methods. The issue (IMO) with fam is that the monitor is fixed to the FilesystemAlterationObserverImpl implementation, rather than any FilesystemAlterationObserver implementation. Also the ability to specify file filters means that there is a need to be able to add more than one observer for the same root directory.

[1] http://svn.apache.org/repos/asf/commons/proper/jci/trunk/fam/",File Listener/Monitor,2,1,,niallp,True,niallp,niallp
commons-io,IO-129,2007-10-13T10:33:29.000+0000,2007-10-15T11:41:59.000+0000,2008-08-22T07:36:50.000+0000,,Fixed,New Feature,Minor,['1.4'],,,,,,,,['Streams/Writers'],[''],"There should be a TeeInputStream class that transparently writes all bytes read from an input stream to a given output stream. Such a class could be used to easily record and log various inputs like incoming network streams, etc. The class would also be nicely symmetric with the existing TeeOutputStream class.",Add TeeInputStream,,,,jukkaz,True,,jukkaz
commons-io,IO-122,2007-06-09T16:04:14.000+0000,2007-10-13T01:22:13.000+0000,2008-08-22T07:36:50.000+0000,,Fixed,New Feature,Minor,['1.4'],,,,,,,,['Streams/Writers'],[''],"Java API docs are typically not very clear on whether a component that accepts an input or output stream will close the stream. This can easily lead to cases where streams are either prematurely closed (which is typically easy to detect) or where an unclosed stream will unnecessarily consume system resources.

The attached patch adds a set of helper classes that allow applications to better control streams even when working with components that don't clearly define whether they close streams or not. The added classes are:

    org.apache.commons.io.input.AutoCloseInputStream
    org.apache.commons.io.input.ClosedInputStream
    org.apache.commons.io.input.CloseShieldInputStream
    org.apache.commons.io.output.ClosedOutputStream
    org.apache.commons.io.output.CloseShieldOutputStream

See the javadocs in the patch for more details and typical use cases. I've included ""@since 1.4"" tags in the javadocs in hope that this patch could be included in the next release.",Helper classes for controlling closing of streams,,,,jukkaz,True,,jukkaz
commons-io,IO-107,2006-12-28T22:43:57.000+0000,2006-12-28T23:21:29.000+0000,2007-03-09T20:38:24.000+0000,,Fixed,New Feature,Major,['1.3'],,,,,,,,['Utilities'],[''],"Add a new method - FileUtils.openOutputStream.

This will centralise the opening of a file in a single place in the source code. The new method will check that the file is not a directory, and will create parent directories if required.",FileUtils.openOutputStream,,,,scolebourne,True,scolebourne,scolebourne
commons-io,IO-106,2006-12-19T12:42:29.000+0000,2006-12-21T03:30:57.000+0000,2007-03-09T20:38:24.000+0000,,Won't Fix,New Feature,Minor,,,,,,,,,['Streams/Writers'],[''],Provide a Writer implementation for outputing XML in fast and efficient manner,Convenience Writer for XML,,,,niallp,True,,niallp
commons-io,IO-95,2006-10-09T00:29:30.000+0000,2006-10-11T15:48:44.000+0000,2007-03-09T20:38:23.000+0000,,Fixed,New Feature,Minor,['1.3'],,,,,,,,['Streams/Writers'],[''],Add a new NullWriter class - Writer equivalent of NullOutputStream,New NullWriter implementation,,,,niallp,True,niallp,niallp
commons-io,IO-94,2006-10-08T08:41:33.000+0000,2006-10-14T10:21:52.000+0000,2006-10-14T10:21:52.000+0000,,Fixed,New Feature,Minor,['1.3'],,,,,,,,['Streams/Writers'],[''],I have created a MockInputStream which can be plugged in for testing parts of systems where the data doesn't matter - the main use I've had for it was testing large files - without actually having the InputStream process large amounts of bytes.,New NullnputStream & NullReader implementations,,,,niallp,True,niallp,niallp
commons-io,IO-92,2006-09-11T07:39:39.000+0000,2007-10-20T05:43:52.000+0000,2008-08-22T07:36:49.000+0000,,Won't Fix,New Feature,Major,['1.4'],,,,,,,,['Streams/Writers'],[''],"I've extended the ThresholdingOutputStream class with a new class which
behaves different from DeferredFileOutputStream:
- when the stream is closed, the content stored in memory is *always* flushed to disk (in DeferredFileOutputStream, instead, if the treshold
is not reached data is lost)
- DeferredFileOutputStream maintains data in memory only until the treshold value has been reached, then it immediately writes every byte
to disk. This new implementation, instead, caches treshold bytes in memory, and every time that value is reached (that is, treshold, 2 * threshold, etc), it flushes data to disk. In other words it acts as a cache.
- It implements the java.io.DataOutput interface, that is, it provides utility methods to write all primitive types (e.g. short, byte, char, int, float, long, double and String in different formats)
",Add DeferredPeriodicOutputStream,1,,,michelemazzucco,True,,michelemazzucco
commons-io,IO-88,2006-07-24T16:20:36.000+0000,2006-10-08T09:12:09.000+0000,2007-03-09T20:38:23.000+0000,,Fixed,New Feature,Major,['1.3'],,,,,,,,,,"Add isFileOlder() methods to FileUtils (counterparts to existing isFileNewer() methods)

Also add Test cases for isFileNewer() which are are missing",Add isFileOlder() methods to FileUtils (counterparts to existing isFileNewer() methods),,,,niallp,True,niallp,niallp
commons-io,IO-87,2006-07-24T02:43:01.000+0000,2006-07-25T00:57:50.000+0000,2007-03-09T20:38:23.000+0000,,Fixed,New Feature,Minor,['1.3'],['1.2'],,,,,,,['Filters'],[''],Provide a convenience mehod in FileFilterUtils to create a Size Range filter - by combining two SizeFileFilter's (one >= minimum and the other <= maximum) with an AndFileFilter,Provide a convenience mehod in FileFilterUtils to create a Size Range filter,,,,niallp,True,niallp,niallp
commons-io,IO-86,2006-07-23T03:17:00.000+0000,2006-12-03T18:12:33.000+0000,2006-12-03T18:12:33.000+0000,,Fixed,New Feature,Major,['1.3'],['1.2'],,,,,,,['Utilities'],[''],"I'd like to propose adding a ""FileFinder"" back into Commons IO. This is a simplified version of what was recently moved out of Commons IO into the ""finder"" component currently in the sandbox.

I believe this is a simpler, more generic implementation than the finder component and therefore would be considered suitable for inclusion in Commons IO. Although simpler it could be used as the basis for achieving the finder component's aims - namely to emulate the unix find command.",Add DirectoryWalker based on FileFinder,1,1,,niallp,True,,niallp
commons-io,IO-85,2006-07-21T10:56:20.000+0000,2006-07-22T16:52:38.000+0000,2007-03-09T20:38:22.000+0000,,Fixed,New Feature,Minor,['1.3'],['1.2'],,,,,,,['Filters'],[''],"I have the following additional IOFileFilter implementations:

1) CanReadFileFilter checks File.canRead()
2) CanWriteFileFilter checks File.canWrite()
3) HiddenFileFilter checks File.isHidden()
4) EmptyFileFilter checks either File contents are empty or directories contain no files

I've been messing around with Commons Finder without realizing the support that already exists in IO for FileFilters :-(

I also have a DepthFileFilter that checks a maximum depth from a reference file thats useful for limiting navigating a directory structure, if that sounds useful.","IOFileFilter implementations for File.canRead(), File.canWrite(), File.isHidden() and empty files/directories",,,,niallp,True,niallp,niallp
commons-io,IO-51,2004-08-14T21:36:29.000+0000,,2008-01-08T04:56:49.000+0000,,,New Feature,Minor,['3.x'],['1.0'],,,,,,,['Streams/Writers'],[''],"Additional classes for the commons-io package.
Three new classes, the new test classes and one example class.

io.Limiter - Core class shared between the input and output streams that provides the central functions 
for the throttle io streams
io.input.ThrottledInputStream - Input Stream implementation
io.output.ThrottledOutputStream - Output Stream implementation 

Gareth",Throttled input and output stream classes,1,2,,gareth@logicalpractice.com,True,,gareth@logicalpractice.com
commons-jelly,JELLY-294,2019-03-21T17:35:43.000+0000,,2019-03-28T15:49:33.000+0000,,,New Feature,Major,,,,,,,,,,,"I think that the jenkins fork is: https://github.com/jenkinsci/jelly

We want to pull their latest code in and do a 2.X jelly release so that we're maintaining the code as opposed to jenkins having to maintain it.",Update Jelly from Jenkins Fork,3,,['GSoC2019'],chtompki,True,chtompki,chtompki
commons-jelly,JELLY-281,2007-09-19T20:03:50.000+0000,,2007-09-19T20:03:50.000+0000,,,New Feature,Major,,"['1.1', '1.0.1', '1.0-beta-4']",,,,,,,['taglib.xml'],[''],"When the dom4j SAXReader is created default encoding is applied. A encoding tag will make it possible to call SAXReader.setEncoding(String encoding).

I have already changed this in my local version of jelly, but I want to put code changes back to the jelly community",support encoding attribute for xml:parse tag,,,,mvanbochove,True,,mvanbochove
commons-jelly,JELLY-229,2006-05-09T05:09:29.000+0000,2006-05-10T03:48:01.000+0000,2006-05-10T20:51:10.000+0000,,Fixed,New Feature,Major,['1.1'],['1.0'],,,,,,,['taglib.interaction'],['Utility to ask console input.'],"This is a follow-up of JELLY-175. I added a method setCompletor(list) allowing to set a list of strings that is used by jline for tab completion. Use it like: <i:ask completor=""${list}"" answer=""a""/>. The list of tab-completion strings is added to the history list, ie new goals typed in console mode will always be tab-completed afterwards.

Please note that I have bumped the jline dependency to the latest 0.9.5. This is not on ibiblio yet, I have created an upload request ( http://jira.codehaus.org/browse/MAVENUPLOAD-883 ), if it is not found, you will have to put the jline-0.9.5.jar into your local repo by hand.

To test it: I have deployed a snapshot of the maven 1 console plugin:

maven plugin:download -Dmaven.repo.remote=http://www.ibiblio.org/maven,http://cvs.apache.org/repository/ -DgroupId=maven -DartifactId=maven-console-plugin -Dversion=1.2-SNAPSHOT

The default value for the completor list is ""clean,java:compile,jar,test,xdoc,site,quit,help"", but you can define your own custom list using the maven.console.completor.goals property.",Add list of possible completions to jelly-tags-interaction,,,,ltheussl,True,polx,ltheussl
commons-jelly,JELLY-225,2005-12-23T03:34:30.000+0000,,2005-12-29T03:12:48.000+0000,,,New Feature,Major,,,,,,,,,['submissions'],[''],"This is a proposal for a new jelly tag library that provides an interface to the new Java 5 Annotation Processing Tool (apt) and its associated Mirror API (see http://java.sun.com/j2se/1.5.0/docs/guide/apt/index.html).

From the official apt documentation: ""apt is a command-line utility for annotation processing. It includes a set of reflective APIs and supporting infrastructure to process program annotations (JSR 175). These reflective APIs provide a build-time, source-based, read-only view of program structure. They are designed to cleanly model the Java programming language's type system after the addition of generics (JSR 14).""

Developers who which to process Java source code are presently limited to working with the Mirror API directly.  If, for example, a developer wished to generate an artifact such as an xml config file or another Java class must do so by writing to an instance of java.io.PrintWriter.

As an admittedly impotent example, to generate a simple class that will print out all methods of all classes in a given source base, the developer would implement instances of com.sun.mirror.apt.AnnotationProcessorFactory and com.sun.mirror.apt.AnnotationProcessor that look something like this:

    package org.apache.commons.jelly.examples;

    import java.util.Collection;
    import java.util.Set;
    import java.util.Collections;
    import java.io.IOException;
    import java.io.PrintWriter;

    import com.sun.mirror.apt.AnnotationProcessorFactory;
    import com.sun.mirror.apt.AnnotationProcessor;
    import com.sun.mirror.apt.AnnotationProcessorEnvironment;
    import com.sun.mirror.declaration.AnnotationTypeDeclaration;
    import com.sun.mirror.declaration.TypeDeclaration;
    import com.sun.mirror.declaration.MethodDeclaration;

    public class ClassAndMethodPrinterAnnotationProcessorFactory implements AnnotationProcessorFactory {

      public Collection<String> supportedOptions() {
        return Collections.EMPTY_LIST;
      }

      public Collection<String> supportedAnnotationTypes() {
        return Collections.EMPTY_LIST;
      }

      public AnnotationProcessor getProcessorFor(Set<AnnotationTypeDeclaration> atds, AnnotationProcessorEnvironment env) {
        return new ClassAndMethodPrinterAnnotationProcessor(env);
      }

      private class ClassAndMethodPrinterAnnotationProcessor implements AnnotationProcessor {

        AnnotationProcessorEnvironment env;

        public ClassAndMethodPrinterAnnotationProcessor(AnnotationProcessorEnvironment env) {
          this.env = env;
        }

        public void process() {
          try {
            PrintWriter writer = env.getFiler().createSourceFile(""org.apache.commons.jelly.examples.ClassAndMethodPrinter"");
            writer.println(""package org.apache.commons.jelly.examples;"");
            writer.println();
            writer.println(""public class ClassAndMethodPrinter {"");
            writer.println();
            writer.println(""  public static void main(String[] args) {"");
            for (TypeDeclaration typeDeclaration : env.getTypeDeclarations()) {
              writer.println(String.format(""    System.out.println(\""Class: %s\"");"", typeDeclaration.getQualifiedName()));
              for (MethodDeclaration methodDeclaration : typeDeclaration.getMethods()) {
                writer.println(String.format(""    System.out.println(\""Method: %s.%s\"");"", typeDeclaration.getQualifiedName(), methodDeclaration.getSimpleName()));
              }
            }
            writer.println(""  }"");
            writer.println();
            writer.println(""}"");
          }
          catch (IOException e) {
            throw new RuntimeException(e);
          }
        }
      }

    }


Any Java programmer with a little bit of experience could testify that using a PrintWriter for large and complex output is significantly heavy and burdensome.  To use a familiar paradigm in J2EE, nobody wants to use a Servlet's PrintWriter for outputting many large and complex html documents.  For this reason, Java Server Pages (JSP) were created to ease development of complex output.

An apt tag library would be to the Mirror API what JSPs are to the Servlet API.  Instead of writing implementations of AnnotationProcessorFactory and AnnotationProcessor, why not just use Jelly to manage your output, like so:


    <j:jelly xmlns:j=""jelly:core"" xmlns:apt=""jelly:apt"">

      <apt:javaSource name=""org.apache.commons.jelly.examples.ClassAndMethodPrinter"">
    package org.apache.commons.jelly.examples;

    public class ClassAndMethodPrinter {

      public static void main(String[] args) {
        <apt:forAllTypes var=""type"">
        System.out.println(""<j:out value=""${type.qualifiedName}""/>"");
          <apt:forAllMethods var=""method"">
        System.out.println(""<j:out value=""${type.qualifiedName}""/>.<j:out value=""${method.simpleName}""/>"");
          </apt:forAllMethods>
        </apt:forAllTypes>
      }
    }
      </apt:javaSource>

    </j:jelly>
    
Not only is the requirement to implement Mirror API classes lifted from the developer, but it's easy to see how easily Jelly provides a much cleaner and easier-to-read abstraction on top of the Mirror API.  Developers could also take advantage of the rich set of jelly tag libararies for managing output.

The new apt tag library would be a replacement for the popular tool XDoclet (see http://xdoclet.sourceforge.net).  Not only would it provide everything that XDoclet currently provides, but it would have many significant advantages over XDoclet, including:

-No tight coupling to Ant
-Availability of a rich set of tag libraries, including all the current jelly tag libararies like jelly core and xml taglibs.
-Complete support for full Java 5 syntax, including generics, annotations, static imports, enums, etc.
-A richer (and significantly cleaner) template language (i.e. Jelly)
-A more complete set of tags for traversing source code classes, including tags like forAllPackages, forAllImportedTypes, forAllNestedTypes, forAllThrownTypes, etc.

I already have everything implemented and integrated into the current maven jelly build.  Attached are a few examples that show what this new library can do.  All examples output properties files (these are examples I use for unit testing).",apt jelly tag library,1,,,stoicflame,True,,stoicflame
commons-jelly,JELLY-222,2005-11-06T11:11:38.000+0000,,2005-11-07T09:16:18.000+0000,,,New Feature,Minor,,['1.0'],,,,,,,['taglib.swing'],[''],Add layout tag for CardLayout,Add CardLayout,,,,hgilde,True,,hgilde
commons-jelly,JELLY-195,2005-01-12T02:35:19.000+0000,,2005-01-22T03:15:02.000+0000,,,New Feature,Major,,['1.0'],,,,,,,['submissions'],[''],"Janino is a pretty embedded compiler for run-time compilation purposes designed by Arno Unkrig under Apache License (<http://www.janino.net>).
It can be a very usefull tool to create java function, entry point (main), class and package on run time.
I would submit to jelly people a new jelly-tag library overview before getting in sandbox processus. This library implements janino compiler in jelly way.

Janino jelly-tag library implements compilation, class body and scriptlet like janino evaluator.

A - compilation

For exemple you can define classes like :

<compile var=""janinoClassLoader"">
<!-- java source code -->
  public class Item {
    private String name;
    private double price;
    			
    public void setName(String str) { this.name = str; }
    public String getName() { return this.name; }    			
    public void setPrice(double p) { 
           if ( p &gt; 0 ) {
                this.price = p; 
           }
    public double getPrice() {return this.price; }
}
</compile>
<j:new classLoader=""${cl}"" className=""foo.test.Customer"" var=""customer""/>
<j:set target=""${customer}"" property=""name"" value=""Charles""/>

B - ClassBody -

<classBody var=""clazz"">
 import java.util.*;
		
 static private int a = 1;
 private int b = 2;
		
 public int func(int c, int d) { return func2(c, d); }

 private static int func2(int e, int f) {
		       return e * f;
 }   
</classBody>
    	
<j:useBean class=""${clazz}"" var=""b""/>
<j:set var=""result"" value=""${b.func(2,3)}""/>

C - scriptlet 
<script var=""script"" result=""s"" returnType=""java.util.ArrayList"" execute=""true"">
 <!-- define expected parameters -->
 <parameter name=""a"" type=""java.lang.String""/>
 <parameter name=""b"" type=""java.util.ArrayList""/>    		
    		
 <!-- pass argument reference -->
 <j:arg value=""foo""/>
 <j:arg value=""${l}""/>
    		
 <!-- java scrip itself -->
 import java.util.ArrayList;
 ArrayList list = new ArrayList();
 list.add(a);
 list.add(b);    		
 return list;
</script>

<evaluate script=""${script}"" result=""myList"">
  <j:arg value=""item0""/>
  <j:arg value=""item1""/>
</evaluate>

<j:forEach var=""item"" items==""${mylist}"">...

D - compilation support - 

A light compilation log writer help you to get through compilation error

Exemple :
11 janv. 2005 18:21:51 org.apache.commons.jelly.janino.JaninoHelper throwDocumentedExcpetion
GRAVE: 0001:import java.util.fooArrayList;
0002:ArrayList list = new ArrayList();
---------------------^
Line 2, Column 17: Expression ""ArrayList"" is not a type
0003:list.add(a);
0004:list.add(b);
0005:return list;


E - Xml customizable bean definition - 
A lot of jelly-tag library can do a lot for bean definition.
In janino tag library, high customizable run time bean definition is a included :

Exemples :

    	<compile var=""cl"" mapName=""classes"" packageName=""foo.test"">
    		<class name=""Human"">
    			<property name=""name""/>
    			<property name=""age"" type=""int""/>
    		</class>    		
    		<class name=""Customer"" extends=""Human"">
    			<property name=""society""/>
    		</class>
    	</compile>

generate simple beans.

But you can go further :

<classBody var=""Person"">
 <property name=""name""/>
 <property name=""forname""/>
 <property name=""prefix""/>

 <property name=""sex"" setter=""true"" declaration=""true"">
   if ( string.equals(""female"") ) { 
     this.prefix = ""Ms"";
   } else {
     this.prefix = ""Mr"";
   }
  </property>
  <property name=""sex"" getter=""true""/>

  
  <property name=""clothing"" getter=""true"" override=""true"" declaration=""true"">
    if ( this.prefix.equals(""Ms"") ) {
      return ""wedding gown"";
    } else {
      return ""smoking"";
    }				
  </property>
</classBody>

<j:useBean class =""${Person}"" var=""jane"" name=""Doo"" forname=""Jane"" sex=""female""/>
<j:useBean class =""${Person}"" var=""john"" name=""Doo"" forname=""John"" sex=""male""/>

<j:file var=""gossip"">
  ${john.prefix} ${john.forname} ${john.name} dressed in ${john.clothing} and 
  ${jane.prefix} ${jane.forname} ${jane.name} in ${jane.clothing} have enlighted this party !
</j:file>



F - of course you can use <compile> to load java class from a source file 
<compile var=""cl"" uri=""./mySources.txt""/>


As I have said, this library is for jelly people interest evaluation. It's rather an alpha library, but I think it can be a great feature for more supple jelly scripting.

Thank for your time.",Janino compiler tag library,1,,,marc_dexet,True,,marc_dexet
commons-jelly,JELLY-194,2005-01-10T10:18:10.000+0000,,2006-02-28T07:44:17.000+0000,,,New Feature,Major,,['1.0'],,,,,,,['taglib.util'],[''],"Hi,

It would be nice if there was a Jelly tag similar to <ant:copy> that would copy text files replacing the Jelly properties.

For instance, I need to copy a datasource XML definition to JBoss before running Cactus (on maven), but each developer in my project has its own database schema. So, I could have a XML file like this:
 
<datasource>
      <username>${ds.username}</username>
      <password>${ds.password}</password>
</datasource>

And then something like this on maven.xml:

<preGoal name=""cactus:test"">
   <util:copyAndReplace file=""datasource.xml"" 
         toFile=""${env.JBOSS_HOME}/server/default/deploy""/> 
</preGoal>

If you think such tag is useful (and hence would be incorporated on Jelly), I could write the tag and test cases and then submit a patch (I'm just not sure what's the best name for it).

-- Felipe
",New tag that copy text files replacing Jelly properties,,,,felipeal,True,,felipeal
commons-jelly,JELLY-192,2005-01-08T01:47:30.000+0000,,2005-01-09T21:24:05.000+0000,,,New Feature,Minor,,,,,,,,,,,"Jelly script can be very hard to read if there several instructions.
I think it would be interesting for users as developers to get a graphical user-friendly viewer.
Each jelly script is an XML well formed document, and each tag is associated to library.
They are designed to get parsed and associated to any alternativ representation.
It's just an idea now.
But what about creating an functional representation of each tag :
We have already library associated to namespace.
Usually 'var' attribute value is an object reference.
With instropection tools we can get setters and object type.
So we have almost everything to establish control graph, reference usage and type maps.
",Graphical jelly script viewer,1,,,marc_dexet,True,,marc_dexet
commons-jelly,JELLY-190,2005-01-06T18:49:11.000+0000,,2005-01-06T20:19:25.000+0000,,,New Feature,Minor,,['1.0-beta-5'],,,,,,,['taglib.util'],[''],"IterateTag is like org.apache.commons.jelly.tags.core.ForEachTag but can iterate over several collections. Where org.apache.commons.jelly.tags.core.ForEachTag iterate over one collection and set Value in one variable like : 
 <forEach var=""oneVar"" items=""oneCollection"">
    <do something value=""${oneVar}""/>
 </forEach>
 
IterateTag is like : 

 <iterate var=""var_1;var_2;...;var_P"" items=""Collection_1;Collection_2;....;Collection_P"">
    <do something value=""${var_1} ${var_2} ${var_3}""/>
 </iterate>
 
Each Collection points at next element at the same time.

Over the n iteration, for j = 1 to P : var_j = Collection_j.get(n);
IterateTag stop on the first ended collection, so n = Min( Collection_1.size(), ..., Collection_P.size());",CLONE -New Tag for parallel iteration,,,,marc_dexet,True,,marc_dexet
commons-jelly,JELLY-183,2005-01-04T16:22:44.000+0000,,2015-05-01T08:20:09.000+0000,,,New Feature,Major,,['Review Patch'],,,,,,,['taglib.ant'],[''],"-- Moved from Bugzilla 28812 --

I have added support for child-anttasks as described in
http://ant.apache.org/manual/develop.html#writingowntask when using a
TaskContainer-implementation. These are used in webtest-project by Canoo for
example.

diff follows:

Index: AntTag.java
===================================================================
RCS file:
/home/cvspublic/jakarta-commons/jelly/jelly-tags/ant/src/java/org/apache/commons/jelly/tags/ant/AntTag.java,v
retrieving revision 1.27
diff -u -r1.27 AntTag.java
--- AntTag.java 25 Feb 2004 01:23:59 -0000      1.27
+++ AntTag.java 6 May 2004 15:35:56 -0000
@@ -181,6 +181,10 @@
                 // now lets set all the attributes of the child elements
                 // XXXX: to do!
     
+               if (parentTask instanceof TaskContainer) {
+                   ((TaskContainer)parentTask).addTask(task);
+               }
+        
                 // now we're ready to invoke the task
                 // XXX: should we call execute() or perform()?
                 task.perform();",jelly-ant-tags TaskContainer Support,,1,,diongillard,True,,diongillard
commons-jelly,JELLY-182,2005-01-04T16:20:57.000+0000,,2005-01-04T16:20:57.000+0000,,,New Feature,Major,,,,,,,,,['taglib.email'],[''],"-- Moved from Bugzilla 26630 --

It would be nice if it were possible to specify the content type for
the email message.  text/plain can be the default, but in some cases
text/html, etc. is useful.",Add parameter contentType to the EmailTag,,,,diongillard,True,,diongillard
commons-jelly,JELLY-181,2005-01-04T16:16:56.000+0000,,2005-01-04T16:18:26.000+0000,,,New Feature,Major,,,,,,,,,['taglib.swt'],[''],"-- Moved from Bugzilla 24466 --

Attached is a patch that adds support for a 'wallpaper' attribute on Controls,
which one can use to set a wallpaper image (similar to a web page).

eg:

<shell .... wallpaper=""image.location.png"" ....>
...
</shell>

Was useful for me, maybe it's useful for others.

Cheers,

Marcus",SWT wallpaper support,,,,diongillard,True,,diongillard
commons-jelly,JELLY-179,2004-12-29T10:46:47.000+0000,,2004-12-29T10:47:42.000+0000,,,New Feature,Minor,,['1.0'],,,,,,,['core / taglib.core'],[''],"Copied from To Do List:

Implementa a META-INF/services mechanism for mapping Jelly libraries to namespace URIs in a similar way to how the commons-discovery and JAXP libraries work. This would allow Jelly libraries to be distributed in a self contained JAR then just put on the classpath and they'd be usable.

Attached patch uses Commons Discovery 0.2 with JDK 1.3 specification for service providers. The namespace URI for the Jelly Tag library is read from a properties file. Example included and all JUnit tests successfull.",META-INF/services mechanism for mapping Jelly libraries to namespace URIs,1,,,jmayrbaeurl,True,,jmayrbaeurl
commons-jelly,JELLY-178,2004-12-18T18:44:54.000+0000,,2004-12-18T19:58:08.000+0000,,,New Feature,Major,,['1.0-beta-5'],,,,,,,,,"Jetty tag lib uses deprecated feature in jetty 5.0 like ""org.mortbay.http.SecurityConstraint.Authenticator""
and some more.",jetty tag lib can't  be used with jetty 5.0,1,,,marc_dexet,True,,marc_dexet
commons-jelly,JELLY-176,2004-12-12T18:52:08.000+0000,,2004-12-12T18:52:08.000+0000,,,New Feature,Major,,,,,,,,,['taglib.fmt'],[''],Jelly fmt taglib should provide an implementation of the fmt:formatNumber tag to comply the JSTL 1.1 specs.,fmt taglib should provide fmt:formatNumber tag,,,,jcreigno,True,,jcreigno
commons-jelly,JELLY-173,2004-12-09T02:48:36.000+0000,,2006-04-05T22:16:00.000+0000,,,New Feature,Major,,,,,,,,,['taglib.xml'],[''],"There is a bug on the maven-ear-plugin regarding support to genera

In order to fix a maven-ear-plugin bug (http://jira.codehaus.org/browse/MPEAR-30), I need to generate a XML-Schema based XML document, but looks like the taglib doesn't allow it.

I can see 2 possible solutions:

1.Add more attributes to the <x:element> tag. Example:

<x:element name=""application"" xmlns=""http://java.sun.com/xml/ns/j2ee""
             xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
             xsi:schemaLocation=""http://java.sun.com/xml/ns/j2ee http://java.sun.com/xml/ns/j2ee/application_1_4.xsd""
             version=""1.4"">

2.Create a new tag specifically for this purpose. Example:

<x:schemaRoot name=""application"" xmlns=""http://java.sun.com/xml/ns/j2ee""
             xsi=""http://www.w3.org/2001/XMLSchema-instance""
             schemaLocation=""http://java.sun.com/xml/ns/j2ee http://java.sun.com/xml/ns/j2ee/application_1_4.xsd""
             version=""1.4"">


If there is another alternative, please let me know.

-- Felipe


",Support for XML Schema,1,,,felipeal,True,,felipeal
commons-jelly,JELLY-164,2004-11-18T17:13:04.000+0000,,2004-11-19T07:49:31.000+0000,,,New Feature,Minor,,['1.0-beta-5'],,,,,,,['core / taglib.core'],[''],"IterateTag is like org.apache.commons.jelly.tags.core.ForEachTag but can iterate over several collections. Where org.apache.commons.jelly.tags.core.ForEachTag iterate over one collection and set Value in one variable like : 
 <forEach var=""oneVar"" items=""oneCollection"">
    <do something value=""${oneVar}""/>
 </forEach>
 
IterateTag is like : 

 <iterate var=""var_1;var_2;...;var_P"" items=""Collection_1;Collection_2;....;Collection_P"">
    <do something value=""${var_1} ${var_2} ${var_3}""/>
 </iterate>
 
Each Collection points at next element at the same time.

Over the n iteration, for j = 1 to P : var_j = Collection_j.get(n);
IterateTag stop on the first ended collection, so n = Min( Collection_1.size(), ..., Collection_P.size());",New Tag for parallel iteration,1,,,marc_dexet,True,,marc_dexet
commons-jelly,JELLY-157,2004-10-13T13:40:38.000+0000,2004-10-26T16:45:32.000+0000,2009-02-20T20:49:34.000+0000,,Fixed,New Feature,Minor,['1.0'],['1.0'],,,,,,,['taglib.util'],[''],"A cool tag would be one that takes one collection (or map) and returns a new collection/map, sorted by a given property (on each of the collection's elements).

This tag could be used, for instance, on Maven's multiproject plugin to generate a sorted report of sub-projects:

<util:sort var=""multiprojectsSortedByName"" items=""${multiprojects}"" sortingProperty=""name""/>

Such tag shouldn't be hard to implement, but I haven't the time to do the full job right now (i.e., writing the tag, test cases, documentation, etc...), but I can give it a shot in the near future, if there is enough interested on such feature (in other words, if someone is willing to commit the changes :-).


-- Felipe
",New tag that generates sorted collection,,1,,felipeal,True,,felipeal
commons-jelly,JELLY-143,2004-09-13T01:09:56.000+0000,,2004-09-13T01:15:59.000+0000,,,New Feature,Major,,,,,,,,,['core / taglib.core'],[''],"The Marmalade guys seem to want/be proud of support for pluggable expression languages. I think that this would be possible in Jelly.

We already have pluggable expression evaluators. The only change would be to add a default evaluator into either the XMLParser or the context. Or both. This default evaluator would have to be given to TagScripts, which would use it to evaluate expressions.

In this way, tags like xslt could continue to use their own expression system while other tags could use a pluggable expression system.
",Support for pluggable expression languages,,,,hgilde,True,,hgilde
commons-jelly,JELLY-118,2004-08-12T04:55:08.000+0000,,2004-08-15T23:52:44.000+0000,,,New Feature,Major,,['1.0-beta-4'],,,,,,,['core / taglib.core'],[''],"It would be really nice if there was a ForTokensTag on Core.

I know you can obtain the same result using a util:tokenizer and core:forEach combo, but that's not trivial for those with JSTL experience.

So, if you are fine with that, I can implement such tag and submit it as a patch.
",Core should have a forTokens tag,1,,,felipeal,True,,felipeal
commons-jelly,JELLY-116,2004-08-10T01:55:48.000+0000,2004-08-12T04:27:33.000+0000,2004-08-16T03:45:50.000+0000,,Fixed,New Feature,Major,['1.0-beta-4'],['1.0-beta-4'],,,,,,,,,"If the method invoked throws a InvocationTargetException, invoke and invokeStatic should allow the caller to obtain the original exception.

So, I'm providing a patch (including test cases) that export that exception if the exceptionVar paramater is set (and in this case, it does not throw a JellyTagException). If that parameter is not set, it assumes the old behaviour (i.e., throws a JellyTagException).
",[PATCH] InvokeTag and InvokeStaticTag should export exception,1,,,felipeal,True,diongillard,felipeal
commons-jelly,JELLY-93,2003-10-17T05:37:09.000+0000,2004-09-07T06:40:18.000+0000,2004-09-07T06:40:18.000+0000,,Fixed,New Feature,Minor,['1.0-beta-4'],,,,600,600,600,,['taglib.swing'],[''],Implementation of an EtchedBorder.,Implemetation of  an EtchedBorder,,,,rmcintosh,True,diongillard,rmcintosh
commons-jelly,JELLY-92,2003-10-17T05:10:23.000+0000,2004-09-07T06:43:30.000+0000,2004-09-07T06:43:29.000+0000,,Fixed,New Feature,Minor,['1.0-beta-4'],,,,,,,,['taglib.swing'],[''],Contributing an implementation of an EmptyBorder tag.,Implementation of EmptyBorder,,,,rmcintosh,True,diongillard,rmcintosh
commons-jelly,JELLY-89,2003-10-01T17:46:55.000+0000,2004-09-07T06:33:08.000+0000,2004-09-07T06:33:08.000+0000,,Fixed,New Feature,Minor,['1.0-beta-4'],['1.0-beta-4'],,,,,,,,,"I think it will be interesting if we can add a description for tag and taglib tag in define tag library. Thus, we could add more documentation in script, in particular, in maven plugin that define some tags.",Add a description for define:tag and define:taglib tags,,,,evenisse,True,,evenisse
commons-jelly,JELLY-82,2003-09-04T19:59:18.000+0000,,2004-09-02T06:40:58.000+0000,,,New Feature,Major,['1.1-beta-1'],['1.0-beta-4'],,,1800,1800,1800,,['core / taglib.core'],[''],"In support of the updated ExpressionTableModel class, adding a Vector tag since DefaultTableModel uses Vectors instead of Lists for the rows.",Add UseVector tag,,,,sirfergy,True,,sirfergy
commons-jelly,JELLY-65,2003-07-25T22:21:50.000+0000,2003-07-28T02:37:14.000+0000,2003-07-28T02:37:14.000+0000,,Fixed,New Feature,Major,,,,,,,,,['taglib.swing'],[''],The purpose of this patch is to include new functionality in the jelly swing tag library.  I created a FocusListenerTag and a KeyListenerTag and modified ComponentTag and SwingTagLibrary.,Key and Focus Listeners added,1,,,sirfergy,True,,sirfergy
commons-jelly,JELLY-60,2003-06-24T21:24:37.000+0000,2004-09-12T16:58:47.000+0000,2004-09-12T16:58:47.000+0000,,Fixed,New Feature,Major,['1.0-beta-5'],,,,,,,,['taglib.soap'],[''],"I decided to go ahead and create a <soap:invoke> tag that takes a whole soap request.  I called it <soap:invokeraw/>.  If anyone has a better name, I'm up for it .  I found this is actually a great capability for testing since it allows me to completely simulate other toolkits - like MS.NET's Compact Framework.  I just capture the request with Axis's tcpmon and throw it into jelly.

So I use it like this:

      <soap:invokeraw endpoint=""http://localhost/some/service""
              var=""response"">

        <soap:Envelope ....>
       <soap:Header>
          Your headers
       </soap:Header>
       <soap:Body>
          <ns:yourMethod />
       </soap:Body>
      </soap:Envelope>
    
      </soap:invokeraw>

Also in the patch is username/password support for the invoke tag.  That  one isn't well tested though.",[SOAP] InvokeRaw tag and Username/Password features for Invoke tag,,,,dandiep,True,,dandiep
commons-jelly,JELLY-59,2003-06-23T04:55:59.000+0000,2004-09-12T15:54:31.000+0000,2004-09-12T15:55:02.000+0000,,Fixed,New Feature,Major,['1.0-beta-5'],,,,14400,14400,14400,,['taglib.http'],[''],"This patch support multi-part mime requests (in the HTTP library).  I will attach the two new files, Part.java and MPPost.java.  Besides those files you just need to modify HttpTagLibrary.java to include the new files:

+        registerTag(""mppost"", MppostTag.class);
+        registerTag(""part"", PartTag.class);

",multi-part mime http request,1,,,wkeese,True,,wkeese
commons-jelly,JELLY-49,2003-05-05T12:48:41.000+0000,2004-09-12T16:42:21.000+0000,2004-09-12T16:42:21.000+0000,,Fixed,New Feature,Major,['1.0-beta-5'],,,,,,,,['submissions'],[''],"This tag library wrap's the ORO regexp engine. It provides two tags that can be used as follows:

<regexp:match
  var=""foo"" 
  text=""ID1234"" 
  expr=""[A-Z][A-Z][0-9]{4}""/>

<regexp:contains
  var=""bar"" 
  text=""Hello World"" 
  expr=""World""/>

The code is not written by me, but by Christian Amor Kvalheim, so give the cred to him (he was too sloppy to submit this himself).",Regexp Tag Library,,,,rinkrank,True,proyal,rinkrank
commons-jelly,JELLY-48,2003-04-15T12:57:26.000+0000,2004-09-05T15:43:17.000+0000,2004-09-05T15:43:16.000+0000,,Fixed,New Feature,Minor,['1.0-beta-4'],,,,,,,,['taglib.swt'],[''],,[SWT Taglibrary] dialog tag,,,,ckl,True,,ckl
commons-jelly,JELLY-42,2003-03-05T19:03:01.000+0000,2004-09-05T15:47:57.000+0000,2004-09-05T15:47:57.000+0000,,Fixed,New Feature,Major,['1.0-beta-4'],,,,,,,,['taglib.swt'],[''],,[SWT taglib] add cTabFolder + cTabItem widgets,,,,ckl,True,jstrachan,ckl
commons-jelly,JELLY-39,2003-02-27T13:31:02.000+0000,2003-09-10T17:50:10.000+0000,2003-09-10T17:50:10.000+0000,,Fixed,New Feature,Minor,['jface-1.0'],,,,,,,,['taglib.jface'],[''],"allow <preferenceDialog parent=""${applicationWindow.Shell}"">

You need this if the preferencesDialog is in a separate file.

It would perhaps be better if 'findAncestorWithClass(ApplicationWindowTag.class)'
would return the ApplicationWindowTag. Then you can use 
<preferenceDialog parent=""${applicationWindow}"">

James, could you also add a taglib.jface and taglib.swt component in JIRA ?",[JFace taglib] PreferenceDialogTag parent patch,,,,ckl,True,jstrachan,ckl
commons-jelly,JELLY-38,2003-02-26T14:25:37.000+0000,,2004-09-12T16:16:29.000+0000,,,New Feature,Major,,,,,,,,,['taglib.jface'],[''],,[JFAce taglib] remove dependency + fix examples,,,,ckl,True,jstrachan,ckl
commons-jelly,JELLY-37,2003-02-26T13:28:42.000+0000,2003-02-26T14:10:28.000+0000,2003-02-26T14:10:28.000+0000,,Fixed,New Feature,Major,,,,,,,,,,,"see
http://www.eclipse.org/documentation/html/plugins/org.eclipse.platform.doc.isv/doc/reference/api/org/eclipse/jface/wizard/package-frame.html

and
http://www.eclipse.org/articles/Article-JFace%20Wizards/wizardArticle.html

",[JFace taglob] JFace wizard implementation,,,,ckl,True,jstrachan,ckl
commons-jelly,JELLY-36,2003-02-25T20:09:26.000+0000,2003-09-04T23:33:41.000+0000,2003-09-04T23:33:41.000+0000,,Fixed,New Feature,Minor,,,,,,,,,['taglib.jface'],[''],"Basically provides a gui for a java.util.Properties object,

see http://www.eclipse.org/documentation/html/plugins/org.eclipse.platform.doc.isv/doc/reference/api/index.html

and

http://www.eclipse.org/articles/Article-Field-Editors/field_editors.html
",[JFace taglib] JFace preferences support,,,,ckl,True,jstrachan,ckl
commons-jelly,JELLY-35,2003-02-21T18:06:17.000+0000,2003-02-26T12:33:23.000+0000,2003-02-26T12:33:23.000+0000,,Fixed,New Feature,Minor,,,,,,,,,,,"<applicationWindow>, <action> , example JFace script and more ...",[JFace taglib part3],,,,ckl,True,jstrachan,ckl
commons-jelly,JELLY-34,2003-02-20T19:39:04.000+0000,2003-09-04T23:22:50.000+0000,2003-09-04T23:22:50.000+0000,,Fixed,New Feature,Minor,,,,,,,,,['taglib.jface'],[''],"This is a first attempt to start adding JFace support to SWT.
This taglib extends the SWT taglib",[SWT JFace] start of JFace taglib,,,,ckl,True,jstrachan,ckl
commons-jelly,JELLY-33,2003-02-19T20:54:10.000+0000,2003-09-04T23:33:07.000+0000,2003-09-04T23:33:07.000+0000,,Fixed,New Feature,Minor,,,,,,,,,['taglib.swt'],[''],,[SWT libtag] Imagetag add image for decorations,,,,ckl,True,jstrachan,ckl
commons-jelly,JELLY-32,2003-02-19T16:08:59.000+0000,2003-09-04T23:27:49.000+0000,2003-09-04T23:27:49.000+0000,,Fixed,New Feature,Minor,,,,,,,,,['taglib.swt'],[''],set background and foreground colors of swt widgets,[SWt taglib] set colors of widgets,,,,ckl,True,jstrachan,ckl
commons-jelly,JELLY-31,2003-02-14T14:50:39.000+0000,2003-02-19T10:29:22.000+0000,2003-02-19T10:29:22.000+0000,,Fixed,New Feature,Minor,,,,,300,300,300,,,,"Adds a <scolledComposite> tag. 
For this widget, the content that will be scrolled, must be set with the setContent method. 
",scolledComposite tag,,,,ckl,True,jstrachan,ckl
commons-jelly,JELLY-30,2003-02-11T15:45:38.000+0000,2003-09-04T23:32:40.000+0000,2003-09-04T23:32:40.000+0000,,Fixed,New Feature,Minor,,,,,300,300,300,,['taglib.swt'],[''],patch for widget size attribute,SWT size attribute,,,,ckl,True,jstrachan,ckl
commons-jelly,JELLY-29,2003-02-07T15:25:04.000+0000,2003-09-04T23:27:04.000+0000,2003-09-04T23:27:04.000+0000,,Fixed,New Feature,Minor,,,,,,,,,['taglib.swt'],[''],,SWT image tag,,,,ckl,True,jstrachan,ckl
commons-jelly,JELLY-20,2003-01-15T19:00:26.000+0000,2003-09-04T23:28:43.000+0000,2003-09-04T23:28:43.000+0000,,Fixed,New Feature,Major,,,,,,,,,['taglib.fmt'],[''],,Create new taglib that mimics JSTL fmt for i18n & Formatting,,,,willievu,True,jstrachan,willievu
commons-jelly,JELLY-1,2002-07-18T20:47:13.000+0000,,2004-12-19T08:50:18.000+0000,,,New Feature,Minor,['1.1-beta-1'],,,,,,,,['core / taglib.core'],[''],"Implement a simpler Scope interface to be used by JellyContext for variable storage and retrieval, to allow for arbitrary integrations for storage.",break variable storage away from the JellyContext,2,4,,bob,True,jstrachan,bob
commons-jexl,JEXL-297,2019-04-18T08:06:39.000+0000,2019-05-27T15:01:56.000+0000,2019-05-27T15:01:56.000+0000,,Not A Problem,New Feature,Minor,,['3.1'],,,,,,,,,"My project uses JEXL to both validate and evaluate expressions.

Let me explain: 

The product has an excel-like feature and two stages:

In the ""design"" stage, a user may enter a literal OR a formula in input fields. fields are either numeric-typed or boolean-typed, so an expression may be numeric or boolean. the expressions may contain variables. In the ""production"" stage, the expressions are evaluated. variables get values from the DB.  

During the design, we are validating the expressions and alert the user if there are syntax errors. at that time, validation is using JEXL to evaluate the expression and examines the result for exceptions or error responses. the variables are assigned some default value. 

The problem I am having is to come up with default values that will not cause errors not related to syntax (like division by zero) I am encountering all kinds of arithmetic exceptions that are because of values and not because of syntax. 

So perhaps JEXL can support ""syntax validation"" feature, where variables can be ""any value that is valid at this position""",validate expression (instead of evaluate),1,,,sharonbn,True,,sharonbn
commons-jexl,JEXL-295,2019-03-27T17:10:54.000+0000,2019-03-29T14:47:45.000+0000,2019-04-01T08:44:36.000+0000,,Fixed,New Feature,Major,['3.2'],['3.1'],,,,,,,,,"WHAT
Introduce a new operator, the unary plus as in '+4' or '+b' expressions. JEXL arithmetic already allows using a unary minus operator (the negate operation) so there is logic in having the counterpart.

The proposed default behaviour would be to:
- call the 'Math.abs(...)' method for numbers
- return true for boolean 
- could uppercase strings ? (should '-' lowercase strings ??)


HOW
- Follow the unary minus syntax and construct in the .jjt / ASTUnaryPlus / ParserVisitor
- Implement Interpreter, Debugger
- Implement abs in JexlArithmetic",Add unary plus operator,2,,,henrib,True,henrib,henrib
commons-jexl,JEXL-292,2019-02-20T08:39:11.000+0000,2019-04-26T08:39:00.000+0000,2019-04-26T09:51:55.000+0000,,Implemented,New Feature,Minor,['3.2'],['3.1'],,,,,,,,,"As of now the default implementation of {{internal.introspection.Uberspect}} class does not allow the usage of anything other than default instance of {{internal.introspection.Permissions}} class, although internally the {{internal.introspection.Introspector}} has a constructor that accesspts the {{Permissions}} object. Subclassing of {{Uberspect}} does not help here since {{Uberspect.base()}} method is decalred {{final}}. We need some way to pass customized {{Permissions}} class to default {{Uberspect}} implementation",Allow to specify custom Permissions class for Uberspect to be used later by Introspector,2,,,dmitri_blinov,True,henrib,dmitri_blinov
commons-jexl,JEXL-284,2018-12-18T18:43:45.000+0000,,2018-12-28T09:26:04.000+0000,,,New Feature,Minor,,['3.1'],,,,,,,,,"It is possible to break down JEXL expressions in order to know how each element (operand) was resolved?

Example :

EXP1 = A * B + C + 200

C = D * F / 2 + FUNCTION(X,Y)

My whish is to print all the values involved in the expression resolution in a handy print to understand the whole expression elements.

Something like :(

 

-EXP1

---A

--------1

---B

--------2

--C

--------D

--------F

--------2

-------- FUNCTION(X,Y)

--------------------------X

---------------------------------3

--------------------------Y

---------------------------------4

--------34

--200

-9999

 

Thanks

 

 

 

 

 

 ",Break down recursive expressions- Handy print,1,,,osymad,True,,osymad
commons-jexl,JEXL-269,2018-08-28T07:02:57.000+0000,,2018-09-20T06:05:49.000+0000,,,New Feature,Minor,,['3.1'],,,,,,,,,"Introduce new extended syntax of 'for' statement, which allows to specify two variables, like the following
{code:java}
for (var i, item : list){code}
Inside the loop, the first variable gets current iteration counter, starting from 0, and the second variable gets current iterated value. The special consideration is taken for iteration over map entries, like the following
{code:java}
for(var key, entry : map){code}
, in this case the first variable is the map key, and the second is the corresponding map value",Indexed for-loop,2,,,dmitri_blinov,True,,dmitri_blinov
commons-jexl,JEXL-268,2018-08-21T09:36:59.000+0000,,2018-09-03T08:28:26.000+0000,,,New Feature,Minor,,['3.1'],,,,,,,,,"Java8 introduced functional interfaces, among them {{Function}} and {{BiFunction}} interfaces which are used as parameters to method calls, for example, {{Map.computeIfAbsent()}} or {{Map.computeIfPresent()}}. In Jexl we have lambdas which in theory are good candidates to construct a {{Function}} for lambda with one parameter, and a {{BiFunction}} for lambda with two parameters.

The problem is Jexl can not currently support Java8 features and all instances of lambda are of single {{internal.Closure}} class, which should not try to implement both interfaces at once. One of the solutions, IMO, is to have a feature to overload lamda creations to construct custom lambda classes without shaking Jexl code tree.",Jexl lambdas as parameters to new Java8 methods,2,,,dmitri_blinov,True,,dmitri_blinov
commons-jexl,JEXL-266,2018-08-09T11:39:14.000+0000,,2018-08-17T09:58:29.000+0000,,,New Feature,Minor,,['3.1'],,,,,,,,,"The for-loop in JEXL provides a convenient way to iterate over different types of collections, however, its not possible for a script writer to utilize underlying
{code:java}
iterator.remove(){code}
method within such a loop. The proposal is to introduce new {{remove}} statement which should be used within for-loops and should internally call {{iterator.remove()}} method and skip the loop to the next element;

For example, the following code should remove items {{1,2,3}} from set and return value {{3}}.
{code:java}
var set = {1,2,3,4,5,6}; for (var item : set) if (item <= 3) remove; return size(set)
{code}
",Allow to remove an element from iterator collection within for-loops,2,,,dmitri_blinov,True,,dmitri_blinov
commons-jexl,JEXL-262,2018-05-23T08:43:19.000+0000,2018-08-16T13:55:17.000+0000,2018-08-20T08:41:02.000+0000,,Won't Do,New Feature,Minor,['3.1'],['3.1'],,,,,,,,,"I wonder is it possible (not difficult) to implement in Jexl a construct that would allow us to initialize object properties in one statement, so that it would be possible to replace the code like
{code}
var i = Address;
i.City = 'NY';
i.Street = '1st Avenue';
i.House = '22';
return i;
{code}
with something like
{code}
Address {City : 'NY', Street : '1st Avenue', House : '22'}
{code}

May be we could also consider a variant for array-style accessors, like
{code}
var i = Address;
i[""City""] = 'NY';
i[""Street""] = '1st Avenue';
i[""House""] = '22';
return i
{code}
would be identical to
{code}
Address {""City"" : 'NY', ""Street"" : '1st Avenue', ""House"" : '22'}
{code}

I think this is somewhat similar to inline map initialization syntax, which is proved to be very useful and productive, so it would be nice to have such a feature for common objects too",Inline object property initialization construct,1,,,dmitri_blinov,True,henrib,dmitri_blinov
commons-jexl,JEXL-253,2018-01-25T16:52:14.000+0000,2019-06-17T14:37:14.000+0000,2019-06-17T18:35:59.000+0000,,Fixed,New Feature,Major,['3.2'],['3.1'],,,,,,,,,"At the moment, the permissions in {{JexlSandbox}} takes the object's class name only into the consideration. So, if someone adds {{java.util.Set}} into the white list, but if the real object is an empty set ({{Collections.emptySet()}}), then it cannot allow invocations on {{#contains(Object)}} operation, for instance.

I think it would be very convenient if it optionally allows to set whites or blacks based on super type (interfaces or base classes).

To minimize the effort, I'd suggest adding {{JexlSandbox#permissionsByType(Class<?> type, ...)}}, where the {{type}} means the object type or any super types.
So, if {{JexlSandbox#permissionsByType(java.util.Set.class, ...)}}, then any invocations on any concrete {{java.util.Set}} objects will be affected by that.

Related e-mail thread: ""[JEXL] white list classes, not by interfaces?"" (10/19/17).",Permissions by super type in JexlSandbox,1,,,woon_san,True,henrib,woon_san
commons-jexl,JEXL-252,2018-01-25T15:25:04.000+0000,2018-02-07T10:09:28.000+0000,2018-02-07T10:09:28.000+0000,,Fixed,New Feature,Minor,['3.2'],['3.1'],,,,,,,,,"Now we have a possibility to access an object property by specifying either {code}object.property{code} or {code}object.""property""{code} expression, but not {code}object.`property`{code} expression. Since interpolated strings can be used anywhere in scripts as ordinal strings, it would be logical to allow them to be used in property access operators too. It would allow to have the property name not necessarily to be a constant, but to be evaluated without using the array access operator, since property access operator and array access operator could be implemented (overloaded) differently for the object.",Allow for interpolated strings to be used in property access operators,2,,,dmitri_blinov,True,henrib,dmitri_blinov
commons-jexl,JEXL-251,2018-01-24T13:57:47.000+0000,2018-01-25T15:10:59.000+0000,2018-01-25T15:10:59.000+0000,,Duplicate,New Feature,Minor,,['3.1'],,,,,,,,,"It would be more convenient for a script writer to have a kind of safe navigation operator, in the form of, for example, ({{object?.property),}} for null pointer dereferences. I think its quite a common feature of many scripting (and compiled languages) now. The safe operator should work like basic navigation operator for non null values and should short-circuit the entire expression evaluation after encountering null value.

I see the difference of new operator from the existing lenient mode evaluation in ability to explicitly define places in an expression where {{null}} deference is allowed, for example the expression {code}Order.Customer?.Name{code} defines that each {{Order}} should have a {{Customer}} and only the {{Name}} of the {{Customer}} is optional.

The difference of new operator from the existing null coalescing {{??}} operator is in its ability to short-circuit evaluation.

So, safe navigation operator would be a helpful syntaxic sugar which in my opinion does not overlap with an existing functionality.
",Safe navigation operator,1,,,dmitri_blinov,True,,dmitri_blinov
commons-jexl,JEXL-250,2018-01-24T13:57:31.000+0000,2018-02-07T10:09:54.000+0000,2018-09-17T20:07:28.000+0000,,Fixed,New Feature,Minor,['3.2'],['3.1'],,,,,,,,,"It would be more convenient for a script writer to have a kind of safe navigation operator, in the form of, for example, ({{object?.property),}} for null pointer dereferences. I think its quite a common feature of many scripting (and compiled languages) now. The safe operator should work like basic navigation operator for non null values and should short-circuit the entire expression evaluation after encountering null value.

I see the difference of new operator from the existing lenient mode evaluation in ability to explicitly define places in an expression where {{null}} deference is allowed, for example the expression {code}Order.Customer?.Name{code} defines that each {{Order}} should have a {{Customer}} and only the {{Name}} of the {{Customer}} is optional.

The difference of new operator from the existing null coalescing {{??}} operator is in its ability to short-circuit evaluation.

So, safe navigation operator would be a helpful syntaxic sugar which in my opinion does not overlap with an existing functionality.
",Safe navigation operator,3,,,dmitri_blinov,True,henrib,dmitri_blinov
commons-jexl,JEXL-249,2018-01-22T08:27:39.000+0000,2018-01-22T13:15:47.000+0000,2018-01-22T13:15:47.000+0000,,Won't Fix,New Feature,Minor,,['3.1'],,,,,,,,,"From one of the latest commits I can see we have started using {{Map}} interface methods that were introduced in Java 1.8, {{Map.putIfAbsent()}} to speak directly. Can we expect that from the 3.2 release anytime soon, we can rely on Java 8 to be minimum supported version?",Java 1.8 as minimum supported version,1,,,dmitri_blinov,True,henrib,dmitri_blinov
commons-jexl,JEXL-229,2017-06-25T18:14:18.000+0000,2017-09-27T15:01:08.000+0000,2017-09-27T15:01:08.000+0000,,Workaround,New Feature,Minor,['3.1'],['3.1'],,,,,,,,,"For the purpose of type checking in jexl, It whould be convenient to have some simple syntax for referring to class types, like Class<String> or Type<Boolean>. Literal Class<T> should refer to general classes, and literal Type<T> should refer to primitive type classes. For literals Class<T> it could be possible to specify partal class name, which should resolve to classes in basic packages like java.lang and java.util, for example.",Introduce new syntax for class literals: Class<T> and Type<T>,2,,,dmitri_blinov,True,,dmitri_blinov
commons-jexl,JEXL-226,2017-06-07T03:26:23.000+0000,2017-06-28T06:55:52.000+0000,2018-02-04T15:57:02.000+0000,,Fixed,New Feature,Major,['3.2'],['Later'],,,3600,3600,3600,,,,"Jexl already supports null-coalescing semantics with the operator ""?:"". So it should be a simple matter to extend it with a different syntax.
https://docs.microsoft.com/en-us/dotnet/csharp/language-reference/operators/null-conditional-operator

https://github.com/apache/commons-jexl/pull/4",add ?? operator support ,3,,['features'],min-mwei,True,henrib,min-mwei
commons-jexl,JEXL-210,2016-08-03T14:42:09.000+0000,2016-08-04T15:00:56.000+0000,2016-08-12T10:52:35.000+0000,,Fixed,New Feature,Minor,['3.1'],['3.0'],,,,,,,,,"I don't see a way now to cancel script execution with some kind of error. Unfortunately it's not possible to just throw an exception from some method as this will rely on current settings of context/engine *strictness* and *verboseness*. Using InterruptedException for this purpose is not an option because I think it has special meaning of cancelling the current thread execution. Yet the task I beleive is quite common - to inform the executing environment that the script has encountered some unavoidable situation and can not continue. Just like *return* statement but returning not a value but an error. 

For this purpose we can simply introduce some new type of exception, for example 
{code}
    public static class Error extends JexlException {
    ...
{code}
and by throwing it from any method the JexlEngine will terminate the current script execution regardless of strictness/verboseness. May be this task even deserves to have a special operator, for example 

{code}
raise 'Something has happended';
{code}",The way to cancel script execution with an error,2,,,dmitri_blinov,True,henrib,dmitri_blinov
commons-jexl,JEXL-200,2016-06-17T12:01:30.000+0000,2016-06-23T12:53:32.000+0000,2016-06-23T12:53:33.000+0000,,Resolved,New Feature,Minor,,['3.0'],,,,,,,,,"Allow for dynamically defined functions inside a script where the definition of the function comes from a string expression, for example:

{code}
var x = function(y) => ""return y+2"";
{code}

the defined function should be accessible after its definition is successfully parsed as ordinary function, and can be evaluated in the current context.

{code}
if (x(40) eq 42) {...
{code}

I think the idea of dynamic script evaluation is not unusual, and though to some extent can be implemented via side-added functions like *eval(expr)* a dedicated syntax for this will benefit from the simplicity of ordinal functions and the ability to control mapping of the arguments between a caller and the defined function.",Support for dynamic scripting in jexl scripts,2,,,dmitri_blinov,True,henrib,dmitri_blinov
commons-jexl,JEXL-185,2016-01-10T09:01:50.000+0000,,2018-12-28T09:26:04.000+0000,,,New Feature,Minor,,['3.0'],,,,,,,,,"Since the rising complexity of the JEXL scripts makes it easier to write sophisticated scripts, and harder to debug overall script evaluation, it would be helpful to provide some way to trace individual statements execution within the script by a callback interface. 

Callback could be applied to JEXL engine as a whole, or to Script.executeScript method as an additional parameter for example.

Callback interface should contain a method which should be invoked by JEXL on completion of individual statement. Method parameters should contain such values as DebugInfo (start..end statement position within the script), statement execution result or exception (which in silent mode may be swallowed), initial context, and possibly a stack frame to be able to peep into local variables.",Ability to trace execution of script statements,1,1,,dmitri_blinov,True,,dmitri_blinov
commons-jexl,JEXL-177,2015-08-21T06:22:12.000+0000,2015-08-27T20:22:13.000+0000,2016-03-21T14:10:14.000+0000,,Fixed,New Feature,Minor,['3.0'],['3.0'],,,,,,,,,"Introduce new string literals to jexl script that allow unified expressions to be used, string literals could use for example triple quoted (single or double) syntax for delimiters, may include newlines and should not escape single quotes

{code}
user = ""Bob""; server = ""Test""; s = """"""Hello ${user}, welcome to ${server}""""""; return s;
{code}",Unified expressions to be used in String literals in Jexl scripts,2,,,dmitri_blinov,True,henrib,dmitri_blinov
commons-jexl,JEXL-175,2015-08-20T16:13:06.000+0000,2018-09-13T16:33:01.000+0000,2018-09-13T16:33:01.000+0000,,Fixed,New Feature,Minor,['3.2'],['2.1.1'],,,,,,,,,"Introduce the pattern operator 
{code}
~/regex/
{code}

to create a java.util.regex.Pattern instance.

The string inside slashes should not require to escape backslashes and quotes as in common string literals, so that regex would be more readable",java.util.regex.Pattern creation operator,2,,,dmitri_blinov,True,henrib,dmitri_blinov
commons-jexl,JEXL-174,2015-08-15T06:00:55.000+0000,2015-08-27T20:17:02.000+0000,2016-03-21T14:10:10.000+0000,,Fixed,New Feature,Minor,['3.0'],['3.0'],,,,,,,,,"In analogy with overloading operators like empty(), size() etc, provide a way to overload property ""get"" and ""set"" operators ([] and .), like 
{code}
   public Object getAt(Object obj, Object index) {...}
   public Object putAt(Object obj, Object index, Object value) {...}
{code}

Overloaded operators should be tried before any standard access stategy, ie MAP, POJO etc.",Overloadable property access operators,2,,,dmitri_blinov,True,henrib,dmitri_blinov
commons-jexl,JEXL-173,2015-08-15T05:51:15.000+0000,2015-08-27T20:16:25.000+0000,2016-03-21T14:10:07.000+0000,,Fixed,New Feature,Minor,['3.0'],['3.0'],,,,,,,,,"Suppose we have a plain java class which implements a ""call"" method
{code}
public class Foo {
   public Object call(Object bar) {
       return null;
   }
}
{code}

In JEXL we could resolve a function() operator on this class directly, in other words use this class as a closure and invoke a ""call"" method.

{code}
x = new (""Foo""); return x(3);
{code}",Duck-typed java closures,2,,,dmitri_blinov,True,henrib,dmitri_blinov
commons-jexl,JEXL-151,2015-03-11T06:01:24.000+0000,2015-07-27T15:22:56.000+0000,2016-03-21T14:10:17.000+0000,,Fixed,New Feature,Minor,['3.0'],['3.0'],,,,,,,,,"New operators to add or remove collection items to/from Collection. Operators can be named as += and -= respectively, or any other way. Operator += can be mapped to Collection.addAll(Collection c) if right argument is Collection or to Collection.add(Object o) otherwise. Operator -= can be mapped to Collection.removeAll(Collection c) if right argument is Collection or to Collection.remove(Object o) otherwise. Extend operators =~  (!~) to support CollectionA =~ CollectionB usage, in which case it should map to B.containsAll(A) operation.

",operators to manipulate j.u.Collection,2,,,dmitri_blinov,True,henrib,dmitri_blinov
commons-jexl,JEXL-149,2015-03-09T13:41:12.000+0000,2015-03-16T16:33:39.000+0000,2016-03-21T14:10:10.000+0000,,Fixed,New Feature,Minor,['3.0'],"['2.1.1', '3.0']",,,,,,,,,"""EL 3.0 Spec Section 2.2.1 Set Construction"" defines the possibility to construct an instance of java.lang.util.Set<Object> by using syntax ""{1, 2, 3}"", the strict grammar is given below.

2.2.1.1 Syntax
SetData := ‘{‘ DataList ‘}’
DataList := (expression (‘,’ expression)* )?",Set Construction as per EL 3.0 spec,2,,,dmitri_blinov,True,henrib,dmitri_blinov
commons-jexl,JEXL-139,2012-07-17T10:21:39.000+0000,2012-07-17T12:58:56.000+0000,2016-03-21T14:17:42.000+0000,,Incomplete,New Feature,Minor,,,,,86400,86400,86400,,,,"Code below.

/**
	 * Iterates over the iterable represented by iterExpression
	 * For each row, evaluate the row-expression
	 * Return a string of all row-results, concatenated by separator.
	 * The current current row is indicated by $$__EVAL_ROW and the current index by $$__EVAL_ROWNUM
	 * @param enumExpression
	 * @param expression
	 * @return
	 */
	public String evalExprOverIter( String iterExpression, String rowExpression, String separator){
		Object objIter = evalExpr( iterExpression);
		if( objIter == null ){ return """";}
		Iterable iterabl = null;
		if( objIter.getClass().isArray()){
			iterabl = Arrays.asList( (Object[]) objIter ); 	
		}
		else if( objIter instanceof Map){
			iterabl = ((Map) objIter).entrySet();
		}
		else {
			iterabl = (Iterable) objIter; 
		}
		Iterator iter = iterabl.iterator();
		StringBuilder sbld = new StringBuilder();
		int rownum = 0;
		while( iter.hasNext()){
			Object row = iter.next(); 
			setVar( ""$$__EVAL_ROW"", row );
			setVar( ""$$__EVAL_ROWNUM"", rownum );
			sbld.append( evalExpr( rowExpression) );
			sbld.append( separator );
			rownum++;
		}
		// remove the temp variables we used.
		remVar( ""$$__EVAL_ROWNUM"" );
		remVar( ""$$__EVAL_ROW"" );
		return sbld.deleteCharAt( sbld.length()-1).toString();
","Loop over an iterable, evalaute and expression per row, and return concatenated results",2,,,mmokashi,True,henrib,mmokashi
commons-jexl,JEXL-118,2011-07-31T11:57:40.000+0000,2011-08-03T13:22:34.000+0000,2011-12-14T09:29:08.000+0000,,Fixed,New Feature,Major,['2.1'],['2.0.1'],,,,,,,,,"It would be very useful to have an IN operator, e.g.:

status in ['A', 'B', 'C']

that would return a boolean. This can currently be done with :

status == 'A' or status == 'B' or status == 'C'

but (especially for longer lists) the IN operator would be more concise and more elegant.

Extra bonus points, of course, if this works with variables, e.g.:

status in possibleStatuses

where possibleStatuses would be an object of type Collection.",Provide an IN operator,,,['features'],maxtardiveau,True,henrib,maxtardiveau
commons-jexl,JEXL-116,2011-07-15T08:32:41.000+0000,2011-07-18T12:55:34.000+0000,2011-12-14T09:29:39.000+0000,,Fixed,New Feature,Major,['2.1'],,,,,,,,,,"The idea is to explicitly allow/disallow which classes, methods, constructors and properties the Uberspect can access.
By building an Uberspect with white/black lists, the JEXL engine would only ""see"" allowed constructs and user scripts would thus be restricted to a controlled set of objects and methods.

See http://apache-commons.680414.n4.nabble.com/jexl-JEXL-Secure-Sandbox-tt3626959.html
","Add control over classes, methods, constructors and properties allowed in scripts",,,,henrib,True,henrib,henrib
commons-jexl,JEXL-115,2011-07-13T20:26:58.000+0000,2011-07-17T18:45:29.000+0000,2011-12-14T09:29:24.000+0000,,Fixed,New Feature,Major,['2.1'],,,,,,,,,,"WHY:
To allow proper sandboxing and let users run their own script, it is necessary to control the time a script is allowed to run and avoid runaway executions (for instance while(true); ).
WHAT:
Support for asynchronuous execution is usually based on Runnable/Callable/Future and Thread interruption. The Callable interface seems the most versatile so a method to transform a Script into a Callable seems in order. Thread interruption is in most cases an indication that must be checked (besides locking methods) somehow.
HOW:
Creating a Callable from a Script, Context (and arguments) is straightforward. Checking thread interruption can be performed in the Interpreter loop at ""key"" points namely getting/setting properties, calling methods/functions, resolving identifiers and of course, within loop constructs (for, while). 
BACKGROUND:
http://apache-commons.680414.n4.nabble.com/jexl-JEXL-Secure-Sandbox-tt3626959.html",Add support for asynchronous script execution and cancellation,,,,henrib,True,henrib,henrib
commons-jexl,JEXL-114,2011-07-11T20:55:38.000+0000,2011-07-17T18:31:07.000+0000,2011-12-14T09:27:42.000+0000,,Fixed,New Feature,Major,['2.1'],['2.0.1'],,,,,,,,,"WHY:
Not having local variables nor return in scripts is inconvenient, especially when dealing with loops.
This also precludes using read-only contexts easily which are really convenient when letting end-users enter their own expressions.

HOW:
This could (will) be implemented by extending the parameters feature (script accept parameters during parsing and thus arguments during evaluation) which is itself based on the notion of 'registers' - an array of objects that the interpreter allocates and uses based on script information.
It only requires adding one keyword (""var"" seems the obvious choice) in the .jjt.
The ""return"" keyword is also an easy .jjt addition; obvious implementation is to use an internal exception to force traversing the stack up.

WHAT:
Add the ""var"" and ""return"" keyword.
Also add methods to extract the variables (global - see JEXL-113), the parameters (used during parsing) and the local variables (declared within) a script to help pinpoint problems or prepare the evaluation of scripts.

",Allow scripts to create local variables // Add return keyword,,,,henrib,True,,henrib
commons-jexl,JEXL-106,2011-02-03T14:33:39.000+0000,2011-02-03T17:01:15.000+0000,2011-12-14T09:30:55.000+0000,,Fixed,New Feature,Major,['2.1'],['2.0.1'],,,,,,,,,"When having an expression X=A / B where
A = BigDecimal(1);
B = BigDecimal(3);

the resulting evaluation of expression for X causes :


java.lang.ArithmeticException: Non-terminating decimal expansion; no exact representable decimal result.
	at java.math.BigDecimal.divide(BigDecimal.java:1603)
	at cz.lbbw.fis.playground.TestMS.big(TestMS.java:44)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:73)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:46)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:180)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:41)
	at org.junit.runners.ParentRunner$1.evaluate(ParentRunner.java:173)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:220)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:46)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)

",When divide two BigDecimal values in an expression it results in java.lang.ArithmeticException: Non-terminating decimal expansion; no exact representable decimal result.,,,,mickel,True,henrib,mickel
commons-jexl,JEXL-102,2010-05-20T11:08:50.000+0000,2010-05-21T15:05:56.000+0000,2011-12-14T09:31:12.000+0000,,Fixed,New Feature,Major,['2.1'],['2.0.1'],,,,,,,,,"Might be useful to add ""jexl2"" (and possibly other variants) as a supported script engine name to make it easier to select JEXL 2 when both JEXL1 and JEXL2 are available.","Add ""jexl2"" as a supported name",,,,sebb,True,henrib,sebb
commons-jexl,JEXL-79,2009-08-06T11:57:41.000+0000,2015-07-23T15:45:18.000+0000,2016-03-21T14:10:15.000+0000,,Fixed,New Feature,Major,['3.0'],,,,,,,,,,"JEXL now has fixed arrays. Trying to access a non-existent entry generates an Exception.

Might be useful to allow for the creation and manipulation of growable arrays, e.g. using ArrayList.

Writing to a non-existent entry should just create the value; reading should return null and empty.

This would need a new syntax.

Perhaps:

list=[1,2,3,...]
and
emptyList=[...]

I did wonder about using (), but that would clash with method invocation.

Other ideas welcome!",Add support for growable arrays (ArrayLists),2,1,,sebb,True,henrib,sebb
commons-jexl,JEXL-70,2009-08-04T21:18:39.000+0000,2009-08-14T18:19:03.000+0000,2010-03-09T15:21:57.000+0000,,Fixed,New Feature,Minor,['2.0'],,,,,,,,,,"I think it would be useful to include a main() class which could be used to test scripts.

Something very simple, just one optional parameter which is the name of the script file.
If omitted, then read expressions from System.in.

The app would evaluate the script/expression and print the return value.

If the JexlEngine is created using JexlScriptEngineFactory, then it would allow easy testing of the JSR-223 features (i.e. JEXL object) as well.

WDYT?",Add main class to allow scripts etc to be tested,,,,sebb,True,,sebb
commons-jexl,JEXL-65,2009-08-01T18:42:17.000+0000,2009-09-03T07:52:52.000+0000,2010-03-09T15:25:30.000+0000,,Fixed,New Feature,Major,['Later'],,,,,,,,,,"See JEXL-50.

It would be useful to have an operator that performs integer division, e.g. 3 intdiv 1 == 1.",Need operator for integer division,,,,sebb,True,henrib,sebb
commons-jexl,JEXL-63,2009-07-31T08:35:20.000+0000,2009-08-14T18:22:06.000+0000,2010-03-09T15:21:57.000+0000,,Fixed,New Feature,Major,['2.0'],,,,,,,,,,"JSR-223 support (through BSF 3.0b2 for JDK < 6)

http://www.nabble.com/-JEXL--2.0-JSR-223-initial-implementation-added---what-next--tt24750307.html
http://www.nabble.com/-JEXL--2.0-and-BSF---JSR-223-tt24706519.html",JSR-223 support,,,,henrib,True,,henrib
commons-jexl,JEXL-58,2009-07-13T21:32:20.000+0000,2009-07-17T18:44:46.000+0000,2010-03-09T15:21:57.000+0000,,Fixed,New Feature,Major,['2.0'],,,,,,,,,," An evaluator similar to the unified EL evaluator used in JSP/JSF based on JEXL.
 It is intended to be used in configuration modules, XML based frameworks or JSP taglibs.
",UnifiedJEXL,,,,henrib,True,rahul@apache.org,henrib
commons-jexl,JEXL-47,2008-07-24T13:28:00.000+0000,2009-07-17T18:00:13.000+0000,2010-03-09T15:21:57.000+0000,,Fixed,New Feature,Major,['2.0'],['1.1'],,,,,,,,,"Comments currently require ##

It might be useful to also allow // for starting a comment.",Allow single-line comments with //,,,,sebb,True,,sebb
commons-jexl,JEXL-46,2008-07-23T20:41:53.000+0000,2010-01-07T18:03:45.000+0000,2010-03-09T15:25:30.000+0000,,Fixed,New Feature,Major,['Later'],['2.0'],,,,,,,,,"would it be possible to add the perl-like ""=~"" and ""!~"" operators to JEXL ?

to be used like:
-----------------------------
str=""this is in here"";

if(str =~ 'is\s+in')
{
 // true here
}
-----------------------------

or the simple (true) expressions:

""this is in here"" =~ 'is\s+in'

""this is in here"" !~ 'not'

i know i could use ""str.match(pattern)"" but for simple users of an application that like to enter simple expressions ""=~"" and ""!~"" would do it without learning java string object methods.
",adding Perl-like regular-expression operators,,,,alfred.reibenschuh@it-austria.com,True,,alfred.reibenschuh@it-austria.com
commons-jexl,JEXL-39,2007-11-10T05:49:45.000+0000,2008-05-15T01:54:15.000+0000,2010-03-09T15:25:30.000+0000,,Won't Fix,New Feature,Major,['2.0.1'],['1.1'],,,,,,,,,"See thread titled ""[JEXL] Any plans for a BSF wrapper?"", started on dev list 09/11/07. On markmail, its at:

    http://commons.markmail.org/message/onjxo7rjmhxnc2as?q=order:date-backward&page=3

This is meant to provide a starting point, if there is interest. Attachment coming, the fix version is purely speculative.
",BSF engine impl,,,,rahul@apache.org,True,,rahul@apache.org
commons-jexl,JEXL-25,2006-10-08T18:51:41.000+0000,2007-10-28T08:29:47.000+0000,2010-03-09T15:21:57.000+0000,,Fixed,New Feature,Critical,['2.0'],['1.1'],,,,,,,,,"Jexl doesn't allow for using method with varargs - method cannot be found and because of that expression returns null. Try following code:

class TestClass  {
 public String testVarArgs (Integer... args) {
  return ""Test"";
 }
}

String jexlExp = ""test.testVarArgs(1,2,3,4,5)"";
Expression e = ExpressionFactory.createExpression(jexlExp);
JexlContext jc = JexlHelper.createContext();
jc.getVars().put(""test"", new TestClass());
Object o = e.evaluate(jc);",Call method with varargs,,,,marek.lewczuk,True,,marek.lewczuk
commons-jexl,JEXL-21,2006-09-19T15:16:57.000+0000,2009-07-17T17:57:11.000+0000,2010-03-09T15:21:57.000+0000,,Fixed,New Feature,Major,['2.0'],,,,,,,,,,"I'd like to use JEXL for a website framework. In order to make the scripting code easy readable it would be nice to let the users add hooks in the expression processing.
such a hook would allow me to add Date operators for example: Date  and Durations addition/substraction.

that would've been very easy for me to implement if I had access to the processing done in let's say ASTAddNode class. 

An easy way to do it is to add a method in the parser like
registerHook(Class astNodeClass, SimpleNode hook)

in ExpressionImpl.evaluate we call node.implValue instead of value. This method can check if  hooks where registered for the class and call for each hook.value before calling the value of the node itself. 
For perfomance reasons the list of these hooks can be stored by the each SimpleNode in the constructor SimpleNode(Parser p, int i) 

that's it, thanks
",operator overloading / hooks on operator processing,,,,acraciun@flashmail.com,True,,acraciun@flashmail.com
commons-jxpath,JXPATH-112,2007-12-23T17:21:42.000+0000,,2014-11-28T02:27:02.000+0000,,,New Feature,Minor,['1.4'],['1.3'],,,,,,,,,"I made a jaxp wrapper (javax.xml.xpath) for jxpath, I attach the code that includes a testsuite. If you like it you can use it.",jaxp wrapper for jxpath,1,,,vivodamichele@hotmail.com,True,,vivodamichele@hotmail.com
commons-jxpath,JXPATH-96,2007-07-15T14:14:29.000+0000,2007-07-18T20:44:48.000+0000,2007-07-18T22:32:06.000+0000,,Fixed,New Feature,Major,['1.3'],['1.2 Final'],,,,,,,,,"Extract VariablePointerFactory interface from JXPathContextReferenceImpl, and pull up using of it to JXPathContext",Extract VariablePointerFactory interface,,,,bsp,True,mbenson,bsp
commons-jxpath,JXPATH-78,2007-02-01T03:40:11.000+0000,2007-02-01T15:41:13.000+0000,2007-02-01T15:41:13.000+0000,,Invalid,New Feature,Major,,['1.2 Final'],,,,,,,,,"I have a bean class ""Cache"" which is generated from jaxb , which has the isEnable method, i can not use the xpath expression ""/cache/enable""to retrive the result, is that means JXpath only support setXXX getXXX? not support isXXX for boolean values?",Boolean Support,,,,maomaode,True,,maomaode
commons-net,NET-665,2019-02-08T16:54:58.000+0000,,2019-05-04T10:43:28.000+0000,,,New Feature,Major,,['3.6'],,,,,,,,,"FTPClient now has a method [*listFiles*|https://commons.apache.org/proper/commons-net/apidocs/org/apache/commons/net/ftp/FTPClient.html#listFiles(java.lang.String,%20org.apache.commons.net.ftp.FTPFileFilter)] that allows a filter to filter out irrelevant files.
I'd like to be able to pass multiple filters.

This way I don't need to combine multiple FileFilters into one (cleaner code)

Current:
{{public FTPFile[] listFiles(String pathname, FTPFileFilter filter) throws IOException}}

New:
{{public FTPFile[] listFiles(String pathname, FTPFileFilter... filter) throws IOException}}",Support for multiple FTPFileFilters,1,,,david tarr,True,,david tarr
commons-net,NET-657,2018-02-14T23:05:55.000+0000,,2018-02-15T22:51:47.000+0000,,,New Feature,Major,,['3.6'],,,,,,,['FTP'],['FTP issues'],"I am using your library to upload photos in android app. When device uses Wi-Fi everything is fine, photos are uploaded to FTP server. But my customer wants this app to be able to do the same when Wi-Fi in app is turned off and device gets net from personal computer(Windows) connected to the device via USB cable. I was able to connect device using ""gnirehtet"":

https://medium.com/genymobile/gnirehtet-reverse-tethering-android-2afacdbdaec7

When I use gnirehtet my app uploads only firsth photo to FTP server and hangs on getting reply code at the end of ""_storeFile"" function in FTPClient class of AppacheCommonsNet. When I use WiFi everything is ok, the problem is only when I use reverse tethering. I checked other reverse thetering solutions and I met the same problem.
 
 
Maybe I do something wrong but I think that your library may not support reverse tethering. Could you consider expanding your library with support for reverse tethering?",Reverse tethering support,1,,,Grzegorz Ostrowski,True,,Grzegorz Ostrowski
commons-net,NET-649,2018-01-08T12:57:51.000+0000,,2019-05-02T13:54:13.000+0000,,,New Feature,Major,['3.7'],['3.6'],1200,1200,,,,100,['FTP'],['FTP issues'],"In a situation where I passed, the FTP server is restoring 227 Entering Passive Mode (0,0,0,0,156,126), so it was obvious to me that the problem is on the server side and not on the API, but using the File Zila client and analyze the log precebi that the same problem was also happening, so he changed the route to use the IP of the server that returned this information and thus manages to make the route correctly.",227 Entering Passive Mode,1,,,fbrissi,True,,fbrissi
commons-net,NET-634,2017-03-27T10:29:55.000+0000,2017-03-27T17:11:47.000+0000,2019-05-04T09:46:10.000+0000,,Fixed,New Feature,Major,['3.7'],['3.6'],,,,,,,['FTP'],['FTP issues'],"I hope I'm not just blind, but I can't find any support for the FTP SIZE command (https://tools.ietf.org/html/rfc3659#page-11).

This was a surprise, indeed.",SIZE command support?,1,,,mauromol,True,,mauromol
commons-net,NET-576,2015-06-18T23:01:02.000+0000,2015-06-19T00:09:57.000+0000,2015-11-26T16:34:46.000+0000,,Fixed,New Feature,Major,['3.4'],['3.3'],,,,,,,,,"At present the FTPClient code assumes that FTPClientConfig always provides the system type.

Generally speaking, the application will know what the system type is.
Also some of the other config settings depend on the system type.
However there may be occasions when it would be useful to provide some config settings (e.g. setUnparseableEntries) but still allow the system type to be determined dynamically.",Allow FTPClient to use SYST response if system type is not specified in configuration,,,,sebb,True,,sebb
commons-net,NET-575,2015-06-18T22:50:19.000+0000,2015-06-18T22:52:38.000+0000,2015-11-26T16:34:48.000+0000,,Fixed,New Feature,Major,['3.4'],['3.3'],,,,,,,,,"FTPClientExample does not currently support setting the date format(s).

It would be useful to be able to set at least the default format, and perhaps also the short date format",FTPClientExample should support setting the date format,,,,sebb,True,,sebb
commons-net,NET-565,2014-12-10T13:31:28.000+0000,2014-12-10T13:41:01.000+0000,2015-11-26T16:34:44.000+0000,,Fixed,New Feature,Major,['3.4'],,,,,,,,,,"FTPClient supports MDTM via the getModificationTime method, but this returns an unparsed String for the date.

It would be useful to have another version that parses the response and returns an FTPFile, similarly to mlistFile()",Add FTPClient method to return an FTPFile from an MDTM command,,,,sebb,True,,sebb
commons-net,NET-536,2014-04-12T23:56:52.000+0000,2014-04-13T01:13:52.000+0000,2015-11-26T16:34:48.000+0000,,Fixed,New Feature,Major,['3.4'],,,,,,,,,,It would be useful to have an IMAP FETCH example that can demonstrate the chunked listener.,IMAP FETCH example,,,,sebb,True,,sebb
commons-net,NET-535,2014-04-09T23:02:55.000+0000,2014-04-10T00:15:54.000+0000,2015-11-26T16:34:46.000+0000,,Fixed,New Feature,Major,['3.4'],['3.3'],,,,,,,,,"A single IMAP request can return huge amounts of data (e.g. FETCH)

An application gets access to this data by calling one of the methods getReplyString() or getReplyStrings() only after the request has completed. 

As well as delaying the response to the application, this may cause the application to run out of memory.

IMAP replies which return large amounts of data use untagged multi-line responses followed by a tagged status line.

It would be useful if applications could register to receive such responses in chunks as they arrived. The array list could then be cleared once the chunk had been processed. e.g. the listener could return a boolean to say whether to clear the array.

If the array was cleared, this would obviously affect any listeners as these are currently only called at the end of the response.
In this case, listeners could be called with a new status of  PARTIAL so they could distinguish the different case if necessary.

The chunk listener could be passed the IMAP instance; this would give access to the getString[s]() methods. This would be more flexible than passing a String or String[] directly.",IMAP FETCH can overflow reply buffer; provide for partial responses,,,,sebb,True,,sebb
commons-net,NET-471,2012-06-27T05:34:13.000+0000,2012-07-14T13:24:42.000+0000,2013-04-17T19:55:43.000+0000,,Won't Fix,New Feature,Major,,['3.1'],,,,,,,['FTP'],['FTP issues'],"When I use ftpclient to push a huge file(>1GB) from one city to another. The Thread will block forever.Then I know the default setting use default socketfactory. And the socket.setSoTimeout only works on read but write.
The commons-net not provide a NIO SocketFactory. So it's a hard work for me to write one.
Can we provide a NIO SocketFactory?",Provide NIO SocketFactory Support,1,,"['NIO', 'SocketManager']",yankay,True,,yankay
commons-net,NET-464,2012-05-16T07:03:28.000+0000,2017-02-08T00:18:20.000+0000,2017-02-08T00:18:20.000+0000,,Later,New Feature,Major,,"['1.3', '3.0.1', '3.1', '3.2']",,,,,,,['Telnet'],['Telnet-related issues'],"Hi,
I am unable to use Telnet Over SSL functionality from the commons-net api but when seeing the api for other protocols like FTP,SMTP,IMAP. It seems that FTPS,SMTPS,IMAPS all have their respective clients like IMAPS Client means IMAP over SSL.SMTPS Client means SMTP over SSL
Whereas not able to find TelnetS Client so it would be bettere if you provide the functionality for Telnet SSL Client.

Regards
RSuri",Not able to use Telnet Over SSL means there is no TelnetS client as IMAPS and FTPS SMTPS. Why?,1,,,ramgsuri,True,,ramgsuri
commons-net,NET-436,2012-01-04T10:53:18.000+0000,2012-01-05T14:10:30.000+0000,2012-02-29T20:35:55.000+0000,,Fixed,New Feature,Major,['3.1'],['3.0.1'],,,,,,,['FTP'],['FTP issues'],"The Rumpus ftp server acts as an ""old"" pre mac os x ftp server.

There are any plans do integrate a Mac OS filelist parsing into the commons net project?
http://cyberduck.ch/ implemnets his own Rumpus parser, 
but it will be great if commons net already support this kind of listing.  

","Support for SYST ""Mac OS""  listing",,,,jjg,True,,jjg
commons-net,NET-379,2011-03-18T01:49:44.000+0000,2011-03-18T01:52:07.000+0000,2011-06-04T19:26:31.000+0000,,Fixed,New Feature,Minor,['3.0'],['3.0'],,,,,,,['FTP'],['FTP issues'],"For FTP commands that don't require a separate data channel, it would be useful to have methods that issued the command and returned the response.

This would allow simpler access to commands which are not yet supported.",FTPClient - support for processing arbitrary commands that only use the control channel,,,,sebb,True,,sebb
commons-net,NET-378,2011-03-18T01:36:53.000+0000,2011-03-18T01:45:46.000+0000,2011-06-04T19:26:30.000+0000,,Fixed,New Feature,Major,['3.0'],['3.0'],,,,,,,['FTP'],['FTP issues'],"It would be useful to add MLST/MLSD parsing support, see [1].


[1] http://tools.ietf.org/html/rfc3659#section-7",FTP listing should support MLST and MLSD,,,,sebb,True,,sebb
commons-net,NET-371,2011-03-13T17:54:46.000+0000,2011-03-13T17:59:00.000+0000,2011-06-04T19:26:29.000+0000,,Fixed,New Feature,Major,['3.0'],,,,,,,,,,"There are currently additional versions of FTPSTrustManager used by different protocols (POP3S and SMTPS); it would be useful to share the code for these.

There should also be a TrustManager that is even more lenient than the FTPSTM (which checks certificate validity).",Create TrustManagerFactory to provide custom TrustManagers,,,,sebb,True,,sebb
commons-net,NET-370,2011-03-13T01:44:39.000+0000,2011-03-18T15:23:32.000+0000,2011-06-04T19:26:35.000+0000,,Fixed,New Feature,Major,['3.0'],['2.2'],,,,,,,,,"MLSD and MSLT are intended for machine parsing of listings, so if the server supports the features it would be beneficial to use them.","Add support for FTP commands MLSD, MLST",,,,sebb,True,,sebb
commons-net,NET-368,2011-03-12T01:20:21.000+0000,2011-03-12T01:46:49.000+0000,2011-06-04T19:26:25.000+0000,,Fixed,New Feature,Major,['3.0'],,,,,,,,,,"Threader.thread() does not actually need a List - it can accept an Iterable, which List implements.

This allows the method to be used with the new iteratorxxx() methods.",Threader.thread should accept an Iterable rather than a List,,,,sebb,True,,sebb
commons-net,NET-361,2011-03-03T02:53:12.000+0000,2011-03-03T13:22:15.000+0000,2011-06-04T19:26:31.000+0000,,Fixed,New Feature,Major,['3.0'],['2.2'],,,,,,,['Telnet'],['Telnet-related issues'],"TelnetClient offers sendAYT() and sendSubnegotiation() methods for sending some types of commands, but there is currently no support for sending AO, IP, EL, EC etc.",Implement Telnet Command sender,,,,sebb,True,,sebb
commons-net,NET-357,2011-02-12T16:51:10.000+0000,2011-03-02T20:24:16.000+0000,2011-06-04T19:26:34.000+0000,,Fixed,New Feature,Minor,['3.0'],['2.2'],,,,,,,['POP3'],[''],The POP3Client class does not support automatic SSL negotiation when connecting to the POP3S port and it does not support TLS negotiation when connecting to the standard POP3 port.,[POP3] [Solution] The POP3 client does not support SSL/TLS connections,,,,bogdro,True,,bogdro
commons-net,NET-356,2011-02-12T16:37:26.000+0000,2011-02-24T01:41:03.000+0000,2011-06-04T19:26:25.000+0000,,Fixed,New Feature,Minor,['3.0'],,,,,,,,['SMTP'],[''],The SMTPClient class does not support automatic SSL negotiation when connecting to the SMTPS port and it does not support TLS negotiation when connecting to the standard SMTP port.,[SMTP] [Solution] The SMTP client does not support SSL/TLS connections,,,,bogdro,True,,bogdro
commons-net,NET-353,2011-01-31T17:20:48.000+0000,2011-02-24T02:37:56.000+0000,2011-06-04T19:26:31.000+0000,,Fixed,New Feature,Minor,['3.0'],,,,,,,,['SMTP'],[''],The SMTPClient java class doesn't support any kind of client authentication.,[SMTP] [PATCH] The SMTPClient does not support authentication,,,,bogdro,True,,bogdro
commons-net,NET-352,2011-01-31T17:15:44.000+0000,2011-03-02T20:41:37.000+0000,2011-06-04T19:26:32.000+0000,,Fixed,New Feature,Minor,['3.0'],['2.2'],,,,,,,['POP3'],[''],The POP3Client does not suppport the SASL PLAIN & CRAM-MD5 authentication mechanisms.,[POP3] [PATCH] SASL PLAIN & CRAM-MD5 authentication,,,,bogdro,True,,bogdro
commons-net,NET-344,2010-11-19T21:12:56.000+0000,2011-03-03T20:57:51.000+0000,2011-06-04T19:26:31.000+0000,,Fixed,New Feature,Major,['3.0'],['2.0'],,,,,,,['Telnet'],['Telnet-related issues'],"I am in the process of trying to implement a Java client for [RFC 2217|http://tools.ietf.org/html/rfc2217], which is the protocol for accessing serial ports over TCP. Unfortunately, the commons-net telnet client is insufficient for this relatively simple task.

There are two missing features in the commons-net telnet client code, one of which is a show stopper (NET-343) and the other of which would be a ""real nice to have"". This issue documents the second problem:

The API I am trying to support requires the ability to notify a listener that new incoming data is available. This would be trivial for me to do if only my library could somehow itself get a listener notification from the {{TelnetInputStream}} reader thread every time it reads new data. Unfortunately, there's no API provided for doing this, even though it would be trivial. So I have to implement a hack that requires a wrapper {{InputStream}} with yet another internal reader thread, etc. Bleh.

I'm going to attempt to come up with a patch and will attach it here if successful.
",Telnet client: Support Listener Notification of Incoming Data,,,,archie172,True,,archie172
commons-net,NET-343,2010-11-19T21:12:20.000+0000,2011-03-02T21:57:34.000+0000,2011-06-04T19:26:37.000+0000,,Fixed,New Feature,Major,['3.0'],['2.0'],,,,,,,['Telnet'],['Telnet-related issues'],"I am in the process of trying to implement a Java client for [RFC 2217|http://tools.ietf.org/html/rfc2217], which is the protocol for accessing serial ports over TCP. Unfortunately, the commons-net telnet client is insufficient for this relatively simple task.

There are two missing features in the commons-net telnet client code, one of which is a show stopper and the other of which would be a ""real nice to have"". This issue documents the first problem:

RFC 2217 specifies that serial port events (such as server notifiying about a change in carrier detect, or the client instructing the server to change the baud rate) are delivered to the peer by subnegotiations. For example, to notify the client about carrier detect, the server sends {{IAC SB COM-PORT-OPTION NOTIFY-MODEMSTATE <value> IAC SE}} to the client; to set the serial port baud rate, the client sends {{IAC SB COM-PORT-OPTION SET-BAUD <value(4)> IAC SE}} to the server. These messages can happen at any time and are not associated with any WILL/WONT/DO/DONT negotiation (according to my understanding).

The problem is that while one can _receive_ such messages via {{TelnetOptionHandler.answerSubnegotiation()}}, the {{TelnetClient}} class doesn't provide any way to _send_ (i.e., intiate) these messages. 

What's needed here is simply to expose {{Telnet._sendSubnegotiation()}} (properly renamed, etc.) as a public method.

I'm going to attempt to come up with a patch and will attach it here if successful.
",Telnet client: Support Client-initiated Subnegotiation Messages,,,,archie172,True,,archie172
commons-net,NET-337,2010-09-11T00:55:33.000+0000,2011-03-13T00:26:58.000+0000,2011-06-04T19:25:16.000+0000,,Fixed,New Feature,Major,['2.2'],['2.0'],,,,,,,['FTP'],['FTP issues'],"Extended Passive Mode  (ESPV) is not supported yet in the current library.

Adding support for it will enable this library to use it to connect ot servers behind routers that are incorrectly configured to use PASV.

I dont know if I should try to place a patch as I am on doubt on what the right approach should be:



a) Create a new EXTENDED_PASSIVE_LOCAL_DATA_CONNECTION_MODE connection mode
     This would probably fulfill the actual requirement, but would also require multiple changes across to support the EPSV command and its result as well.


b) create a public property  ""ignorePassiveHost""  that can be used in FTPClient# __parsePassiveModeReply() as follows
        if(__ignorePassiveHost){
      	  __passiveHost = getRemoteAddress().getHostAddress();
        }else{
      	  __passiveHost = parts[0] + '.' + parts[1] + '.' + parts[2] + '.' + parts[3];
        }
        
c) externalize (as a protected method)  the call to  create socket in FTPClient#_openDataConnection_   to a protected method ""createSocket)
        This would allow user code to override the default connection behavior to accomplish a similar functionality.


d) make __parsePassiveModeReply  a protected method and also add a protected method for setting the value on __passiveHost
        This would allow user code to override the default behavior to accomplish a similar functionality.


Thanks for any comments / work on this. 

I can submit a patch on any of these options if you need me to.



",Support for Extended Passive Mode ,,2,,csilva,True,,csilva
commons-net,NET-332,2010-07-26T09:36:36.000+0000,2011-02-25T22:21:59.000+0000,2011-06-04T19:26:25.000+0000,,Fixed,New Feature,Major,['3.0'],['2.0'],,,,,,,['FTP'],['FTP issues'],"When parser is of unknown type, commons ftp throws an exception and does not enable you to use the ftp even though every other ftp client is able to do handle this. My suggestion is to allow the default factory to be overriden through a VM parameter so that i can use a different parser even in ant. For non ant, the solution is easy as i outline here: http://groovyjavathoughts.blogspot.com/2009/06/ftpclient-advantage-of-open-source-code.html",Commons net ftp cannot handle unknown type parser and should allow override of parser through vm argument,1,,,drosenstark,True,,drosenstark
commons-net,NET-285,2009-07-15T08:53:55.000+0000,2010-03-10T10:19:37.000+0000,2010-10-15T09:20:31.000+0000,,Fixed,New Feature,Major,['2.2'],,,,7200,7200,7200,,,,"If you use FTP tools like FileZilla, you see that in Active mode,
 the external IP address (when client has multiple network cards) and 
 port range (when client has firewall setup) are supported.
This is very essential for an enterprise class FTP client.

I see that a small modification in FTPClient's _openDataConnection_() method will support all this.
_serverSocketFactory_.createServerSocket(0, 1, getLocalAddress());

Can I also go ahead creating a patch for this?
",FTP should support external IP address and port range,1,1,,binoyme,True,,binoyme
commons-net,NET-281,2009-06-15T21:29:17.000+0000,2009-07-20T23:20:34.000+0000,2011-06-04T19:26:37.000+0000,,Fixed,New Feature,Trivial,['3.0'],,,,,,,,,,"Implement a size method in the SubnetUtils.SubnetInfo class.

Something similiar to this:

{code}
public int getSize() {
    return (this.broadcast - this.network) + 1;
}
{code}

We add one so that we include the network address as being in the network range.  This will also return the same value as the similiar Net::IP class in perl (I was converting a perl module to a java class that uses the Net::IP size method and was very supripsed this library didn't have one).",Implement a Network Size method,,,,zwei133,True,,zwei133
commons-net,NET-267,2009-03-26T10:57:57.000+0000,2014-12-12T17:18:56.000+0000,2014-12-12T17:18:56.000+0000,,Fixed,New Feature,Minor,,['2.0'],,,,,,,['FTP'],['FTP issues'],In response to a recent email on the user list: add support for FTP proxying over HTTP (not SOCKS).,Support HTTP Proxying,2,1,,rwinston@eircom.net,True,,rwinston@eircom.net
commons-net,NET-244,2008-11-18T15:55:19.000+0000,2010-07-12T16:48:45.000+0000,2011-06-04T19:25:16.000+0000,,Fixed,New Feature,Minor,['2.2'],['2.0'],,,,,,,['FTP'],['FTP issues'],"It would be nice if there was a way to list *only* those files I'm interested in.  I've had to write a loop a couple of times to figure out which files had the right naming conventions.  If the API handled that for me, I would have less boilerplate code to write.

I would attach a patch but as of this time I cannot download the source due to an internal server error.

I was imagining the introduction of a new interface {{org.apache.commons.net.ftp.FTPFileFilter}} that would be analogous to [java.io.FileFilter|http://java.sun.com/javase/6/docs/api/java/io/FileFilter.html]:
{code}
package org.apache.commons.net.ftp;

public interface FTPFileFilter {
   public boolean accept(FTPFile file);
}
{code}

A new method on FTPClient would need to be created to support it.  Here's a code sample using API calls:
{code}
public FTPFile[] listFiles(FTPFileFilter filter) throws IOException {
   FTPFile files = listFiles();
   List<FTPFile> fileList = new ArrayList<FTPFile>(files.length);
   for (FTPFile file : files) {
      if (filter.accept(file)) {
         fileList.add(file);
      }
   }
   return fileList.toArray(new FTPFile[fileList.size()]);
}
{code}

See [java.io.File.listFiles(java.io.FileFilter)|http://java.sun.com/javase/6/docs/api/java/io/File.html#listFiles(java.io.FileFilter)] for comparison.",Add a FTPClient.listFiles(FTPFileFilter) method,1,,,mjlachman,True,,mjlachman
commons-net,NET-237,2008-11-04T18:31:44.000+0000,2011-03-10T01:57:05.000+0000,2011-06-04T19:26:26.000+0000,,Fixed,New Feature,Minor,['3.0'],"['1.0', '1.1', '1.1.1', '1.2', '1.2.1', '1.2.2', '1.3', '1.4', '2.0']",,,,,,,['NNTP'],[''],"NNTPClient.listNewNews(..) can use a lot of memory for large groups.  I recommend adding streaming version of the same.

  Iterator<String> iterateNewNews(NewGroupsOrNewsQuery query);

One could also add streaming versions of the *Newsgroups methods, but these are expected to have smaller results than some large newsgroups.

  Iterator<NewsgroupInfo> iterateNewNewsgroups(NewGroupsOrNewsQuery query);
  Iterator<NewsgroupInfo> iterateNewsgroups();
  Iterator<NewsgroupInfo> iterateNewsgroups(String wildmat);

I'll work on iterateNewNews(..) and submit a patch if it's good.",add streaming methods (corresponding to array methods) to NNTPClient,,,,kgeis,True,,kgeis
commons-net,NET-229,2008-07-23T21:23:16.000+0000,2011-02-26T02:15:14.000+0000,2011-06-04T19:26:31.000+0000,,Fixed,New Feature,Major,['3.0'],,,,,,,,['FTP'],['FTP issues'],"The DefaultFTPFileEntryParserFactory classes could be updated to use a properties file to translate between reported system type and parser type if the existing checks fail.

This would allow new OS types to be quickly supported if there is already a suitable parser - just create / update the properties file.",Use properties file to handle new OS-type auto-detection.,,,,sebb,True,,sebb
commons-net,NET-189,2008-02-28T16:46:52.000+0000,2008-03-02T16:50:34.000+0000,2008-03-02T16:50:34.000+0000,,Fixed,New Feature,Minor,['2.0'],,,,,,,,,,"Java is sorely lacking in a good quality open source APi for translating standard IP netmasks, e.g.

10.1.0.0/16 
83.217.112.18/255.255.255.240
10.1.0.*

etc etc

These sorts of masks are commonly used in the likes of Apache web server, and I would really like to use them in java.  A utlity to translate them into sequences of InetAddress objects would be perfect.

Commons-Net would seem like a good project to put them in.

Thoughts?",Add netmask translation feature,,,,skaffman,True,,skaffman
commons-net,NET-187,2008-02-23T23:33:08.000+0000,2008-02-24T21:42:10.000+0000,2008-03-10T17:44:39.000+0000,,Fixed,New Feature,Minor,['2.0'],['1.4'],,,,,,,,,Bug for attaching TFTP Server.,TFTP Server for commons net ,,,,armbrust,True,,armbrust
commons-net,NET-163,2007-07-12T12:03:45.000+0000,2009-03-22T19:16:13.000+0000,2009-03-22T19:16:13.000+0000,,Won't Fix,New Feature,Major,,,,,,,,,,,"Hi all,
I developed DNS methods for resolving IP address by host name and vice verse; receiving mail exchange and authoritative name servers for domain; and receiving other information about domain from DNS server.

JDK provides above functionality in JNDI DNS service provider [1]. But JNDI technology is too abstract and obfuscates communication with DNS servers, which leads to users don't use JNDI DNS SP.

Attached classes provide evident API for receiving information from DNS server as wrapper for JNDI DNS SP. Also these classes provide necessary additional parsing of answers.

Examples how to use created API can be found in DNSClientTest unit test. This library requires only JDK and JUnit to run the test.

[1] http://java.sun.com/javase/6/docs/technotes/guides/jndi/jndi-dns.html",[contribution] DNS utils,,,,niks,True,,niks
commons-net,NET-156,2007-04-10T02:39:53.000+0000,2011-02-25T15:03:23.000+0000,2011-06-04T19:26:27.000+0000,,Fixed,New Feature,Minor,['3.0'],['1.4'],,,,,,,['FTP'],['FTP issues'],A project I'm on had a requirement which I needed to traverse each directory from the users home directory (after logging in to the ftp server) and download a specific file format. I created a method listDirectories() which will return a String[] of directory names which reside in the current working directory. I thought this could be a nice addition to the API and would like to contribute the code I wrote.,New FTPClient method to retrieve all directory names in the current working directory.,,,,wb138834,True,,wb138834
commons-ognl,OGNL-189,2011-10-23T23:34:06.000+0000,,2014-01-02T08:04:55.000+0000,,,New Feature,Minor,['4.0'],,,,,,,,,,"At the moment, OGNL provides 4 different kind of cache implementation.
After some performance tests I realized that there is no the best approach: the efficiency depending strongly on the use of the cache itself.
Hence give the freedom of choice should be a good thing.",Allow user to choose his preferred cache implementation,2,,,maurizio.cucchiara,True,maurizio.cucchiara,maurizio.cucchiara
commons-ognl,OGNL-191,2011-10-15T18:31:46.000+0000,2011-10-16T17:43:37.000+0000,2013-03-27T12:37:15.000+0000,,Fixed,New Feature,Major,['4.0'],,,,,,,,,,"Using the Visitor pattern allows for a cleaner implementation of toString().

I have a patch which will remove toString() from all AST classes, and replace it with a single toString() in ""SimpleNode"" which delegates to a ToStringVisitor to build the String efficiently.  

This patch can also be used as an example of how to move other business logic out of the AST classes into their own visitor classes.","Move ""toString"" implementations into visitor pattern.",,,,danielpitts,True,simone.tripodi,danielpitts
commons-ognl,OGNL-190,2011-09-26T02:07:52.000+0000,2011-10-09T13:19:09.000+0000,2013-03-27T12:37:10.000+0000,,Fixed,New Feature,Major,['4.0'],,,,,,,,,,"Supporting the visitor pattern on the AST tree allows for more modular support of features and easier refactoring in the future.

Patch to be attached.",Support visitor pattern on AST,1,,,danielpitts,True,,danielpitts
commons-ognl,OGNL-188,2011-09-01T21:42:40.000+0000,,2012-06-18T09:08:33.000+0000,,,New Feature,Major,,,,,,,,,,,"First, let me describe in abstract terms my use case:

I have several ""resources"" which I need to retrieve (by URI).  I'm using Ognl to describe the URI of these resources.  

Some of the resource URI's might be dependent on content in other resources.  I'd like to be able to set up my own queuing mechanism which will put on hold any such dependent URI until the resource is retrieved. 

Right now, the Ognl expression is evaluated using the Java stack, which makes it extremely difficult to ""pause"" an expression.  

It would be nice if the expression state could be  externalized and run iteratively instead of recursively.  ","Ability to programatically ""step"" through an Ognl execution.",1,,"['feature', 'runtime']",danielpitts,True,,danielpitts
commons-ognl,OGNL-145,2008-02-25T10:23:33.000+0000,,2014-10-06T13:26:53.000+0000,,,New Feature,Major,['4.0'],['2.7.3'],,,,,,,['Core Runtime'],['Anything having to deal with the core utility functions provided by things like OgnlRuntime / OgnlContext / etc..'],"The static state management in OgnlRuntime is causing havoc with classloaders where people run multiple instances of tapestry apps with shared jar references to OGNL.  Need to finally do the ""big refactor"" that removes all static state from OGNL.",Remove all static state. ,7,2,,jkuhnert,True,jkuhnert,jkuhnert
commons-ognl,OGNL-7,2007-02-01T22:27:09.000+0000,2007-11-19T14:26:52.000+0000,2007-11-19T14:45:17.000+0000,,Fixed,New Feature,Major,['2.7.2'],,,,,,,,['Core Runtime'],['Anything having to deal with the core utility functions provided by things like OgnlRuntime / OgnlContext / etc..'],"Although xwork has been able to work around it, it seems like it would be better if OGNL supported 1.5 out-of-the-box. Particularly it seems like there are places where we should be passing ParameterizedType instead of Class so the generic type is passed along with the base class type.
",Full Java 5 Support for OGNL,1,,,tschneider22,True,,tschneider22
commons-ognl,OGNL-1,2005-11-08T17:36:59.000+0000,2007-02-17T12:38:15.000+0000,2007-02-17T12:38:15.000+0000,,Duplicate,New Feature,Major,,,,,,,,,['Core Runtime'],['Anything having to deal with the core utility functions provided by things like OgnlRuntime / OgnlContext / etc..'],http://forums.opensymphony.com/thread.jspa?threadID=8981&tstart=0,Type conversion improvements,,,,plightbo,True,plightbo,plightbo
santuario,SANTUARIO-492,2018-08-15T03:32:13.000+0000,2018-08-15T08:25:06.000+0000,2018-08-15T08:25:06.000+0000,,Fixed,New Feature,Major,,"['Java 1.5.9', 'Java 2.0.11', 'Java 2.1.3']",,,3600,3600,3600,,['Java'],['Java issues.']," Please add OWASP Dependency Check to the build (pom.xml).  OWASP DC makes an outbound REST call to MITRE Common Vulnerabilities & Exposures (CVE) to perform a lookup for each dependant .jar to list any/all known vulnerabilities for each jar.  This step is needed because a manual MITRE CVE lookup/check on the main component does not include checking for vulnerabilities in components or in dependant libraries.

OWASP Dependency check : https://www.owasp.org/index.php/OWASP_Dependency_Check has plug-ins for most Java build/make types (ant, maven, ivy, gradle).   

Also, add the appropriate command to the nightly build to generate a report of all known vulnerabilities in any/all third party libraries/dependencies that get pulled in. example : mvn -Powasp -Dtest=false -DfailIfNoTests=false clean aggregate

Generating this report nightly/weekly will help inform the project's development team if any dependant libraries have a newly discovered & reported (known) vulnerailities.  Project teams that keep up with removing known vulnerabilities on a weekly basis will help protect businesses that rely on these open source componets.",Please add OWASP Dependency Check to the build (pom.xml),1,,"['build', 'easy-fix', 'security']",ABakerIII,True,coheigea,ABakerIII
santuario,SANTUARIO-458,2016-11-04T07:42:00.000+0000,2017-01-19T14:53:44.000+0000,2017-08-28T13:51:23.000+0000,,Fixed,New Feature,Minor,"['Java 2.1.0', 'Java 2.0.9']",,,,,,,,['Java'],['Java issues.'],"I'm building an client for an API that has special requirements with regards to the XML messages that are excepted. Some of these requirements were not possible to configure with the current XMLSecurityProperties for the STAX interface.

The interface required that the document is schema valid and that means in this case that the elements are not allowed to have an ""Id"" attribute on the document, signature and key info elements.

The interface required that the Reference URI is empty when the entire message is signed.

The interface requires that when the transform ""<Transform Algorithm=""http://www.w3.org/2000/09/xmldsig#enveloped-signature"" />"" is present, the digest transform is left empty to indicate the ""default"" transform.

I've implemented new XMLSecurityProperties to be able to configure these features. With defaults set to ensure current behaviour.
    private boolean signatureGenerateIds = true;
    private boolean signatureIncludeDigestTransform = true;
    private String signatureDefaultCanonicalizationTransform;

",Add signature configuration options to control generating Id attributes and adding default transformations,3,,,htrippaers,True,coheigea,htrippaers
santuario,SANTUARIO-453,2016-10-11T16:37:30.000+0000,2016-11-08T16:29:27.000+0000,2016-12-05T10:00:46.000+0000,,Fixed,New Feature,Major,['Java 2.0.8'],,,,,,,,['Java'],['Java issues.'],The signature trust is currently done based on a preconfigured SignatureValidationKey. However when KeyName is used it is possible to have multiple messages with different KeyNames. The inbound processing should be able to deal with this and select the right certificate to use when validating a message.,Support selection of a signature validation certificate based on KeyName,2,,,htrippaers,True,coheigea,htrippaers
santuario,SANTUARIO-452,2016-10-11T07:34:37.000+0000,2016-10-11T15:17:03.000+0000,2016-12-05T10:00:45.000+0000,,Fixed,New Feature,Minor,['Java 2.0.8'],,,,,,,,['Java'],['Java issues.'],I’m working on a project that uses KeyName to identify the key used to verify or sign the signature. I’m using the santuario library through the XmlSecIn/OutInterceptors in the CXF project. Currently the KeyName identifier is not supported for outgoing messages. ,Add support for KeyName to the STAX output processor,2,,,htrippaers,True,coheigea,htrippaers
santuario,SANTUARIO-417,2015-02-11T17:58:58.000+0000,,2015-04-13T09:05:22.000+0000,,,New Feature,Major,,['Java 2.0.3'],,,,,,,['Java'],['Java issues.'],"Please provide a possibility for changing the namespace for the element dsig:Signature.
",Provide a way to change the Signature prefix for the streaming code,4,,,tobiaswolf,True,coheigea,tobiaswolf
mina-sshd,SSHD-900,2019-02-23T10:45:05.000+0000,2019-02-24T12:56:54.000+0000,2019-03-04T18:03:02.000+0000,,Won't Fix,New Feature,Minor,,,,,,,,,,,"MDC logging support (e.g., remote IP, username, etc.) would really be a great enhancement. We checked out the MINA filters ([http://mina.apache.org/mina-project/userguide/ch5-filters/ch5-filters.html)] but it seems they not integrate nicely with SSHD. 

 

 ",Support for MDC Logging,3,,,marcozapletal,True,,marcozapletal
mina-sshd,SSHD-850,2018-10-16T20:45:50.000+0000,2018-10-21T04:26:36.000+0000,2018-10-21T20:53:50.000+0000,,Fixed,New Feature,Minor,['2.1.1'],"['2.0.0', '2.1.0']",,,,,,,,,"In openssh, the ssh config entry NumberOfPasswordPrompts controls the number of times the ssh client keeps asking for a password if the one entered was invalid in two cases:
 # keyboard-interactive authentication, and
 # asking for passwords for encrypted private keys in identity files in pubkey authentication (see [openssh sources; sshconnect2.c|https://github.com/openssh/openssh-portable/blob/1a4a9cf/sshconnect2.c#L1380]).

sshd-core only has support for (1) through setting the property {{ClientAuthenticationManager.PASSWORD_PROMPTS}} in the session's properties.

There doesn't seem to be any support for FilePasswordProvider to make it respect this value.

{{AbstractPEMResourceKeyPairParser.extractkeyPairs()}} and also {{BouncyCastleKeyPairResourceParser.loadKeyPair()}} call {{FilePasswordProvider.getPassword()}} exactly once.

So how can I write a ssh client using sshd that asks the user NumberOfPasswordPrompts times? Either I'm missing something, or there is some support for this missing in sshd.",Add capability to retry a failed private key decryption when client is decrypting private key file(s),2,,,wolft,True,lgoldstein,wolft
mina-sshd,SSHD-837,2018-07-30T00:57:37.000+0000,2018-08-05T17:30:32.000+0000,2018-08-05T17:30:32.000+0000,,Fixed,New Feature,Major,['2.0.1'],"['2.0.0', '2.1.0', '2.0.1']",,,3600,3600,3600,,,,"Please add OWASP Dependency Check to the build (pom.xml). OWASP DC makes an outbound REST call to MITRE Common Vulnerabilities & Exposures (CVE) to perform a lookup for each dependant .jar to list any/all known vulnerabilities for each jar. This step is needed because a manual MITRE CVE lookup/check on the main component does not include checking for vulnerabilities in components or in dependant libraries.

OWASP Dependency check : https://www.owasp.org/index.php/OWASP_Dependency_Check has plug-ins for most Java build/make types (ant, maven, ivy, gradle).

Also, add the appropriate command to the nightly build to generate a report of all known vulnerabilities in any/all third party libraries/dependencies that get pulled in. example : mvn -Powasp -Dtest=false -DfailIfNoTests=false clean aggregate

Generating this report nightly/weekly will help inform the project's development team if any dependant libraries have a reported known vulnerailities. Project teams that keep up with removing vulnerabilities on a weekly basis will help protect businesses that rely on these open source componets.",Please add OWASP Dependency Check to the build (pom.xml),2,,"['build', 'easy-fix', 'security']",ABakerIII,True,lgoldstein,ABakerIII
mina-sshd,SSHD-821,2018-04-18T08:24:27.000+0000,2018-04-19T06:47:44.000+0000,2018-04-19T06:47:44.000+0000,,Fixed,New Feature,Major,['2.0.0'],,,,,,,,,,,Support for async authentication,2,,,gnt,True,gnt,gnt
mina-sshd,SSHD-817,2018-04-16T21:01:57.000+0000,2018-04-19T06:47:29.000+0000,2018-04-24T12:52:04.000+0000,,Fixed,New Feature,Major,['2.0.0'],,,,,,,,,,,Support for netty i/o provider,1,,,gnt,True,gnt,gnt
mina-sshd,SSHD-815,2018-04-13T11:36:55.000+0000,2018-04-16T11:48:05.000+0000,2018-04-24T12:52:03.000+0000,,Fixed,New Feature,Major,['2.0.0'],,,,,,,,,,,Extract SFTP support in its own module,3,,,gnt,True,gnt,gnt
mina-sshd,SSHD-812,2018-03-27T08:18:05.000+0000,2018-04-19T15:09:03.000+0000,2018-04-23T07:56:23.000+0000,,Fixed,New Feature,Minor,['2.0.0'],['1.7.0'],,,,,,,,,"I used SSHD as a middleman between client and target sftp server.

I found that, when filezilla client directly connect to the target sftp server, it transfers fast. When filezilla client connect to the middleman, it transfers slow.

I analyzed the source code of org.apache.sshd.server.subsystem.sftp.SftpSubsystem#doRead, and I found it behaves like block mode, and client's other SSH_FXP_READ request blocked in the same thread.

 

my middleman code:

 [^Main.java]

 ",support asynchronous mode for sftp subsystem,4,,"['asynchronous', 'sftp']",smoking,True,gnt,smoking
mina-sshd,SSHD-806,2018-03-05T16:25:17.000+0000,2018-03-06T12:39:12.000+0000,2018-04-17T08:08:46.000+0000,,Fixed,New Feature,Major,['2.0.0'],,,,,,,,,,,Split mina io support in its own module,1,,,gnt,True,gnt,gnt
mina-sshd,SSHD-805,2018-03-05T09:49:47.000+0000,2018-03-05T16:24:53.000+0000,2018-09-24T00:09:43.000+0000,,Fixed,New Feature,Major,['2.0.0'],,,,,,,,,,,RFC 8268 support,2,,,gnt,True,gnt,gnt
mina-sshd,SSHD-791,2018-01-02T12:59:48.000+0000,2018-01-03T07:43:33.000+0000,2018-01-03T07:43:33.000+0000,,Won't Fix,New Feature,Minor,,,,,,,,,,,"There is a situation for me to use SshClient to connect remote server via a socks5 proxy server with Java code. I found the issue https://issues.apache.org/jira/browse/SSHD-656 , and I want to implement interface ClientProxyConnector to suit my issue, but I found it's impossible because socks5 protocol is not as simple as just send a packet, but need to send and receive multiple times.",Support the socks protocol,2,,['socks'],suzl,True,,suzl
mina-sshd,SSHD-790,2017-12-27T04:44:38.000+0000,2017-12-27T11:53:10.000+0000,2018-01-04T06:20:06.000+0000,,Fixed,New Feature,Major,['1.7.0'],['1.6.0'],,,,,,,,,"SFTP client does not support filename encoding other than UTF-8.
The implementation (org.apache.sshd.client.subsystem.sftp.AbstractSftpClient) uses the default UTF-8 encoding and no chance to modify.",SFTP client does not support filename encoding other than UTF-8.,2,,,lfn,True,lgoldstein,lfn
mina-sshd,SSHD-787,2017-12-15T12:14:56.000+0000,2017-12-20T10:09:02.000+0000,2017-12-24T10:56:26.000+0000,,Fixed,New Feature,Minor,['1.7.0'],,,,,,,,,,"Hello!

mina-sshd iss awesomely configurable tool.  It is so powerful, that  it allows  programmer to provide VirtualFileSystem or even more, ScpFileOpener for custom stream.

However, before any of above comes to play,  the target path is checked by simple nio.File.exists, which successfully kills any afterwards customisations

This is caused by following chain:
receiveFile is called first, and alwys calls LocalFileScpTargetStreamResolver
https://github.com/apache/mina-sshd/blob/master/sshd-core/src/main/java/org/apache/sshd/common/scp/ScpHelper.java#L334
This is imo the core of bug - the StreamResolver should be replaceable. Or at least it should take in consideration VirtualFileSystem and/or ScpFileOpener, ending like https://github.com/apache/mina-sshd/blob/master/sshd-core/src/main/java/org/apache/sshd/common/scp/helpers/LocalFileScpTargetStreamResolver.java#L91  or any of  many checks *before* there is actual call to custom StreamOpener. Those checks consist from simple path,parene, path.isDirecotry path.isWriteable and simialr.   IMho nothing of this should happen when custom ScpFileOpener is in play. But more likely I'm underestimating some security check.

I will be happy to implement those fix if we agree on approach.
Currnetly I have in mind possiblity of custom ScpTargetStreamResolver set in simialr way as StramOpener is, or fixing LocalFileScpTargetStreamResolver so it do nto check if custom StramOpener is in play.

Background
I'm usign mina as moreover simple scp-l;ike upload.  Now the system where it runs changed dramatically, so - based on the target path - I need to store the file on completely different place (""backward comaptible upload:) ) or - in addition - to send it over network .
Due to changes of destinations, I was unsuccessful to use VirtualFileSystem and in case of network forwarding I think I'm domed to StreamOpener anyway.


Best regards
  j.

",Secure trasnfer file's directoy existence is alway checked (on real FS) no metter of any other settings,3,,,jvanek,True,lgoldstein,jvanek
mina-sshd,SSHD-768,2017-08-30T06:46:30.000+0000,2017-09-07T05:54:58.000+0000,2017-09-07T05:54:58.000+0000,,Workaround,New Feature,Major,['1.7.0'],['1.6.0'],,,,,,,,,Our application runs an Apache MINA server to provide SSH support. We are seeing {{OutOfMemoryError}} s when certain clients establish a session with a large {{Window}} size. Particularly clients like TortoiseGit (which uses TortoisePlink which in turn seems to use Putty) use an initial window size of 2GB. From heap dumps we can see that the {{DefaultWriteRequestQueue}} is filling up with {{WriteRequest}} instances and taking up gigabytes of space until the heap blows. ,Avoid unbounded message queueing when sending large amounts of data to slow clients,3,,"['channel', 'stability', 'stream', 'throttle']",jpalacios,True,lgoldstein,jpalacios
mina-sshd,SSHD-767,2017-08-29T06:19:52.000+0000,,2017-09-14T12:07:18.000+0000,,,New Feature,Minor,,['1.7.0'],,,,,,,,,"According to [ssh_config(5)|https://www.freebsd.org/cgi/man.cgi?ssh_config(5)]:
{quote}
VerifyHostKeyDNS - Specifies whether to verify the remote key using DNS and SSHFP resource records. If this option is set to ``yes'', the client will implicitly trust keys that match a secure fingerprint from DNS.
{quote}",Add support for verifying server keys via DNS and SSHFP records,1,,"['client', 'dns', 'fingerprint', 'future', 'key-management', 'ssh']",lgoldstein,True,lgoldstein,lgoldstein
mina-sshd,SSHD-757,2017-07-26T12:35:37.000+0000,2018-11-22T05:23:34.000+0000,2018-11-22T05:23:34.000+0000,,Fixed,New Feature,Minor,['2.1.1'],['1.6.0'],,,,,,,,,"[SSH 2.3|http://www.onlamp.com/pub/a/onlamp/excerpt/ssh_8/] seems to have added the capability to use PGP keys as authorized ones:
{quote}
SSH2 Version 2.0.13 introduced support for PGP authentication. Your authorization file may also include {{PgpPublicKeyFile, PgpKeyName, PgpKey Fingerprint}}, and {{PgpKeyId}} lines. A Command line may follow them, just as it may follow Key:

{noformat}
# SSH2 only
PgpKeyName my-key
Command ""/bin/echo PGP authentication was detected"" 
{noformat}
{quote}

Some examples of how to use _Bouncycastle_ to facilitate this:
* [Sample code|https://github.com/damico/OpenPgp-BounceCastle-Example]
* [jpgpj Library wrapper|https://github.com/justinludwig/jpgpj]

_Python_ [converter pgp->ssh|https://raw.githubusercontent.com/fincham/ssh-to-pgp/master/ssh-to-pgp]

_openssh-gpg_ [configuration|http://www.red-bean.com/~nemo/openssh-gpg/]

[Calculating PGP key fingerprint|https://crypto.stackexchange.com/questions/32087/how-to-generate-fingerprint-for-pgp-public-key]",Add support for PGP authorized keys usage,1,,"['authorization', 'key', 'key-management', 'pgp', 'ssh']",lgoldstein,True,lgoldstein,lgoldstein
mina-sshd,SSHD-756,2017-07-23T04:24:06.000+0000,2017-07-23T04:52:11.000+0000,2017-07-23T04:52:11.000+0000,,Implemented,New Feature,Minor,,['1.6.0'],,,86400,86400,86400,,,,"Provide a simple ""hook"" for implementing an SSH-based GIT server using {{org.eclipse.jgit.lib.Repository}} and {{org.eclipse.jgit.transport.resolver.RepositoryResolver}}",Provide simple back-end SSH JGit server support,1,,['git'],lgoldstein,True,lgoldstein,lgoldstein
mina-sshd,SSHD-750,2017-05-27T19:02:45.000+0000,2017-05-28T16:11:46.000+0000,2017-05-28T16:11:46.000+0000,,Fixed,New Feature,Minor,['1.6.0'],"['1.4.0', '1.5.0']",,,,,,,,,"I debated listing this as a bug, given clients advertising 1.99 are SSH 2.0 clients and are being rejected, I believe, somewhat accidentally. However, on further research I see that the RFC doesn't really cover this situation (only the opposite: a server advertising 1.99 to a 2.0 client). Ultimately, I do think this should be supported given:

1) the server/client will still talk 2.0, nobody is suggesting adding a mode whereby they talk 1.x, so there shouldn't be a security issue here
2) MINA SSH client already supports it (inverse situation), so this creates alignment within the code between client/server capabilities
3) OpenSSH server supports it (not that everybody has to copy OpenSSH behavior, but it is certainly a widely supported implementation to look for advice on how to behave)
4) the patch is a one liner, easy to understand, and was easily tested (aka it works)

Github pull request:
https://github.com/apache/mina-sshd/pull/31",Accept SSH clients advertising vesion 1.99,1,,,nu11ptr,True,lgoldstein,nu11ptr
mina-sshd,SSHD-746,2017-05-09T15:34:08.000+0000,2018-01-09T14:11:05.000+0000,2018-01-22T06:55:08.000+0000,,Fixed,New Feature,Minor,['1.7.0'],['1.2.0'],,,,,,,,,"Seems a lot of the code is not aware of IPv6 patterns

{code}
java.lang.IllegalArgumentException: Invalid host pattern char in 0:0:0:0:0:0:0:1
	at org.apache.sshd.common.util.ValidateUtils.throwIllegalArgumentException(ValidateUtils.java:138)
	at org.apache.sshd.common.util.ValidateUtils.checkTrue(ValidateUtils.java:127)
	at org.apache.sshd.client.config.hosts.HostPatternsHolder.toPattern(HostPatternsHolder.java:247)
	at org.apache.sshd.client.config.hosts.HostPatternsHolder.parsePatterns(HostPatternsHolder.java:221)
	at org.apache.sshd.client.config.hosts.HostConfigEntry.setHost(HostConfigEntry.java:138)
	at org.apache.sshd.client.config.hosts.HostConfigEntry.<init>(HostConfigEntry.java:123)
	at org.apache.sshd.client.SshClient.connect(SshClient.java:505)
{code}",IPv6 addresses are not supported as client sessions connection target,3,,,stephenc,True,lgoldstein,stephenc
mina-sshd,SSHD-745,2017-05-09T09:16:50.000+0000,2019-03-27T13:47:17.000+0000,2019-03-27T13:47:17.000+0000,,Fixed,New Feature,Minor,['2.1.0'],['1.2.0'],,,,,,,,,"My known hosts file contains lines like: (note I have sanitized the lines and switched entries to documentation reserved IP addresses and DNS names)

{code}
[host.example.com]:7999,[192.0.2.3]:7999 ssh-rsa AAAA...Z45M/
2001:db8:85a3::8a2e:370:7334%en1 ecdsa-sha2-nistp256 AAAAE2...q9zmk=
{code}

Both of which will bomb out with errors like:

{code}
WARNING: Failed (StreamCorruptedException) to reload server keys from /Users/stephenc/.ssh/known_hosts: Failed (IllegalArgumentException) to parse line #96 '2001:db8:85a3::8a2e:370:7334%en1 ecdsa-sha2-nistp256 AAAAE2...q9zmk=': Invalid host pattern char in 2001:db8:85a3::8a2e:370:7334%en1
{code}

or

(This next one is a thrown away ssh server running in docker and the container is gone so I can provide the full unedited line)
{code}
WARNING: Failed (StreamCorruptedException) to reload server keys from /Users/stephenc/.ssh/known_hosts: Failed (IllegalArgumentException) to parse line #60 '[127.0.0.1]:49153 ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC5ntfSGAQOO2O/8ma7mtk9UaC6JWVlJ+6WAjnN3V510wc4nkkUcL66hZZyPLI0TYhk2CHNNQPeXkc6Cq7enSJab7SI06MhCQhF9yeuBQnphNBsUa01twPgycQOdrYey+4hIVXRF5BbckuU5nVAuFsPv7utNROmo+XoWoAdopSiejWs6Mrox+VIwgeN6peJ5l5jPqCjMEBTP9iNWgZMa+wQN7cTWpi7lIbK+a2hnzHz5P8oWFN8j8jREm+/6NK1IdW3c0CsLRWyee50GYYQOEi0zo1FfGweia5CbzfjRtfd0XFtUxGtULJpPoj9x/7xHuFUL1rYoKAGaTPzPdBHJTiJ': Invalid host pattern char in [127.0.0.1]:49153
{/code}

https://en.wikibooks.org/wiki/OpenSSH/Client_Configuration_Files seems to indicate that {{[host]:port}} is the syntax for when the ssh server is using a non-standard port 

https://serverfault.com/a/771355/147023 reports that the % at the end of the IPv6 address is indicating the local link (in my case %en1 being WIFI on my macbook, but I have seem %eth0 on modern linux servers)

Completely blowing up for these entries seems rather wrong, I would expect either skipping such entries or actually being able to parse them.

Checking the current code on master it seems this issue has not been resolved yet:

IOW https://github.com/apache/mina-sshd/blob/5b0b163c5500fedc2a6bae0fc5e260d64fcf370a/sshd-core/src/main/java/org/apache/sshd/client/config/hosts/HostPatternsHolder.java#L289-L306 still will reject a host name containing either the non-standard port specification or the link specifier",KnownHostEntry cannot parse [host]:port or IPv6 with %link entries,2,,,stephenc,True,,stephenc
mina-sshd,SSHD-741,2017-04-17T14:43:30.000+0000,2017-06-04T09:30:54.000+0000,2017-06-04T09:32:29.000+0000,,Fixed,New Feature,Minor,['1.6.0'],['1.4.0'],,,,,,,,,The current implementation relies on _JSch_ - we should see if we can provide a simple replacement for the [Spring integration SFTP adapter|http://docs.spring.io/spring-integration/docs/4.3.8.RELEASE/reference/html/sftp.html],Provide seamless replacement for Spring integation SFTP adapter,1,,"['features', 'sftp', 'spring']",lgoldstein,True,lgoldstein,lgoldstein
mina-sshd,SSHD-729,2017-02-24T17:27:32.000+0000,,2017-02-24T17:27:32.000+0000,,,New Feature,Minor,,['1.5.0'],,,,,,,,,See [rfc4819|https://filezilla-project.org/specs/rfc4819.txt] and [rfc7076|https://tools.ietf.org/html/rfc7076],Add support for 'publickey' subsystem,1,,['features'],lgoldstein,True,lgoldstein,lgoldstein
mina-sshd,SSHD-710,2016-10-23T15:41:58.000+0000,2017-06-01T17:00:05.000+0000,2017-06-01T21:19:04.000+0000,,Fixed,New Feature,Major,['1.6.0'],['1.4.0'],,,,,,,,,"It seems that if the SSHD client is using an _ed25519_ identity or the SSHD server presents an _ed25519_ key, then the generated signature is wrong either when (client) public key authentication is executed or (server) KEX.",Cannot connect standard OpenSSH client/server using ed25519 keys,4,,['security'],lgoldstein,True,lgoldstein,lgoldstein
mina-sshd,SSHD-700,2016-09-25T14:39:27.000+0000,2017-07-16T08:04:34.000+0000,2017-07-16T23:50:34.000+0000,,Fixed,New Feature,Minor,['1.7.0'],['1.2.0'],,,,,,,,,"I use MINA SSHD for both server side and client side:

SSH client  --> MINA SSHD Server - MINA SSHD Client --> Target Linux Server

I use XShell (http://www.netsarang.com/) as SSH client, and use XAgent with XShell for target server authentication (Public Key Access with Agent Forwarding).
I have tried PuTTY (with pagent), SecureCRT, and openssh client in linux, they are all passed. But when I try XShell with XAgent, the agent forwarding phase is failed.

When I check the debug log and source code of MINA SSHD, I find that SSHD only handle the ""auth-agent-req@openssh.com"" request type (in org.apache.sshd.server.channel.ChannelSession#handleInternalRequest), which is OK for PuTTY, SecureCRT and openssh client. But XShell send a ""auth-agent-req"" request (without ""@openssh.com""), so SSHD not handle the request.
I have try to change the source code, add ""auth-agent-req"" to the ""switch-case"" in handleInternalRequest, but the authentication is blocked.

I have attached my code, please help me solve the problem.

Thanks a lot.",SSHD does not suppot agent forwarding for XShell and XAgent,3,,,lfn,True,lgoldstein,lfn
mina-sshd,SSHD-693,2016-08-23T17:00:45.000+0000,2016-12-09T13:48:02.000+0000,2016-12-09T13:48:03.000+0000,,Won't Fix,New Feature,Minor,,['1.2.0'],,,,,,,,,The current API does not provide an {{InputStream download(String remotePath) throws IOException;}} method,Provide a way to retrieve an input stream when downloading via SCP,1,,,lgoldstein,True,lgoldstein,lgoldstein
mina-sshd,SSHD-685,2016-07-30T14:32:39.000+0000,2016-08-02T03:14:28.000+0000,2016-08-03T15:26:12.000+0000,,Fixed,New Feature,Minor,['1.3.0'],,,,,,,,,,Provide a configurable option to display the server's key as [randomArt|http://superuser.com/questions/22535/what-is-randomart-produced-by-ssh-keygen] in the welcome banner (see also [The drunken bishop|http://www.dirk-loss.de/sshvis/drunken_bishop.pdf]),Provide support for VisualHostKey (a.k.a. randomArt),1,,,lgoldstein,True,lgoldstein,lgoldstein
mina-sshd,SSHD-657,2016-02-25T20:49:14.000+0000,2016-02-26T05:48:35.000+0000,2016-02-26T05:48:35.000+0000,,Fixed,New Feature,Major,['1.2.0'],"['0.11.0', '0.12.0', '0.13.0', '0.14.0', '1.0.0', '1.2.0']",,,14400,14400,14400,,,,"Due to the upload requirements of implementing a message protocol over sftp where truncated
messages aren't easily detected by the receiving system, the extension is required to avoid
data corruption. In particular, the operation permits us to upload to a scratch name to be
ignored by the pickup process, and then rename to a name that will pick up, while atomically
erroring if the pickup name already exists.

In particular, the only safe upload sequence is:

rm working.@tmp@
put working.@tmp@
do {
   ln working.@tmp@ serial.txt
   increment serial
} while (ln reported file exists)

The ln operation is hardlink@openssh.com. We cannot substitute a ren operation here because
ren is allowed to clobber an existing file. Even if your code checks for this, there is beneath
it a race condition because the rename() system call doesn't check, and we must be race condition
free.

Anyway, I didn't think there were any more sftp providers left standing that didn't implement
hardlink@openssh.com; turns out I was wrong. I whipped up a quick patch to the hosting provider
involved to demonstrate how easy this is to implement. The patch probably works, but I have no
place to try it. I'm fairly confident because I just implemented a client side version a few
months ago.",missing hardlink@openssh.com sftp extension,2,,"['easyfix', 'features', 'newbie', 'patch']",joshudson,True,lgoldstein,joshudson
mina-sshd,SSHD-656,2016-02-24T17:29:00.000+0000,2017-04-09T13:11:04.000+0000,2017-06-29T09:34:14.000+0000,,Fixed,New Feature,Minor,['1.6.0'],,,,,,,,,,"Load Balancing and other higher availability services are included between client and SSHD server and works on TCP level. This makes an actual client address shown in the SSHD server to be a load balancer address, not a real client address. This makes it hard to use SSHD for multi-node production scenarios. 
 
There are several ways to solve the issue.
The first one is to include complex TCP routing to have specific packets delivered correctly. This is too hard to setup

It looks like using {{The PROXY Protocol}} is the possible, easy and more or less standard way to pass actual client/server addresses to the server over TCP.  The protocol is  implemented by a number of TCP-based servers (including nginx, Amazon Load Balancer, Apache, github enterprise, see the link below for details)

Protocol specification is here 
http://www.haproxy.org/download/1.6/doc/proxy-protocol.txt",Support The PROXY protocol,3,,,jonnyzzz,True,lgoldstein,jonnyzzz
mina-sshd,SSHD-594,2015-11-19T06:30:25.000+0000,,2015-11-19T06:31:06.000+0000,,,New Feature,Minor,,,,,,,,,,,See [RFC 5656|https://tools.ietf.org/html/rfc5656],Add support for ecmqv-sha2 key exchange,1,,,lgoldstein,True,,lgoldstein
mina-sshd,SSHD-593,2015-11-18T09:08:57.000+0000,,2016-03-01T08:34:45.000+0000,,,New Feature,Major,,['0.14.0'],,,,,,,,,RFC 6187 specifies how to use X.509 v3 public  key certificates in public key algorithms in the Secure Shell protocol. Would be great if SSHD can support this out of the box. Other ways to employ Certificate authentication are also welcome.,RFC 6187 Support in MINA SSHD,2,,,kbhatk,True,,kbhatk
mina-sshd,SSHD-559,2015-08-31T06:01:11.000+0000,2015-09-02T05:40:41.000+0000,2015-09-02T05:40:41.000+0000,,Fixed,New Feature,Major,['1.1.0'],,,,,,,,,,"Allow users to define 'aliases' which refer to hosts configured according to the [ssh_config|http://www.gsp.com/cgi-bin/man.cgi?topic=ssh_config] format so that when the code invokes _connect(host, port, etc...)_ the _host_ value may refer to an *alias* which has been read from the configuration file.",Add support for reading specific host configuration from config file,1,,,lgoldstein,True,lgoldstein,lgoldstein
mina-sshd,SSHD-550,2015-08-18T07:26:03.000+0000,2015-08-18T13:02:23.000+0000,2015-08-18T13:02:23.000+0000,,Fixed,New Feature,Major,['1.0.0'],,,,,,,,,,,Support sending window change messages from PtyCapableChannelSession,1,,,gnt,True,gnt,gnt
mina-sshd,SSHD-440,2015-03-27T09:56:19.000+0000,2016-10-11T15:01:09.000+0000,2017-02-01T05:31:39.000+0000,,Fixed,New Feature,Major,['1.4.0'],['0.14.0'],,,,,,,,,"As an administrator of a production system running with Apache Karaf I want to secure the system as much as possible.

Based on the following article I would like to use the ""curve25519-sha256"" key exchange mechanism and Ed25519 keys for the SSH server and client:
* https://stribika.github.io/2015/01/04/secure-secure-shell.html

\\
The probably most stable implementation in pure Java is currently residing in I2P:
* https://github.com/i2p/i2p.i2p/tree/master/core/java/src/net/i2p/crypto/eddsa

\\
...although it seems to have originated at the following standalone library:
* https://github.com/str4d/ed25519-java
",Support Ed25519 keys and ed25519-sha-512 signature type,10,6,,ancoron,True,lgoldstein,ancoron
mina-sshd,SSHD-408,2015-02-11T13:36:49.000+0000,2015-02-23T15:32:12.000+0000,2015-02-23T15:32:12.000+0000,,Fixed,New Feature,Major,['1.0.0'],,,,,,,,,,,"Implement sftp v4, v5 and v6",2,,,gnt,True,gnt,gnt
mina-sshd,SSHD-392,2014-12-17T15:13:01.000+0000,2016-02-18T14:46:03.000+0000,2016-02-18T14:46:03.000+0000,,Won't Fix,New Feature,Minor,,,,,,,,,,,"Implement an option to limit the bandwidth sum of all downloads / uploads, to ensure that the SFTP server does not use the entire bandwidth.
I only use the sftp subsystem so I have no idea if this this makes sense for entire sshd.",Add transfer limit for server,2,,,Stefan Mueller,True,,Stefan Mueller
mina-sshd,SSHD-390,2014-12-16T20:48:45.000+0000,2014-12-29T08:56:18.000+0000,2014-12-29T08:56:18.000+0000,,Fixed,New Feature,Major,['0.14.0'],,,,,,,,,,"This is interesting for data transfers using scp / sftp that do not really need encryption.
The authentication phase is always encrypted (unless the none cipher is the only one configured).",Support switching to a none cipher on the client side for performances,1,,,gnt,True,gnt,gnt
mina-sshd,SSHD-383,2014-12-12T09:50:00.000+0000,2014-12-12T13:19:40.000+0000,2014-12-12T13:19:40.000+0000,,Fixed,New Feature,Minor,['0.14.0'],,,,,,,,,,,Support for loading ecdsa keys in the client demo,1,,,gnt,True,gnt,gnt
mina-sshd,SSHD-382,2014-12-12T09:47:47.000+0000,2014-12-12T13:19:27.000+0000,2014-12-12T13:19:27.000+0000,,Fixed,New Feature,Minor,['0.14.0'],,,,,,,,,,,Add support for custom properties on the command line for client and server,1,,,gnt,True,gnt,gnt
mina-sshd,SSHD-379,2014-12-01T15:43:21.000+0000,2015-05-06T12:06:00.000+0000,2015-05-06T12:06:00.000+0000,,Fixed,New Feature,Major,['1.0.0'],,,,,,,,,,,Require JDK 1.7,2,,,gnt,True,lgoldstein,gnt
mina-sshd,SSHD-378,2014-12-01T15:42:36.000+0000,2015-02-23T15:31:46.000+0000,2015-05-07T04:10:04.000+0000,,Fixed,New Feature,Major,['1.0.0'],,,,,,,,,,,Switch to nio FileSystem api for commands (scp and sftp subsystem),1,,,gnt,True,gnt,gnt
mina-sshd,SSHD-377,2014-12-01T15:41:44.000+0000,2015-02-23T15:31:23.000+0000,2015-02-23T15:31:23.000+0000,,Fixed,New Feature,Major,['1.0.0'],,,,,,,,,,,Create a nio FileSystem implementation for sftp,1,,,gnt,True,gnt,gnt
mina-sshd,SSHD-371,2014-11-17T15:44:56.000+0000,2014-11-17T20:15:27.000+0000,2014-11-17T20:15:27.000+0000,,Fixed,New Feature,Major,['0.14.0'],,,,,,,,,,,Support Socks proxy with ssh tunnelling on the client side,1,,,gnt,True,gnt,gnt
mina-sshd,SSHD-358,2014-10-10T11:54:12.000+0000,2014-10-21T08:04:43.000+0000,2014-10-21T09:47:37.000+0000,,Fixed,New Feature,Minor,['0.13.0'],,,,,,,,,,I want to extend them and use default configuration. Right now it's impossible without copying all configuration code to my class.,Create Builders for SShServer and SShClient so they can be extended properly,2,,,pskierczynski,True,gnt,pskierczynski
mina-sshd,SSHD-341,2014-08-25T14:37:56.000+0000,2014-09-17T06:30:53.000+0000,2014-10-17T06:44:18.000+0000,,Fixed,New Feature,Major,['0.13.0'],['0.12.0'],,,,,,,,,Facilities should exist for providing UserInteractive implementations on a per-session basis. This would align more with the mechanics of the other authentication mechanics supported (e.g. password) by providing method(s) on the ClientSession interface and making use of these per-session UserInteractive instances within AuthUserKeyboardInteractive.,Support UserInteractive implementations on a per-session basis,2,,,invinity,True,gnt,invinity
mina-sshd,SSHD-327,2014-06-04T18:44:18.000+0000,2015-05-06T14:01:10.000+0000,2015-05-07T04:16:09.000+0000,,Fixed,New Feature,Major,['1.0.0'],,,,,,,,,,,Support for OpenSSH config file in ~/.ssh/config,1,,,gnt,True,lgoldstein,gnt
mina-sshd,SSHD-312,2014-04-21T09:36:13.000+0000,2014-06-05T16:12:25.000+0000,2014-06-05T16:12:25.000+0000,,Fixed,New Feature,Major,['0.12.0'],,,,,,,,,,"The OutputStream returned by ClientChannel#getInvertedIn() can block while flushing, because of a wait for free space in the remote window.
The same is true for Command.",Provide fully asynchronous interfaces for ClientChannel and Command,2,1,,gnt,True,gnt,gnt
mina-sshd,SSHD-287,2014-01-31T10:48:57.000+0000,2014-01-31T10:52:32.000+0000,2014-01-31T10:52:32.000+0000,,Fixed,New Feature,Major,['0.10.0'],,,,,,,,,,,Support client-side partial authentication,1,,,gnt,True,gnt,gnt
mina-sshd,SSHD-283,2014-01-29T15:54:04.000+0000,2014-01-29T23:48:58.000+0000,2014-01-30T08:45:55.000+0000,,Fixed,New Feature,Major,['0.10.0'],,,,,,,,,,,Support key re-exchange,1,,,gnt,True,gnt,gnt
mina-sshd,SSHD-277,2014-01-27T13:53:33.000+0000,2014-01-27T23:41:36.000+0000,2014-01-27T23:41:36.000+0000,,Fixed,New Feature,Major,['0.10.0'],,,,,,,,,,,Add RFC 4419 (DH Group Exchange) support,1,,,gnt,True,gnt,gnt
mina-sshd,SSHD-273,2013-12-22T19:39:52.000+0000,2014-01-24T10:45:23.000+0000,2014-03-20T10:46:50.000+0000,,Fixed,New Feature,Major,['0.10.0'],['0.10.0'],,,,,,,,,"Adds support for HMAC with SHA-2 hashes (SHA-256 and SHA-512) as specified in RFC 6668. Both HmacSHA256 and HmacSHA512 are added as they are supported by Sun/Oracle JDK, OpenJDK, and BouncyCastle.",Add RFC 6668 (HMAC SHA-2) support,4,,,kruton,True,gnt,kruton
mina-sshd,SSHD-272,2013-12-22T19:15:48.000+0000,2014-01-23T21:07:24.000+0000,2014-01-25T13:50:08.000+0000,,Fixed,New Feature,Major,['0.10.0'],['0.10.0'],,,,,,,,,"Add support for RFC 5656 by enabling use of EC keys for ECDSA signatures and ECDH for key exchange. This should support the three mandatory curves specified by the RFC at the least: NIST P-256, P-384, and P-521",Add RFC 5656 (ECDSA and ECDH) support,2,,,kruton,True,gnt,kruton
mina-sshd,SSHD-266,2013-12-03T14:42:07.000+0000,2016-02-14T14:17:36.000+0000,2016-03-01T08:34:22.000+0000,,Fixed,New Feature,Major,['1.2.0'],,,,,,,,,,"* See http://www.manpagez.com/man/8/sshd/ for the format
* See http://nms.lcs.mit.edu/projects/ssh/README.hashed-hosts and https://pthree.org/2011/12/30/making-sense-of-hashed-hosts-in-sshknown_hosts/ for the hosts hashing",Support for known hosts,2,,,gnt,True,lgoldstein,gnt
mina-sshd,SSHD-265,2013-12-02T20:48:50.000+0000,2013-12-02T21:08:52.000+0000,2013-12-02T21:08:52.000+0000,,Fixed,New Feature,Major,['0.10.0'],['0.9.0'],,,,,,,,,,Provide a file system that can be locked to a dir (eventually user dependant),1,,,gnt,True,gnt,gnt
mina-sshd,SSHD-245,2013-07-26T00:34:10.000+0000,2013-07-26T08:45:35.000+0000,2013-07-26T10:48:42.000+0000,,Fixed,New Feature,Minor,['0.9.0'],,,,,,,,,,"I can't find a way to force a unix ssh client to send an interrupt signal, but it seems easy to force the break.
Unfortunately, the break message isn't implemented.

It was easy to patch in my needs...  

In ChannelSession:

    protected boolean handleRequest(String type, Buffer buffer) throws IOException {
        if (""env"".equals(type)) {
            return handleEnv(buffer);
        }
        if (""pty-req"".equals(type)) {
            return handlePtyReq(buffer);
        }
        if (""window-change"".equals(type)) {
            return handleWindowChange(buffer);
        }
        if (""signal"".equals(type)) {
            return handleSignal(buffer);
        }
        if (""break"".equals(type)) {
            return handleBreak(buffer);
        }

....

    protected boolean handleBreak(Buffer buffer) throws IOException {
        boolean wantReply = buffer.getBoolean();
        int length = buffer.getInt();
        log.debug(""Break received on channel {}: {}"", id, length);

        getEnvironment().signal( Signal.INT );

        if (wantReply) {
            buffer = session.createBuffer(SshConstants.Message.SSH_MSG_CHANNEL_SUCCESS, 0);
            buffer.putInt(recipient);
            session.writePacket(buffer);
        }
        return true;
    }

",Support Break Extension (RFC 4335),1,,,glennmcg,True,gnt,glennmcg
mina-sshd,SSHD-244,2013-07-25T12:18:04.000+0000,2013-07-26T08:58:12.000+0000,2013-07-26T10:48:41.000+0000,,Fixed,New Feature,Major,['0.9.0'],,,,,,,,,,,Abstract the IO layer and provide two providers: mina and plain nio2,1,,,gnt,True,gnt,gnt
mina-sshd,SSHD-241,2013-07-23T15:09:09.000+0000,2014-06-04T18:32:46.000+0000,2014-06-04T18:32:46.000+0000,,Fixed,New Feature,Major,['0.12.0'],,,,,,,,,,,Git/SSH command support,1,,,gnt,True,gnt,gnt
mina-sshd,SSHD-239,2013-07-22T18:39:32.000+0000,2013-07-22T18:39:59.000+0000,2013-07-22T18:39:59.000+0000,,Fixed,New Feature,Major,['0.9.0'],,,,,,,,,,,Provide an SFTP client,1,,,gnt,True,gnt,gnt
mina-sshd,SSHD-235,2013-07-17T14:32:48.000+0000,2014-01-31T10:52:26.000+0000,2014-01-31T10:52:26.000+0000,,Fixed,New Feature,Major,['0.10.0'],,,,,,,,,,,Refactor client side authentication,1,,,gnt,True,gnt,gnt
mina-sshd,SSHD-234,2013-07-17T09:34:12.000+0000,2013-07-17T14:34:02.000+0000,2013-07-17T19:08:59.000+0000,,Fixed,New Feature,Major,['0.9.0'],,,,,,,,,,"Authentication may require multiple methods to actually grant access.
This is currently not supported on the server nor the client side.",Support partial authentication,1,,,gnt,True,gnt,gnt
mina-sshd,SSHD-195,2012-10-10T14:13:53.000+0000,2013-01-07T16:07:30.000+0000,2013-01-07T16:07:30.000+0000,,Fixed,New Feature,Trivial,['0.9.0'],['0.8.0'],,,,,,,,,"When a user connects, sometimes a welcome banner is required to let the user know the site that they've connected to and what the rules of the site are.
This patch allows a banner to be set in the code after an SshServer has been instantiated:
		SshServer sshd = SshServer.setUpDefaultServer();
		sshd.getProperties().put(SshServer.WELCOME_BANNER, ""This is the Banner"");
",Display a Welcome Banner after a user connects,2,,,tpaulsz,True,gnt,tpaulsz
mina-sshd,SSHD-187,2012-09-17T17:17:35.000+0000,2013-07-18T07:32:36.000+0000,2013-07-18T07:32:36.000+0000,,Fixed,New Feature,Major,['0.9.0'],,,,,,,,,,,Support for keyboard interactive authentication,3,2,,gnt,True,gnt,gnt
mina-sshd,SSHD-185,2012-09-14T10:23:50.000+0000,2012-09-14T12:42:46.000+0000,2019-06-25T17:33:34.000+0000,,Fixed,New Feature,Major,['0.8.0'],,,,,,,,,,,Support client heartbeat to keep the connection alive,4,,,gnt,True,gnt,gnt
mina-sshd,SSHD-175,2012-06-12T01:42:12.000+0000,2012-09-14T20:08:42.000+0000,2012-09-14T20:08:43.000+0000,,Duplicate,New Feature,Minor,,,,,,,,,,,"Add client support for port-forwarding

Patch attached; it creates a new channel type (direct-tcpip) and also has two strategies for using it.

The first strategy opens a local socket and relays incoming connections over the SSH channel, it is useful for compatibility, but the implementation is thread-intensive and so not really preferred.

The preferred strategy subclasses Socket, and allows for connection over an SSH tunnel without opening a local port.

https://github.com/platformlayer/mina-sshd/commit/6f798f46b1b79b8066a78b5323aa412978a18b1e",Support client port-forwarding,1,,,justinsb,True,gnt,justinsb
mina-sshd,SSHD-173,2012-06-05T06:16:28.000+0000,2015-11-24T08:22:27.000+0000,2015-11-24T08:22:27.000+0000,,Fixed,New Feature,Major,['1.1.0'],,,,,,,,,,Similar to FTPLet(s) ...,Add event listener interfaces,3,,,gnt,True,,gnt
mina-sshd,SSHD-165,2012-05-14T08:22:27.000+0000,2012-05-14T09:28:15.000+0000,2013-05-02T02:29:53.000+0000,,Fixed,New Feature,Major,['0.7.0'],,,,,,,,,,,"Improved support for SSH agents, including a local proxy without the needs for native libraries (for unix sockets or windows pipes)",,,,gnt,True,gnt,gnt
mina-sshd,SSHD-161,2012-03-22T09:50:44.000+0000,2012-05-21T08:43:47.000+0000,2012-05-21T08:43:47.000+0000,,Won't Fix,New Feature,Major,,,,,,,,,,,,"After login using SSHServer program ...It displays ""bash: no job control in this shell""...How to solve this problem????",2,,,michael27,True,gnt,michael27
mina-sshd,SSHD-150,2011-11-04T18:10:58.000+0000,2013-12-03T21:08:05.000+0000,2016-11-08T22:11:51.000+0000,,Fixed,New Feature,Major,,['0.6.0'],,,,,,,,,"I need an API to configure a user and his home directory, just like you can with Apache's FTP server BaseUser class (http://mina.apache.org/ftpserver/): 

{code:java}
        final BaseUser user = (BaseUser) userManager.getUserByName(""test"");
        user.setHomeDirectory(getTestDirectory());
{code}

My goal is to embed an SSHd server in the SFTP unit tests for Apache Commons VFS.

I need to point the test user's home directory to a subdirectory in the projects's resources.",Need an API to configure a user's home directory,4,1,,ggregory,True,,ggregory
mina-sshd,SSHD-92,2010-08-02T18:47:37.000+0000,2010-09-22T14:56:55.000+0000,2010-09-22T14:56:55.000+0000,,Fixed,New Feature,Major,['0.5.0'],['0.4.0'],,,,,,,,,"This patch provides a pluggable way to check the server key along with some very basic policies.  It also exposes the exit code when an SSH execute completes.

I've also added a 'metadata' dictionary to the ClientSession, so that applications can easily store their own information.  This is used so that the ClientSession can contain a ServerKeyVerifier, so we can do per-server verification even when multiple ClientSessions are run in the same SshClient instance.","Patch: Server Key Verification, Expose exit code",1,,,justinsb,True,gnt,justinsb
mina-sshd,SSHD-90,2010-06-24T16:02:16.000+0000,2013-07-22T18:38:03.000+0000,2013-07-22T18:38:03.000+0000,,Fixed,New Feature,Major,['0.9.0'],,,,,,,,,,Adding client-side SCP tools.,Client-side SCP,2,1,,brainlounge,True,gnt,brainlounge
mina-sshd,SSHD-81,2010-03-29T09:53:10.000+0000,2010-03-31T14:06:52.000+0000,2010-03-31T14:07:09.000+0000,,Fixed,New Feature,Major,['0.4.0'],,,,,,,,,,,Support for X11 forwarding on the server side,,,,abbierwolf,True,gnt,abbierwolf
mina-sshd,SSHD-60,2009-11-16T23:24:29.000+0000,2009-11-17T03:27:20.000+0000,2009-11-17T03:27:20.000+0000,,Fixed,New Feature,Major,['0.3.0'],['0.3.0'],,,,,,,,,"Most users of Gerrit Code Review want to block all port forwarding to the internal SSH daemon as it might permit external end users connecting to the daemon to open arbitrary network connections within the server's otherwise firewalled network.

Add an interface which permits the process which created and configured the SshServer to make TCP/IP forwarding decisions, and block requests it does not want to permit.",need interface to filter TCP/IP forwarding,,,,spearce,True,,spearce
mina-sshd,SSHD-55,2009-11-13T17:01:45.000+0000,2010-04-29T07:50:08.000+0000,2010-04-29T07:50:07.000+0000,,Fixed,New Feature,Major,['0.4.0'],,,,,,,,,,,Support for SFTP on the server side,1,2,,gnt,True,gnt,gnt
mina-sshd,SSHD-45,2009-10-28T09:00:34.000+0000,2012-09-14T20:10:04.000+0000,2012-09-14T20:10:04.000+0000,,Fixed,New Feature,Major,['0.8.0'],,,,,,,,,,,Support local/remote port forwarding on the client side,1,,,gnt,True,gnt,gnt
mina-sshd,SSHD-40,2009-08-11T16:20:04.000+0000,2009-10-28T09:01:19.000+0000,2009-10-28T09:01:19.000+0000,,Fixed,New Feature,Major,['0.3.0'],,,,,,,,,,,Support local/remote port forwarding on the server side,,,,rajika,True,gnt,rajika
mina-sshd,SSHD-27,2009-06-08T15:50:02.000+0000,2009-06-08T15:50:43.000+0000,2009-07-27T07:23:49.000+0000,,Fixed,New Feature,Major,['0.1.0'],,,,,,,,,,,Support the exec command mode on the client side,,,,gnt,True,gnt,gnt
mina-sshd,SSHD-26,2009-06-08T06:45:35.000+0000,2015-09-09T04:05:01.000+0000,2015-09-09T04:05:01.000+0000,,Fixed,New Feature,Major,['1.1.0'],,,,,,,,,,,Add a synchronous API on the client side for easier use,1,,,gnt,True,lgoldstein,gnt
mina-sshd,SSHD-25,2009-06-08T06:45:10.000+0000,2009-12-04T18:41:33.000+0000,2009-12-04T18:42:46.000+0000,,Fixed,New Feature,Major,['0.4.0'],,,,,,,,,,,Support key based authentication on the client side,,,,gnt,True,gnt,gnt
mina-sshd,SSHD-11,2008-12-31T16:16:52.000+0000,2009-01-20T18:41:58.000+0000,2009-07-27T07:23:49.000+0000,,Fixed,New Feature,Major,['0.1.0'],,,,,,,,,,"Some authenticators may need to record state information associated with the key they authenticated against for later reference by command implementations.  This is necessary for example to honor the forced-command feature of the OpenSSH authorized_keys file format, but may also be helpful to remember the database's row key when a public key authentication is checked against a SQL database and Command implementations need that same row key to perform their actions.",Expose the ServerSession to PublickeyAuthenticators,,,,spearce,True,gnt,spearce
mina-sshd,SSHD-8,2008-12-23T11:02:24.000+0000,2009-12-17T13:36:21.000+0000,2009-12-17T13:36:21.000+0000,,Fixed,New Feature,Major,['0.4.0'],,,,,,,,,,"See http://www.unixwiz.net/techtips/ssh-agent-forwarding.html and http://www.securityfocus.com/infocus/1812

To be compatible with openssh ssh client, we'd have to use Unix Domain Sockets which require JNI.
It could be done using tomcat-apr Local class, or importing the needed bits into our own native library.

We could also have two different implementations, one using a local tcp socket and another using unix sockets.",Implement agent forwarding,,,,gnt,True,gnt,gnt
mina-sshd,SSHD-5,2008-12-22T15:16:23.000+0000,2008-12-22T16:00:51.000+0000,2009-07-27T07:23:51.000+0000,,Duplicate,New Feature,Major,['0.1.0'],,,,,,,,,,"Within my Command class I need to know the username that was authenticated; e.g. the value returned by ServerSession.getUsername():

  class AnyCommand implements Command, NeedsServerSession {
    protected ServerSession session;
    ...
    public void setServerSession(final ServerSession session) {
      this.session = session;
    }

    public void start() throws IOException {
      System.err.println(""From "" + session.getUsername());
      System.err.println(""{"" + command + ""}"");
      ...
    }
  }
",Make the ServerSession available to Command implementations,,,,spearce,True,gnt,spearce
mina-sshd,SSHD-2,2008-12-12T10:57:34.000+0000,2008-12-22T10:49:05.000+0000,2009-07-27T07:23:50.000+0000,,Fixed,New Feature,Major,['0.1.0'],,,,,,,,,,,Use future on the client side for asynchronous operations,,,,gnt,True,gnt,gnt
commons-validator,VALIDATOR-456,2019-04-03T15:04:20.000+0000,,2019-04-03T15:04:20.000+0000,,,New Feature,Major,,,,,,,,,,,"for a validated iban, we can extract the routingNo and AccountNo 

example for Austria
 {{ATkk bbbb bccc cccc cccc}} - {{bbbbb}} is the routingNo and {{ccc...c}} is the AccountNo

Benifit : with routingNo we can find the bank the iban belongs to.
Example : AT611904300234573201 - the routingNo is 19043 - this is UniCredit Bank AG

regards",provide an extractor of routingNo and AccountNo for a valid IBAN,1,,,eugenh,True,,eugenh
commons-validator,VALIDATOR-423,2017-04-25T11:37:41.000+0000,2017-04-25T14:27:11.000+0000,2017-04-25T14:27:11.000+0000,,Fixed,New Feature,Major,['1.7'],['1.6'],,,,,,,['Routines'],['Validation Routines'],"There is an ISINCheckDigit class, but as yet no ISINValidator class.",Add ISINValidator,,,,sebb,True,,sebb
commons-validator,VALIDATOR-415,2017-02-12T15:52:06.000+0000,2017-02-13T10:56:25.000+0000,2017-02-22T16:36:27.000+0000,,Fixed,New Feature,Major,['1.6'],,,,,,,,,,"Credit card validators currently rely on regexes.
Whilst these are flexible enough for the purpose, it is difficult to define the REs and hard to read them.

It would be simpler if the user could provide the prefix ranges and lengths, for example:

Amex: ""34"", length 15
Discover: ""644"" -> ""65"", length 16

(both the above have other ranges which would need to be provided separately)

This can then be converted internally into the appropriate validation routines.",Simplify building new CreditCard validators,,,,sebb,True,,sebb
commons-validator,VALIDATOR-402,2016-08-23T09:53:04.000+0000,2017-02-05T17:49:57.000+0000,2017-02-05T17:49:57.000+0000,,Implemented,New Feature,Major,,,,,,,,,,,"I wrote an ISIN validator. Might be a good contribute (or not). I did not really ask the dev list first.

https://github.com/apache/commons-validator/pull/7",ISIN validator,1,,,raupach,True,,raupach
commons-validator,VALIDATOR-394,2016-05-07T23:01:18.000+0000,2017-02-05T17:55:31.000+0000,2017-02-22T16:36:27.000+0000,2016-05-21,Fixed,New Feature,Major,['1.6'],,,,86400,86400,86400,,['Routines'],['Validation Routines'],"Discussion on the commons dev mailing list about a CheckDigit implementation for German identity cards prompted a general  Modulus 10 CheckDigit implementation where the weighting factors could be specified.

See http://markmail.org/message/zqxsep327ios2r4t",General Modulus Ten Check Digit Implementation,1,,,niallp,True,,niallp
commons-validator,VALIDATOR-357,2015-01-15T14:53:13.000+0000,,2015-11-26T16:42:04.000+0000,,,New Feature,Minor,,"['1.1.3 Release', '1.2.0 Release', '1.3.0 Release', '1.3.1 Release', '1.4.0 Release', '1.4.1 Release']",,,,,,,['Framework'],['Validator Framework'],"Validator 1.41 depends on BeanUtils 1.8.3.  This has a ""potential security issue"", see http://commons.apache.org/proper/commons-beanutils/javadocs/v1.9.2/RELEASE-NOTES.txt  Also, see http://www.cvedetails.com/cve-details.php?t=1&cve_id=cve-2014-0114

Even if this issue doesn't affect Validator, BeanUtils should be upgraded so that issue issue doesn't affect other users of BeanUtils given the screwy way some builders (e.g. Maven) resolve conflicting dependencies.",Upgrade BeanUtils,2,,,ddillard,True,,ddillard
commons-validator,VALIDATOR-342,2014-10-10T12:57:14.000+0000,2015-01-06T23:52:16.000+0000,2015-01-13T20:02:31.000+0000,,Fixed,New Feature,Major,['1.4.1 Release'],,,,,,,,,,It seems the (valid) subdomain is the problem because it returns true for http://example.com,URLValidator returns false for http://example.rocks,1,1,,domurtag,True,,domurtag
commons-validator,VALIDATOR-341,2014-08-25T05:31:41.000+0000,2015-11-05T00:43:12.000+0000,2018-12-07T12:14:53.000+0000,,Fixed,New Feature,Major,['1.5.0'],,,,,,,,['Framework'],['Validator Framework'],"Since the list of valid TLDs changes from time to time, it makes sense to let users configure the list of valid TLDs.",Make TLD list configurable,6,3,"['TLD', 'domains', 'email']",britter,True,,britter
commons-validator,VALIDATOR-325,2013-12-02T12:21:05.000+0000,2015-11-07T23:50:26.000+0000,2015-11-07T23:50:26.000+0000,,Fixed,New Feature,Major,,,,,,,,,,,"When I try to validate an incorrect belgian IBAN code ""BE7436302152834"", than it passes the validation without any problem although even the length of the code is invalid.",Improve IBAN validation with format checks,2,,,wvlg,True,,wvlg
commons-validator,VALIDATOR-312,2012-10-09T11:55:21.000+0000,2015-01-05T01:22:43.000+0000,2015-01-05T01:22:43.000+0000,,Duplicate,New Feature,Major,,['1.4.0 Release'],,,,,,,['Routines'],['Validation Routines'],"The validation of the country code in DomainValidator, used in EmailValidator, is done according to a fixed list of country code list.

This is not practical as this list is usually updated, so the check fails for new countries (e.g. South Sudan '.ss').

The DomainValidator is singleton with private constructor so it is not possible to subclass it. We need to provide in someway an updated list so check will not fail.",Extending country code list in DomainValidator,1,,['patch'],haythammb,True,,haythammb
commons-validator,VALIDATOR-290,2010-12-08T12:43:20.000+0000,2015-01-04T21:30:02.000+0000,2015-01-13T20:02:26.000+0000,,Fixed,New Feature,Major,['1.4.1 Release'],"['1.3.1 Release', '1.4.0 Release']",,,,,,,['Routines'],['Validation Routines'],"UrlValidation at the moment use old rfc2396 and doesn't check IDN (through punycode or something)
",Create new url validation using rfc3986 and IDN,3,2,,arytov,True,,arytov
commons-validator,VALIDATOR-280,2009-09-04T14:32:03.000+0000,2016-01-07T22:46:44.000+0000,2016-01-07T22:46:44.000+0000,,Won't Fix,New Feature,Major,,,,,,,,,,,"with the fail-safe-mode enabled, every input has a maxlength validation with at most 100 characters.

So, if a developer forgets to specify a rule, this would apply in any case.

This is kind of a shift from an opt-in paradigm to an opt-out paradigm.

If for a specific field the max length is different from 100, then it is the task of the developer to specify this in validation.xml",create a fail-safe mode (maxlength),,,,hauser@acm.org,True,,hauser@acm.org
commons-validator,VALIDATOR-279,2009-09-02T16:18:45.000+0000,,2009-09-02T16:18:45.000+0000,,,New Feature,Major,['2.0'],['2.0'],,,,,,,,,"Upgrade/rework the validator code to create a JSR-303 compliant implementation, which can then be used by Apache Geronimo, OpenJPA and MyFaces, instead of the RedHat Hibernate RI.
Work will be performed in the sandbox as 2.0-SNAPSHOT, until we have made enough progress to consider moving it to trunk.
",Create an implementation of the JSR-303 Bean Validation Spec,,1,,drwoods,True,drwoods,drwoods
commons-validator,VALIDATOR-270,2008-08-26T05:25:08.000+0000,2017-02-05T18:53:55.000+0000,2017-02-05T18:53:55.000+0000,,Invalid,New Feature,Major,,['1.3.1 Release'],,,,,,,,,"in our struts application, a form field that is for example too long can not be named depending on the locale alone, but also on other factors 

in order to allow for this, a call-back method would need to access the HttpServlet and HttpServletRequest in order to be able to access the ServletContext and HttpSession that allow to determine which resource to take.

Therefore, it would be great if the DTD had 3 more optional attributes to the <msg> tag:

1) ""callBackMethod""  (e.g. ""org.apache.myApp.NameChooser.determine"")
2) ""callBackParameters"" (e.g. ""javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServlet"" - commaSeparated), obviously, only classes allowed that are available at the point where a validate() is executed
3) ""callBackNames"" (e.g. in a struts action or jsp, this would be ""request,servlet"" or if the Parameter were ""javax.servlet.ServletContext"", the name in a jsp would be ""pageContext"")

Alternatively, the arguments might be handed over in a generic object array?",allow to determine NLS resource by other inputs via call-back method,,,,hauser@acm.org,True,,hauser@acm.org
commons-validator,VALIDATOR-258,2008-03-26T23:25:36.000+0000,2017-02-05T20:06:21.000+0000,2017-02-05T20:06:21.000+0000,,Implemented,New Feature,Minor,,"['1.1.0 (alpha)', '1.1.1 (alpha)', '1.1.2 (alpha)', '1.1.3 Release', '1.1.4 Release', '1.2.0 Release', '1.3.0 Release', '1.3.1 Release', '1.4.0 Release', 'Nightly Builds', '2.0']",,,60,60,60,,['Framework'],['Validator Framework'],"Add support for a textfile containing TLDs. If the file is not found, use the existing code (i.e. return true).",EmailValidator: the tld should be checked against some sort of configurable list,2,1,,hdiwan,True,,hdiwan
commons-validator,VALIDATOR-250,2007-11-14T14:55:52.000+0000,2010-02-04T21:34:25.000+0000,2014-01-30T14:08:11.000+0000,,Fixed,New Feature,Major,['1.4.0 Release'],['1.3.1 Release'],,,,,,,,,"I've had a number of CheckDigit implementations sitting on my hard drive for a while - which I created just for fun (sad I know!). I haven't used them in anger (and am unlikely to) so I didn't commit them when I checked in the rest of the CheckDigit stuff. Thought I would post them here in case anyone else thinks they would be good to add or would find a use for them

1) ABA Number
============
ABA Numbers (or Routing Transit Numbers) are a nine digit numeric code used to identify American financial institutions for things such as checks or deposits (ABA stands for the American Bankers Association).
    http://en.wikipedia.org/wiki/Routing_transit_number

2) CUSIP
========
CUSIP Numbers are 9 character alphanumeric codes used to identify North American Securities.
    http://en.wikipedia.org/wiki/CUSIP

3) IBAN
=======
IBAN (International Bank Account Number) Check Digit calculation/validation based on the ISO 7064 Mod 97,10 check digit caluclation routine.
    http://en.wikipedia.org/wiki/International_Bank_Account_Number

4) ISIN
=======
ISIN (International Securities Identifying Number) Numbers are 12 character alphanumeric codes used to identify Securities.
    http://en.wikipedia.org/wiki/ISIN

5) SEDOL
========
SEDOL Numbers are 7 character alphanumeric codes used to identify UK Securities (SEDOL stands for Stock Exchange Daily Official List).
    http://en.wikipedia.org/wiki/SEDOL


","Banking CheckDigit implementations: ABA, CUSIP, IBAN, ISIN and Sedol",,,,niallp,True,niallp,niallp
commons-validator,VALIDATOR-244,2007-11-01T01:50:32.000+0000,,2007-11-12T18:00:18.000+0000,,,New Feature,Major,['2.0'],,,,,,,,['Framework'],['Validator Framework'],"A new feature in Struts 1.4 is the ability to post-process the configuration before it is set. I believe the Validator Framework should allow the same. As a benefit, a developer could code substitution variables -- ${variable-name} -- and then read them in from a property file or database. This would allow easy aligning with data constraints that are already in place but in a remote resource.",Validator post-processing,,,,pbenedict,True,,pbenedict
commons-validator,VALIDATOR-241,2007-10-18T18:18:30.000+0000,2007-11-01T04:47:11.000+0000,2007-11-12T19:25:31.000+0000,,Fixed,New Feature,Major,['1.4.0 Release'],,,,,,,,['Routines'],['Validation Routines'],"Create a new InetAdress validator implementation. The existing email and url validators duplicate this functionality. Refactoring it into a separate validator means that it can be re-used by both those implementations and also provides another useful validator implementation to the users.

See http://svn.apache.org/viewvc?view=rev&revision=585762",New InetAdress Validator implementation,,,,niallp,True,bens,niallp
commons-validator,VALIDATOR-237,2007-09-20T19:00:08.000+0000,,2007-09-27T05:36:17.000+0000,,,New Feature,Minor,,['1.4.0 Release'],,,,,,,['Framework'],['Validator Framework'],"Apache Commons validator can be ABSTRACTED to a common framework called ACTIONS framework.
 
The Actions framework will act in similar manner as validation framework but will be independent of the way in which actions are execute on the value.
 
Abstraction points.
1.Validation can be treated as any action
2.The value to be validated is treated as source value
3.The return value true/false can be treated as a target value
4.Source Value ---pass to--- > list of actions to be executed ----results in---> Target Value
 
We have already done this in our enterprise product, we use apache validation framework even for transforming values in our custom ETL framework.
 
Once the apache validator is abstracted as actions framework one can use the same to not only validate the source value but to transform the value or even in describing and modifying styles of controls in UI and much more. The advantage here is we use the same digester , loading of elements, extending forms and stuff, this code remains common.",Abstract Validation framework to Actions framework.,1,,,bhupeshnaik,True,,bhupeshnaik
commons-validator,VALIDATOR-232,2007-07-08T04:40:06.000+0000,2007-10-31T05:01:06.000+0000,2007-11-12T19:25:30.000+0000,,Fixed,New Feature,Minor,['1.4.0 Release'],,,,,,,,['Framework'],['Validator Framework'],"Add a script=""true|false"" attribute to <field> to control whether JavaScript should be generated.

Also see: https://issues.apache.org/struts/browse/STR-1888





",Add script attribute to control script generation,,,,pbenedict,True,bens,pbenedict
commons-validator,VALIDATOR-230,2007-05-28T20:39:45.000+0000,2015-11-05T22:45:04.000+0000,2015-11-05T22:45:04.000+0000,,Won't Fix,New Feature,Major,['2.0'],,,,,,,,,,"An ""enum"" validator which validates a value is member of an Commons Enum type or JDK5 type. Attachment forthcoming.",Enum validator,,,,pbenedict,True,,pbenedict
commons-validator,VALIDATOR-219,2006-12-23T02:57:04.000+0000,2015-01-07T00:01:35.000+0000,2015-01-07T00:07:19.000+0000,,Won't Fix,New Feature,Minor,,,,,,,,,['JavaScript'],['JavaScript Validation'],"Hi,

I would like to suggest to make an enhancement to the client side validation process so that the client validation error message is displayed the same way the client side validation messages are displayed. Personally I would prefer making this the default behavour and eliminating the JavaScript alert(). Another option might be to change the value passed through to the client attribute of the s:commonsValidator tag to multiple options like ""alert"" and ""message"".

When message would be chosen, the JavaScript will populate the <div> representing the h:message tag.

I think this will result in a more elegant notification to the enduser.

example:
--------------------------
<h:inputText id=""email"" value=""#{contact.email}"">
		<s:commonsValidator type=""required"" message=""#{labels['err.requiredField']}"" server=""true"" client=""message""/>
</h:inputText>
<h:message for=""email"" styleClass=""error""/>
--------------------------

We currently have a fairly elaborate system in place where empty JavaScript Array's are generated for each validator type. Whenever a value needs to be validated, the input object will be added to the specific validator Array. On submit each array will be checked for their correct values , messages are dysplayed acoringly and the request is stopped.

How do you think we should implement this feature, and what part of this process belongs in Shale, and what part belongs in the commons validator. I also posted this RFE in the shale issue tracker (SHALE-373).

Thank you,
Joost ",New display method for client validation error,1,,,joostschouten,True,,joostschouten
commons-vfs,VFS-720,2019-06-27T15:26:56.000+0000,2019-06-27T15:27:22.000+0000,2019-06-27T15:27:22.000+0000,,Fixed,New Feature,Major,['2.4'],,,,,,,,,,Implement {{Closeable}} for {{RandomAccessContent}} #66,Implement Closeable for RandomAccessContent #66,1,,,ggregory,True,ggregory,ggregory
commons-vfs,VFS-719,2019-06-27T15:20:43.000+0000,2019-06-27T15:27:07.000+0000,2019-06-27T15:27:07.000+0000,,Fixed,New Feature,Major,['2.4'],,,,,,,,,,"Add methods to get the contents of file objects as strings as I keep on coding the same patterns in different projects:
- org.apache.commons.vfs2.FileContent.getString(Charset)
- org.apache.commons.vfs2.FileContent.getString(String)
- org.apache.commons.vfs2.util.FileObjectUtils.getContentAsString(FileObject, Charset)
- org.apache.commons.vfs2.util.FileObjectUtils.getContentAsString(FileObject, String)
",Add methods to get the contents of file objects as strings,1,,,ggregory,True,ggregory,ggregory
commons-vfs,VFS-716,2019-05-16T17:13:24.000+0000,2019-05-16T17:15:09.000+0000,2019-05-16T17:15:09.000+0000,,Fixed,New Feature,Major,['2.4'],,,,,,,,,,Fix AbstractFileName.getURI returning unencoded #-sign #64,Fix AbstractFileName.getURI returning unencoded #-sign #64,1,,,ggregory,True,ggregory,ggregory
commons-vfs,VFS-715,2019-05-10T15:18:45.000+0000,2019-05-10T15:54:39.000+0000,2019-05-10T15:54:39.000+0000,,Fixed,New Feature,Major,['2.4'],,,,,,,,,,"Add {{org.apache.commons.vfs2.FileContent.getByteArray()}}.

This is a refactoring of {{org.apache.commons.vfs2.FileUtil.getContent(FileObject)}} to a new default method {{FileContent.getByteArray()}}.",Add org.apache.commons.vfs2.FileContent.getByteArray(),1,,,ggregory,True,ggregory,ggregory
commons-vfs,VFS-713,2019-05-07T17:25:05.000+0000,2019-05-07T17:29:24.000+0000,2019-05-07T17:29:24.000+0000,,Fixed,New Feature,Major,['2.4'],,,,,,,,,,"Add:

- org.apache.commons.vfs2.util.FileObjectUtils.readProperties(FileObject)
- org.apache.commons.vfs2.util.FileObjectUtils.readProperties(FileObject, Properties)

As:
{code:java}
    /**
     * Reads the given file into a new {@link Properties}.
     *
     * @param fileObject the file to read
     * @return a new {@link Properties}.
     * @throws IOException
     * @throws FileSystemException On error getting this file's content.
     * @throws IOException On error getting this file's content.
     * @since 2.4
     */
    public static Properties readProperties(final FileObject fileObject) throws FileSystemException, IOException {
        return readProperties(fileObject, new Properties());
    }

    /**
     * Reads the given file into a new given {@link Properties}.
     *
     * @param fileObject the file to read
     * @param properties the destination
     * @return a new {@link Properties}.
     * @throws FileSystemException On error getting this file's content.
     * @throws IOException On error getting this file's content.
     * @since 2.4
     */
    public static Properties readProperties(final FileObject fileObject, final Properties properties)
            throws FileSystemException, IOException {
        if (fileObject == null) {
            return properties;
        }
        try (InputStream inputStream = fileObject.getContent().getInputStream()) {
            properties.load(inputStream);
        }
        return properties;
    }
{code}

",Add FileObjectUtils.readProperties(FileObject) method to read a .properties file.,1,,,ggregory,True,ggregory,ggregory
commons-vfs,VFS-712,2019-05-07T16:56:29.000+0000,2019-05-07T17:22:37.000+0000,2019-05-07T17:22:37.000+0000,,Fixed,New Feature,Major,['2.4'],,,,,,,,,,"Add null-safe \{{org.apache.commons.vfs2.util.FileObjectUtils.exists(FileObject)}}:

{code:java}
    /**
     * Null-safe call to {@link FileObject#exists()}.
     * 
     * @param fileObject
     * @return false if {@code fileObject} is null, otherwise, see {@link FileObject#exists()}.
     * @throws FileSystemException On error determining if this file exists.
     */
    public static boolean exists(final FileObject fileObject) throws FileSystemException {
        return fileObject != null && fileObject.exists();
    }
{code}",Add null-safe org.apache.commons.vfs2.util.FileObjectUtils.exists(FileObject),1,,,ggregory,True,ggregory,ggregory
commons-vfs,VFS-706,2019-04-25T21:24:10.000+0000,2019-04-30T12:57:51.000+0000,2019-04-30T12:59:04.000+0000,,Fixed,New Feature,Major,['2.4'],,,,,,,,,,"Add ability to specify buffer sizes #59.

""FileContent.getInputStream""'s documentation specifies that ""The input stream is buffered"" which is true (because the implementation in ""DefaultFileContent"" wraps it in ""FileContentInputStream"" which inherits ""MonitorInputStream"" which in turn ""BufferedInputStream"". However, there is no way to configure the buffer size and sometimes the default one which is just 8 KB might not be a good choice. Adding an ability to specify a different buffer size will help greatly boost the performance when talking to slow providers.",Add ability to specify buffer sizes #59,1,,,ggregory,True,ggregory,ggregory
commons-vfs,VFS-699,2019-04-11T15:04:32.000+0000,2019-04-13T23:10:58.000+0000,2019-04-13T23:10:58.000+0000,,Fixed,New Feature,Major,['2.4'],,,,,,,,,,Add setting for FTP encoding autodetection #58.,Add setting for FTP encoding autodetection #58,1,,,ggregory,True,,ggregory
commons-vfs,VFS-696,2019-03-21T21:20:28.000+0000,2019-03-21T21:33:39.000+0000,2019-03-21T21:33:40.000+0000,,Fixed,New Feature,Major,['2.4'],,,,,,,,,,SFTP HTTP and SOCKS proxy authentication #49,SFTP HTTP and SOCKS proxy authentication #49,1,,,ggregory,True,ggregory,ggregory
commons-vfs,VFS-690,2019-02-04T22:19:44.000+0000,2019-02-04T22:21:26.000+0000,2019-02-04T22:21:26.000+0000,,Fixed,New Feature,Major,['2.4'],,,,,,,,,,Allow to set key exchange algorithm explicitly. GitHub #32.,Allow to set key exchange algorithm explicitly. GitHub #32.,1,,,ggregory,True,ggregory,ggregory
commons-vfs,VFS-687,2019-01-02T03:04:52.000+0000,,2019-01-03T10:28:40.000+0000,,,New Feature,Major,,,,,,,,,,,"As of VFS-360, we have two different http client based file system providers: ""http3(s)"" vs. ""http4(s)"".
Since HttpClient 5 is coming out in the near future (HttpClient 5 Beta 2 is already available), it would be nice if we have ""http5(s)"" file system provider as well.",HttpClient 5 based FileSystemProvider,3,,,woon_san,True,,woon_san
commons-vfs,VFS-661,2018-04-17T08:01:21.000+0000,,2018-04-19T15:32:06.000+0000,,,New Feature,Major,,['2.2'],,,,,,,,,"On case-insensitive file systems (local FS on Windows; Samba; etc) resolving a file ignores the case that is used. For example, if there is a folder like: *smb://localhost/share/folder* and is resolved with *smb://localhost/share/FOLDER* it would work and return the same folder. However, there is no method in the *FileObject* interface that allows us to get the ""real""/""physical"" name of the folder - i.e. *folder*. All of the methods would return *FOLDER* in this case.

We have a major usecase where we need that. The only solution I can think of is getting the parent folder, going through its children and thus finding the correct case but the performance of that would be horrible.","Ability to get ""real""/""native""/""physical"" file name",3,,,boris-petrov,True,,boris-petrov
commons-vfs,VFS-654,2018-02-27T16:25:53.000+0000,,2018-02-28T12:38:18.000+0000,,,New Feature,Major,,['2.2'],,,,,,,,,Currently the DefaultFileMonitor walks the whole file system and notifies when something changes. For large file systems and (near) real-time requirements this is _extremely_ inefficient. Some file systems have an events API which could be exposed (like *inotify*). Has any work towards that ever been done?,File system events API,3,,,boris-petrov,True,,boris-petrov
commons-vfs,VFS-653,2018-02-18T20:23:45.000+0000,2018-02-18T20:25:33.000+0000,2018-11-07T17:57:08.000+0000,,Fixed,New Feature,Major,['1.0'],['2.2'],,,,,,,,,Replace use of deprecated APIs in HDFS provider.,Replace use of deprecated APIs in HDFS provider,1,,,ggregory,True,ggregory,ggregory
commons-vfs,VFS-649,2017-11-22T15:52:54.000+0000,,2018-02-04T20:19:15.000+0000,,,New Feature,Minor,,['2.2'],,,,,,,,,"Hi,

based on this issue: [https://issues.apache.org/jira/browse/VFS-635] I created an implementation of [https://github.com/hierynomus/smbj] into vfs 2.2

I named the provider SMB2, so the scheme I used is also smb2. I dont know if it's better to rename the existing smb to cifs and use smb for the smb v2+ implementation.

All the tests from the ProviderTests pass but I had to remove the test (exclude in pom.xml) because the test needs an external smb share.

Thanks for any feedback or further advice how to proceed with this pr,
André",SMB v. 2 / 3 integration based on SMBJ,6,4,['SMBv2'],bonsu,True,,bonsu
commons-vfs,VFS-638,2017-07-07T07:04:41.000+0000,,2017-07-07T16:30:26.000+0000,,,New Feature,Minor,,['2.1'],,,,,,,,,"It would be great to get new fail-fast hasChildren() method to dertmine if a FileObject really has Children or not. You often only need to know if a FileObject has minimum one child but don't need to know how many.
I've tried to simulate this in different ways:
# fo.getChildren().size() > 0 returns the whole list first and therefore takes some time to get the basic information. Too long for only this information
# fo.iterator().next() != fo; (iterator returns the original fo as last child again so you need to check if there are other file objects in folder) doesn't work either, because the iterator also collects all children before, many also these from the subfolders.
# fo.getType().hasChildren() is nothing more than a static flag in an enumeration of types with no real life information

Many thanks in advance
Guido",FileObject.hasChildren(),2,,,gschnepp,True,,gschnepp
commons-vfs,VFS-621,2016-08-09T23:55:42.000+0000,2016-08-10T00:04:36.000+0000,2016-08-11T17:01:26.000+0000,,Fixed,New Feature,Major,['2.2'],,,,,,,,,,"Add API {{VFS.setManager(FileSystemManager)}}.

Initialize VFS with the right class loader for OSGi types of scenarios to work. Must be called before any VFS APIs are called.

Instead of:

{code:java}
        final StandardFileSystemManager fsm = new StandardFileSystemManager();
        final ClassLoader classLoader = fsm.getClass().getClassLoader();
        fsm.setClassLoader(classLoader);
        try {
            FieldUtils.writeDeclaredStaticField(VFS.class, ""instance"", fsm, true);
            fsm.init();
        } catch (final FileSystemException | IllegalAccessException e) {
            ...
        }
{code}

do:

{code:java}
        final StandardFileSystemManager fsm = new StandardFileSystemManager();
        final ClassLoader classLoader = fsm.getClass().getClassLoader();
        fsm.setClassLoader(classLoader);
        VFS.setManager(fsm);
        fsm.init();
{code}",Add API VFS.setManager(FileSystemManager),2,,,ggregory,True,ggregory,ggregory
commons-vfs,VFS-570,2015-05-21T20:17:38.000+0000,,2019-05-01T20:45:09.000+0000,,,New Feature,Major,,,600,600,,,,100,,,"Currently, the HDFS Provider only supports reading of files. Write ability should be added to facilitate broader uses of VFS with respect to HDFS. ",Write Ability for HDFS Provider,5,,,a.lurie,True,,a.lurie
commons-vfs,VFS-500,2013-11-08T00:38:30.000+0000,,2019-01-28T13:11:50.000+0000,,,New Feature,Minor,['2.4'],['2.0'],,,,,,,,,"the VFSClassLoader.findResources(String) method is a dummy implementation returning an empty Enumeration.

I have a working implementation and will support the patch for it, this is the JIRA to track it.",VFSClassLoader.findResources missing,4,,,b.eckenfels,True,b.eckenfels,b.eckenfels
commons-vfs,VFS-445,2012-11-15T03:08:01.000+0000,2012-11-15T03:11:23.000+0000,2016-05-19T18:25:38.000+0000,,Fixed,New Feature,Major,['2.1'],,,,,,,,,,Add FileSystemManager.resolveFile(URI) and resolveFile(URL).,Add FileSystemManager.resolveFile(URI) and resolveFile(URL),2,,,ggregory,True,,ggregory
commons-vfs,VFS-442,2012-10-29T17:54:31.000+0000,2013-01-09T04:08:51.000+0000,2016-05-19T18:25:57.000+0000,,Fixed,New Feature,Minor,['2.1'],['2.0'],,,,,,,,,,Add an HDFS FileSystem Provider,5,,"['accumulo', 'hdfs']",dlmarion,True,ggregory,dlmarion
commons-vfs,VFS-438,2012-09-19T15:38:18.000+0000,,2017-06-07T07:17:45.000+0000,,,New Feature,Major,,['2.0'],,,,,,,,,"Current smb provider in the sandbox is mature enough to proceed with promotion out of sandbox for general use. 

Detail discussion is here http://apache-commons.680414.n4.nabble.com/Promote-vfs-cift-out-of-sandbox-td4636519.html

The sandbox also tests hookup to current test suite, all we need is to provide a external URI using VFS test suite instructions.

There are currently 2 classloader specific failures

Tests in error:
  testLoadClass(org.apache.commons.vfs2.impl.test.VfsClassLoaderTests):
code.ClassToLoad
  testSealing(org.apache.commons.vfs2.impl.test.VfsClassLoaderTests):
code.sealed.AnotherClass

Tests run: 1745, Failures: 0, Errors: 2, Skipped: 0

require cifs  1.3.17 with one line change at 

SmbFileObject.java line number 227 to 

if (e.getNtStatus() == SmbException.NT_STATUS_NO_SUCH_FILE)


",Please promote vfs-smb currently in the sandbox,8,4,,danttran,True,,danttran
commons-vfs,VFS-425,2012-06-27T14:59:29.000+0000,2012-06-27T15:00:38.000+0000,2016-05-19T18:26:21.000+0000,,Fixed,New Feature,Major,['2.1'],,,,,,,,,,Add API FileObject.isExecutable(),Add API FileObject.isExecutable(),2,,,ggregory,True,ggregory,ggregory
commons-vfs,VFS-417,2012-05-17T20:00:14.000+0000,2012-05-17T20:07:26.000+0000,2016-05-19T18:26:28.000+0000,,Fixed,New Feature,Major,['2.1'],,,,,,,,,,"Add and implement new API: RandomAccessContent.setLength(long).

This is easily implemented for the [RAM] and [Local] providers.",[RAM][Local] Add and implement new API: RandomAccessContent.setLength(long),2,,,ggregory,True,ggregory,ggregory
commons-vfs,VFS-414,2012-05-12T13:19:13.000+0000,2012-05-12T20:07:47.000+0000,2016-05-19T18:25:37.000+0000,,Fixed,New Feature,Major,['2.1'],,,,,,,,,,"[FTP] Add config API to set the file type:
{code}
FtpFileSystemConfigBuilder.getInstance().setFileType(opts, FtpFileType.ASCII);
FtpFileSystemConfigBuilder.getInstance().setFileType(opts, FtpFileType.BINARY);
FtpFileSystemConfigBuilder.getInstance().setFileType(opts, FtpFileType.EBCDIC);
FtpFileSystemConfigBuilder.getInstance().setFileType(opts, FtpFileType.LOCAL);
{code}",[FTP] Add config API to set the file type,2,,,ggregory,True,ggregory,ggregory
commons-vfs,VFS-400,2012-02-06T09:32:55.000+0000,2012-05-16T16:05:10.000+0000,2016-05-19T18:26:05.000+0000,,Fixed,New Feature,Minor,['2.1'],['2.1'],,,,,,,,,"In the long todo list there was a post about adding a file selector based on regular expressions. I had need for that for a specific project so I built a simple class that seems to work. I'm kind of new to open source contribution though so I'm not sure if i should just commit it to trunk. Here is the code:
{code:title=FileRegexSelector.java|borderStyle=solid}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the ""License""); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.commons.vfs2;

import java.util.regex.Matcher;
import java.util.regex.Pattern;

/**
 * A {@link FileSelector} that selects based on regular expressions matched against base filename.
 * 
 * @since 2.1
 */
public class FileRegexSelector implements FileSelector
{

    /**
     * The extensions to select.
     */
    private Pattern pattern = null;

    /**
     * Creates a new selector for the given extensions.
     * 
     * @param extensions
     *            The extensions to be included by this selector.
     */
    public FileRegexSelector(String regex)
    {
    	this.pattern = Pattern.compile(regex);
    }

    /**
     * Determines if a file or folder should be selected.
     * @param fileInfo
     *            The file selection information.
     * @return true if the file should be selected, false otherwise.
     */
    public boolean includeFile(final FileSelectInfo fileInfo)
    {
        if (this.pattern == null)
        {
            return false;
        }
    	Matcher matcher = this.pattern.matcher(fileInfo.getFile().getName().getBaseName());
        return matcher.matches();
    }

    /**
     * Determines whether a folder should be traversed.
     * 
     * @param fileInfo
     *            The file selection information.
     * @return true if descendents should be traversed, fase otherwise.
     */
    public boolean traverseDescendents(final FileSelectInfo fileInfo)
    {
        return true;
    }
}
{code}",Add a FileSelector based on regular expressions,2,,,rikard.oxenstrand,True,,rikard.oxenstrand
commons-vfs,VFS-397,2011-12-30T15:33:49.000+0000,,2017-06-03T14:45:02.000+0000,,,New Feature,Major,,,,,,,,,,,"Hello,

Could you please add support for listing CIFS Directory content.

Thanks in advance!",Add support for CIFS Directory content,3,2,"['cifs', 'commons-vfs', 'content', 'directory', 'listing']",stounfree,True,,stounfree
commons-vfs,VFS-389,2011-11-18T15:22:41.000+0000,2011-11-18T15:27:57.000+0000,2016-05-19T18:26:15.000+0000,,Fixed,New Feature,Major,['2.1'],['2.0'],,,,,,,,,"h3. Replace

{code:java}
org.apache.commons.vfs2.FileSystemException.FileSystemException(String, Object[])
{code}

with:

{code:java}
org.apache.commons.vfs2.FileSystemException.FileSystemException(String, Object...)
{code}

This is a binary compatible change.

h3. Add

{code:java}
org.apache.commons.vfs2.FileSystemException.FileSystemException(String, Throwable, Object...)
{code}

and deprecate:

{code:java}
org.apache.commons.vfs2.FileSystemException.FileSystemException(String, Object[], Throwable)
{code}
",Use variable argument lists in FileSystemException instead of Object[]s,1,,,ggregory,True,ggregory,ggregory
commons-vfs,VFS-381,2011-11-06T18:04:32.000+0000,2011-11-06T18:18:02.000+0000,2016-05-19T18:26:01.000+0000,,Fixed,New Feature,Major,['2.1'],,,,,,,,,,"Iterate over a FileObject using the Java ""foreach"" statement, to provide all descendents of a FileObject.","Iterate over a FileObject using the Java ""foreach"" statement, to provide all descendents of a FileObject.",1,,,ggregory,True,ggregory,ggregory
commons-vfs,VFS-373,2011-10-31T17:47:00.000+0000,2011-10-31T18:35:56.000+0000,2016-05-19T18:25:38.000+0000,,Fixed,New Feature,Major,['2.1'],"['1.0', '2.0']",,,,,,,,,"Add APIs to write FileContent objects to an OutputStream, FileContent, and FileObject.

In 1.0, 2.0, all writing must be done manually via FileContent#getOutputStream() or FileUtil. 

Make writing a first class citizen in the API. 

Make FileUtil reuse the new FileContent API.

{code:java}
write(FileContent)
write(FileObject)
write(OutputStream)
write(OutputStream, int)
{code}

In more detail:

{code:java}
    /**
     * Writes this content to another FileContent.
     * 
     * @param output
     *            The target OutputStream.
     * @throws IOException
     *             if an error occurs writing the content.
     * @since 2.1             
     */
    long write(FileContent output) throws IOException;
    
    /**
     * Writes this content to another FileObject.
     * 
     * @param file
     *            The target FileObject.
     * @throws IOException
     *             if an error occurs writing the content.
     * @since 2.1             
     */
    long write(FileObject file) throws IOException;
    
    /**
     * Writes this content to an OutputStream.
     * 
     * @param output
     *            The target OutputStream.
     * @throws IOException
     *             if an error occurs writing the content.
     * @since 2.1             
     */
    long write(OutputStream output) throws IOException;
    
    /**
     * Writes this content to an OutputStream.
     * 
     * @param output
     *            The target OutputStream.
     * @param bufferSize
     *            The buffer size to write data chunks.
     * @throws IOException
     *             if an error occurs writing the file.
     * @since 2.1             
     */
    long write(OutputStream output, int bufferSize) throws IOException;
{code}",Add FileContent write APIs,1,,,ggregory,True,ggregory,ggregory
commons-vfs,VFS-371,2011-10-27T06:34:15.000+0000,2011-11-06T18:58:49.000+0000,2016-05-19T18:25:54.000+0000,,Fixed,New Feature,Major,['2.1'],,,,,,,,,,"I see a lot of call sites within VFS 2 and some in my work server that do:

{code:java}
fileObject.delete(Selectors.SELECT_ALL);
{code}

It therefore seems like a sensible refactoring.

Add to {{FileObject}}:

{code:java}
    /**
     * Deletes all descendents of this file.  
     * Does nothing if this file does not exist.
     * Shorthand for {@code delete(Selectors.Selectors.SELECT_ALL)}
     * <p/>
     * <p>This method is not transactional.  If it fails and throws an
     * exception, this file will potentially only be partially deleted.
     *
     * @return the number of deleted objects
     * @throws FileSystemException If this file or one of its descendents is read-only, or on error
     *                             deleting this file or one of its descendents.
     */
    int deleteAll() throws FileSystemException;
{code}

And to {{AbstractFileObject}}:

{code:java}
    /**
     * Deletes this file, and all children.
     *
     * @return the number of deleted files.
     * @throws FileSystemException if an error occurs.
     */
    public int deleteAllDescendents() throws FileSystemException
    {
        return this.delete(Selectors.SELECT_ALL);
    }
{code}


Thoughts?

Attaching patch.
",Add FileObject API deleteAll(),1,,,ggregory,True,ggregory,ggregory
commons-vfs,VFS-370,2011-10-27T06:08:53.000+0000,2011-10-27T06:19:13.000+0000,2016-05-19T18:26:17.000+0000,,Fixed,New Feature,Major,['2.1'],,,,,,,,,,"Add a {{FileExtensionSelector}} class, a {{FileSelector}} that selects based on file extensions.",Add a FileExtensionSelector class,1,,,ggregory,True,ggregory,ggregory
commons-vfs,VFS-344,2010-11-23T12:21:05.000+0000,2011-09-24T07:49:30.000+0000,2011-09-24T07:49:30.000+0000,,Won't Fix,New Feature,Major,,['2.0'],,,,,,,,,"Allow Clirr to be used to compare VFS2 against VFS

POM (to follow) uses Shade to convert VFS2 to VFS to allow use of Clirr.",Allow Clirr to be used to compare VFS2 against VFS,,,,sebb,True,,sebb
commons-vfs,VFS-312,2010-07-12T19:47:28.000+0000,,2010-07-12T21:06:40.000+0000,,,New Feature,Minor,,,,,10800,10800,10800,,,,"Hello,

I ran into a problem while copying a file with a colon ("":"") in the file name from a Unix SFTP FileSystem to a window LocalFileSystem.  Windows does not permit the colon as part of the file name, and it causes the output stream to fail.  Is there currently a way to fix this?

What I would like to propose is to add a org.apache.commons.vfs.FileObjectResolver interface, which would provide methods to resolve an input to a (FileSystem) FileObject instance.  Then, add the setFileObjectResolver() and removeFileObjectResolver() methods to the FileSystem interface to accept this optional resolver.  When attempting to resolve the FileObject, the FileSystem would check to see if an optional FileObjectResolver was set, and if so, use that.  A custom FileObjectResolver would obviously add additional functionality than simply file name conversions, but that's the use case we can start with =).

I built a similar model that actually extends the FileSystemManager (instead of FileSystem) and adds the FileObjectResolver to this class for simplicity (I REALLY don't want to have to make an extension for each provider =) and it's working great (except, of course, I have to use the FSM to resolve correctly rather than directly the FS - a bug I'm hoping to fix by adding this to the core.

I've downloaded the core from the community SVN and have built a patch I can contribute, if there is interest.  I would be quite happy to contribute the patch.

Thanks for taking a look!

Steve",Add FileObjectResolver to provide custom FileObject resolution (FileSystem.resolveFile(...)),,,,smsiebe,True,,smsiebe
commons-vfs,VFS-290,2009-12-09T18:46:48.000+0000,,2016-07-27T21:04:32.000+0000,,,New Feature,Minor,['2.0'],['1.0'],,,,,,,,,We have a need to supply our own implementation of HttpClient in order to track the network traffic.   To do this was a very small change; I have a patch and I intended to attach it to this bug report.  But right now I can't see anywhere to do so.  Perhaps that happens after I submit it initially.,Allow pluggable HttpClient implementation for providers using http,,,,sporritt,True,,sporritt
commons-vfs,VFS-264,2009-06-11T16:48:09.000+0000,2010-09-07T21:13:28.000+0000,2010-09-07T21:14:18.000+0000,,Fixed,New Feature,Minor,['2.0'],['2.0'],,,,,,,,,The current version of VFS doesn't support FTPS as a provider,FTPS support for VFS,4,2,,scottbjer,True,jwcarman,scottbjer
commons-vfs,VFS-232,2008-12-18T07:48:37.000+0000,,2016-07-27T21:04:51.000+0000,,,New Feature,Minor,['2.0'],['1.0'],,,,,,,,,"Add gmailfs to the list of future supported filesystems and prioritize / set target date / version.

ref: Linux gmailfs
http://richard.jones.name/google-hacks/gmail-filesystem/gmail-filesystem.html
",Addition of gmail filesystem gmailfs,,,,hanasaki,True,,hanasaki
commons-vfs,VFS-199,2008-02-26T01:20:22.000+0000,,2016-07-27T21:04:21.000+0000,,,New Feature,Major,,['1.0'],,,,,,,,,"I think this was lodged at some time ago as VFS-79: http://www.mail-archive.com/commons-dev@jakarta.apache.org/msg83701.html.

Essentially the ability to browse http via the directory listing structure is reasonably important for a number of use cases. 

Mario noted in that previous bug that the parsing of the html isn't necessarily trivial but I've submitted a patch that meets my requirements. The patch uses java regular expressions which has the requirement of JVM > 1.4. Submitting in the hope that other users will find it useful and maybe we'll be able to tidy it to the point where it can be integrated into the vfs mainline.",Http/Https listChildren browse support,5,4,,arbfranklin,True,,arbfranklin
commons-vfs,VFS-180,2007-11-13T17:34:24.000+0000,,2019-05-01T20:45:08.000+0000,,,New Feature,Major,,"['1.0', '2.0']",600,600,,,,100,,,"The Slide Webdav lib supports encrypted HTTPS connections. So it should be possible to add https support to vfs too. currently the webdav provider creates http urls (in WebdavClientFactory.java).

maybe some provider like 'webdavs' could be added to switch to HttpsUrl.


regards

werner



",Support HTTPS / SSL for Webdav,9,3,['patch'],werner.mueller@mimacom.ch,True,,werner.mueller@mimacom.ch
commons-vfs,VFS-170,2007-07-13T01:56:52.000+0000,,2007-07-13T01:58:59.000+0000,,,New Feature,Minor,,['1.0'],,,,,,,,,"See file attached.

This will allow to use one-liners like this:

import org.apache.commons.io.IOCase;
import org.apache.commons.io.filefilter.WildcardFileFilter;

...

FileObject[] configFileObjects = dirObject.findFiles(new FilenameFilterFileSelector(new WildcardFileFilter(""*config*.xml"", IOCase.SYSTEM)))

To name a few FilenameFilters from Commons IO that can be used here:
AndFileFilter
DelegateFileFilter
NameFileFilter
NotFileFilter
OrFileFilter
PrefixFileFilter
SuffixFileFilter
WildcardFileFilter
FalseFileFilter
TrueFileFilter

Very useful. :-)",FileSelector that delegates all work to java.io.FilenameFilter,1,,,bas,True,,bas
commons-vfs,VFS-168,2007-07-11T00:19:56.000+0000,,2007-07-11T00:24:49.000+0000,,,New Feature,Major,,,,,,,,,,,"It would be nice to have Spring FactoryBean that configures FileSystemOptions object and provides it as a bean reference.

I have created simple FactoryBean to do this kind of stuff (FileSystemOptionsFactoryBean.java attached).

For example, configuration would be like this:
<bean class=""FileSystemOptionsFactoryBean"">
    <property name=""fileSystemConfig"">
      <map>
        <entry key=""org.apache.commons.vfs.impl.DefaultFileSystemConfigBuilder"">
          <map>
            <entry key=""userAuthenticator"">
              <bean class=""org.apache.commons.vfs.auth.StaticUserAuthenticator"">
                <constructor-arg index=""0""><null/></constructor-arg>
                <constructor-arg index=""1"" value=""username""/>
                <constructor-arg index=""2"" value=""password""/>
              </bean>
            </entry>
          </map>
        </entry>
        <entry key=""org.apache.commons.vfs.provider.sftp.SftpFileSystemConfigBuilder"">
          <map>
            <entry key=""proxyType"">
              <util:constant static-field=""org.apache.commons.vfs.provider.sftp.SftpFileSystemConfigBuilder.PROXY_HTTP""/>
            </entry>
            <entry key=""proxyHost"" value=""proxyhost""/>
            <entry key=""proxyPort"" value=""8080""/>
            <entry key=""userInfo"">
              <bean class=""org.apache.commons.vfs.provider.sftp.TrustEveryoneUserInfo""/>
            </entry>
            <entry key=""strictHostKeyChecking"" value=""no""/>
            <entry key=""userDirIsRoot"" value=""true""/>
            <entry key=""timeout"" value=""3000""/>
          </map>
        </entry>
      </map>
    </property>
  </bean>

All properties are optional with ""smart"" defaults.

I'm planing to make it smart enough to accept just map of properties (FileSystem options) without reference to FileSystemConfigBuilder objects. It'll introspect all available FileSystemConfigBuilder implementations and apply those options using proper FileSystemConfigBuilder implementation.",FileSystemOptionsFactoryBean implementation to configure FileSystemOptions object in the Spring context,1,,,bas,True,,bas
commons-vfs,VFS-154,2007-05-14T15:23:29.000+0000,2007-05-14T19:31:16.000+0000,2016-07-27T21:04:35.000+0000,,Fixed,New Feature,Minor,['2.0'],['1.0'],,,,,,,,,"Add CombinedResources, which fetches all Resources.properties.",Find all Resources.properties and merge them.,,,,doogie,True,,doogie
commons-vfs,VFS-149,2007-05-14T14:37:11.000+0000,2007-05-14T18:37:42.000+0000,2016-07-27T21:04:37.000+0000,,Fixed,New Feature,Minor,['2.0'],['1.0'],,,,,,,,,See $summary and patch.  Makes FileContent more like a map.,Add FileContent.attributeExists and removeAttribute.,,,,doogie,True,,doogie
commons-vfs,VFS-148,2007-05-14T14:32:37.000+0000,,2016-07-27T21:04:37.000+0000,,,New Feature,Trivial,,['1.0'],,,,,,,,,"See $summary, and patch.",Make DefaultFileSelectInfo public,,,,doogie,True,,doogie
commons-vfs,VFS-146,2007-05-14T14:25:12.000+0000,2007-05-14T19:37:26.000+0000,2016-07-27T21:04:39.000+0000,,Fixed,New Feature,Trivial,['2.0'],['1.0'],,,,,,,,,"See $summary and patch; this allows utility code to find the real underlying file, in the case we need the real file for processing(for code that requires java.io.File, etc).",Add DelegateFileObject.getDelegateFile(),,,,doogie,True,,doogie
commons-vfs,VFS-133,2007-05-14T04:51:48.000+0000,2007-05-14T05:51:31.000+0000,2016-07-27T21:04:55.000+0000,,Fixed,New Feature,Trivial,['2.0'],['later'],,,,,,,,,"As $summary, that returns ""r"", ""w"", ""rw"", or """", as appropriate.",Add RandomAccessMode.getModeString(),,,,doogie,True,,doogie
commons-vfs,VFS-132,2007-05-14T04:45:13.000+0000,2007-05-14T05:46:18.000+0000,2016-07-27T21:04:54.000+0000,,Fixed,New Feature,Major,['2.0'],['1.0'],,,,,,,,,"The set and list methods don't lowercase attribute names; why should get?
",Attributes are case-senstive,,,,doogie,True,,doogie
commons-vfs,VFS-106,2007-01-04T07:30:46.000+0000,2007-03-19T19:24:58.000+0000,2010-02-05T09:50:30.000+0000,,Incomplete,New Feature,Major,,,,,,,,,,,"Attached is a quicky truezip provider to allow for read/write access to zip/tar/... archives.
See https://truezip.dev.java.net/ for details on truezip.

Let me know how you want to proceed with something like this.",VFS  Truezip provider,2,,,filipdef,True,,filipdef
commons-vfs,VFS-103,2006-12-19T20:25:07.000+0000,2007-03-19T20:39:19.000+0000,2008-03-25T07:52:18.000+0000,,Won't Fix,New Feature,Minor,,,,,,,,,,,"Mac OS X, and Windows has special purpose directories used to organizes specific types of files.  In Mac OS X, these directories include ""Documents"", ""Movies"", ""Pictures"", ""Music""; Windows uses ""My Documents"", ""My Pictures"", ""My Videos"" and ""My Music"".

Provide a consistent approach for getting these directories either as a static factory method, or as individual methods. i.e. something along the lines of  ""public static FileObject getRoot(int rootType)"" where rootType is a constant that defines which one of the directorires (specified above) that you want.",Add support for Local File System reserved directories,,,,mfortner,True,,mfortner
commons-vfs,VFS-97,2006-10-25T11:32:32.000+0000,,2016-07-27T21:04:28.000+0000,,,New Feature,Major,['2.0'],,,,,,,,,,"FTP Object should return child by name, even if it's name of unlisted virtual directory.

IIS support creation FTP virtual folders. Such foldres is not listed in LS command, but can be accessed by name (CWD/LS) as well as files in it.",Microsoft FTP Virtual Folder support,1,2,,bsp,True,,bsp
commons-vfs,VFS-94,2006-10-06T12:07:17.000+0000,2006-10-21T07:13:06.000+0000,2006-10-21T07:13:06.000+0000,,Won't Fix,New Feature,Major,,,,,,,,,,,"The FISH protocol comes handy when you need to transfer files from a remote computer, but all you have at your disposal is SSH access.

See http://en.wikipedia.org/wiki/Files_transferrer_over_shell_protocol",Provide support for the FISH protocol,,,,elifarley@yahoo.com,True,,elifarley@yahoo.com
commons-vfs,VFS-92,2006-10-05T19:17:12.000+0000,2006-10-06T05:34:00.000+0000,2008-03-25T07:52:18.000+0000,,Won't Fix,New Feature,Major,,,,,,,,,,,"Integrating FUSE (http://fuse.sourceforge.net/) and Commons-VFS would ease the creation of new file system drivers for the GNU/linux and FreeBSD platforms.

Besides, it would be really cool to mount a partition whose handling is done in Java ;-)",FUSE integration,1,,,elifarley@yahoo.com,True,,elifarley@yahoo.com
commons-vfs,VFS-90,2006-10-05T13:23:28.000+0000,2007-03-19T20:30:07.000+0000,2016-07-27T21:04:40.000+0000,,Fixed,New Feature,Minor,['2.0'],,,,,,,,,,"Some existing libraries and applications rely on a RandomAccessFile instance to process its IO tasks.
They could be benefited by an adapter class providing a ""RandomAccessFile"" view of an arbitrary RandomAccessContent.

Example: a database server using a RandomAccessFile instance to access its data from a local file would automatically be able to access it from a remote resource accessed through HTTP.

I have already created such a class. I'll try to add it to this issue.",RandomAccessFile backed by a RandomAccessContent instance,1,,,elifarley@yahoo.com,True,imario,elifarley@yahoo.com
zookeeper,ZOOKEEPER-3454,2019-07-02T09:54:09.000+0000,,2019-07-04T09:24:46.000+0000,,,New Feature,Major,,,,,,,,,['java client'],['The java client interface for ZooKeeper'],"./etcdctl -w table --endpoints=127.0.0.1:2379,127.0.0.1:3379,127.0.0.1:4379 endpoint status
+----------------+------------------+---------+---------+-----------+-----------+------------+
| ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | RAFT TERM | RAFT INDEX |
+----------------+------------------+---------+---------+-----------+-----------+------------+
| 127.0.0.1:2379 | 2d6d41b3de34869d | 3.3.11 | 20 kB | true | 46 | 18 |
| 127.0.0.1:3379 | fa452fb497312bf1 | 3.3.11 | 20 kB | false | 46 | 18 |
| 127.0.0.1:4379 | a20a53b92b8e7e56 | 3.3.11 | 20 kB | false | 46 | 18 |
+----------------+------------------+---------+---------+-----------+-----------+------------+
[用name会报错]:Error: dial tcp: lookup etcd-1 on 10.9.255.1:53: no such host
./etcdctl -w table --endpoints=etcd-1:2379,etcd-2:3379,etcd-3:4379 endpoint status


./etcdctl endpoint health
./etcdctl --endpoints=127.0.0.1:2379,127.0.0.1:3379,127.0.0.1:4379 health
127.0.0.1:2379 is healthy: successfully committed proposal: took = 1.454841ms

./etcdctl member list
 3fcd54d1766f30b4: name=etcd-2 peerURLs=http://127.0.0.1:2381 clientURLs=http://127.0.0.1:3379,http://127.0.0.1:3379 isLeader=false
 48e24310bdb358ce: name=etcd-1 peerURLs=http://127.0.0.1:2380 clientURLs=http://127.0.0.1:2379,http://127.0.0.1:2379 isLeader=true
 66846ef509b1c4d7: name=etcd-3 peerURLs=http://127.0.0.1:2382 clientURLs=http://127.0.0.1:4379,http://127.0.0.1:4379 isLeader=false",add a new CLI: quorumInfo,1,,,maoling,True,maoling,maoling
zookeeper,ZOOKEEPER-3449,2019-07-01T13:53:30.000+0000,,2019-07-01T13:53:30.000+0000,,,New Feature,Major,,,,,,,,,['server'],['General issues with the ZooKeeper server.'],"ZooKeeper is often used for [service discovery|https://curator.apache.org/curator-x-discovery/index.html].  However, how are clients to discover the ZooKeeper quorum itself?

 Provide a mechanism for clients to automatically discover a ZooKeeper quorum.

[https://en.wikipedia.org/wiki/Zero-configuration_networking]",Zero Configuration Discovery of ZooKeeper Quorum,1,,,belugabehr,True,,belugabehr
zookeeper,ZOOKEEPER-3447,2019-06-28T11:03:39.000+0000,,2019-07-08T12:21:57.000+0000,,,New Feature,Major,['3.6.0'],,1200,1200,,,,100,['documentation'],"['Cross cutting documentation, including docs/site/wiki.']",,add a doc: zookeeperMonitor.md,1,,['pull-request-available'],maoling,True,maoling,maoling
zookeeper,ZOOKEEPER-3419,2019-06-06T22:31:48.000+0000,,2019-06-06T23:56:33.000+0000,,,New Feature,Major,,['3.6.0'],,,,,,,['server'],['General issues with the ZooKeeper server.'],"Historically ZooKeeper has no intrinsic support for backup and restore. The usual approach of doing backup and restore is through customized scripts to copy data around, or through some 3rd party tools (exhibitor, etc), which introduces operation burden. 

This Jira will introduce another option: a direct support of backup and restore from ZooKeeper itself. It's completely built into ZooKeeper, support point in time recovery of an entire tree rooted after an oops event, support recovery partial tree for test/dev purpose, and can help replay history for bug investigation. It will try to provide a generic interface so the backups can be directed to different data storage systems (S3, Kafka, HDFS, etc).

This same system has been in production at Twitter for X years and proved to be quite helpful for various use cases mentioned earlier. This will be a relative big patch, we'll try break the feature down and incrementally submit the patches when they are ready.",Backup and recovery support,3,,['Twitter'],hanm,True,hanm,hanm
zookeeper,ZOOKEEPER-3417,2019-06-06T02:02:05.000+0000,,2019-07-02T05:37:33.000+0000,,,New Feature,Major,,['3.6.0'],2400,2400,,,,100,['documentation'],"['Cross cutting documentation, including docs/site/wiki.']",,add the new doc:zookeeperProtocols to introduce the implementation details of ZAB comparing with raft,1,,['pull-request-available'],maoling,True,maoling,maoling
zookeeper,ZOOKEEPER-3377,2019-05-07T09:29:22.000+0000,2019-05-08T02:36:33.000+0000,2019-05-08T02:37:36.000+0000,,Invalid,New Feature,Minor,,['3.6.0'],,,,,,,['scripts'],[''],[zk: 127.0.0.1:2180(CONNECTED) 0] exit                                                                                     ~bin,Add a new CLI:exit,1,,,maoling,True,maoling,maoling
zookeeper,ZOOKEEPER-3371,2019-04-22T22:58:59.000+0000,,2019-07-12T23:30:36.000+0000,,,New Feature,Major,,['3.6.0'],15600,15600,,,,100,['security'],[''],"This issue provides the Jetty admin server with port unification, meaning both secure and insecure connections can be established on the same port. By default, this feature is disable. It can be enabled by passing ""zookeeper.admin.portUnification"" as a command line argument.",Port unification for admin server,2,,['pull-request-available'],ericlee123,True,,ericlee123
zookeeper,ZOOKEEPER-3368,2019-04-18T02:28:25.000+0000,2019-05-05T03:24:24.000+0000,2019-05-05T03:24:24.000+0000,,Duplicate,New Feature,Minor,,['3.6.0'],7800,7800,,,,100,,,"[zk: 127.0.0.1:2180(CONNECTED) 0] version
ZooKeeper version: 3.6.0-SNAPSHOT-29f9b2c1c0e832081f94d59a6b88709c5f1bb3ca, built on 04/16/2019 09:16 GMT",Add a new cli:version,1,,['pull-request-available'],maoling,True,maoling,maoling
zookeeper,ZOOKEEPER-3352,2019-04-08T20:58:34.000+0000,,2019-05-09T21:23:00.000+0000,,,New Feature,Critical,['4.0.0'],,,,,,,,['server'],['General issues with the ZooKeeper server.'],"Use LevelDB for managing data stored in ZK (transaction logs and snapshots).

https://stackoverflow.com/questions/6779669/does-leveldb-support-java
",Use LevelDB For Backend,5,,,belugabehr,True,belugabehr,belugabehr
zookeeper,ZOOKEEPER-3349,2019-04-04T22:55:42.000+0000,2019-05-20T23:07:15.000+0000,2019-05-20T23:07:24.000+0000,,Not A Problem,New Feature,Trivial,['3.6.0'],['3.6.0'],8400,8400,,,,100,['quorum'],['Quorum determination for ZooKeeper'],"QuorumCnxManager member variable 'socketTimeout' is not used anywhere in the class. It's clear from the context that it should either be removed entirely or invoked in QuorumCnxManager::setSockOpts. Since the QuorumPeer syncLimit can be changed by jmx, I'm thinking that the former is the better solution.

 ",QuorumCnxManager socketTimeout unused,1,,['pull-request-available'],nixon,True,nixon,nixon
zookeeper,ZOOKEEPER-3344,2019-03-30T08:52:04.000+0000,,2019-06-08T05:37:28.000+0000,,,New Feature,Major,,['3.6.0'],600,600,,,,100,['scripts'],[''],"write a new script:*zkSnapShotToolkit.sh* to encapsulate *SnapshotFormatter.java*
just like*: zkTxnLogToolkit.sh* for the users' convenience.",write a new script:zkSnapShotToolkit.sh to encapsulate SnapshotFormatter and doc the usage,1,,['pull-request-available'],maoling,True,maoling,maoling
zookeeper,ZOOKEEPER-3343,2019-03-30T08:26:12.000+0000,2019-05-03T15:08:57.000+0000,2019-05-03T21:46:02.000+0000,,Fixed,New Feature,Major,['3.6.0'],['3.5.4'],3000,3000,,,,100,['documentation'],"['Cross cutting documentation, including docs/site/wiki.']","write zookeeper tools[3.7], which includes the:

   - list all usages of the shells under the zookeeper/bin. (e.g zkTxnLogToolkit.sh,zkCleanup.sh)

   - benchmark tool

   - backup tool

   - test tools:jepsen",Add a new doc: zookeeperTools.md,3,,['pull-request-available'],maoling,True,maoling,maoling
zookeeper,ZOOKEEPER-3331,2019-03-22T22:23:03.000+0000,2019-05-06T16:32:52.000+0000,2019-05-06T19:51:15.000+0000,,Fixed,New Feature,Trivial,['3.6.0'],['3.6.0'],1800,1800,,,,100,['server'],['General issues with the ZooKeeper server.'],"NIOServerCnxn automatically adds the client's address as an auth token under the ""ip"" scheme. Extend that functionality to the NettyServerCnxn as well to bring parity to the two approaches.",Automatically add IP authorization for Netty connections,3,,['pull-request-available'],nixon,True,,nixon
zookeeper,ZOOKEEPER-3318,2019-03-16T07:10:39.000+0000,,2019-06-15T14:14:26.000+0000,,,New Feature,Major,,,3000,3000,,,,100,['other'],[''],"We already had some workaround ways for the backup, e.g:
scenario 1: just write a cron shell to copy the snapshots periodically. 
scenario 2: use the observer as the role of backup, then write the snapshots to distributed file system. (e.g HDFS)

this issue is aiming to implement a complete backup mechanism for zookeeper internal:
the init propose:
1. for realtime backup.
write a new CLI:snapshot
1.1
[zk: 127.0.0.1:2180(CONNECTED) 0] snapshot backupDataDir
[zk: 127.0.0.1:2180(CONNECTED) 1] snapshot
 ***************************************************************************************************************
1.2 
if no parameter, the default backupDataDir is the dataDir. the format of the backup-snapshot is just like: snapshot.f9f800002834 which is the same as the original one.
when recovering,moving the snapshot.f9f800002834 to the dataDir, then restart the ensemble.
1.3
don't worry about exposing the takeSnap() api to the client.Look at this two references:
https://github.com/etcd-io/etcd/blob/master/clientv3/snapshot/v3_snapshot.go
https://github.com/xetorthio/jedis/blob/master/src/main/java/redis/clients/jedis/commands/BasicCommands.java#L68
2. for no-realtime backup.
2.1 
write a new tool/shell: zkBackup.sh which is the reverse proces of the zkCleanup.sh for no-realtime backup.",Add a complete backup mechanism for zookeeper internal,3,1,['pull-request-available'],maoling,True,maoling,maoling
zookeeper,ZOOKEEPER-3311,2019-03-14T00:04:13.000+0000,2019-05-22T09:39:31.000+0000,2019-05-22T17:16:40.000+0000,,Fixed,New Feature,Minor,['3.6.0'],['3.6.0'],7200,7200,,,,100,['server'],['General issues with the ZooKeeper server.'],"The SyncRequestProcessor flushes writes to disk either when 1000 writes are pending to be flushed or when the processor fails to retrieve another write from its incoming queue. The ""flush when queue empty"" condition operates poorly under many workloads as it can quickly degrade into flushing after every write -- losing all benefits of batching and leading to a continuous stream of flushes + fsyncs which overwhelm the underlying disk.
 
A configurable flush delay would ensure flushes do not happen more frequently than once every X milliseconds. This can be used in-place of or jointly with batch size triggered flushes.",Allow a delay to the transaction log flush ,3,,['pull-request-available'],nixon,True,nixon,nixon
zookeeper,ZOOKEEPER-3287,2019-02-22T04:30:58.000+0000,,2019-02-22T04:30:58.000+0000,,,New Feature,Trivial,,['3.6.0'],,,,,,,['server'],['General issues with the ZooKeeper server.'],"Add a new command to dump the set of ACLs currently applied on the data tree. 

 

Used by an admin to check what controls are being set for an ensemble. A flat list with no connection to the data will suffice - will have to think whether any details ought to be emitted as a cryptographic hash to preserve secrecy.",admin command to dump currently known ACLs,1,,,nixon,True,,nixon
zookeeper,ZOOKEEPER-3282,2019-02-19T11:50:32.000+0000,,2019-02-19T12:04:52.000+0000,,,New Feature,Major,,,,9000,,,,,['documentation'],"['Cross cutting documentation, including docs/site/wiki.']","Hi guys:

I'am working on doing a big refactor for the documetations.it aims to 

 - 1.make a better reading experiences and help users know more about zookeeper quickly,as good as other projects' doc(e.g redis,hbase).

 - 2.have less changes to diff with the original docs as far as possible.

 - 3.solve the problem when we have some new features or improvements,but cannot find a good place to doc it.

 

The new catalog may looks kile this:

* is new one added.

** is the one to keep unchanged as far as possible.

*** is the one modified.

--------------------------------------------------------------

|---Overview

    |---Welcome ** [1.1]

    |---Overview ** [1.2]

    |---Getting Started ** [1.3]

    |---Release Notes ** [1.4]

|---Developer

    |---API *** [2.1]

    |---Programmer's Guide ** [2.2]

    |---Recipes *** [2.3]

    |---Clients * [2.4]

    |---Use Cases * [2.5]

|---Admin & Ops

    |---Administrator's Guide ** [3.1]

    |---Quota Guide ** [3.2]

    |---JMX ** [3.3]

    |---Observers Guide ** [3.4]

    |---Dynamic Reconfiguration ** [3.5]

    |---Zookeeper CLI * [3.6]

    |---Shell * [3.7]

    |---Configuration flags * [3.8]

    |---Troubleshooting & Tuning  * [3.9]

|---Contributor Guidelines

    |---General Guidelines * [4.1]

    |---ZooKeeper Internals ** [4.2]

|---Miscellaneous

    |---Wiki ** [5.1]

    |---Mailing Lists ** [5.2]

--------------------------------------------------------------










The Roadmap is:

1.(I pick up it : D)

  1.1 write API[2.1], which includes the： 

    1.1.1  original API Docs which is a Auto-generated java doc,just give a link.

    1.1.2. Restful-api (the apis under the /zookeeper-contrib-rest/src/main/java/org/apache/zookeeper/server/jersey/resources)

  1.2 write Clients[2.4], which includes the: 

      1.2.1 C client 

      1.2.2 zk-python, kazoo

      1.2.3 Curator etc.......

      look at an example from: https://redis.io/clients




 #  write Recipes[2.3], which includes the:

  - integrate ""Java Example"" and ""Barrier and Queue Tutorial""(Since some bugs in the examples and they are obsolete，we may delete something) into it.

  - suggest users to use the recipes implements of Curator and link to the Curator's recipes doc.

 
 #  write Zookeeper CLI[3.6], which includes the:

  - about how to use the zk command line interface [./zkCli.sh]

    e.g ls /; get ; rmr;create -e -p etc.......

  - look at an example from redis: https://redis.io/topics/rediscli

 
 #  write shell[3.7], which includes the:

   - list all usages of the shells under the zookeeper/bin. (e.g zkTxnLogToolkit.sh,zkCleanup.sh)

 
 #  write Configuration flags[3.8], which includes the:

   - list all usages of configurations properties(e.g zookeeper.snapCount): 

   - move the original Advanced Configuration part of zookeeperAdmin.md into it.

     look at an example from:https://coreos.com/etcd/docs/latest/op-guide/configuration.html

   
 #  write Troubleshooting & Tuning[3.9], which includes the:

   - move the original ""Gotchas: Common Problems and Troubleshooting"" part of Administrator's Guide.md into it.

   - move the original ""FAQ"" into into it.

   - add some new contents （e.g https://www.yumpu.com/en/document/read/29574266/building-an-impenetrable-zookeeper-pdf-github）.

   look at an example from:https://redis.io/topics/problems

                              https://coreos.com/etcd/docs/latest/tuning.html

 
 #  write General Guidelines[4.1], which includes the:

  - move the original ""Logging"" part of ZooKeeper Internals into it as the logger specification.

  - write specifications about code, git commit messages,github PR  etc ...

    look at an example from:

    http://hbase.apache.org/book.html#hbase.commit.msg.format

 
 #  write Use Cases[2.5], which includes the:

  - just move the context from: https://cwiki.apache.org/confluence/display/ZOOKEEPER/PoweredBy into it.

  - add some new contents.(e.g Apache Projects:Spark;Companies:twitter,fb)

 

--------------------------------------------------------------

BTW:

- Any insights or suggestions are very welcomed.After the dicussions,I will create a series of tickets(An umbrella)

- Since these works can be done parallelly, if you are interested in them, please don't hesitate,just assign to yourself, pick it up. (Notice: give me a ping to avoid the duplicated work).",a big refactor for the documetations,1,,,maoling,True,maoling,maoling
zookeeper,ZOOKEEPER-3281,2019-02-18T01:51:01.000+0000,,2019-03-16T09:34:09.000+0000,,,New Feature,Major,,['3.6.0'],1800,1800,,,,100,['scripts'],[''],"Terminal1:t1;
 Terminal2:t2;

PART1:
 --------------[-d] test for data change------------------------
 [t1]:
 watch -d /testwatch
 [t2]:
 set /testwatch mydata
 [t1]: result:
 WatchedEvent state:SyncConnected
 type:NodeDataChanged
 path:/testwatch
 new data:mydata
 ----------------------------------------------------------------
 [t1]:
 watch -d /testwatch
 [t2]:
 delete /testwatch
 [t1] result:
 WatchedEvent state:SyncConnected
 type:NodeDeleted
 path:/testwatch

PART2:
 --------------[-c] test for child change------------------------
 [t1]:
 watch -c /testwatch
 [t2]
 create /testwatch/child_1 mydata
 [t1] reslut:
 WatchedEvent state:SyncConnected
 type:NodeChildrenChanged
 path:/testwatch
 new child list:[child_1]
 ----------------------------------------------------------------
 [t1]:
 watch -c /testwatch
 [t2]:
 delete /testwatch/child_1
 [t1]:
 WatchedEvent state:SyncConnected
 type:NodeChildrenChanged
 path:/testwatch
 new child list:[]

PART3:
 ----------------[-e]test for exist watch----------------------
 [t2]:
 delete /testwatch
 [t1]:
 watch -e /testwatch
 [t2]:
 create /testwatch mydata
 [t1] result:
 WatchedEvent state:SyncConnected
 type:NodeCreated
 path:/testwatch
 ----------------------------------------------------------------
 [t1]:
 watch -e /testwatch
 [t2]:
 delete /testwatch
 WatchedEvent state:SyncConnected
 type:NodeDeleted
 path:/testwatch
 ----------------------------------------------------------------
 [t1]:
 watch -e /testwatch
 [t2]:
 set /testwatch mydata666666666
 [t1]:
 WatchedEvent state:SyncConnected
 type:NodeDataChanged
 path:/testwatch

----------------------------------------------------------------
a test for watching a non-existent key
[t1]: 
watch -d /non-existent_key
Node does not exist: /non-existent_key
watch -c /non-existent_key
Node does not exist: /non-existent_key
watch -e /non-existent_key
[t2]:
create /non-existent_key mydata
[t1]:
WatchedEvent state:SyncConnected
type:NodeCreated
path:/non-existent_key
----------------------------------------------------------------
the test for other watchedEvent state: e.g. Disconnected
[t1]:
watch -c /testwatch
#kill the zk server
WatchedEvent state:Disconnected
type:None
path:null",Add a new CLI:watch,1,,['pull-request-available'],maoling,True,maoling,maoling
zookeeper,ZOOKEEPER-3270,2019-02-04T04:06:57.000+0000,,2019-02-05T02:37:29.000+0000,,,New Feature,Major,,,,,,,,,['other'],[''],"There are metrics being published for connected clients. But if we want to find why connection count went up or down, it would be better to expose connections per user stats. (i.e. Some auth info for the connection)

Introduce new `clnt` command which is similar to `cons` except this also displays the user-agent to identify the type of client used to connect. List full connection/session details for all clients connected to this server. Includes information on numbers of packets received/sent, session id, operation latencies, last operation performed, user-agent, etc...",Publish metrics for connected clients with user stats,1,,['features'],dineshappavoo,True,dineshappavoo,dineshappavoo
zookeeper,ZOOKEEPER-3269,2019-02-03T22:41:41.000+0000,2019-02-06T08:46:56.000+0000,2019-02-06T11:27:34.000+0000,,Fixed,New Feature,Major,['3.6.0'],,10200,10200,,,,100,['java client'],['The java client interface for ZooKeeper'],"For testing and other reasons it would be very useful to add a way to inject an event into ZooKeeper's event queue. ZooKeeper already has the {{Testable}} for features such as this (low level, backdoor, testing, etc.). This queueEvent method would be particularly helpful to Apache Curator and we'd very much appreciate its inclusion.

The method should have the signature:

{code}
void queueEvent(WatchedEvent event);
{code}

Calling this would have the affect of queueing an event into the clients queue.",Testable facade would benefit from a queueEvent() method,3,,['pull-request-available'],randgalt,True,randgalt,randgalt
zookeeper,ZOOKEEPER-3264,2019-01-31T03:57:10.000+0000,,2019-07-07T10:27:12.000+0000,,,New Feature,Major,,,600,600,,,,100,['other'],[''],"Reference:
https://github.com/etcd-io/etcd/blob/master/tools/benchmark/cmd/range.go
https://github.com/antirez/redis/blob/unstable/src/redis-benchmark.c
https://github.com/phunt/zk-smoketest/blob/master/zk-latencies.py
https://github.com/brownsys/zookeeper-benchmark/blob/master/src/main/java/edu/brown/cs/zkbenchmark/ZooKeeperBenchmark.java",Add a benchmark tool for zookeeper,3,,['pull-request-available'],maoling,True,maoling,maoling
zookeeper,ZOOKEEPER-3259,2019-01-28T11:43:38.000+0000,,2019-02-19T12:04:52.000+0000,,,New Feature,Major,,,,,,,,,['documentation'],"['Cross cutting documentation, including docs/site/wiki.']","1.1 write API[2.1], which includes the： 
    1.1.1  original API Docs which is a Auto-generated java doc,just give a link.
    1.1.2. Restful-api (the apis under the /zookeeper-contrib-rest/src/main/java/org/apache/zookeeper/server/jersey/resources)",create a separate document:client.md about the usage of the client.,2,,,maoling,True,maoling,maoling
zookeeper,ZOOKEEPER-3247,2019-01-12T23:29:25.000+0000,,2019-01-12T23:29:25.000+0000,,,New Feature,Major,,,,,,,,,['leaderElection'],['Leader election algorithm for ZooKeeper'],Add lest admin command to get the last leader election time. ,New lest admin command to get leader election time,1,,['features'],dineshappavoo,True,,dineshappavoo
zookeeper,ZOOKEEPER-3246,2019-01-12T07:38:48.000+0000,,2019-01-12T07:38:48.000+0000,,,New Feature,Major,,,,,,,,,['other'],[''],"Problem
There is no way to currently tell whether a learner gets a diff or full snapshot, or does a truncate when it connects with the leader, also how long syncing with the leader takes. There is no explicit and general indicator of how often a learner has to connect to a leader without greping through zookeeper.log

Solution
Start tracking and exporting the following three items:
 * counter incremented each time a learner sync's with a leader
 * the type of sync that was needed
 * how long the sync took","Publish more stats when learner gets a diff, full snapshot, or does a truncate when it connects with the leader",1,,['features'],dineshappavoo,True,,dineshappavoo
zookeeper,ZOOKEEPER-3244,2019-01-12T00:28:27.000+0000,2019-05-14T15:11:35.000+0000,2019-05-15T21:08:56.000+0000,,Fixed,New Feature,Minor,['3.6.0'],,13200,13200,,,,100,['server'],['General issues with the ZooKeeper server.'],"Currently, ZooKeeper only takes snapshot based on the snap count. If the workload on an ensemble includes large txns then we'll end up with large amount data kept on disk, and might have a low disk space issue. 

Add a maximum limit on the total size of the log files between each snapshot. This will change the snap frequency, which means with the same snap retention number a server will eat up less disk.

 ",Add option to snapshot based on log size,2,,['pull-request-available'],nixon,True,nixon,nixon
zookeeper,ZOOKEEPER-3241,2019-01-10T21:48:29.000+0000,,2019-01-12T07:26:49.000+0000,,,New Feature,Major,,,,,,,,,['c client'],['The c client interface to ZooKeeper'],There is as new api `getEphemerals()` [https://github.com/apache/zookeeper/pull/735] being introduced in Zookeeper server. Update C client for the API with different parameters.,Update C client for the new getEphemerals api,1,,,dineshappavoo,True,,dineshappavoo
zookeeper,ZOOKEEPER-3209,2018-12-07T23:25:09.000+0000,2019-01-16T13:21:49.000+0000,2019-01-17T01:58:05.000+0000,,Fixed,New Feature,Major,['3.6.0'],,9000,9000,,,,100,['other'],[''],"New API `getEphemerals()` to get all the ephemeral nodes created by the session by providing the prefix path. 
* get the prefix path as a input parameter and return a list of string (ephemeral nodes)
* If the prefix path is `/` return all the ephemeral nodes created by the session
* Provide synchronous and asynchronous API's with same functionality
",New `getEphemerals` api to get all the ephemeral nodes created by the session,2,,"['features', 'pull-request-available']",dineshappavoo,True,dineshappavoo,dineshappavoo
zookeeper,ZOOKEEPER-3167,2018-10-13T14:09:26.000+0000,2019-02-07T10:26:29.000+0000,2019-02-07T16:00:43.000+0000,,Fixed,New Feature,Minor,['3.6.0'],"['3.4.5', '3.5.0']",9600,9600,,,,100,,,"1. In production environment, there will be always a situation that there are a lot of recursive sub nodes of one node. We need to count total number of it.

2. Now, we can only use API getChildren  which returns the List<String> of first level of sub nodes. We need to iterate every sub node to get recursive sub nodes. It will cost a lot of time.

3.  In zookeeper server side, it uses Hasp<String, DataNode> to store node.  The key of the map represents the path of the node. We can iterate the map get total number of all levels of sub nodes of one node.",add an API and the corresponding CLI to get total count of recursive sub nodes under a specific path,4,,"['patch', 'pull-request-available']",TyqITstudent,True,maoling,TyqITstudent
zookeeper,ZOOKEEPER-3160,2018-10-02T15:46:02.000+0000,2019-01-25T13:32:45.000+0000,2019-01-25T19:23:33.000+0000,,Fixed,New Feature,Minor,['3.6.0'],['3.5.4'],48600,48600,,,,100,['java client'],['The java client interface for ZooKeeper'],"The Zookeeper libraries currently allow you to set up your SSL Context via system properties such as ""zookeeper.ssl.keyStore.location"" in the X509Util. This covers most simple use cases, where users have software keystores on their harddrive.

There are, however, a few additional scenarios that this doesn't cover. Two possible ones would be:
 # The user has a hardware keystore, loaded in using PKCS11 or something similar.
 # The user has no access to the software keystore, but can retrieve an already-constructed SSLContext from their container.

For this, I would propose that the X509Util be extended to allow a user to set a property such as ""zookeeper.ssl.client.context"" to provide a class which supplies a custom SSL context. This gives a lot more flexibility to the ZK client, and allows the user to construct the SSLContext in whatever way they please (which also future proofs the implementation somewhat).

I've already completed this feature, and will put in a PR soon for it.",Custom User SSLContext,2,,"['features', 'pull-request-available', 'ready-to-commit']",Kenco,True,Kenco,Kenco
zookeeper,ZOOKEEPER-3140,2018-09-07T23:28:26.000+0000,2018-12-09T02:17:31.000+0000,2018-12-09T06:21:26.000+0000,,Fixed,New Feature,Minor,['3.6.0'],['3.6.0'],30000,30000,,,,100,['server'],['General issues with the ZooKeeper server.'],"Observers function simple as non-voting members of the ensemble, sharing the Learner interface with Followers and holding only a slightly difference internal pipeline. Both maintain connections along the quorum port with the Leader by which they learn of all new proposals on the ensemble. 
 
 There are benefits to allowing Observers to connect to the Followers to plug into the commit stream in addition to connecting to the Leader. It shifts the burden of supporting Observers off the Leader and allow it to focus on coordinating the commit of writes. This means better performance when the Leader is under high load, particularly high network load such as can happen after a leader election when many Learners need to sync. It also reduces the total network connections maintained on the Leader when there are a high number of observers. One the other end, Observer availability is improved since it will take shorter time for a high number of Observers to finish syncing and start serving client traffic.
 
 The current implementation only supports scaling the number of Observers into the hundreds before performance begins to degrade. By opening up Followers to also host Observers, over a thousand observers can be hosted on a typical ensemble without major negative impact under both normal operation and during post-leader election sync.",Allow Followers to host Observers,4,1,['pull-request-available'],nixon,True,nixon,nixon
zookeeper,ZOOKEEPER-3137,2018-08-31T22:43:13.000+0000,2018-09-14T22:04:16.000+0000,2018-11-24T19:55:35.000+0000,,Fixed,New Feature,Trivial,['3.6.0'],['3.6.0'],7800,7800,,,,100,,,"Add a utility that allows an admin to truncate a given transaction log to a specified zxid. This can be similar to the existent LogFormatter. 

Among the benefits, this allows an admin to put together a point-in-time view of a data tree by manually mutating files from a saved backup.",add a utility to truncate logs to a zxid,2,,['pull-request-available'],nixon,True,nixon,nixon
zookeeper,ZOOKEEPER-3114,2018-08-08T01:02:52.000+0000,,2018-09-19T03:13:57.000+0000,,,New Feature,Major,['3.6.0'],,,19200,,,,,['quorum'],['Quorum determination for ZooKeeper'],"The correctness of ZooKeeper was kind of proved in theory in ZAB paper, but the implementation is a bit different from the paper, for example, save the currentEpoch and proposals/commits upon to NEWLEADER is not atomic in current implementation, so the correctness of ZooKeeper is not actually proved in reality.

Also bugs could be introduced during implementation, issues like sending NEWLEADER packet too early reported in ZOOKEEPER-3104 might be there since the beginning (didn't check exactly when this was introduced). 

More correctness issues were introduced when adding new features, like on disk txn sync, local session, retain database, etc, both of these features added inconsistency bugs on production.

To catch the consistency issue earlier, internally, we're running external consistency checkers to compare nodes (digest), but that's not efficient (slow and expensive) and there are corner cases we cannot cover in external checker. For example, we don't know the last zxid before epoch change, which makes it's impossible to check it's missing txn or not. Another challenge is the false negative which is hard to avoid due to fuzzy snapshot or expected txn gap during snapshot syncing, etc.

This Jira is going to propose a built-in real time consistency check by calculating the digest of DataTree after applying each txn, and sending it over to learner during propose time so that it can verify the correctness in real time. 

The consistency check will cover all phases, including loading time during startup, syncing, and broadcasting. It can help us avoid data lost or data corrupt due to bad disk and catch bugs in code.

The protocol change will make backward compatible to make sure we can enable/disable this feature transparently.

As for performance impact, based on our testing, it will add a bit overhead during runtime, but doesn't have obvious impact in general.
h2.  ",Built-in data consistency check inside ZooKeeper,3,2,,lvfangmin,True,lvfangmin,lvfangmin
zookeeper,ZOOKEEPER-3092,2018-07-17T22:09:32.000+0000,2019-06-07T12:20:06.000+0000,2019-06-07T12:20:06.000+0000,,Fixed,New Feature,Major,['3.6.0'],,,111000,,,,,['metric system'],[''],"ZooKeeper should provide a pluggable metrics system such that various metrics can be collected and reported using different approaches that fit production monitoring / alert / debugging needs. 
Historically ZooKeeper provides four letter words and JMX which exposes certain stats / metrics but they are not very flexible in terms of programmatically accessing metrics and connecting metrics to different reporting systems.

There are other projects that's already doing this which can be used for reference, such as bookkeeper metrics service providers and hadoop metrics2.",Pluggable metrics system for ZooKeeper,4,1,,hanm,True,eolivelli,hanm
zookeeper,ZOOKEEPER-3091,2018-07-17T13:38:59.000+0000,2019-06-12T08:25:40.000+0000,2019-06-16T09:15:40.000+0000,,Fixed,New Feature,Major,['3.6.0'],['3.4.6'],28200,28200,,,,100,"['jmx', 'metric system']","['JMX Support', '']","Feature Request to add Prometheus /metrics http endpoint for monitoring integration:

[https://prometheus.io/docs/prometheus/latest/configuration/configuration/#%3Cscrape_config%3E]

Prometheus metrics format for that endpoint:

[https://github.com/prometheus/docs/blob/master/content/docs/instrumenting/exposition_formats.md]",Prometheus.io integration,4,,['pull-request-available'],harisekhon,True,,harisekhon
zookeeper,ZOOKEEPER-3066,2018-06-19T13:46:12.000+0000,2018-06-26T09:19:35.000+0000,2019-05-20T17:50:42.000+0000,,Fixed,New Feature,Major,"['3.6.0', '3.5.5']","['3.5.4', '3.6.0']",22800,22800,,,,100,"['jmx', 'leaderElection', 'quorum']","['JMX Support', 'Leader election algorithm for ZooKeeper', 'Quorum determination for ZooKeeper']","It will be useful to add to JMX beans published on Follower Peers to have an information about the current ""leader"".

This information is only available using 4 letter words",Expose on JMX of Followers the id of the current leader,3,,['pull-request-available'],eolivelli,True,eolivelli,eolivelli
zookeeper,ZOOKEEPER-2994,2018-03-07T15:31:51.000+0000,2018-04-23T22:26:15.000+0000,2018-07-17T04:49:57.000+0000,,Fixed,New Feature,Major,"['3.5.4', '3.6.0', '3.4.13']","['3.5.4', '3.6.0', '3.4.13']",,,,,,,,,"In the even that the zookeeper transaction log or snapshot become corrupted and fail CRC checks (preventing startup) we should have a mechanism to get the cluster running again.

Previously we achieved this by loading the broken transaction log with a modified version of ZK with disabled CRC check and forced it to snapshot.

It'd very handy to have a tool which can do this for us. LogFormatter and SnapshotFormatter have already been designed to dump log and snapshot files, it'd be nice to extend their functionality and add ability for such recovery.

It has proven that once you end up with the corrupt txn log there is no way to recover except manually modifying the crc check. That's basically why the tool is needed.",Tool required to recover log and snapshot entries with CRC errors,3,,,andorm,True,andorm,andorm
zookeeper,ZOOKEEPER-2941,2017-11-15T08:01:18.000+0000,,2017-11-15T08:01:18.000+0000,,,New Feature,Major,,,,,,,,,,,"right now, zookeeper only promise channel FIFO order,  in this case the third operation my arrive before the second operation:

since zookeeper promise that these operation are sending in pipeline, so the later operation don't need to wait the prior's confirmation. so the three operations
1. set a = 1
2. set b = 1
3. set ready = true

these three operations are sending in pipeline, the first operation set a = 1 is process ok, and the second operation set b = 1 is on the way. then there is something wrong with the leader, then the client connect a new tcp connection with the leader. And then the client send the last operation, since there is two tcp connection from client to server, even through the first is closed from the client's view, but there maybe still some redidual data, so we can't promise whether the second operation will arrive to the leader, and we also can't promise that the second operation arrive to the leader before the third one or after the third one. so this violate the client FIFO order.

we know that http://atomix.io/copycat/docs/client-interaction/#preserving-program-order   provide client level FIFO order. 
How about support client level FIFO order

Thank you ","support client FIFO client order, not only channel FIFO client order",3,,,baotiao,True,,baotiao
zookeeper,ZOOKEEPER-2933,2017-11-06T16:22:17.000+0000,2018-10-02T09:38:45.000+0000,2019-05-20T17:50:48.000+0000,,Fixed,New Feature,Major,"['3.6.0', '3.5.5']",,,5400,,,,,"['jute', 'server']","['Data marshalling code generator for ZooKeeper', 'General issues with the ZooKeeper server.']","This is related to jute.maxbuffer problems on the server side when Leader generates a proposal that doesn't fit into Follower's Jute buffer causing the quorum to be broken.

Proposed solution is to add the following new JMX Beans:

1. Add getJuteMaxBuffer to ZookeeperServerBean which monitors the current jute.maxbuffer setting,
2. Add get last/min/max ProposalSize to LeaderBean which monitors the size of the latest/min/max proposal.

The rationale behind this new feature is to add capability to JMX monitoring API to determine what is the current/min/max usage of the Jute buffer. This will let third party monitoring tools to get samples of buffer usage and create some statistics or generate alerts if it breaches a particular value.

This will not solve the problems related to jute.maxbuffer setting on its own, but it's intended to be the first step towards better handling or preventing production issues to happen.

Subtasks have been created to separately implement client and server side buffer size monitoring.",Ability to monitor the jute.maxBuffer usage in real-time,5,,"['buffer', 'buffer-length']",andorm,True,andorm,andorm
zookeeper,ZOOKEEPER-2912,2017-10-02T11:18:02.000+0000,,2017-10-02T11:18:02.000+0000,,,New Feature,Major,,,,,,,,,"['leaderElection', 'quorum', 'server']","['Leader election algorithm for ZooKeeper', 'Quorum determination for ZooKeeper', 'General issues with the ZooKeeper server.']","In our system, we're having to deploy a single zookeeper cluster across multiple datacentres. In this situation, we're running into problems with latency across the sites.

One thing that would help is if there was the capability to deploy an arbiter zookeeper node that did not store/update data or serve client requests, could not become leader, and did not determine quorum for updates, but participated in leadership elections (very similar to arbiters for mongo, https://docs.mongodb.com/manual/tutorial/add-replica-set-arbiter/).

This arbiter could then be deployed on a separate arbiter site that did not need a fast network link to the rest of the cluster, but would determine the active cluster in split-brain situations across the 2 main sites.

Currently, there's nothing stopping a zookeeper deployed on the arbiter site from becoming leader, and then the relatively high latencies involved cause problems across the cluster. Observers don't really fit our use case at the moment either.",Allow arbiter zookeeper nodes to be deployed,2,1,,thecoop1984,True,,thecoop1984
zookeeper,ZOOKEEPER-2875,2017-08-15T07:04:09.000+0000,2017-09-11T04:35:26.000+0000,2017-09-16T19:50:32.000+0000,,Fixed,New Feature,Major,"['3.4.11', '3.5.4', '3.6.0']","['3.4.10', '3.5.3', '3.6.0']",,,,,,,,,"The OWASP dependency check is a tool ""that identifies project dependencies and checks if there are any known, publicly disclosed, vulnerabilities"". We could run this tool periodically to make sure that we are not shipping any security vulnerabilities through our dependencies. ",Add ant task for running OWASP dependency report,3,,,abrahamfine,True,abrahamfine,abrahamfine
zookeeper,ZOOKEEPER-2858,2017-07-26T14:03:47.000+0000,,2017-11-17T22:12:44.000+0000,,,New Feature,Major,,['3.4.6'],,,,,,,['java client'],['The java client interface for ZooKeeper'],"I have the following setup:
- zookeeper server running in docker container
- kerberos auth

When client setup sasl connection it creates service principal name as:
- ""principalUserName+""/""+addr.getHostName()"",

where:
- addr.getHostName is the reverse DNS of original server host.

If zookeeper nodes will be deployed behind the firewall or software defined network (the docker case), then reverse DNS host won't match original server host. And this is done by design.

If these hosts won't match, then principals won't match and Kerberos auth will fail.

Is it possible to introduce some configuration parameter to disable reverse DNS lookups?",Disable reverse DNS lookup for java client,6,1,,dernasherbrezon,True,,dernasherbrezon
zookeeper,ZOOKEEPER-2765,2017-04-19T15:43:08.000+0000,2019-01-30T16:15:34.000+0000,2019-01-30T21:49:28.000+0000,,Won't Fix,New Feature,Major,,,2400,2400,,,,100,['c client'],['The c client interface to ZooKeeper'],"We should add a modern C++ (i.e. C++14, C++17, etc.) client library that wraps the existing C client.  A future issue may replace the C client itself.",modern C++ client,5,,['pull-request-available'],ecarter,True,ecarter,ecarter
zookeeper,ZOOKEEPER-2755,2017-04-13T04:51:04.000+0000,2018-01-17T08:02:27.000+0000,2018-01-17T08:02:27.000+0000,,Won't Fix,New Feature,Major,,['3.5.2'],,,,,,,"['java client', 'server']","['The java client interface for ZooKeeper', 'General issues with the ZooKeeper server.']","ClientCnxnSocketNetty and NettyServerCnxn use explicitly InetSocketAddress class to work with network addresses.

We can do a little refactoring to use only SocketAddress and make it possible to create subclasses of ClientCnxnSocketNetty and NettyServerCnxn which leverage built-in Netty 'local' channels. 

Such Netty local channels do not create real sockets and so allow a simple ZooKeeper server + ZooKeeper client to be run on the same JVM without binding to real TCP endpoints.

Usecases:

Ability to run concurrently on the same machine tests of projects which use ZooKeeper (usually in unit tests the server and the client run inside the same JVM) without dealing with random ports and in general using less network resources

Run simplified (standalone, all processes in the same JVM) versions of applications which need a working ZooKeeper ensemble to run.

Note:
Embedding ZooKeeper server + client on the same JVM has many risks and in general I think we should encourage users to do so, so I in this patch I will not provide official implementations of ClientCnxnSocketNetty and NettyServerCnxn. There will be implementations only inside the test packages, in order to test that most of the features are working with custom socket factories and in particular with the 'LocalAddress' specific subclass of SocketAddress.

Note:
the 'Local' sockets feature will be available on Netty 4 too
",Allow to subclass ClientCnxnSocketNetty and NettyServerCnxn in order to use Netty Local transport,3,,,eolivelli,True,eolivelli,eolivelli
zookeeper,ZOOKEEPER-2748,2017-04-06T22:03:43.000+0000,,2019-01-30T13:37:04.000+0000,,,New Feature,Minor,,,600,600,,,,100,['server'],['General issues with the ZooKeeper server.'],"In certain circumstances, it would be useful to be able to move clients from one server to another.

One example: a quorum that consists of 3 servers (A,B,C) with 1000 active client session, where 900 clients are connected to server A, and the remaining 100 are split over B and C (see example below for an example of how this can happen).
A will do a lot more work than B, C. 
Overall throughput will benefit by having the clients more evenly divided.
In case of A failure, all its client will create an avalanche by migrating en masse to a different server.

There are other possible use cases for a mechanism to move clients: 
 - Migrate away all clients before a server restart
 - Migrate away part of clients in response to runtime metrics (CPU/Memory usage, ...)
 - Shuffle clients after adding more server capacity (i.e. adding Observer nodes)

The simplest form of rebalancing which does not require major changes of protocol or client code consists of requesting a server to voluntarily drop some number of connections.
Clients should be able to transparently move to a different server.

Patch introducing 4-letter commands to shed clients:
https://github.com/apache/zookeeper/pull/215


-- -- --


How client imbalance happens in the first place, an example.

Imagine servers A, B, C and 1000 clients connected.
Initially clients are spread evenly (i.e. 333 clients per server).
A: 333 (restarts: 0)
B: 333 (restarts: 0)
C: 334 (restarts: 0)

Now restart servers a few times, always in A, B, C order (e.g. to pick up a software upgrades or configuration changes).

Restart A:
A: 0 (restarts: 1)
B: 499 (restarts: 0)
C: 500 (restarts: 0)

Restart B:
A: 250 (restarts: 1)
B: 0 (restarts: 1)
C: 750 (restarts: 0)

Restart C:
A: 625 (restarts: 1)
B: 375 (restarts: 1)
C: 0 (restarts: 1)

The imbalance is pretty bad already. C is idle while A has a lot of work.
A second round of restarts makes the situation even worse:

Restart A:
A: 0 (restarts: 2)
B: 688 (restarts: 1)
C: 313 (restarts: 1)

Restart B:
A: 344 (restarts: 2)
B: 657 (restarts: 1)
C: 0 (restarts: 1)

Restart C:
A: 673 (restarts: 2)
B: 328 (restarts: 1)
C: 0 (restarts: 1)

Large cluster (5, 7, 9 servers) make the imbalance even more evident.",Admin command to voluntarily drop client connections,4,,['pull-request-available'],mprime,True,mprime,mprime
zookeeper,ZOOKEEPER-2741,2017-03-28T18:52:15.000+0000,,2017-03-28T18:54:13.000+0000,,,New Feature,Minor,,,,,,,,,['contrib'],[''],"Perfect-ZooKeeper is a Swift class wrapper of zookeeper C connector:

Source Code:
https://github.com/PerfectlySoft/Perfect-ZooKeeper

Document:
http://www.perfect.org/docs/ZooKeeper.html

Perfect is an open source Server Side Swift framework supported by PerfectlySoft Inc. since 2015.

We are happy to share this new components as a community contribution.",A Swift Connector of ZooKeeper is available now: Perfect-ZooKeeper,1,,,rockford,True,,rockford
zookeeper,ZOOKEEPER-2719,2017-03-12T17:38:58.000+0000,2017-03-17T15:05:10.000+0000,2019-01-21T13:41:12.000+0000,,Fixed,New Feature,Major,['3.5.3'],,,,,,,,"['java client', 'server']","['The java client interface for ZooKeeper', 'General issues with the ZooKeeper server.']",ZOOKEEPER-2169 is a useful feature that should be deployed sooner than later. Take the work done in the master branch and port it to the 3.5 branch,Port ZOOKEEPER-2169 (TTL Nodes) to 3.5 branch,4,3,['ttl_nodes'],randgalt,True,randgalt,randgalt
zookeeper,ZOOKEEPER-2698,2017-02-16T20:28:43.000+0000,2017-03-02T18:36:26.000+0000,2017-03-02T18:36:26.000+0000,,Duplicate,New Feature,Major,,,,,,,,,,,,SSL support for server to server communication,2,,,abrahamfine,True,abrahamfine,abrahamfine
zookeeper,ZOOKEEPER-2593,2016-09-19T15:40:53.000+0000,,2019-05-08T06:13:47.000+0000,,,New Feature,Major,,,,,,,,,"['java client', 'server']","['The java client interface for ZooKeeper', 'General issues with the ZooKeeper server.']","Currently in ZooKeeper when quota limit exceeds, a warning is logged. There are many user scenarios where it is desired to throw exception in case quota limits exceed.
We should make it configurable whether to throw exception or just log the warning when quota limits exceed.
*Implementation:*
add new properties
{code}
enforce.number.quota
enforce.byte.quota
{code}
add new error codes
{code}
KeeperException.Code.NUMBERQUOTAEXCEED
KeeperException.Code.BYTEQUOTAEXCEED
{code}
add new exception
{code}
KeeperException.NumberQuotaExceedException
KeeperException.ByteQuotaExceedException
{code}
    
*Basic Scenarios:*
# If enforce.number.quota=true and number quota exceed, then server should send NUMBERQUOTAEXCEED error code and client should throw NumberQuotaExceedException
# If enforce.byte.quota=true and byte quota exceed, then server should send BYTEQUOTAEXCEED error code and client should throw ByteQuotaExceedException

*Impacted APIs:*
create 
setData",Enforce the quota limit,4,,,arshad.mohammad,True,arshad.mohammad,arshad.mohammad
zookeeper,ZOOKEEPER-2543,2016-09-02T04:37:37.000+0000,2016-10-08T18:20:34.000+0000,2019-01-21T14:53:46.000+0000,,Duplicate,New Feature,Major,['3.6.0'],['3.6.0'],,,,,,,['c client'],['The c client interface to ZooKeeper'],"ZOOKEEPER-2169 introduces a new feature for creation nodes with TTL, which is supported by Java client with new Java API. Similar API should be added to C client as well.",Add C API for creating nodes with TTL,1,,['ttl_nodes'],hanm,True,,hanm
zookeeper,ZOOKEEPER-2462,2016-07-01T14:50:01.000+0000,,2018-07-10T14:07:09.000+0000,,,New Feature,Minor,,,,,,,,,['server'],['General issues with the ZooKeeper server.'],"This change introduces two new config options to force authorization and authentication:

1. disableWorldACL
The purpose of this option is disable the builtin mechanism which authorizes everyone.
If it is turned on than the world/anyone usage is ignored. ZooKeeper will not check operations based on world/anyone.
This option is useful to force some kind of authorization mechanism. This restriction is useful in a strictly audited environment.

2. forceAuthentication
If this option is turned on than ZooKeeper won't authorize any operation if the user has not authenticated either with SASL or with addAuth.
There is way to enforce SASL authentication but currently there is no way to enforce authentication using the plugin mechanism. Enforcing authentication for that is more tricky since authentication can come any time later. This option doesn't drop the connection if there was no authentication. It is only throwing NoAuth for any operation until the Auth packet arrives.

",force authentication/authorization,4,1,,botond.hejj,True,,botond.hejj
zookeeper,ZOOKEEPER-2454,2016-06-27T09:16:43.000+0000,,2016-10-10T01:54:35.000+0000,,,New Feature,Minor,,,,,,,,,['server'],['General issues with the ZooKeeper server.'],"ZooKeeper currently can limit connection count from clients coming from the same ip. It is a great feature to malfunctioning clients DOS-ing the server with many requests.

I propose additional safegurads for ZooKeeper. 
It would be great if optionally connection count could be limited for a specific user or a specific user on an ip.
This is great in cases where ZooKeeper ensemble is shared by multiple users and these users share the same client ips. This can be common in container based cloud deployment where external ip of multiple clients can be the same.
",Limit Connection Count based on User,8,,,botond.hejj,True,botond.hejj,botond.hejj
zookeeper,ZOOKEEPER-2350,2015-12-28T01:33:06.000+0000,,2016-02-29T05:31:34.000+0000,,,New Feature,Minor,,['3.5.0'],,,,,,,['scripts'],[''],"Starting with 3.5.0, ZooKeeper has already supported a very convenient dynamic reconfiguration. However, the procedure is slightly complicated. So I propose adding a script to build ensemble with ease.

Usage:
{noformat}
$ ./bin/zkEnsemble.sh help
usage: ./bin/zkEnsemble.sh {start|stop|status} <parameters>
  Commadns:
    start                   Start a node of ensemble.
      Parameters:
        --seed              Specify the IP address and port of an existing ensemble node that required for 2nd and subsequent nodes.
                            This is not required for the 1st node. (Example: 127.0.0.1:2181)
        --ip                Normally, you do not need to specify because it is automatically detected.
                            If it seems the wrong IP address is found automatically, you can over ride the IP address with this option.
        --clientport        The port is used to client connections (2181 by default).
        --peerport          The port is used to talk to each other (2888 by default).
                            If omitted, it will use the minimum port number that is available between 2888 to 3142.
        --electionport      The port is used to leader election (3888 by default).
                            If omitted, it will use the minimum port number that is available between 3888 to 4142.
        --role              The role of node, it can be participant or observer (participant by default).
        --clientip          The IP address for client connections (0.0.0.0 by default).
                            If omitted, it will use the minimum port number that is available between 2181 to 2435.
        --confdir           Specify a base conf directory (/Users/mosuka/git/zookeeper/conf by default).
        --datadir           Specify a base data directory (/tmp/zookeeper by default).
        --foreground        Start node in foreground.

    stop                    Stop a node of ensemble.
      Parameters:
        --ip                Normally, you do not need to specify because it is automatically detected.
                            If it seems the wrong IP address is found automatically, you can over ride the IP address with this option.
        --clientport        The port is used to client connections (2181 by default).

    status                  Show ensemble nodes.
      Parameters:
        --seed              Specify the IP address and port of a existing ensemble node (Example: 127.0.0.1:2181).

    help                    Display this message.
{noformat}

Example:
1. Start a 1st node of ensemble on host1(192.168.33.11)
{noformat}
$ ./bin/zkEnsemble.sh start
ZooKeeper JMX enabled by default
Using config: /Users/minoru/zookeeper/zookeeper-3.5.0/conf/server.1.cfg
Starting zookeeper ... STARTED
{noformat}

2. Start a 2nd node of ensemble on host2(192.168.33.12).
{noformat}
$ ./bin/zkEnsemble.sh start --seed=192.168.33.11:2181
ZooKeeper JMX enabled by default
Using config: /Users/minoru/zookeeper/zookeeper-3.5.0/conf/server.2.cfg
Starting zookeeper ... STARTED
{noformat}

3. Start a 3rd node of ensemble on host3(192.168.33.13).
{noformat}
$ ./bin/zkEnsemble.sh start --seed=192.168.33.11:2181
ZooKeeper JMX enabled by default
Using config: /Users/minoru/zookeeper/zookeeper-3.5.0/conf/server.3.cfg
Starting zookeeper ... STARTED
{noformat}

4. Show ensemble nodes on host1(192.168.33.11).
{noformat}
$ ./bin/zkEnsemble.sh status --seed=192.168.33.11:2181
server.1=192.168.33.11:2888:3888:participant;0.0.0.0:2181
server.2=192.168.33.12:2888:3888:participant;0.0.0.0:2181
server.3=192.168.33.13:2888:3888:participant;0.0.0.0:2181
{noformat}

5. Stop a 2nd node of ensemble on host2(192.168.33.12).
{noformat}
$ ./bin/zkEnsemble.sh stop
Using config: /Users/minoru/zookeeper/zookeeper-3.5.0/conf/server.2.cfg
Stopping zookeeper ... STOPPED
{noformat}

6. Show ensemble nodes on host1(192.168.33.11).
{noformat}
$ ./bin/zkEnsemble.sh status --seed=192.168.33.11:2181
server.1=192.168.33.11:2888:3888:participant;0.0.0.0:2181
server.3=192.168.33.13:2888:3888:participant;0.0.0.0:2181
{noformat}

7. Start a 2nd node of ensemble on host2(192.168.33.12).
{noformat}
$ ./bin/zkEnsemble.sh start --seed=192.168.33.11:2181
ZooKeeper JMX enabled by default
Using config: /Users/minoru/zookeeper/zookeeper-3.5.0/conf/server.2.cfg
Starting zookeeper ... STARTED
{noformat}

8. Show ensemble nodes on host1(192.168.33.11).
{noformat}
$ ./bin/zkEnsemble.sh status --seed=192.168.33.11:2181
server.1=192.168.33.11:2888:3888:participant;0.0.0.0:2181
server.2=192.168.33.12:2888:3888:participant;0.0.0.0:2181
server.3=192.168.33.13:2888:3888:participant;0.0.0.0:2181
{noformat}",Script that provide a way to build the ensemble with ease.,3,1,,minoru,True,,minoru
zookeeper,ZOOKEEPER-2289,2015-10-10T22:50:30.000+0000,,2015-10-16T22:46:39.000+0000,,,New Feature,Major,,,,,,,,,"['java client', 'server']","['The java client interface for ZooKeeper', 'General issues with the ZooKeeper server.']","The current codebase supports use of SASL for authenticating connections, but it does not allow specifying the desired SASL Quality of Protection (QOP).  It always uses the default QOP, which is ""auth"" (authentication only).  This issue proposes to support the full set of available QOP settings by adding support for ""auth-int"" (authentication and integrity) and ""auth-conf"" (authentication and integrity and privacy/encryption).","Support use of SASL ""auth-int"" and ""auth-conf"" quality of protection.",4,1,,cnauroth,True,,cnauroth
zookeeper,ZOOKEEPER-2287,2015-09-30T11:17:41.000+0000,2016-06-29T19:01:03.000+0000,2016-06-29T19:01:03.000+0000,,Duplicate,New Feature,Major,,,,,,,,,,,"As of now Zookeeper does not support auditing the user operations
This is a very important tracability in distributed cluster to trace the operations

We can have a separate logger and log file.
Can start with normal node change operations.

Please share your thoughts ? ",Audit logging the zookeeper operations,3,,,nijel,True,arshad.mohammad,nijel
zookeeper,ZOOKEEPER-2260,2015-08-28T00:29:18.000+0000,,2019-04-02T13:55:37.000+0000,,,New Feature,Major,"['3.6.0', '3.5.6']","['3.4.6', '3.5.0']",600,600,,,,100,,,"Add pagination support to the getChildren() call, allowing clients to iterate over children N at the time.

Motivations for this include:
  - Getting out of a situation where so many children were created that listing them exceeded the network buffer sizes (making it impossible to recover by deleting)[1]
 - More efficient traversal of nodes with large number of children [2]

I do have a patch (for 3.4.6) we've been using successfully for a while, but I suspect much more work is needed for this to be accepted. 


[1] https://issues.apache.org/jira/browse/ZOOKEEPER-272
[2] https://issues.apache.org/jira/browse/ZOOKEEPER-282",Paginated getChildren call,23,4,"['api', 'features', 'pull-request-available']",mprime,True,mprime,mprime
zookeeper,ZOOKEEPER-2250,2015-08-17T23:50:47.000+0000,,2017-02-07T16:19:55.000+0000,,,New Feature,Major,,,,,,,,,['java client'],['The java client interface for ZooKeeper'],"Connecting to ZooKeeper via a SOCKS proxy is often useful for debugging systems over an SSH dynamic port forward.

It is possible to do this today with some hacking by setting ""zookeeper.clientCnxnSocket"", but that is difficult because ClientCnxnSocket is package-private and is quite low-level.",Support client connections using a SOCKS proxy,5,3,,electrum,True,,electrum
zookeeper,ZOOKEEPER-2209,2015-06-08T07:19:03.000+0000,2015-10-31T07:24:31.000+0000,2016-10-13T20:38:32.000+0000,,Won't Fix,New Feature,Major,,['3.4.6'],,,,,,,,,"Inspired by the work of [~ewhauser] .
I propose a C# Client that supports the current stable version of ZK 3.4.6.
It was built by using static code conversion tools followed by manual editing and C# implementations of java selector and other java constructs. 
A great measure was taken to follow the logic of the java version. In fact, the code is almost identical. Thus allowing easy evolution alongside the java version. 

Main features:
* fully .NET async, no explicit threads used
* all relevant unit tests have been converted and passing consistently
* Code is 100% CoreCLR compliant
* [NuGet package|https://www.nuget.org/packages/ZooKeeperNetEx] is already integrated in [Microsoft Project Orleans|https://github.com/dotnet/orleans] as the only open-source membership provider.
* [Nuget package for recipes|https://www.nuget.org/packages/ZooKeeperNetEx.Recipes]
",A .NET C# version of ZooKeeper client,6,,"['.NET', 'CoreCLR', 'async', 'c#']",shayhatsor,True,shayhatsor,shayhatsor
zookeeper,ZOOKEEPER-2169,2015-04-16T20:21:47.000+0000,2016-10-09T18:36:34.000+0000,2019-01-21T13:40:55.000+0000,,Fixed,New Feature,Major,['3.6.0'],['3.6.0'],,1800,,,,,"['c client', 'java client', 'jute', 'server']","['The c client interface to ZooKeeper', 'The java client interface for ZooKeeper', 'Data marshalling code generator for ZooKeeper', 'General issues with the ZooKeeper server.']","As a user, I would like to be able to create a node that is NOT tied to a session but that WILL expire automatically if action is not taken by some client within a time window.

I propose this to enable clients interacting with ZK via http or other ""thin clients"" to create ephemeral-like nodes.

Some ideas for the design, up for discussion:

The node should support all normal ZK node operations including ACLs, sequential key generation, etc, however, it should not support the ephemeral flag. The node will be created with a TTL that is updated via a refresh operation. 

The ZK quorum will watch this node similarly to the way that it watches for session liveness; if the node is not refreshed within the TTL, it will expire.

QUESTIONS:

1) Should we let the refresh operation set the TTL to a different base value?
2) If so, should the setting of the TTL to a new base value cause a watch to fire?
3) Do we want to allow these nodes to have children or prevent this similar to ephemeral nodes?

",Enable creation of nodes with TTLs,20,8,['ttl_nodes'],fournc,True,randgalt,fournc
zookeeper,ZOOKEEPER-2163,2015-04-13T18:19:28.000+0000,2015-06-21T00:06:16.000+0000,2019-03-19T13:12:33.000+0000,,Fixed,New Feature,Major,"['3.5.1', '3.6.0']",['3.5.0'],600,6600,,,,100,"['c client', 'java client', 'server']","['The c client interface to ZooKeeper', 'The java client interface for ZooKeeper', 'General issues with the ZooKeeper server.']","BACKGROUND
============
A recurring problem for ZooKeeper users is garbage collection of parent nodes. Many recipes (e.g. locks, leaders, etc.) call for the creation of a parent node under which participants create sequential nodes. When the participant is done, it deletes its node. In practice, the ZooKeeper tree begins to fill up with orphaned parent nodes that are no longer needed. The ZooKeeper APIs don’t provide a way to clean these. Over time, ZooKeeper can become unstable due to the number of these nodes.

CURRENT SOLUTIONS
===================
Apache Curator has a workaround solution for this by providing the Reaper class which runs in the background looking for orphaned parent nodes and deleting them. This isn’t ideal and it would be better if ZooKeeper supported this directly.

PROPOSAL
=========
ZOOKEEPER-723 and ZOOKEEPER-834 have been proposed to allow EPHEMERAL nodes to contain child nodes. This is not optimum as EPHEMERALs are tied to a session and the general use case of parent nodes is for PERSISTENT nodes. This proposal adds a new node type, CONTAINER. A CONTAINER node is the same as a PERSISTENT node with the additional property that when its last child is deleted, it is deleted (and CONTAINER nodes recursively up the tree are deleted if empty).

CANONICAL USAGE
================

{code}
while ( true) { // or some reasonable limit
    try {
        zk.create(path, ...);
        break;
    } catch ( KeeperException.NoNodeException e ) {
        try {
            zk.createContainer(containerPath, ...);
        } catch ( KeeperException.NodeExistsException ignore) {
       }
    }
}
{code}",Introduce new ZNode type: container,22,6,"['container_znode_type', 'pull-request-available']",randgalt,True,randgalt,randgalt
zookeeper,ZOOKEEPER-2130,2015-02-24T10:49:13.000+0000,,2015-03-17T10:25:40.000+0000,,,New Feature,Minor,['3.6.0'],,,,,,,,,,"It is good to have a command which can give complete summary of zookeeper ensemble. Ensemble summary should give information about who is leader, which are followers, observers.

Consider a zookeeper cluster with following configurations
server.1=localhost:33230:33235:participant;localhost:33222
server.2=localhost:33231:33236:participant;localhost:33223
server.3=localhost:33232:33237:participant;localhost:33224
server.4=localhost:33233:33238:participant;localhost:33225
server.5=localhost:33234:33239:participant;localhost:33226

When four servers are running and we execute esum(Ensemble Summary Command) command we should get status of all the servers and their roles

Example:
{quote}
server.1=localhost:33230:33235:participant;localhost:33222 {color:green}FOLLOWING{color}
Client Connections:1

server.2=localhost:33231:33236:participant;localhost:33223 {color:green}FOLLOWING{color}
Client Connections:0

server.3=localhost:33232:33237:participant;localhost:33224 {color:red}NOT RUNNING{color}

server.4=localhost:33233:33238:participant;localhost:33225 {color:green}FOLLOWING{color}
Client Connections:0

server.5=localhost:33234:33239:participant;localhost:33226 {color:blue}LEADING{color}
Client Connections:0
{quote}

",Command to get ensemble summary,1,,,arshad.mohammad,True,,arshad.mohammad
zookeeper,ZOOKEEPER-2120,2015-02-17T20:20:45.000+0000,,2015-05-13T17:08:40.000+0000,,,New Feature,Major,,,,,,,,,,,"As we discussed in ZOOKEEPER-2094, the SSL work would be divided into several subtask:

1. Provide implementation of X509 AuthenticationProvider
2. Modify ZooKeeper Netty server and client to support SSL
3. Modify ZooKeeperServerMain to support SSL

This is the umbrella task.",SSL feature on Netty,3,,,hdeng,True,hdeng,hdeng
zookeeper,ZOOKEEPER-2103,2015-01-07T20:55:11.000+0000,2016-08-08T14:42:17.000+0000,2016-08-08T14:42:17.000+0000,,Duplicate,New Feature,Minor,['3.5.2'],['3.6.0'],,,,,,,['java client'],['The java client interface for ZooKeeper'],"I ran into an issue when connecting to two ZooKeeper clusters from the same JVM application. One of the clusters required SASL authentication while the other one did not. Unfortunately the client uses System properties to configure authentication and the client was attempting to authenticate on the non-auth cluster, preventing a connection. 

To solve it, I implemented a base config class with helper methods for parsing config settings as well as a client specific subclass that parsed the system system values but allowed for overriding via programatic values or via a file. There are also new Zookeeper constructors to use this config object. I implemented it so that it's completely backwards compatible so it shouldn't break existing installs (and it hasn't yet with my testing).

If folks like this, we could use the same config base for server configs and migrate away from system properties to per object configs. It would also be helpful to centralize more of the ""zookeeper.*"" strings.

Let me know what ya'll think and thanks!",ZooKeeper Client Configuration,3,1,"['features', 'patch']",manolamancha,True,manolamancha,manolamancha
zookeeper,ZOOKEEPER-2077,2014-11-12T08:44:46.000+0000,,2015-11-17T06:14:18.000+0000,,,New Feature,Major,,,,,,,,,['java client'],['The java client interface for ZooKeeper'],"We had an use-case where we had to list nodes matching a particular pattern from a given path. While looking at the ZK client commands, it seems that it does not support wildcard/regex. 

I did try to overcome this by making some basic changes to the LSCommand.java and adding a ""-m"" switch which accepts regex. Since I implemented this using java.util.regex, it supports everything that Java regex supports. 

I was thinking such functionality can be useful for 'ls' as well as 'delete' (and deleteall). Though I implemented this at the client code for ls - this can be done at the server side code as well and I have a preliminary plan on top of my head to do this for ls, delete, deleteall. 

Will it be worthwhile addition to make to zookeeper client? If so, I can work on submitting a patch.

Points to consider in case such a support can be implemented:
1. Do we support Java regex or Unix Shell wildcards ( * )?
2. Right now, create allows creating nodes with characters like * - we need to make sure that such a change does not break or create confusion (Unix too allows creating a directory with * BTW).

Any thoughts on whether this will be a worthwhile addition to Zookeeper client?",Wild-card/Regex Support for Zookeeper client commands,5,2,,vivekpm,True,,vivekpm
zookeeper,ZOOKEEPER-2063,2014-10-17T20:34:38.000+0000,,2017-10-24T04:46:28.000+0000,,,New Feature,Major,,,,39000,,,,,,,"ZooKeeper currently have netty option on server side. We want to support netty on client side too. After that, we could add ssl support based on netty channel.",Netty+SSL support for client-server communication,10,1,,hdeng,True,,hdeng
zookeeper,ZOOKEEPER-1962,2014-07-10T11:29:42.000+0000,2016-09-08T20:53:24.000+0000,2017-05-18T03:43:57.000+0000,,Fixed,New Feature,Minor,"['3.5.3', '3.6.0']",['3.4.6'],,,86400,86400,86400,,['java client'],['The java client interface for ZooKeeper'],"When troubleshooting applications where znodes can be multiple levels deep  (eg. HBase replication), it is handy to see all child znodes recursively rather than run an ls for each node manually.

So I propose adding an option to the ""ls"" command (-r) which will list all child nodes under a given znode. ",Add a CLI command to recursively list a znode and children,17,2,,ggop,True,ggop,ggop
zookeeper,ZOOKEEPER-1931,2014-06-01T20:09:11.000+0000,,2016-01-15T01:12:01.000+0000,,,New Feature,Major,,,,,,,,,,,"The goal of this project is to define an interface for replication protocol and implement the interface using ZAB. This project will most likely be done outside of ZooKeeper to avoid impacting the stability of the ZooKeeper code base, but I'm opening a JIRA here to gauge interest and get feedback from ZooKeeper community.

There are 2 main motivations for this project:

1. There are many use cases that need a replication protocol like ZAB, but ZooKeeper's hierarchical data model doesn't work well. It's difficult to use ZAB without ZooKeeper with the way ZooKeeper code is currently structured.
2. It's valuable to have a common interface for replication protocol to build services on. This allows you to plug in different implementations for benchmarking and testing for correctness. This point is related to ZOOKEEPER-30.

The project is roughly broken into 4 pieces:

1. Define the interface for replication protocol. It's very important to get the interface right. I'd appreciate if you guys can help define the interface.
2. Implement the interface with single node ZAB.
3. Implement a simple reference service, something like a key-value store or a benchmark tool.
4. Implement ZAB, either from scratch or by refactoring / curving off unnecessary parts from the ZooKeeper code base.

I have some questions:

- How do things like session tracker and dynamic reconfiguration fit into this? Should they be separate optional interfaces?
- Where should this project belong to? Is it worth making this an incubator project, or should I just put the code on github? I'd like to make it easy for people from different organizations to collaborate (in terms of license grant and all) from the beginning.
",intern project idea: implement zab,6,,,michim,True,michim,michim
zookeeper,ZOOKEEPER-1925,2014-05-08T19:53:05.000+0000,2016-12-03T00:48:53.000+0000,2019-01-21T14:49:52.000+0000,,Duplicate,New Feature,Major,['3.6.0'],,,,,,,,,,Today whenever a znode is created; it stays there for ever. We all know that there is a limitation in terms of how many nodes a system can handle. It would be nice to have a way to specify expiry time for every znode thereby stale zondes are cleandup automatically after sufficiently large time. any thoughts?,Time to Live or auto expiration of zookeeper node,9,2,['ttl_nodes'],ojoshi,True,,ojoshi
zookeeper,ZOOKEEPER-1887,2014-02-24T01:00:12.000+0000,2014-04-16T06:16:20.000+0000,2018-12-16T14:50:13.000+0000,,Fixed,New Feature,Major,['3.5.0'],,,,,,,,['c client'],['The c client interface to ZooKeeper'],This is equivalent for ZOOKEEPER-442's Java impl. ,C implementation of removeWatches,8,1,['remove_watches'],rgs,True,rgs,rgs
zookeeper,ZOOKEEPER-1829,2013-12-05T04:57:10.000+0000,2014-04-04T17:52:12.000+0000,2018-12-16T14:35:50.000+0000,,Fixed,New Feature,Critical,['3.5.0'],,,,,,,,"['java client', 'server']","['The java client interface for ZooKeeper', 'General issues with the ZooKeeper server.']",,Umbrella jira for removing watches that are no longer of interest,3,1,['remove_watches'],rakeshr,True,rakeshr,rakeshr
zookeeper,ZOOKEEPER-1760,2013-09-22T23:02:04.000+0000,2013-09-25T08:26:43.000+0000,2013-09-25T09:44:09.000+0000,,Won't Fix,New Feature,Major,['3.5.0'],,,,,,,,['java client'],['The java client interface for ZooKeeper'],"The idea of this JIRA is to discuss the check version interface which is used to see the existence of a node for the specified version. Presently only multi transaction api has this interface, this umbrella JIRA is to make 'check version' api part of ZooKeeper# main apis and cli command.
",Provide an interface for check version of a node,6,,,rakeshr,True,rakeshr,rakeshr
zookeeper,ZOOKEEPER-1721,2013-07-06T15:51:01.000+0000,,2013-07-09T15:46:42.000+0000,,,New Feature,Major,,['3.4.5'],,,,,,,['server'],['General issues with the ZooKeeper server.'],I use zookeeper for cluster synchronization. We have no need for keeping persistent state across zookeeper restarts. For performance enhancement would be good to have possibility to run without writing snapshots and logs.,Ability to run without writing to disk,2,,,hsn,True,,hsn
zookeeper,ZOOKEEPER-1704,2013-05-09T21:04:22.000+0000,,2013-05-09T21:04:22.000+0000,,,New Feature,Trivial,,['3.4.5'],,,,,,,['documentation'],"['Cross cutting documentation, including docs/site/wiki.']",There's no obvious way to download the source file other than copy/paste.,Please add download link for tutorial,1,,,haydens,True,,haydens
zookeeper,ZOOKEEPER-1703,2013-05-09T21:03:05.000+0000,2017-10-13T23:22:00.000+0000,2017-10-13T23:57:47.000+0000,,Fixed,New Feature,Minor,"['3.4.11', '3.5.4', '3.6.0']",['3.4.5'],,,,,,,['documentation'],"['Cross cutting documentation, including docs/site/wiki.']",There's no instructions for running the tutorial. ,Please add instructions for running the tutorial,5,,['newbie'],haydens,True,andorm,haydens
zookeeper,ZOOKEEPER-1688,2013-04-10T19:28:37.000+0000,,2014-05-07T14:35:50.000+0000,,,New Feature,Major,,['3.5.0'],,,,,,,,,"We propose to introduce optional transparent encryption of snapshots and commit logs on disk. The goal is to protect against the leakage of sensitive information from files at rest, due to accidental misconfiguration of filesystem permissions, improper decommissioning, or improper disk disposal. This change would introduce a new ServerConfig option that allows the administrator to select the desired persistence implementation by classname, and new persistence classes extending the File* classes that wrap current formats in encrypted containers. Otherwise and by default the current File* classes will be used without change. If enabled, transparent encryption of all on disk structures will be accomplished with a shared cluster key made available to the quorum peers via the Java Keystore (supporting various store options, including hardware security module integration). Small modifications to the LogFormatter and SnapshotFormatter utilities will be needed. A new utility for offline key rotation will also be provided.

These changes will not introduce any new dependencies. The standard Java Cryptographic Extensions (JCE) are sufficient for implementation and can benefit from potential acceleration options provided by JCE now or future.",Transparent encryption of on-disk files,9,,,apurtell,True,,apurtell
zookeeper,ZOOKEEPER-1665,2013-03-14T04:08:29.000+0000,2014-04-02T18:38:38.000+0000,2014-04-02T18:38:38.000+0000,,Won't Fix,New Feature,Major,,,,,,,,,,,"Use case in HBase is that we need to recursively delete multiple subtrees:
{code}
    ZKUtil.deleteChildrenRecursively(watcher, acquiredZnode);
    ZKUtil.deleteChildrenRecursively(watcher, reachedZnode);
    ZKUtil.deleteChildrenRecursively(watcher, abortZnode);
{code}
To achieve high consistency, it is desirable to use multi for the above operations.

This JIRA adds support for recursive deletion in multi.",Support recursive deletion in multi,5,,,yuzhihong@gmail.com,True,,yuzhihong@gmail.com
zookeeper,ZOOKEEPER-1634,2013-01-30T19:35:57.000+0000,,2019-06-17T17:19:25.000+0000,,,New Feature,Major,['3.6.0'],['3.4.5'],13200,13200,246000,259200,246000,5,"['security', 'server']","['', 'General issues with the ZooKeeper server.']","Up to the version of 3.4.5, ZooKeeperServer doesn't force the authentication if the client doesn't give any auth-info through ZooKeeper#addAuthInfo method invocation.  Hence, every znode should have at least one ACL assigned otherwise any unauthenticated client can do anything on it.

The current authentication/authorization mechanism of ZooKeeper described above has several points at issue:
1. At security standpoint, a maleficent client can access a znode which doesn't have any proper authorization access control set.
2. At runtime performance standpoint, authorization for every znode to every operation is unnecessarily but always evaluated against the client who bypassed the authentication phase.

In other words, the current mechanism doesn't address a certain requirement at below:
""We want to protect a ZK server by enforcing a simple authentication to every client no matter which znode it is trying to access.  Every connection (or operation) from the client won't be established but rejected if it doesn't come with a valid authentication information.  As we don't have any other distinction between znodes in term of authorization, we don't want any ACLs on any znode.""

To address the issues mentioned above, we propose a feature called ""authentication enforcement"" to the ZK source.  The idea is roughly but clearly described in a form of patch in the attached file (zookeeper_3.4.5_patch_for_authentication_enforcement.patch): which makes ZooKeeperServer enforce the authentication with the given 2 configurations: authenticationEnforced (boolean) and enforcedAuthenticationScheme (string) against every operation coming through ZooKeeperServer#processPacket method except for OpCode.auth operation.  The repository base of the patch is ""http://svn.apache.org/repos/asf/zookeeper/tags/release-3.4.5/""",A new feature proposal to ZooKeeper: authentication enforcement,13,4,['pull-request-available'],choijw1,True,hanm,choijw1
zookeeper,ZOOKEEPER-1508,2012-07-09T23:12:12.000+0000,,2012-07-12T21:54:52.000+0000,,,New Feature,Major,,,,,,,,,,,"Currently ZooKeeper requires 3 servers to provide both reliability and availability. This is fine for large internet scale clusters, but there are lots of two node clusters that could benefit from ZooKeeper.  There are also single server use cases where it is highly desirable to have ZooKeeper survive a disk failure, but availability is not as important. 

This feature would allow the configuration of multiple destinations for logs and snapshots. A transaction is committed when a majority of the log writes complete successfully. If one log gets an error on write, then it is taken offline until an administrator brings it online or replaces it with a new destination. ZooKeeper continues to run as long as a quorum of disks can be written.

High availability can be provided with a two node cluster. When the ZooKeeper node dies, the  disks are switched to the surviving node and a new ZooKeeper starts. Faster switch over can be done if there is an observer already running in the new node.
",Reliable standalone mode through redundant databases,3,,,bbridge,True,,bbridge
zookeeper,ZOOKEEPER-1482,2012-06-11T06:14:33.000+0000,2014-05-21T20:12:24.000+0000,2014-05-21T20:12:24.000+0000,,Duplicate,New Feature,Major,"['3.5.0', '4.0.0']","['3.3.2', '3.4.3']",,,,,,,['server'],['General issues with the ZooKeeper server.'],"Now,Zookeeper doesn't have batch get feature,so i add this feature.
The method is getChildrenData,we can use getChildrenData fetch some znode's children's data.",Batch get to improve perfermance,7,,,zhiyuan.daizy,True,zhiyuan.daizy,zhiyuan.daizy
zookeeper,ZOOKEEPER-1428,2012-03-20T17:01:48.000+0000,,2013-07-08T17:27:23.000+0000,,,New Feature,Major,,,,,,,,,"['java client', 'scripts']","['The java client interface for ZooKeeper', '']","See discussion entitled 'ZOOKEEPER-1059 Was: Does the rolling-restart.sh script work?' on zookeerper-dev

HBase bin/rolling-restart.sh depends on zkcli returning non-zero exit code for non-existing znode.
Jonathan Hsieh found that rolling-restart.sh no longer works using zookeeper 3.4.x

From Patrick Hunt:

I think what we need is to have a tool that's intended for use both
programmatically and by humans, with more strict requirements about
input, output formatting and command handling, etc... Please see the
work Hartmut has been doing as part of 271 on trunk (3.5.0). Perhaps
we can augment these new classes to also support such a tool. However
it should instead be a true command line tool, rather than a shell.",Create command line tool to utilize the new classes introduced in ZOOKEEPER-271,3,,['newbie'],zhihyu@ebaysf.com,True,,zhihyu@ebaysf.com
zookeeper,ZOOKEEPER-1383,2012-02-01T00:09:13.000+0000,,2019-06-17T02:36:29.000+0000,,,New Feature,Major,,,,,,,,,['server'],['General issues with the ZooKeeper server.'],"Quotas exist for size (node count and size in bytes); it would be useful to track and support quotas on update throughput (bytes per second) as well. This can be tracked on both a node/subtree level for quota support as well as on the server level for monitoring.

In addition, the existing quotas log a warning when they are exceeded but allow the transaction to proceed (soft quotas). It would also be useful to support a corresponding set of hard quota limits that fail the transaction.
",Create update throughput quotas and add hard quota limits,3,,,shrauner,True,thawan,shrauner
zookeeper,ZOOKEEPER-1355,2012-01-09T22:16:15.000+0000,2012-11-17T14:03:36.000+0000,2013-05-02T02:30:58.000+0000,,Fixed,New Feature,Major,['3.5.0'],,,,,,,,"['c client', 'java client']","['The c client interface to ZooKeeper', 'The java client interface for ZooKeeper']","When the set of servers changes, we would like to update the server list stored by clients without restarting the clients.
Moreover, assuming that the number of clients per server is the same (in expectation) in the old configuration (as guaranteed by the current list shuffling for example), we would like to re-balance client connections across the new set of servers in a way that a) the number of clients per server is the same for all servers (in expectation) and b) there is no excessive/unnecessary client migration.

It is simple to achieve (a) without (b) - just re-shuffle the new list of servers at every client. But this would create unnecessary migration, which we'd like to avoid.

We propose a simple probabilistic migration scheme that achieves (a) and (b) - each client locally decides whether and where to migrate when the list of servers changes. The attached document describes the scheme and shows an evaluation of it in Zookeeper. We also implemented re-balancing through a consistent-hashing scheme and show a comparison. We derived the probabilistic migration rules from a simple formula that we can also provide, if someone's interested in the proof.",Add zk.updateServerList(newServerList) ,13,3,,shralex,True,shralex,shralex
zookeeper,ZOOKEEPER-1320,2011-12-06T10:43:04.000+0000,2012-03-18T06:28:41.000+0000,2012-03-18T06:28:43.000+0000,,Incomplete,New Feature,Major,,['3.3.3'],,,604800,604800,604800,,['server'],['General issues with the ZooKeeper server.'],Add the feature to zookeeper so that administrator can set the list of ips that limit which nodes can connect to the zk servers and which connected clients can operate on data.,Add the feature to zookeeper allow client limitations by ip.,,,"['client,server,limited,ipfilter']",nileader,True,nileader,nileader
zookeeper,ZOOKEEPER-1312,2011-11-30T23:46:39.000+0000,,2011-12-03T00:54:18.000+0000,,,New Feature,Major,,,,,,,,,,,"It would be extremely useful to be able to have a ""getChildrenWithStat"" method.  This method would behave exactly the same as getChildren but in addition to returning the list of all child znode names it would also return a Stat for each child.  I'm sure there are quite a few use cases for this but it could save a lot of extra reads for my application.","Add a ""getChildrenWithStat"" operation",1,,['newbie'],dlord,True,,dlord
zookeeper,ZOOKEEPER-1310,2011-11-30T12:47:56.000+0000,,2012-05-06T04:51:21.000+0000,,,New Feature,Major,,,,,,,,,['c client'],['The c client interface to ZooKeeper'],"I would like to ZooKeeper let know my watcher (which I'm giving to zookeeeper_init) about CONNECTION_LOSS, right the given watcher doesn't know that connection is lost due to what I can't do my stuff.

What you think? If so I could try to create a patch.",C Api should use state CONNECTION_LOSS,1,,,kuebk,True,,kuebk
zookeeper,ZOOKEEPER-1297,2011-11-11T12:51:38.000+0000,2012-12-19T18:17:35.000+0000,2013-12-24T00:10:41.000+0000,,Fixed,New Feature,Major,['3.5.0'],['3.3.3'],,,,,,,['java client'],['The java client interface for ZooKeeper'],In order to get a Stat object after creation one has to do another exists() call. This leaves client code vulnerable to a possible update window by another writer. All synchronous methods but the create() method allow to pass in a Stat object for population. It would be nice if the create() method would also allow this.,Add stat information to create() call,3,,['newbie'],gunnar,True,lskuff,gunnar
zookeeper,ZOOKEEPER-1290,2011-11-05T02:04:25.000+0000,,2011-11-05T06:06:57.000+0000,,,New Feature,Major,,,,,,,,,['c client'],['The c client interface to ZooKeeper'],"Our use of zookeeper requires high scalability, and the underlying data set is small and changes infrequently. A persisted cache is ideal for solving scalability. We want to treat a restart as if it were a prolonged reconnect - that is, maintain the last known zxid and watch list. We would like to expose a new zookeeper_init_with_watches api that allows the zhandle to be initialized with the watch list and last known zxid. The change would reuse the current reconnect logic.",zookeeper_init_with_watches,,,,marccelani,True,,marccelani
zookeeper,ZOOKEEPER-1289,2011-11-05T01:48:47.000+0000,,2011-11-30T23:08:39.000+0000,,,New Feature,Major,,,,,,,,,"['c client', 'java client', 'server']","['The c client interface to ZooKeeper', 'The java client interface for ZooKeeper', 'General issues with the ZooKeeper server.']","Caches built on top of zookeeper clients can become inconsistent because of lack of multi op watches. Our clients receive watch notifications for paths one at a time, and in their watch handling, invalidate the path in the cache. However, the cache now has an inconsistent view of zookeeper, since it is receiving the notifications one at a time. In general, the watch handling semantics do not conform with the idea of a multi op. If changes can be made to multiple paths atomically, all clients should be notified of that change atomically.",Multi Op Watch Events,,,,marccelani,True,,marccelani
zookeeper,ZOOKEEPER-1260,2011-10-27T17:49:26.000+0000,,2019-04-02T13:56:03.000+0000,,,New Feature,Major,"['3.6.0', '3.5.6']",,7200,7200,,,,100,['server'],['General issues with the ZooKeeper server.'],Lots of users have had questions on debugging which client changed what znode and what updates went through a znode. We should add audit logging as in Hadoop (look at Namenode Audit logging) to log which client changed what in the zookeeper servers. This could just be a log4j audit logger.,Audit logging in ZooKeeper servers.,14,6,['pull-request-available'],mahadev,True,arshad.mohammad,mahadev
zookeeper,ZOOKEEPER-1215,2011-10-06T21:19:20.000+0000,,2011-12-21T17:05:56.000+0000,,,New Feature,Major,,,,,,,,,['c client'],['The c client interface to ZooKeeper'],"Motivation:
1.  Reduce the impact of client restarts on zookeeper by implementing a persisted cache, and only fetching deltas on restart
2.  Reduce unnecessary calls to zookeeper.
3.  Improve performance of gets by caching on the client
4.  Allow for larger caches than in memory caches.

Behavior Change:
Zookeeper clients will not have the option to specify a folder path where it can cache zookeeper gets.  If they do choose to cache results, the zookeeper library will check the persisted cache before actually sending a request to zookeeper.  Watches will automatically be placed on all gets in order to invalidate the cache.  Alternatively, we can add a cache flag to the get API - thoughts?  On reconnect or restart, zookeeper clients will check the version number of each entries into its persisted cache, and will invalidate any old entries.  In checking version number, zookeeper clients will also place a watch on those files.  In regards to watches, client watch handlers will not fire until the invalidation step is completed, which may slow down client watch handling. Since setting up watches on all files is necessary on initialization, initialization will likely slow down as well.

API Change:
The zookeeper library will expose a new init interface that specifies a folder path to the cache.  A new get API will specify whether or not to use cache, and whether or not stale data is safe to return if the connection is down.

Design:
The zookeeper handler structure will now include a cache_root_path (possibly null) string to cache all gets, as well as a bool for whether or not it is okay to serve stale data.  Old API calls will default to a null path (which signifies no cache), and signify that it is not okay to serve stale data.

The cache will be located at a cache_root_path.  All files will be placed at cache_root_path/file_path.  The cache will be an incomplete copy of everything that is in zookeeper, but everything in the cache will have the same relative path from the cache_root_path that it has as a path in zookeeper.  Each file in the cache will include the Statstructure and the file contents.

zoo_get will check the zookeeper handler to determine whether or not it has a cache.  If it does, it will first go to the path to the persisted cache and append the get path.  If the file exists and it is not invalidated, the zookeeper client will read it and return its value.  If the file does not exist or is invalidated, the zookeeper library will perform the same get as is currently designed.  After getting the results, the library will place the value in the persisted cache for subsequent reads.  zoo_set will automatically invalidate the path in the cache.

If caching is requested, then on each zoo_get that goes through to zookeeper, a watch will be placed on the path. A cache watch handler will handle all watch events by invalidating the cache, and placing another watch on it.  Client watch handlers will handle the watch event after the cache watch handler.  The cache watch handler will not call zoo_get, because it is assumed that the client watch handlers will call zoo_get if they need the fresh data as soon as it is invalidated (which is why the cache watch handler must be executed first).

All updates to the cache will be done on a separate thread, but will be queued in order to maintain consistency in the cache.  In addition, all client watch handlers will not be fired until the cache watch handler completes its invalidation write in order to ensure that client calls to zoo_get in the watch event handler are done after the invalidation step.  This means that a client watch handler could be waiting on SEVERAL writes before it can be fired off, since all writes are queued.

When a new connection is made, if a zookeeper handler has a cache, then that cache will be scanned in order to find all leaf nodes.  Calls will be made to zookeeper to check if all of these nodes still exist, and if they do, what their version number is.  Any inconsistencies in version will result in the cache invalidating the out of date files.  Any files that no longer exist will be deleted from the cache.
 
If a connection fails, and a zoo_get call is made on a zookeeper handler that has a cache associated with it, and that cache tolerates stale data, then the stale data will be returned from cache - otherwise, all zoo_gets will error out as they do today.",C client persisted cache,3,1,,marccelani,True,marccelani,marccelani
zookeeper,ZOOKEEPER-1164,2011-08-29T17:59:43.000+0000,,2013-05-02T02:29:43.000+0000,,,New Feature,Major,,['3.5.0'],,,,,,,['c client'],['The c client interface to ZooKeeper'],"If ZooKeeper is going to switch to netty for connections to support encryption, then C binding library and other language bindings should be updated to support communication through netty to support encryption.",Support encryption for C binding,,,,eyang,True,,eyang
zookeeper,ZOOKEEPER-1161,2011-08-24T20:17:43.000+0000,2012-03-06T08:23:35.000+0000,2012-03-07T10:58:06.000+0000,,Fixed,New Feature,Major,['3.5.0'],,,,,,,,"['scripts', 'server']","['', 'General issues with the ZooKeeper server.']","Currently if ZK starts and doesn't see and existing dataDir it tries to create it. There should be an option to tweak this behavior. As for default, my personal opinion is to NOW allow autocreate.",Provide an option for disabling auto-creation of the data directory,2,,,rvs,True,phunt,rvs
zookeeper,ZOOKEEPER-1130,2011-07-21T12:25:19.000+0000,,2014-04-23T22:15:13.000+0000,,,New Feature,Major,['3.6.0'],['3.4.0'],,,,,,,['contrib'],[''],I have ported Patrick's zookeeper smoke test to Java so that it can be run on windows machines (since I couldn't find any way of getting the python bindings for windows).  The port provides the same functionality as the python varient as of 21st June 2011.,Java port of PHunt's zk-smoketest,,,,cgsmithe,True,cgsmithe,cgsmithe
zookeeper,ZOOKEEPER-1112,2011-06-30T22:13:29.000+0000,,2016-01-21T01:52:11.000+0000,,,New Feature,Major,,,,,,,,,,,"Hopefully this would leverage the SASL server-side support provided by ZOOKEEPER-938. It would be similar to the Java SASL client support also provided in ZOOKEEPER-938.

Java has built-in SASL support, but I'm not sure what C libraries are available for SASL and if so, are they compatible with the Apache license.",Add support for C client for SASL authentication,11,2,,ekoontz,True,klonik_t,ekoontz
zookeeper,ZOOKEEPER-1107,2011-06-23T14:48:22.000+0000,2011-09-02T20:51:29.000+0000,2011-11-23T19:22:43.000+0000,,Fixed,New Feature,Major,['3.4.0'],['3.3.3'],,,,,,,['server'],['General issues with the ZooKeeper server.'],"I like to have ZK itself manage the amount of snapshots and logs kept,  instead of relying on the PurgeTxnLog utility.
",automating log and snapshot cleaning,4,3,,junrao,True,lakshman,junrao
zookeeper,ZOOKEEPER-1098,2011-06-20T13:34:29.000+0000,2014-04-23T22:21:31.000+0000,2014-07-16T20:43:36.000+0000,,Duplicate,New Feature,Minor,['3.5.0'],,,,,,,,,,HBase is planning to make use of the native ZooKeeper libraries in order to have small session timeouts that aren't affected by GC pauses (see HBASE-1316). The current patch uses a custom maven packaging of the ZooKeeper native libraries. It would be nice if ZooKeeper published those artifacts as part of its release process.,Upload native libraries as Maven artifacts,5,,,fwiffo,True,,fwiffo
zookeeper,ZOOKEEPER-1080,2011-05-30T13:24:16.000+0000,2014-05-17T12:03:02.000+0000,2014-05-17T12:03:02.000+0000,,Duplicate,New Feature,Major,['3.5.0'],['3.3.2'],,,,,,,['contrib'],[''],"Currently Hadoop components such as NameNode and JobTracker are single point of failure.
If Namenode or JobTracker goes down, there service will not be available until they are up and running again. If there was a Standby Namenode or JobTracker available and ready to serve when Active nodes go down, we could have reduced the service down time. Hadoop already provides a Standby Namenode implementation which is not fully a ""hot"" Standby. 
The common problem to be addressed in any such Active-Standby cluster is Leader Election and Failure detection. This can be done using Zookeeper as mentioned in the Zookeeper recipes.
http://zookeeper.apache.org/doc/r3.3.3/recipes.html


+Leader Election Service (LES)+

Any Node who wants to participate in Leader Election can use this service. They should start the service with required configurations. The service will notify the nodes whether they should be started as Active or Standby mode. Also they intimate any changes in the mode at runtime. All other complexities can be handled internally by the LES.
",Provide a Leader Election framework based on Zookeeper recipe,21,6,,harivishnu,True,harivishnu,harivishnu
zookeeper,ZOOKEEPER-1045,2011-04-06T22:01:58.000+0000,2016-12-06T00:20:13.000+0000,2019-07-14T16:39:29.000+0000,,Fixed,New Feature,Critical,['3.4.10'],,,,,,,,"['quorum', 'security']","['Quorum determination for ZooKeeper', '']","ZOOKEEPER-938 addresses mutual authentication between clients and servers. This bug, on the other hand, is for authentication among quorum peers. Hopefully much of the work done on SASL integration with Zookeeper for ZOOKEEPER-938 can be used as a foundation for this enhancement.

Review board: https://reviews.apache.org/r/47354/",Support Quorum Peer mutual authentication via SASL,31,2,,ekoontz,True,rakeshr,ekoontz
zookeeper,ZOOKEEPER-1024,2011-03-19T10:20:16.000+0000,,2011-05-03T03:42:53.000+0000,,,New Feature,Major,,,,,,,,,['server'],['General issues with the ZooKeeper server.'],"let path be binary, not string. there are overhead to hold string.
the overhead is obvious when there are millions of nodes.

some time ZK can be used as a highly available meta database.
some data are binary, if converting to string, there is also obvious overhead. ",let path be binary,1,,,wjiangwen,True,,wjiangwen
zookeeper,ZOOKEEPER-1022,2011-03-19T10:13:28.000+0000,,2011-04-29T15:34:31.000+0000,,,New Feature,Major,,,,,,,,,['server'],['General issues with the ZooKeeper server.'],"let the children under a ZNode in order. and user can specify a comparator for each parent ZNode.
some time we only need get some children, not all, like getting first children.
and some application can leverage the order, like in HBase, the meta table can put into ZK. ",let the children under a ZNode in order.,3,2,,wjiangwen,True,,wjiangwen
zookeeper,ZOOKEEPER-1020,2011-03-15T14:39:48.000+0000,2011-03-17T01:01:00.000+0000,2011-11-23T19:22:33.000+0000,,Fixed,New Feature,Minor,['3.4.0'],,,,,,,,['c client'],['The c client interface to ZooKeeper'],"On occasion it might be useful to determine which host your Zookeeper client is currently connected to, be it for debugging purposes or otherwise. A possible signature for that function:

const char* zoo_get_connected_host(zhandle_t *zh, char *buffer, size_t buffer_size, unsigned short *port);

Clients could use it like below:

  char buffer[33];
  unsigned short port = 0;
  if (!zoo_get_connected_host(zh, buffer, sizeof(buffer), &port))
    return EXIT_FAILURE;

  printf(""The connected host is: %s:%d\n"", buffer, port);",Implement function in C client to determine which host you're currently connected to.,,,,tyree731,True,tyree731,tyree731
zookeeper,ZOOKEEPER-1012,2011-03-11T20:12:40.000+0000,2011-03-16T17:17:58.000+0000,2012-01-27T01:58:56.000+0000,,Fixed,New Feature,Trivial,['3.4.0'],,,,300,300,300,,['server'],['General issues with the ZooKeeper server.'],"1. Sometimes you might want to run zkServer.sh with different JVMFLAGS than for clients. Make zkServer.sh consult the SERVER_JVMFLAGS variable and, if it exists, add it to the beginning of the existing JVMFLAGS setting.

2. Sometimes you might want to run zkCli.sh with different JVMFLAGS than for servers. Make zkCli.sh consult the CLIENT_JVMFLAGS variable and, if it exists, add it to the beginning of the existing JVMFLAGS setting.",support distinct JVMFLAGS for zookeeper server in zkServer.sh and zookeeper client in zkCli.sh,,,,ekoontz,True,ekoontz,ekoontz
zookeeper,ZOOKEEPER-999,2011-02-22T01:36:14.000+0000,2011-08-29T21:52:58.000+0000,2011-11-23T19:22:05.000+0000,,Fixed,New Feature,Major,['3.4.0'],,,,,,,,['build'],['Build issues for ZooKeeper'],This goal of this ticket is to generate a set of RPM/debian package which integrate well with RPM sets created by HADOOP-6255.,Create an package integration project,2,,,eyang,True,eyang,eyang
zookeeper,ZOOKEEPER-992,2011-02-17T16:29:24.000+0000,2011-07-19T00:59:54.000+0000,2011-11-23T19:22:00.000+0000,,Fixed,New Feature,Major,['3.4.0'],,,,,,,,['c client'],['The c client interface to ZooKeeper'],"This is an extention of the work in  https://issues.apache.org/jira/browse/ZOOKEEPER-859
",MT Native Version of Windows C Client ,4,2,,fournc,True,dheerajagrawal,fournc
zookeeper,ZOOKEEPER-965,2010-12-28T00:18:46.000+0000,2011-06-30T22:54:23.000+0000,2011-11-23T19:22:26.000+0000,,Fixed,New Feature,Major,['3.4.0'],['3.3.3'],,,,,,,,,"The basic idea is to have a single method called ""multi"" that will accept a list of create, delete, update or check objects each of which has a desired version or file state in the case of create.  If all of the version and existence constraints can be satisfied, then all updates will be done atomically.

Two API styles have been suggested.  One has a list as above and the other style has a ""Transaction"" that allows builder-like methods to build a set of updates and a commit method to finalize the transaction.  This can trivially be reduced to the first kind of API so the list based API style should be considered the primitive and the builder style should be implemented as syntactic sugar.

The total size of all the data in all updates and creates in a single transaction should be limited to 1MB.

Implementation-wise this capability can be done using standard ZK internals.  The changes include:

- update to ZK clients to all the new call

- additional wire level request

- on the server, in the code that converts transactions to idempotent form, the code should be slightly extended to convert a list of operations to idempotent form.

- on the client, a down-rev server that rejects the multi-update should be detected gracefully and an informative exception should be thrown.

To facilitate shared development, I have established a github repository at https://github.com/tdunning/zookeeper  and am happy to extend committer status to anyone who agrees to donate their code back to Apache.  The final patch will be attached to this bug as normal.",Need a multi-update command to allow multiple znodes to be updated safely,14,,,tdunning,True,tdunning,tdunning
zookeeper,ZOOKEEPER-938,2010-11-23T17:25:18.000+0000,2011-08-18T22:05:54.000+0000,2014-04-28T04:03:58.000+0000,,Fixed,New Feature,Major,['3.4.0'],,,,,,,,"['java client', 'server']","['The java client interface for ZooKeeper', 'General issues with the ZooKeeper server.']","Support Kerberos authentication of clients. 

The following usage would let an admin use Kerberos authentication to assign ACLs to authenticated clients.

1. Admin logs into zookeeper (not necessarily through Kerberos however). 

2. Admin decides that a new node called '/mynode' should be owned by the user 'zkclient' and have full permissions on this.

3. Admin does: zk> create /mynode content sasl:zkclient@FOOFERS.ORG:cdrwa

4. User 'zkclient' logins to kerberos using the command line utility 'kinit'.

5. User connects to zookeeper server using a Kerberos-enabled version of zkClient (ZookeeperMain).

6. Behind the scenes, the client and server exchange authentication information. User is now authenticated as 'zkclient'.

7. User accesses /mynode with permissions 'cdrwa'.",Support Kerberos authentication of clients.,15,,,ekoontz,True,ekoontz,ekoontz
zookeeper,ZOOKEEPER-920,2010-11-08T07:53:59.000+0000,,2019-04-02T13:55:23.000+0000,,,New Feature,Minor,"['3.6.0', '3.5.6']",['3.3.1'],,,,,,,['c client'],['The c client interface to ZooKeeper'],"Zookeeper is used in applications where fault tolerance is important. Its client i/o thread send/recv heartbeats to/fro Zookeeper ensemble to stay connected. However healthy heartbeat does not always means that the application that uses Zookeeper client is in good health, it only means that ZK client thread is in good health.

This I needed something that can tagged onto Zookeeper ping that represents L7 (application) health as well.
I have modified C client source to support this in minimal way. I am new to Zookeeper, so please code review this code.  I am actually using this code in our in-house solution.

https://github.com/tru64ufs/zookeeper/commit/2196d6d5114a2fd2c0a3bc9a55f4494d47d2aece

Thank you very much.

",L7 (application layer) ping support,4,1,,tru64ufs,True,tru64ufs,tru64ufs
zookeeper,ZOOKEEPER-911,2010-10-21T15:02:23.000+0000,,2019-04-02T13:54:25.000+0000,,,New Feature,Major,"['3.6.0', '3.5.6']",,,,,,,,['java client'],['The java client interface for ZooKeeper'],"Copied from my email to the ZK dev list from 2010/05/26:

For my current code I'm using zkclient[1] and have also looked at cages[2] for 
some ZK usage examples. I observed, that there's a common pattern to wrap ZK 
operations in callables and feed them to a ""retryUntilConnected"" executor.

Now my idea is, that ZK should already come with operations in classes, e.g.:

o.a.z.operation.Create extends Operation implements callable{
  
  private path, data[], acl, createMode

  public Create( .. all kind of ctors .. )

  public call(){
    .. move code from Zookeeper.create() here
  }
}

Similiar classes should be provided for getChildren, delete, exists, getData, 
getACL, setACL and setData.

One could then feed such operations to an ZkExecutor, which has the necessary 
knowledge about the ZkConnection and can execute a command either 
synchronously or asynchronously.

One could also wrap operations in an ExceptionCatcher to ignore certain 
Exceptions or in a RetryPolicy.

This is only an idea so far, but I wanted to share my thoughts before starting 
to try it out. (BTW: You can meet me at BerlinBuzzwords.de)

[1] http://github.com/sgroschupf/zkclient
[2] http://code.google.com/p/cages/

And a reply from Patrick Hunt at my mail:

Hi Thomas, you might take a look at this JIRA
https://issues.apache.org/jira/browse/ZOOKEEPER-679

there's definitely been interest in this area, however there are some 
real challenges as well. Most users do end up wrapping the basic api 
with some code, esp the ""retry"" metaphor is a common case, so I think it 
would be valuable. At the same time getting the semantics right is hard 
(and covering all the corner cases). Perhaps you could sync up with 
Aaron/Chris, I'd personally like to see this go into contrib, but I 
understand the extra burden the patch process presents -- it may make 
more sense to rapidly iterate on something like github and then move to 
contrib once you have something less frequently changing, where the 
patch issue would be less of a problem (see 679, there's discussion on 
this there). Regardless which way you take it we'd be happy to work with 
you.",move operations from methods to individual classes,3,,,thkoch,True,thkoch,thkoch
zookeeper,ZOOKEEPER-892,2010-10-08T10:38:18.000+0000,,2014-04-21T20:24:23.000+0000,2011-05-15,,New Feature,Major,['3.6.0'],['3.4.0'],,,9676800,9676800,9676800,,['server'],['General issues with the ZooKeeper server.'],"ZooKeeper is a highly available and scalable system for distributed synchrony and is frequently used for cluster management. In its current incarnation it has issues with communication and data replication across extended geographic locations. Presently, the only way to distribute ZooKeeper across multiple data centers is to maintain a cross-colo Quorum using Observer members, leading to unnecessary consumption of bandwidth and performance impacts. As the title suggests, this work aims to to provide replication of ZooKeeper data from one site to others using a new type of ZooKeeper member called a Publisher. The broad idea is to have a complete instance of the current ZooKeeper at each geographic location in a master-slave setup. The Publisher will be a part of the Master ZooKeeper Site and will push changes to a FIFO queue and make it available to any interested client. The slave ZooKeeper runs client application called Replicator which receives and replays the changes to slave instance. Multiple slave Replicators can subscribes to the master Publisher and receive changes with guaranteed ordering. It will be asynchronous, non-intrusive, loosely-coupled and can be applied to a subset of the data. This scheme will bring about many of the benefits of database replication such as resilience to site failure and localized serving across data centers. In short, the goal is to provide remote (sub-tree) data replication with guaranteed ordering, without affecting the Master ZooKeeper performance.",Remote replication of Zookeeper data,18,3,,r_anirban,True,r_anirban,r_anirban
zookeeper,ZOOKEEPER-873,2010-09-15T15:57:06.000+0000,,2012-07-25T18:46:33.000+0000,,,New Feature,Minor,,,,,,,,,,,"Currently, the leader is elected based on the length of its history. In heterogeneous settings, other processes can be better suited to serve as a leader, e.g., the process running on the node with best links to a majority. POLE (Performance Oriented Leader Election)  will  be a leader election implementation that takes into account multiple factors when selecting the leader.
   ",Performance oriented leader election (POLE),1,1,,dioog,True,,dioog
zookeeper,ZOOKEEPER-869,2010-09-14T07:39:07.000+0000,,2010-09-21T13:44:08.000+0000,,,New Feature,Minor,,,,,,,,,,,"Currently, the leader election algorithm implemented guarantees that the leader has the maximum zxid of the ensemble. The state synchronization after the election was built based on this assumption. However, other leader elections algorithms might elect leaders with arbitrary zxid. 

To support other leader election algorithms, the state synchronization should allow the leader to have an arbitrary zxid.",Support for election of leader with arbitrary zxid,,,,dioog,True,,dioog
zookeeper,ZOOKEEPER-866,2010-09-05T00:12:55.000+0000,,2014-04-23T19:56:13.000+0000,,,New Feature,Major,['3.6.0'],,,,,,,,,,"Its been seen that some folks would like to use zookeeper for very fine grained locking. Also, in there use case they are fine with loosing all old zookeeper state if they reboot zookeeper or zookeeper goes down. The use case is more of a runtime locking wherein forgetting the state of locks is acceptable in case of a zookeeper reboot. Not logging to disk allows high throughput on and low latency on the writes to zookeeper. This would be a configuration option to set (ofcourse the default would be logging to disk).
",Adding no disk persistence option in zookeeper.,13,6,,mahadev,True,mahadev,mahadev
zookeeper,ZOOKEEPER-859,2010-08-31T18:52:49.000+0000,2011-07-14T02:49:29.000+0000,2011-11-23T19:22:38.000+0000,,Duplicate,New Feature,Major,['3.4.0'],['3.3.1'],,,,,,,['c client'],['The c client interface to ZooKeeper'],"Use windows sockets and the win32 API for implementing the c client.  This would be only useful for the ""single-threaded"" model, where the IO waiting is taken care of in the calling code.",Native Windows version of C client,1,,,bcollins,True,bcollins,bcollins
zookeeper,ZOOKEEPER-834,2010-08-06T19:58:48.000+0000,2019-03-20T15:28:03.000+0000,2019-03-20T15:28:03.000+0000,,Duplicate,New Feature,Major,,,,,,,,,"['c client', 'java client', 'server']","['The c client interface to ZooKeeper', 'The java client interface for ZooKeeper', 'General issues with the ZooKeeper server.']","Ephemeral znodes are automatically removed when the client session is closed or expires and this behavior makes them very useful when you want to publish status information from active / connected clients. 

But there is a catch. Right now ephemerals can't have children znodes and because of that clients need to serialize status information as byte strings. This serialization renders that information almost invisible to generic zookeeper clients and hard / inefficient to update. 

Most of the time the status information can be expressed as a bunch of (key, value) pairs and we could easily store that using child znodes. Any ZooKeeper client can read that info without the need to reverse the serialization process and we can also easily update it. 

I suggest that the server should allow the ephemeral znodes to have children znodes. Each child should also be an ephemeral znode owned by the same session - parent ephemeralOwner session.

Mail Archive: 
http://www.mail-archive.com/zookeeper-dev@hadoop.apache.org/msg09819.html

Another discussion about the same topic:
http://www.mail-archive.com/zookeeper-dev@hadoop.apache.org/msg08165.html
",Allow ephemeral znodes to have children created only by the owner session. ,12,3,['container_znode_type'],savu.andrei,True,rakeshr,savu.andrei
zookeeper,ZOOKEEPER-829,2010-07-29T17:25:19.000+0000,,2012-12-13T08:09:27.000+0000,,,New Feature,Major,,,,,,,,,['server'],['General issues with the ZooKeeper server.'],"For some use cases in HBase (HBASE-1316 in particular) we'd like the ability to forcible expire someone else's ZK session. Patrick and I discussed on IRC and came up with an idea of creating nodes in /zookeeper/sessions/<session id> that can be read in order to get basic stats about a session, and written in order to manipulate one. The manipulation we need in HBase is the ability to write a command like ""kill"", but others might be useful as well.",Add /zookeeper/sessions/* to allow inspection/manipulation of client sessions,12,1,,tlipcon,True,marshall,tlipcon
zookeeper,ZOOKEEPER-823,2010-07-19T21:57:03.000+0000,,2019-04-02T13:55:21.000+0000,,,New Feature,Major,"['3.6.0', '3.5.6']",,,,,,,,['java client'],['The java client interface for ZooKeeper'],This jira will port the client side connection code to use netty rather than direct nio.,update ZooKeeper java client to optionally use Netty for connections,10,2,,phunt,True,phunt,phunt
zookeeper,ZOOKEEPER-816,2010-07-16T14:55:29.000+0000,,2010-07-20T20:48:28.000+0000,,,New Feature,Minor,,,,,,,,,,,"Complex distributed systems like Zookeeper tend to fail in strange ways that are hard to diagnose. The objective is to build a tool that helps understand when and where these problems occurred based on Zookeeper's traces (i.e., logs in TRACE level). Minor changes to the server code will be needed.",Detecting and diagnosing elusive bugs and faults in Zookeeper,1,,,mpc,True,,mpc
zookeeper,ZOOKEEPER-808,2010-07-10T14:38:08.000+0000,2010-08-18T05:53:21.000+0000,2011-11-23T19:22:03.000+0000,,Fixed,New Feature,Major,['3.4.0'],,,,,,,,['contrib'],[''],"Implement a web-based administrative interface that should allow the user to perform all the tasks that can be done using the interactive shell (zkCli.sh) from a browser. It should also display cluster and individual server info extracted using the 4letter word commands. 

I'm going to build starting from the http://github.com/phunt/zookeeper_dashboard implemented by Patrick Hunt. ",Web-based Administrative Interface,,,,savu.andrei,True,savu.andrei,savu.andrei
zookeeper,ZOOKEEPER-806,2010-07-07T14:44:59.000+0000,2013-02-22T05:48:00.000+0000,2013-02-22T05:48:00.000+0000,,Later,New Feature,Major,,,,,,,,,,,"Hello, we have built a cluster management layer on top of Zookeeper here at the SNA team at LinkedIn: 

http://sna-projects.com/norbert/

We were wondering ways for collaboration as this is a very useful application of zookeeper.",Cluster management with Zookeeper - Norbert,1,,,john.wang@gmail.com,True,,john.wang@gmail.com
zookeeper,ZOOKEEPER-799,2010-06-29T21:13:36.000+0000,2010-07-14T06:41:01.000+0000,2012-09-17T13:21:12.000+0000,,Fixed,New Feature,Major,['3.4.0'],,,,,,,,['contrib'],[''],"Tools and Recipes for Monitoring ZooKeeper using Cacti, Nagios or Ganglia. ",Add tools and recipes for monitoring as a contrib,2,,,savu.andrei,True,savu.andrei,savu.andrei
zookeeper,ZOOKEEPER-781,2010-05-26T18:00:13.000+0000,,2019-04-02T13:55:43.000+0000,,,New Feature,Major,"['3.6.0', '3.5.6']",,,,,,,,"['c client', 'java client']","['The c client interface to ZooKeeper', 'The java client interface for ZooKeeper']","A connection strategy allows control over the way that ZooKeeper clients (we would implement this for both c and java apis) connect to a serving ensemble. Today we have two strategies, randomized round robin (default) and ordered round robin, both of which are hard coded into the client implementation. We would generalize this interface and allow users to create their own.

See this page for more detail: http://wiki.apache.org/hadoop/ZooKeeper/ConnectionStrategy","provide a generalized ""connection strategy"" for ZooKeeper clients",2,1,,phunt,True,creatstar,phunt
zookeeper,ZOOKEEPER-775,2010-05-18T04:21:30.000+0000,2010-08-19T21:29:46.000+0000,2011-11-23T19:22:33.000+0000,,Fixed,New Feature,Major,['3.4.0'],,,,,,,,['contrib'],[''],we have developed a large scale pub/sub system based on ZooKeeper and BookKeeper.,A large scale pub/sub system,15,,,breed,True,breed,breed
zookeeper,ZOOKEEPER-747,2010-04-22T03:17:33.000+0000,2010-05-03T21:50:41.000+0000,2012-01-30T17:25:20.000+0000,,Fixed,New Feature,Major,['3.4.0'],,,,,,,,['jute'],['Data marshalling code generator for ZooKeeper'],"The following patch adds a new language, C#, to the Jute code generation.  The code that is generated does have a dependency on a third party library, Jon Skeet's MiscUtil, which is Apache licensed.  The library is necessary because C# does not provide big endian support in the base class libraries.

As none of the existing Jute code has any unit tests, I have not added tests for this patch.",Add C# generation to Jute,5,,,ewhauser,True,ewhauser,ewhauser
zookeeper,ZOOKEEPER-744,2010-04-19T20:14:42.000+0000,2010-07-05T18:22:10.000+0000,2011-11-23T19:22:45.000+0000,,Fixed,New Feature,Major,['3.4.0'],['3.4.0'],,,,,,,['server'],['General issues with the ZooKeeper server.'],"Filing a feature request based on a zookeeper-user discussion.

Zookeeper should have a new four-letter word that returns key-value pairs appropriate for importing to a monitoring system (such as Ganglia which has a large installed base)

This command should initially export the following:

(a) Count of instances in the ensemble.
(b) Count of up-to-date instances in the ensemble.

But be designed such that in the future additional data can be added. For example, the output could define the statistic in a comment, then print a key ""space character"" value line:

""""""
# Total number of instances in the ensemble
zk_ensemble_instances_total 5
# Number of instances currently participating in the quorum.
zk_ensemble_instances_active 4
""""""

From the mailing list:

""""""
Date: Mon, 19 Apr 2010 12:10:44 -0700
From: Patrick Hunt <phunt@apache.org>
To: zookeeper-user@hadoop.apache.org
Subject: Re: Recovery issue - how to debug?

On 04/19/2010 11:55 AM, Travis Crawford wrote:
> It would be a lot easier from the operations perspective if the leader
> explicitly published some health stats:
>
> (a) Count of instances in the ensemble.
> (b) Count of up-to-date instances in the ensemble.
>
> This would greatly simplify monitoring&  alerting - when an instance
> falls behind one could configure their monitoring system to let
> someone know and take a look at the logs.

That's a great idea. Please enter a JIRA for this - a new 4 letter word 
and JMX support. It would also be a great starter project for someone 
interested in becoming more familiar with the server code.

Patrick
""""""",Add monitoring four-letter word,3,,,traviscrawford,True,savu.andrei,traviscrawford
zookeeper,ZOOKEEPER-730,2010-03-26T22:38:12.000+0000,,2014-04-25T08:24:53.000+0000,,,New Feature,Major,['3.6.0'],,,,,,,,['c client'],['The c client interface to ZooKeeper'],"ZOOKEEPER-729 talks about recursively deleting a znode in java.  Once the review is complete and frozen, equivalent functionality need to be available in C client as well. 

Tracker jira for the same. ",C cli: Add a command to recursively delete a znode ,4,,,kaykay.unique,True,,kaykay.unique
zookeeper,ZOOKEEPER-729,2010-03-26T19:30:44.000+0000,2010-04-15T07:26:48.000+0000,2011-12-28T16:13:38.000+0000,,Fixed,New Feature,Major,['3.4.0'],,,,,,,,['java client'],['The java client interface for ZooKeeper'],"Recursively delete a given znode in zookeeper, from the command-line. 

New operation ""rmr"" added to zkclient. 

$ ./zkCli.sh rmr /node ",Recursively delete a znode  - zkCli.sh rmr /node,2,,,kaykay.unique,True,kaykay.unique,kaykay.unique
zookeeper,ZOOKEEPER-723,2010-03-23T17:32:42.000+0000,2019-03-19T13:14:33.000+0000,2019-03-19T13:14:33.000+0000,,Duplicate,New Feature,Major,,,,,,,,,['server'],['General issues with the ZooKeeper server.'],"ephemeral znodes have the nice property of automatically cleaning up after themselves when the creator goes away, but since they can't have children it is hard to build subtrees that will cleanup after the clients that are using them are gone.

rather than changing the semantics of ephemeral nodes, i propose ephemeral parents: znodes that disappear when they have no more children. this cleanup would happen automatically when the last child is removed. an ephemeral parent is not tied to any particular session, so even if the creator goes away, the ephemeral parent will remain as long as there are children.

the when an ephemeral parent is created it will have an initial child, so that it doesn't get immediately removed. i think this child should be an ephemeral znode with a predefined name, ""firstChild"".",ephemeral parent znodes,15,7,['container_znode_type'],breed,True,dferro,breed
zookeeper,ZOOKEEPER-712,2010-03-18T15:39:08.000+0000,2011-02-19T04:01:22.000+0000,2011-11-23T19:22:39.000+0000,,Fixed,New Feature,Major,"['3.3.3', '3.4.0']",,,,,,,,['contrib-bookkeeper'],['Bookkeeper - reliable distributed transactional log.'],Recover the ledger fragments of a bookie once it crashes.,Bookie recovery,,,,fpj,True,erwin.tam,fpj
zookeeper,ZOOKEEPER-679,2010-02-24T18:41:14.000+0000,,2019-04-02T13:55:09.000+0000,,,New Feature,Major,"['3.6.0', '3.5.6']",,,,,,,,"['contrib', 'java client', 'tests']","['', 'The java client interface for ZooKeeper', 'For issues with test cases']","Following up on my conversations with Patrick and Mahadev (http://n2.nabble.com/Might-I-contribute-a-Node-design-for-the-Java-API-td4567695.html#a4567695).

This patch includes the implementation as well as unit tests. The first unit test gives a simple high level demo of using the node API.

The current implementation is simple and is only what I need withe current project I am working on. However, I am very open to any and all suggestions for improvement.

This is a proposal to support a simplified node (or File) like API into a Zookeeper tree, by wrapping the Zookeeper Java client. It is similar to Java's File API design.

Although, I'm trying to make it easier in a few spots. For example, deleting a Node recursively is done by default. I also lean toward resolving Exceptions ""under the hood"" when it seems appropriate. For example, if you ask a Node if it exists, and its parent doesn't even exist, you just get a false back (rather than a nasty Exception).

As for watches and ephemeral nodes, my current work does not need these things so I currently have no handling of them. But if potential users of  the ""Node a.k.a. File"" design want these things, I'd be open to supporting them as reasonable.",Offers a node design for interacting with the Java Zookeeper client.,4,,,dirtyvagabond,True,dirtyvagabond,dirtyvagabond
zookeeper,ZOOKEEPER-678,2010-02-22T20:00:39.000+0000,2010-03-09T01:07:23.000+0000,2010-04-27T16:17:05.000+0000,,Fixed,New Feature,Major,['3.3.0'],['3.3.0'],,,,,,,,,"An application which shows a tree view of the nodes currently in a zookeeper instance and allow the user to view and update the contents of the nodes as well as allowing users to add and remove nodes from the tree, similar in use to the Luke application in the Lucene project.

I have a list of other features that I want to add to this application but I wanted to gauge the response before I implemented them all.  I have found this useful when debugging my application and thought that it may be useful to others.

I was going to submit this as a patch file but I have used some icon files and one library which isn't available in the maven/ivy repositories and these don't seem to work when creating a patch file using subversion.  Because of this I have attached a zip containing this application to this issue.  If there is a better way to submit this please let me know.

The zip contains two directories, the src directory contains the source as it would be added to the contrib folder and the build folder contains a build version of the with a runnable jar.",Browser application to view and edit the contents of a zookeeper instance,3,,,cgsmithe,True,cgsmithe,cgsmithe
zookeeper,ZOOKEEPER-661,2010-01-30T18:42:39.000+0000,,2012-02-03T17:02:51.000+0000,,,New Feature,Minor,,,,,,,,,['contrib-bindings'],['Client bindings contained within src/contrib'],"Add Ruby bindings to the ZooKeeper distribution.

Ruby presents special threading difficulties for asynchronous ZK calls (aget, watchers, etc).  It looks like the simplest workaround is to patch the ZK C API.

Proposed approach will be described in comment.

Please use this ticket for discussion and suggestions.
",Add Ruby bindings,4,,,reynhout,True,,reynhout
zookeeper,ZOOKEEPER-646,2010-01-15T20:51:15.000+0000,,2011-10-26T02:07:46.000+0000,,,New Feature,Major,,,,,,,,,,,"Tracking JIRA for namespace partitioning in ZK 


From the mailing list (- courtesy: Mahadev / Flavio ) , discussion during Jan 2010 - 

""Hi, Mahadev said it all, we have been thinking about it for a while, but
>> haven't had time to work on it. I also don't think we have a jira open for
>> it; at least I couldn't find one. But, we did put together some comments:
>>
>>    http://wiki.apache.org/hadoop/ZooKeeper/PartitionedZookeeper
>>
>> One of the main issues we have observed there is that partitioning will
>> force us to change our consistency guarantees, which is far from ideal.
>> However, some users seem to be ok with it, but I'm not sure we have
>> agreement.
>>
>> In any case, please feel free to contribute or simply express your
>> interests so that we can take them into account.
>>
>> Thanks,
>> -Flavio
>>
>>
>> On Jan 15, 2010, at 12:49 AM, Mahadev Konar wrote:
>>
> >>> Hi kay,
> >>>  the namespace partitioning in zookeeper has been on a back burner for a
> >>> long time. There isnt any jira open on it. There had been some
> >>> discussions
> >>> on this but no real work. Flavio/Ben have had this on there minds for a
> >>> while but no real work/proposal is out yet.
> >>>
> >>> May I know is this something you are looking for in production?
> >>>
> >>> Thanks
> >>> mahadev
""",Namespace partitioning in ZK ,9,5,,kaykay.unique,True,,kaykay.unique
zookeeper,ZOOKEEPER-643,2010-01-14T09:18:54.000+0000,2010-01-14T17:59:53.000+0000,2010-01-14T17:59:53.000+0000,,Duplicate,New Feature,Major,,,,,,,,,,,add zkServer.bat for windows beside zkServer.sh and other shell ,please add support for windows os,1,,,quaff,True,,quaff
zookeeper,ZOOKEEPER-635,2009-12-22T08:12:31.000+0000,2010-03-09T05:14:46.000+0000,2011-07-22T23:07:49.000+0000,,Fixed,New Feature,Major,['3.3.0'],['3.2.1'],,,,,,,['server'],['General issues with the ZooKeeper server.'],"The issue in maililist is located:
http://mail-archives.apache.org/mod_mbox/hadoop-zookeeper-user/200912.mbox/%3c4ac0d28c0912210242g58230a9ds1c55361561c70d61@mail.gmail.com%3e

I have checked the server size code, seems no this option provided. This feature is useful when we have more than two network interfaces, one for Internet and others for intranet. We want to run ZooKeeper in our intranet and not be exposed  to outside world.",Server supports listening on a specified network address,2,,,stvchu,True,phunt,stvchu
zookeeper,ZOOKEEPER-616,2009-12-09T22:32:20.000+0000,,2014-04-23T22:12:46.000+0000,,,New Feature,Minor,['3.6.0'],"['3.0.0', '3.0.1', '3.1.0', '3.1.1', '3.2.0', '3.2.1']",,,,,,,"['c client', 'java client']","['The c client interface to ZooKeeper', 'The java client interface for ZooKeeper']","Given a zookeeper path and knowing it was created with the SEQUENCE flag, it would be nice to be able to get the sequence number and the name.  Currently, it is not documented how many bytes the sequence number uses in the path (Mahadev told me 10 for 3.1.1 for example), and having a function to retrieve this data would hide the actual number of bytes used and provide the useful functionality for users.",Provide a function to parse out the name and the sequence number from a zknode path,,,,aching,True,thkoch,aching
zookeeper,ZOOKEEPER-584,2009-11-19T15:16:46.000+0000,,2013-05-02T02:29:22.000+0000,,,New Feature,Major,,,,,,,,,['server'],['General issues with the ZooKeeper server.'],When Zookeeper is booted in an OSGi framework by {{ManagedService}} it would be quite handy to have {{ManagedService}} register a management interface for that ZooKeeper instance into the OSGi service registry.,ZooKeeper service instance should be registered in the OSGi registry,4,2,,maguro,True,,maguro
zookeeper,ZOOKEEPER-550,2009-10-09T22:59:26.000+0000,2009-11-12T21:27:12.000+0000,2010-03-26T17:25:02.000+0000,,Fixed,New Feature,Minor,['3.3.0'],['3.2.1'],,,,,,,['java client'],['The java client interface for ZooKeeper'],"This patch adds a recipe for creating a distributed queue with ZooKeeper similar to the WriteLock recipe and some sequential tests.  This early attempt follows the Java BlockingQueue interface, though it doesn't implement it since I don't think there's a good reason for it to be Iterable.  ",Java Queue Recipe,2,,,stevechy,True,stevechy,stevechy
zookeeper,ZOOKEEPER-546,2009-10-08T16:40:26.000+0000,2010-09-06T07:43:41.000+0000,2011-11-19T01:18:09.000+0000,,Duplicate,New Feature,Major,,,,,,,,,['server'],['General issues with the ZooKeeper server.'],"In some cases there is no need to have the ZK data persisted to disk. For example if all you are doing is group membership and leadership
election the data is totally ephemeral, storing on disk is unnecessary. We've also seen cases where any non-ephemeral data can be
easily recovered (say configuration data that's generated/read and loaded into zk) and there is less need to worry about recovery of the
data in the case of catastrophic failure (meaning _all_ replicas are lost, remember, recovery is automatic if 2n+1 servers and <= n servers
fail, even if > n fail manual recovery is still possible as long as at least 1 replica, or replica backup can be recovered)

In these cases it makes sense to have a ""diskless"" zookeeper ensemble. The result should be improved write performance
an less moving parts (no disk to fail!), simplifiying ops in cases where this can be applied.
","add ""diskless"" ensemble support",,,,phunt,True,,phunt
zookeeper,ZOOKEEPER-539,2009-10-01T20:54:19.000+0000,2009-10-05T22:38:05.000+0000,2010-03-26T17:25:01.000+0000,,Fixed,New Feature,Critical,['3.3.0'],,,,,,,,['build'],['Build issues for ZooKeeper'],"Enable eclipse project generation via ant target.
",generate eclipse project via ant target,1,,,phunt,True,phunt,phunt
zookeeper,ZOOKEEPER-520,2009-09-04T00:06:07.000+0000,,2010-01-15T22:21:35.000+0000,,,New Feature,Major,,,,,,,,,"['c client', 'java client']","['The c client interface to ZooKeeper', 'The java client interface for ZooKeeper']","Occasionally people (typically ops) has asked for the ability to start a ZK client with a hardcoded, local, non cluster based session. Meaning that you can bring up a particular client with a hardcoded/readonly view of the ZK namespace even if the zk cluster is not available. This seems useful for a few reasons:

1) unforseen problems - a client might be brought up and partial application service restored even in the face of catastrophic cluster failure

2) testing - client could be brought up with a hardcoded configuration for testing purposes. we might even be able to extend this idea over time to allow ""simulated changes"" ie - simulate other clients making changes in the namespace, perhaps simulate changes in the state of the cluster (testing state change is often hard for users of the client interface)

Seems like this shouldn't be too hard for us to add. The session could be established with a URI for a local/remote file rather than a URI of the cluster servers. The client would essentially read this file which would be a simple representation of the znode namespace.

/foo/bar ""abc""
/foo/bar2 ""def""
etc...

In the pure client readonly case this is simple. We might also want to allow writes to the namespace (essentially back this with an in memory hash) for things like group membership (so that the client continues to function).

Obv this wouldn't work in some cases, but it might work in many and would allow further options for users wrt building a relable/recoverable service on top of ZK.
",add static/readonly client resident serverless zookeeper,1,,,phunt,True,,phunt
zookeeper,ZOOKEEPER-496,2009-08-03T18:51:25.000+0000,2010-01-12T22:02:43.000+0000,2010-03-26T17:24:56.000+0000,,Fixed,New Feature,Major,['3.3.0'],,,,604800,604800,604800,,['contrib'],[''],"==========================================
zktreeutil - Zookeeper Tree Data Utility
Author: Anirban Roy
Organization: Yahoo Inc.
==========================================

zktreeutil program is intended to manage and manipulate zk-tree data quickly, effi-
ciently and with ease. The utility operates on free-form ZK-tree and hence can be used
for any cluster managed by Zookeeper. Here are the basic functionalities -

EXPORT: The whole/partial ZK-tree is exported into a XML file. This helps in
capturing a current snapshot of the data for backup/analysis. For a subtree
export, one need to specify the path to the ZK-subtree with proper option.

IMPORT: The ZK-tree can be imported from XML into ZK cluster. This helps in priming
the new ZK cluster with static configuration. The import can be non-intrusive by
making only the additions in the existing data. The import of subtree is also
possible by optionally providing the path to the ZK-subtree.

DIFF: Creates a diff between live ZK data vs data saved in XML file. Diff can ignore
some ZK-tree branches (possibly dynamic data) on reading the optional ignore flag
from XML file. Diffing on a ZK-subtree achieved by providing path to ZK-subtree with
diff command.

UPDATE: Make the incremental changes into the live ZK-tree from saved XML, essentia-
lly after running the diff.

DUMP: Dumps the ZK-tree on the standard output device reading either from live ZK
server or XML file. Like export, ZK-subtree can be dumped with optionaly
providing the path to the ZK-subtree, and till a certain depth of the (sub)tree.

The exported ZK data into XML file can be shortened by only keeping the static ZK
nodes which are required to prime a cluster. The dynamic zk nodes (created on-the-
fly) can be ignored by setting a 'ignore' attribute at the root node of the dynamic
subtree (see tests/zk_sample.xml), possibly deleting all inner ZK nodes under that.
Once ignored, the whole subtree is ignored during DIFF, UPDATE and WRITE.","zookeeper-tree utility for export, import and incremental updates",6,,,r_anirban,True,r_anirban,r_anirban
zookeeper,ZOOKEEPER-465,2009-07-13T20:24:41.000+0000,2011-02-17T21:53:48.000+0000,2011-11-23T19:22:27.000+0000,,Fixed,New Feature,Major,"['3.3.3', '3.4.0']",,,,,,,,['contrib-bookkeeper'],['Bookkeeper - reliable distributed transactional log.'],"It is currently easy to know how many entries a ledger has, but there is no easy way to know the total number of bytes in a ledger. The idea of this jira is to add a method that gives the number of bytes in a closed ledger. My current idea is to simply have the writer counting the number of bytes written and store it to ZooKeeper.  ",Ledger size in bytes,4,,,fpj,True,fpj,fpj
zookeeper,ZOOKEEPER-464,2009-07-13T20:20:34.000+0000,2010-05-13T20:24:49.000+0000,2011-11-23T19:22:04.000+0000,,Fixed,New Feature,Major,['3.4.0'],,,,,,,,['contrib-bookkeeper'],['Bookkeeper - reliable distributed transactional log.'],"An application using BookKeeper is likely to use a large number of ledgers over time. Such an application might not need all ledgers created over time and might want to delete some of these ledgers to free up some space on bookies. The idea of this jira is to implement a procedure that enables an application to garbage-collect unwanted ledgers.

To garbage-collect a ledger, we need to delete the ledger metadata on ZooKeeper, and delete the ledger data on corresponding bookies. ",Need procedure to garbage collect ledgers,,,,fpj,True,erwin.tam,fpj
zookeeper,ZOOKEEPER-423,2009-06-01T16:10:15.000+0000,,2011-04-29T16:07:12.000+0000,,,New Feature,Major,,,,,,,,,"['contrib-bindings', 'documentation', 'java client', 'server']","['Client bindings contained within src/contrib', 'Cross cutting documentation, including docs/site/wiki.', 'The java client interface for ZooKeeper', 'General issues with the ZooKeeper server.']","When building the distributed queue for my tutorial blog post, it was pointed out to me that there's a serious inefficiency here. 

Informally, the items in the queue are created as sequential nodes. For a 'dequeue' call, all items are retrieved and sorted by name by the client in order to find the name of the next item to try and take. This costs O( n ) bandwidth and O(n.log n) sorting time - per dequeue call! Clearly this doesn't scale very well. 

If the servers were able to maintain a data structure that allowed them to efficiently retrieve the children of a node in order of the zxid that created them this would make successful dequeue operations O( 1 ) at the cost of O( n ) memory on the server (to maintain, e.g. a singly-linked list as a queue). This is a win if it is generally true that clients only want the first child in creation order, rather than the whole set. 

We could expose this to the client via this API: getFirstChild(handle, path, name_buffer, watcher) which would have much the same semantics as getChildren, but only return one znode name. 

Sequential nodes would still allow the ordering of znodes to be made explicitly available to the client in one RPC should it need it. Although: since this ordering would now be available cheaply for every set of children, it's not completely clear that there would be that many use cases left for sequential nodes if this API was augmented with a getChildrenInCreationOrder call. However, that's for a different discussion. 

A halfway-house alternative with more flexibility is to add an 'order' parameter to getFirstChild and have the server compute the first child according to the requested order (creation time, update time, lexicographical order). This saves bandwidth at the expense of increased server load, although servers can be implemented to spend memory on pre-computing commonly requested orders. I am only in favour of this approach if servers maintain a data-structure for every possible order, and then the memory implications need careful consideration.

[edit - JIRA interprets ( n ) without the spaces as a thumbs-down. cute.]",Add getFirstChild API,9,4,,henryr,True,,henryr
zookeeper,ZOOKEEPER-395,2009-05-08T23:59:24.000+0000,2009-05-14T06:38:38.000+0000,2012-05-21T07:17:19.000+0000,,Fixed,New Feature,Major,['3.2.0'],['3.2.0'],,,,,,,,,"ZooKeeper doesn't have Python bindings. Having them would be useful, and would complement the extant Perl bindings. ",Python bindings,2,,,henryr,True,henryr,henryr
zookeeper,ZOOKEEPER-383,2009-04-27T20:35:44.000+0000,2009-05-28T05:01:29.000+0000,2013-05-02T02:29:24.000+0000,,Fixed,New Feature,Major,['3.2.0'],,,,,,,,['contrib-bookkeeper'],['Bookkeeper - reliable distributed transactional log.'],"While there are async versions for read and write, there is no async version for creating a ledger. This can cause applications to have to change their whole thread design. 

It should be easier and more consistent to add an async version of createLedger().",Asynchronous version of createLedger(),1,,,utkarsh,True,fpj,utkarsh
zookeeper,ZOOKEEPER-378,2009-04-20T17:36:04.000+0000,2009-04-27T18:09:37.000+0000,2009-07-08T20:24:03.000+0000,,Fixed,New Feature,Major,['3.2.0'],,,,,,,,,,"As per the conversation in ZOOKEEPER-364, I submitted a Software Grant Agreement to the ASF last week for Net::ZooKeeper by fax and surface mail; it might take a few more days for it to show up in the private grants.txt file, I believe.

I also created a Net::ZooKeeper package, version 0.33, with licenses and a NOTICE file as required by the ASF for any ASF project.  It is attached to this issue and also available at http://people.apache.org/~chrisd/projects/net_zookeeper/.",contribution of Net::ZooKeeper Perl module,,,,cdarroch,True,cdarroch,cdarroch
zookeeper,ZOOKEEPER-368,2009-04-06T20:58:06.000+0000,2009-11-18T19:08:19.000+0000,2010-03-26T17:24:54.000+0000,,Fixed,New Feature,Major,['3.3.0'],,,,,,,,['quorum'],['Quorum determination for ZooKeeper'],"Edit (Henry Robinson/henryr) 12/11/09:

This JIRA specifically concerns the implementation of non-voting peers called Observers, their documentation and their tests. 

Explicit goals are 1. not breaking any current ZK functionality, 2. enabling at least one deployment scenario involving Observers, 3. documentation describing how to use the feature and 4. tests validating the correct behaviour of 2. 

Non goals of this JIRA are 1. performance optimizations specific to Observers, 2. compatibility with every feature of ZooKeeper (in particular all leader election protocols), which are both to be addressed in future JIRAs. 

See http://wiki.apache.org/hadoop/ZooKeeper/Observers for more detail of use cases, proposed design and usage.

See http://wiki.apache.org/hadoop/ZooKeeper/Observers/ReviewGuide for a brief commentary on the current patch. 

-------------

Currently, all servers of an ensemble participate actively in reaching agreement on the order of ZooKeeper transactions. That is, all followers receive proposals, acknowledge them, and receive commit messages from the leader. A leader issues commit messages once it receives acknowledgments from a quorum of followers. For cross-colo operation, it would be useful to have a third role: observer. Using Paxos terminology, observers are similar to learners. An observer does not participate actively in the agreement step of the atomic broadcast protocol. Instead, it only commits proposals that have been accepted by some quorum of followers.

One simple solution to implement observers is to have the leader forwarding commit messages not only to followers but also to observers, and have observers applying transactions according to the order followers agreed upon. In the current implementation of the protocol, however, commit messages do not carry their corresponding transaction payload because all servers different from the leader are followers and followers receive such a payload first through a proposal message. Just forwarding commit messages as they currently are to an observer consequently is not sufficient. We have a couple of options:

1- Include the transaction payload along in commit messages to observers;
2- Send proposals to observers as well.

Number 2 is simpler to implement because it doesn't require changing the protocol implementation, but it increases traffic slightly. The performance impact due to such an increase might be insignificant, though.

For scalability purposes, we may consider having followers also forwarding commit messages to observers. With this option, observers can connect to followers, and receive messages from followers. This choice is important to avoid increasing the load on the leader with the number of observers. 

",Observers: core functionality ,8,2,,fpj,True,henryr,fpj
zookeeper,ZOOKEEPER-364,2009-04-03T17:38:03.000+0000,2014-04-23T20:34:59.000+0000,2014-04-23T20:34:59.000+0000,,Invalid,New Feature,Major,['3.5.0'],"['3.0.0', '3.0.1', '3.1.0', '3.1.1']",,,,,,,,,currently we have a shell based interface for zookeeper (which again isnt well published). we should have a well published cli based interface for zookeeper.,command line interface for zookeeper.,2,,,mahadev,True,phunt,mahadev
zookeeper,ZOOKEEPER-361,2009-04-02T21:30:15.000+0000,2009-04-14T04:58:11.000+0000,2009-07-08T20:24:02.000+0000,,Fixed,New Feature,Major,['3.2.0'],"['3.0.0', '3.0.1', '3.1.0', '3.1.1']",,,,,,,['build'],['Build issues for ZooKeeper'],we need to test the c tests as part of our hudson patch testing process.,integrate cppunit testing as part of hudson patch process.,,,,mahadev,True,gkesavan,mahadev
zookeeper,ZOOKEEPER-338,2009-03-17T22:01:50.000+0000,,2017-01-16T06:38:47.000+0000,,,New Feature,Major,,"['3.0.0', '3.0.1', '3.1.0']",,,,,,,"['c client', 'java client']","['The c client interface to ZooKeeper', 'The java client interface for ZooKeeper']","The list of host names passed to ZK init method is resolved only once. Had a corresponding DNS entry been changed, it
would not be refreshed by the ZK library,effectively preventing from proper load balancing.

",zk hosts should be resolved periodically for loadbalancing amongst zk servers.,11,7,,mahadev,True,,mahadev
zookeeper,ZOOKEEPER-321,2009-02-19T02:23:32.000+0000,2010-03-08T04:36:07.000+0000,2011-12-05T05:28:28.000+0000,,Won't Fix,New Feature,Major,,['3.2.2'],,,,,,,"['c client', 'java client', 'server']","['The c client interface to ZooKeeper', 'The java client interface for ZooKeeper', 'General issues with the ZooKeeper server.']",sometimes a lot of zookeeper clients are read only. For such clients we do not need the session tracking in zookeeper. Getting rid of session tracking for such clients will help us sclae much better.,optmize session tracking in zookeeper.,,,['session'],mahadev,True,mahadev,mahadev
zookeeper,ZOOKEEPER-301,2009-02-06T09:57:00.000+0000,2009-04-07T11:40:02.000+0000,2010-03-26T17:30:48.000+0000,,Fixed,New Feature,Minor,,,,,,,,,['contrib-bookkeeper'],['Bookkeeper - reliable distributed transactional log.'],This patch provides a class named org.apache.bookkeeper.util.ZookeeperInitialization which provides basic initialization for bookies onto zookeeper reading from a text file. An example configuration file is provided in conf/zookeeper-initialize-bookies.conf ,Util class to initialize Zookeeper with a set of user-specified bookies ,,,,lucat,True,fpj,lucat
zookeeper,ZOOKEEPER-276,2009-01-20T11:54:00.000+0000,2009-01-30T19:30:58.000+0000,2009-02-13T21:18:54.000+0000,,Fixed,New Feature,Major,['3.1.0'],,,,,,,,,,"BookKeeper is a system to reliably log streams of records. In BookKeeper, servers are ""bookies"", log streams are ""ledgers"", and each unit of a log (aka record) is a ""ledger entry"". BookKeeper is designed to be reliable; bookies, the servers that store ledgers can be byzantine, which means that some subset of the bookies can fail, corrupt data, discard data, but as long as there are enough correctly behaving servers the service as a whole behaves correctly; the meta data for BookKeeper is stored in ZooKeeper.
",Bookkeeper contribution,7,,,lucat,True,fpj,lucat
zookeeper,ZOOKEEPER-256,2008-12-13T06:58:25.000+0000,2008-12-22T23:48:23.000+0000,2009-02-13T21:18:53.000+0000,,Fixed,New Feature,Major,['3.1.0'],,,,,,,,['jmx'],['JMX Support'],"Log4j has code to support management at runtime using JMX. Need to add hooks to register the beans.
",support use of JMX to manage log4j configuration at runtime,,,,phunt,True,phunt,phunt
zookeeper,ZOOKEEPER-237,2008-11-24T21:28:56.000+0000,2009-06-24T23:00:04.000+0000,2009-07-08T20:23:56.000+0000,,Fixed,New Feature,Minor,['3.2.0'],,,,,,,,"['c client', 'java client']","['The c client interface to ZooKeeper', 'The java client interface for ZooKeeper']","It would be nice to be able to root ZooKeeper handles at specific points in the namespace, so that applications that use ZooKeeper can work in their own rooted subtree.

For example, if ops decides that application X can use the subtree /apps/X and application Y can use the subtree /apps/Y, X can to a chroot to /apps/X and then all its path references can be rooted at /apps/X. Thus when X creates the path ""/myid"", it will actually be creating the path ""/apps/X/myid"".

There are two ways we can expose this mechanism: 1) We can simply add a chroot(String path) API, or 2) we can integrate into a service identifier scheme for example zk://server1:2181,server2:2181/my/root. I like the second form personally.",Add a Chroot request,,,,breed,True,mahadev,breed
zookeeper,ZOOKEEPER-235,2008-11-24T21:15:23.000+0000,,2017-03-02T23:32:08.000+0000,,,New Feature,Minor,,,,,,,,,"['c client', 'java client', 'server']","['The c client interface to ZooKeeper', 'The java client interface for ZooKeeper', 'General issues with the ZooKeeper server.']","ZooKeeper should be able to support SSL for ZooKeeper clients. As part of the implementation we should also add an X509AuthenticationProvider so that client side certifications can be used for authentication.

The tricky part of the implementation will be integrating with the non-blocking NIO calls that we use. There are various web pages that describe how to do it.",SSL Support for clients,18,11,,breed,True,,breed
zookeeper,ZOOKEEPER-233,2008-11-24T20:56:56.000+0000,,2014-08-29T14:04:06.000+0000,,,New Feature,Trivial,['3.6.0'],,,,,,,,"['build', 'java client']","['Build issues for ZooKeeper', 'The java client interface for ZooKeeper']",Patrick request I open up this in issue in this [email thread|http://n2.nabble.com/ActiveMQ-is-now-using-ZooKeeper-td1573272.html],Create a slimmer jar for clients to reduce their disk footprint.,8,3,,chirino,True,,chirino
zookeeper,ZOOKEEPER-231,2008-11-21T21:13:13.000+0000,2009-02-03T05:26:51.000+0000,2009-02-13T21:18:50.000+0000,,Fixed,New Feature,Major,['3.1.0'],['3.0.0'],,,,,,,,,creating quota's in zookeeper so that a runaway application does not bring down the zookeeper cluster.,Quotas in zookeeper,1,,,mahadev,True,mahadev,mahadev
zookeeper,ZOOKEEPER-215,2008-11-05T00:57:53.000+0000,2009-01-31T01:04:36.000+0000,2009-02-13T21:18:49.000+0000,,Fixed,New Feature,Major,['3.1.0'],,,,,,,,['tests'],['For issues with test cases'],"Currently our system tests are lumped in with our unit tests. It would be great to have a system test environment where we could run larger scale testing. Say you have 20 hosts, and you would like to test a serving ensemble with 7 servers and 100 clients running particular operations. It should be easy to test this scenario. Additionally during the test it should be possible to simulate serving node failure, etc...

I've had a brief conversation with Ben about this and he's going to take this JIRA.
",expand system test environment,,,,phunt,True,breed,phunt
zookeeper,ZOOKEEPER-153,2008-10-01T22:42:26.000+0000,,2014-01-15T19:37:53.000+0000,,,New Feature,Minor,,,,,,,,,"['c client', 'documentation', 'java client', 'server', 'tests']","['The c client interface to ZooKeeper', 'Cross cutting documentation, including docs/site/wiki.', 'The java client interface for ZooKeeper', 'General issues with the ZooKeeper server.', 'For issues with test cases']","Subscribe Method
(note, this was moved from http://zookeeper.wiki.sourceforge.net/SubscribeMethod)

Outline of the semantics and the requirements of a yet-to-be-implemented subscribe() method.

Background

ZooKeeper uses a very light weight one-time notification method for notifying interested clients of changes to ZooKeeper data nodes (znode). Clients can set a watch on a node when they request information about a znode. The watch is atomically set and the data returned, so that any subsequent changes to the znode that affect the data returned will trigger a watch event. The watch stays in place until triggered or the client is disconnected from a ZooKeeper server. A disconnect watch event implicitly triggers all watches.

ZooKeeper users have wondered if they can set permanent watches rather than one time watches. In reality such permanent watches do not provide any extra benefit over one time watches. Specifically, no data is included in a watch event, so the client still needs to do a query operation to get the data corresponding to a change; even then, the znode can change yet again after the event is received and before the client sends the query operation. Even the number of of changes to a znode can be found using one time watches and checking the mzxid in the stat structure of the znode. And the client will still miss events that happen when the client switches ZooKeeper servers.

There are use cases that require clients to see every change to a ZooKeeper node. The most general case is when a client behaves like a state machine and each change to the znode changes the state of the client. In these cases ZooKeeper is much more like a publish/subscribe system than a distributed register. To support this case we need not only reliable permanent watches (we even get the events that happen while switching servers) but also the data that caused the change, so that the client doesn't miss data that occurs between rapid fire changes.

Semantics

The subscribe(String path) causes ZooKeeper to register a subscription for a znode. The initial value of the znode and any subsequent changes to that znode will cause a watch event with the data to be sent to the client. The client will see all changes in order. If a client switches servers, any missed events with the corresponding data will be sent to the client when the client reconnects to a server.

There are three ways to cancel a subscription:

   1. Calling unsubscribe(String path)
   2. Closing the ZooKeeper session or letting it expire
   3. Falling too far behind. If the server decides that a client is not processing the watch events fast enough, it will cancel the subscription and send a SUBSCRIPTION_CANCELLED watch event.


Requirements

There are a couple of things that make it hard to implement the subscribe() method:

   1. Servers must have complete transaction logs - Currently ZooKeeper servers just need to have their data trees and in flight transaction logs in sync. When a follower syncs to a leader, the leader can just blast down a new snapshot of its data tree; it does not need to send past transactions that the follower might have missed. However in order to send changes that might have been missed by a client, the ZooKeeper server must be able to look into the past to send missed changes.
   2. Servers must be able to send clients information about past changes - Currenly ZooKeeper servers just send clients information about the current state of the system. However, to implement subscribe clients must be able to go back into the log and send watches for past changes.


Implementation Hints

There are things that work in our favor. ZooKeeper does have a bound on the amount of time it needs to look into the past. A ZooKeeper server bounds the session expiration time. The server does not need to keep a record of transactions older than this bound.

ZooKeeper also keeps a log of transactions. As long as the log is complete enough (as all the transaction back to the longest expiration time) the server has the information it needs and it isn't hard to process.

We do not want to cause the log disk to seek while looking at past transactions. There are two complimentary approaches to handling this problems: keep a few of the transactions from the recent past in memory and log to two disks. The first log disk will be synced before letting requests proceed and the second disk will not be synced. Recovery uses the first log disk and ensures that the second log disk has the same log at recovery time. The second log disk is to look into the past. Using the two disks in this way allows synchronous logging to be fast because seeks are avoided on the disk with the synchronous log.","add api support for ""subscribe"" method",11,6,,phunt,True,,phunt
zookeeper,ZOOKEEPER-135,2008-09-04T15:58:19.000+0000,2009-01-08T22:32:54.000+0000,2009-02-13T21:18:49.000+0000,,Fixed,New Feature,Minor,['3.1.0'],,,,,,,,['build'],['Build issues for ZooKeeper'],For testing and experimentation purposes it would be nice to have everything in a self contained executable jar file that you can plop down on a machine and run.,Fat jar build target,,,,breed,True,phunt,breed
zookeeper,ZOOKEEPER-129,2008-08-29T17:11:36.000+0000,,2008-08-29T21:01:10.000+0000,,,New Feature,Trivial,,,,,,,,,,,"This is a basic server that acts as a proxy between a thrift interface and ZooKeeper. It doesn't support watches, and I'm not entirely convinced the session handling makes sense, but it works for basic operations like creating, listing and deleting nodes. The patch puts the code and (basic) build file under src/contrib/thrift; the build file requires the thrift compiler to be in the path, and the thrift and zookeeper libraries in the lib dir.",Thrift proxy,2,,,md87,True,,md87
zookeeper,ZOOKEEPER-119,2008-08-11T18:10:59.000+0000,,2010-06-14T23:43:51.000+0000,,,New Feature,Major,,,,,,,,,['server'],['General issues with the ZooKeeper server.'],"The server should not allow clients to create .zookeeper nodes in the node hierarchy. These nodes should be reserved for zk future use, some ideas:

* /.zookeeper/proc
* /.zookeeper/stats
* /.zookeeper/...

* /.../.zookeeper/... (disallow both at root as well as child nodes)
","Reserve "".zookeeper"" node for server use.",2,,,phunt,True,,phunt
zookeeper,ZOOKEEPER-104,2008-07-25T23:53:04.000+0000,2010-12-08T20:39:48.000+0000,2010-12-08T20:39:48.000+0000,,Not A Problem,New Feature,Minor,,,,,,,,,['java client'],['The java client interface for ZooKeeper'],Here is an implementation of a ZooKeeper backed Java Set. It should be generally useful.,KeptSet: a distributed data stucture backed by the children of a ZooKeeper node,1,,,anthonyu,True,anthonyu,anthonyu
zookeeper,ZOOKEEPER-79,2008-07-17T16:43:35.000+0000,2008-09-26T17:41:14.000+0000,2008-10-26T01:10:41.000+0000,,Fixed,New Feature,Major,['3.0.0'],['3.0.0'],,,,,,,['documentation'],"['Cross cutting documentation, including docs/site/wiki.']","The following discussion occurred on the zookeeper-user list. We need to formalize this recipe and document on the wiki recipes page:

---------------------from jacob ----------------
Avinash

 

The following protocol will help you fix the observed misbehavior. As Flavio points out, you cannot rely on the order of nodes in getChildren, you must use an intrinsic property of each node to determine who is the leader. The protocol devised by Runping Qi and described here will do that.

 

First of all, when you create child nodes of the node that holds the leadership bids, you must create them with the EPHEMERAL and SEQUENCE flag. ZooKeeper guarantees to give you an ephemeral node named uniquely and with a sequence number larger by at least one than any previously created node in the sequence. You provide a prefix, like ""L_"" or your own choice, and ZooKeeper creates nodes named ""L_23"", ""L_24"", etc. The sequence number starts at 0 and increases monotonously.

 

Once you've placed your leadership bid, you search backwards from the sequence number of *your* node to see if there are any preceding (in terms of the sequence number) nodes. When you find one, you place a watch on it and wait for it to disappear. When you get the watch notification, you search again, until you do not find a preceding node, then you know you're the leader. This protocol guarantees that there is at any time only one node that thinks it is the leader. But it does not disseminate information about who is the leader. If you want everyone to know who is the leader, you can have an additional Znode whose value is the name of the current leader (or some identifying information on how to contact the leader, etc.). Note that this cannot be done atomically, so by the time other nodes find out who the leader is, the leadership may already have passed on to a different node.

 

Flavio

 

Might it make sense to provide a standardized implementation of leader election in the library code in Java?

 

--Jacob

 

From: zookeeper-user-bounces@lists.sourceforge.net [mailto:zookeeper-user-bounces@lists.sourceforge.net] On Behalf Of Flavio Junqueira
Sent: Friday, July 11, 2008 1:02 AM
To: zookeeper-user@lists.sourceforge.net
Cc: zookeeper-user@hadoop.apache.org
Subject: Re: [Zookeeper-user] Leader election

 

Hi Avinash, getChildren returns a list in lexicographic order, so if you are updating the children of the election node concurrently, then you may get a different first node with different clients. If you are using the sequence flag to create nodes, then you may consider stripping the prefix of the node name and using the sufix value to determine order.

Hope it helps.

-Flavio

 

----- Original Message ----
From: Avinash Lakshman <avinash.lakshman@gmail.com>
To: zookeeper-user@lists.sourceforge.net
Sent: Friday, July 11, 2008 7:20:06 AM
Subject: [Zookeeper-user] Leader election

Hi

I am trying to elect leader among 50 nodes. There is always one odd guy who seems to think that someone else distinct from what some other nodes see as leader. Could someone please tell me what is wrong with the following code for leader election:

public void electLeader()
        {           
            ZooKeeper zk = StorageService.instance().getZooKeeperHandle();
            String path = ""/Leader"";
            try
            {
                String createPath = path + ""/L-"";                               
                LeaderElector.createLock_.lock();
                while( true )
                {
                    /* Get all znodes under the Leader znode */
                    List<String> values = zk.getChildren(path, false);
                    /*
                     * Get the first znode and if it is the
                     * pathCreated created above then the data
                     * in that znode is the leader's identity.
                    */
                    if ( leader_ == null )
                    {
                        leader_ = new AtomicReference<EndPoint>( EndPoint.fromBytes( zk.getData(path + ""/"" + values.get(0), false, null) ) );
                    }
                    else
                    {
                        leader_.set( EndPoint.fromBytes( zk.getData(path + ""/"" + values .get(0), false, null) ) );
                        /* Disseminate the state as to who the leader is. */
                        onLeaderElection();
                    }
                    logger_.debug(""Elected leader is "" + leader_ + "" @ znode "" + ( path + ""/"" + values.get(0) ) );                  
                    Collections.sort(values);
                    /* We need only the last portion of this znode */
                    String[] peices = pathCreated_.split(""/"");
                    int index = Collections.binarySearch(values, peices[peices.length - 1]);                  
                    if ( index > 0 )
                    {
                        String pathToCheck = path + ""/"" + values.get(index - 1);
                        Stat stat = zk.exists(pathToCheck, true);
                        if ( stat != null )
                        {
                            logger_.debug(""Awaiting my turn ..."");
                            condition_.await();
                            logger_.debug(""Checking to see if leader is around ..."");
                        }
                    }
                    else
                    {
                        break;
                    }
                }
            }
            catch ( InterruptedException ex )
            {
                logger_.warn(LogUtil.throwableToString(ex));
            }
            catch ( KeeperException ex )
            {
                logger_.warn(LogUtil.throwableToString(ex));
            }
            finally
            {
                LeaderElector.createLock_.unlock();
            }
        }
    }

Thanks
Avinash

 



-------------------------------------------------------------------------
Sponsored by: SourceForge.net Community Choice Awards: VOTE NOW!
Studies have shown that voting for your favorite open source project,
along with a healthy diet, reduces your potential for chronic lameness
and boredom. Vote Now at http://www.sourceforge.net/community/cca08



_______________________________________________
Zookeeper-user mailing list
Zookeeper-user@lists.sourceforge.net
https://lists.sourceforge.net/lists/listinfo/zookeeper-user
",Document jacob's leader election on the wiki recipes page,,,,phunt,True,fpj,phunt
zookeeper,ZOOKEEPER-78,2008-07-17T15:35:42.000+0000,2009-04-23T21:41:38.000+0000,2009-07-08T20:23:56.000+0000,,Fixed,New Feature,Major,['3.2.0'],['3.0.0'],,,,,,,['java client'],['The java client interface for ZooKeeper'],Here's a patch which adds a little WriteLock helper class for performing leader elections or creating exclusive locks in some directory znode. Note its an early cut; am sure we can improve it over time. The aim is to avoid folks having to use the low level ZK stuff but provide a simpler high level abstraction.,added a high level protocol/feature - for easy Leader Election or exclusive Write Lock creation,2,1,,jstrachan,True,mahadev,jstrachan
zookeeper,ZOOKEEPER-66,2008-07-04T01:51:27.000+0000,2008-07-22T17:46:24.000+0000,2008-07-22T17:46:24.000+0000,,Won't Fix,New Feature,Minor,,,,,,,,,['java client'],['The java client interface for ZooKeeper'],"I found that I was repeating this code all over the place. Here is a NullWatcher class, a Watcher that does nothing at all.","Add NullWatcher, a watcher that does nothing",,,,anthonyu,True,,anthonyu
zookeeper,ZOOKEEPER-41,2008-06-10T22:48:50.000+0000,2008-07-03T21:31:30.000+0000,2008-10-26T01:10:38.000+0000,,Fixed,New Feature,Major,['3.0.0'],,,,,,,,['scripts'],[''],"Moved from SourceForge to Apache.
http://sourceforge.net/tracker/index.php?func=detail&aid=1951806&group_id=209147&atid=1008546",Sample startup script,,,,phunt,True,mahadev,phunt
zookeeper,ZOOKEEPER-40,2008-06-10T22:47:13.000+0000,,2013-12-24T23:42:23.000+0000,,,New Feature,Major,,,,,,,,,['quorum'],['Quorum determination for ZooKeeper'],"Moved from SourceForge to Apache.
http://sourceforge.net/tracker/index.php?func=detail&aid=1962503&group_id=209147&atid=1008547",Go read-only on loss of quorum,3,,,phunt,True,,phunt
zookeeper,ZOOKEEPER-39,2008-06-10T22:43:45.000+0000,2008-07-24T21:56:34.000+0000,2008-07-25T10:42:06.000+0000,,Fixed,New Feature,Major,['3.0.0'],,,,,,,,"['c client', 'java client']","['The c client interface to ZooKeeper', 'The java client interface for ZooKeeper']","Moved from SourceForge to Apache.
http://sourceforge.net/tracker/index.php?func=detail&aid=1962501&group_id=209147&atid=1008547",Use Watcher objects rather than boolean on read operations,,,,phunt,True,akornev,phunt
zookeeper,ZOOKEEPER-38,2008-06-10T22:42:12.000+0000,2008-10-01T06:28:35.000+0000,2008-10-26T01:10:37.000+0000,,Fixed,New Feature,Major,['3.0.0'],,,,,,,,['server'],['General issues with the ZooKeeper server.'],"Moved from SourceForge to Apache.
http://sourceforge.net/tracker/index.php?func=detail&aid=1961767&group_id=209147&atid=1008547",headers (version+) in log/snap files,,,,phunt,True,mahadev,phunt
zookeeper,ZOOKEEPER-37,2008-06-10T22:41:06.000+0000,,2008-06-10T22:41:06.000+0000,,,New Feature,Major,,,,,,,,,,,"Moved from SourceForge to Apache.
http://sourceforge.net/tracker/index.php?func=detail&aid=1961764&group_id=209147&atid=1008547",WebDAV access to ZooKeeper,,,,phunt,True,phunt,phunt
zookeeper,ZOOKEEPER-36,2008-06-10T22:39:28.000+0000,2009-05-08T05:28:53.000+0000,2009-07-08T20:23:55.000+0000,,Fixed,New Feature,Major,['3.2.0'],,,,,,,,['contrib'],[''],"Moved from SourceForge to Apache.
http://sourceforge.net/tracker/index.php?func=detail&aid=1961763&group_id=209147&atid=1008547",REST access to ZooKeeper,1,,,phunt,True,phunt,phunt
zookeeper,ZOOKEEPER-35,2008-06-10T22:38:04.000+0000,,2008-06-10T22:38:04.000+0000,,,New Feature,Major,,,,,,,,,['server'],['General issues with the ZooKeeper server.'],"Moved from SourceForge to Apache.
http://sourceforge.net/tracker/index.php?func=detail&aid=1958121&group_id=209147&atid=1008547", Replay logs,,,,phunt,True,,phunt
zookeeper,ZOOKEEPER-34,2008-06-10T22:37:06.000+0000,,2014-04-23T19:52:56.000+0000,,,New Feature,Major,['3.6.0'],,,,,,,,['server'],['General issues with the ZooKeeper server.'],"Moved from SourceForge to Apache.
http://sourceforge.net/tracker/index.php?func=detail&aid=1952497&group_id=209147&atid=1008547",Optimize string deserialization,3,,,phunt,True,,phunt
zookeeper,ZOOKEEPER-33,2008-06-10T22:34:27.000+0000,2008-10-14T17:48:22.000+0000,2011-09-07T14:49:25.000+0000,,Fixed,New Feature,Major,['3.0.0'],,,,,,,,['server'],['General issues with the ZooKeeper server.'],"Moved from SourceForge to Apache.
http://sourceforge.net/tracker/index.php?func=detail&aid=1945175&group_id=209147&atid=1008547",Better ACL management,,,,phunt,True,mahadev,phunt
zookeeper,ZOOKEEPER-32,2008-06-10T22:32:50.000+0000,2008-10-01T20:51:45.000+0000,2008-10-26T01:10:37.000+0000,,Fixed,New Feature,Major,['3.0.0'],,,,,,,,['server'],['General issues with the ZooKeeper server.'],"Moved from SourceForge to Apache.
http://sourceforge.net/tracker/index.php?func=detail&aid=1945106&group_id=209147&atid=1008547",CRCs for ZooKeeper data,,,,phunt,True,mahadev,phunt
zookeeper,ZOOKEEPER-31,2008-06-10T22:17:32.000+0000,2010-12-12T03:55:27.000+0000,2010-12-12T03:55:27.000+0000,,Duplicate,New Feature,Major,,,,,,,,,,,"Moved from SourceForge to Apache.
http://sourceforge.net/tracker/index.php?func=detail&aid=1941112&group_id=209147&atid=1008547",Need deleteRecursively method,,,,phunt,True,,phunt
zookeeper,ZOOKEEPER-30,2008-06-10T22:15:16.000+0000,,2013-04-11T21:36:24.000+0000,,,New Feature,Major,,,,,,,,,['quorum'],['Quorum determination for ZooKeeper'],"Moved from SourceForge to Apache.
http://sourceforge.net/tracker/index.php?func=detail&aid=1938788&group_id=209147&atid=1008547",Hooks for atomic broadcast protocol,6,1,,phunt,True,mahadev,phunt
zookeeper,ZOOKEEPER-29,2008-06-10T22:12:38.000+0000,2009-04-24T18:28:16.000+0000,2009-07-08T20:23:55.000+0000,,Fixed,New Feature,Major,['3.2.0'],,,,,,,,['server'],['General issues with the ZooKeeper server.'],"Moved from SourceForge to Apache.
http://sourceforge.net/tracker/index.php?func=detail&aid=1938782&group_id=209147&atid=1008547",Flexible quorums,,,,phunt,True,fpj,phunt
zookeeper,ZOOKEEPER-28,2008-06-10T22:09:34.000+0000,,2010-11-29T08:57:13.000+0000,,,New Feature,Major,,,,,,,,,"['c client', 'java client', 'server']","['The c client interface to ZooKeeper', 'The java client interface for ZooKeeper', 'General issues with the ZooKeeper server.']","Moved from SourceForge to Apache.
http://sourceforge.net/tracker/index.php?func=detail&aid=1937084&group_id=209147&atid=1008547",Incompatible client and server list detection,,,,phunt,True,,phunt
zookeeper,ZOOKEEPER-27,2008-06-10T21:58:07.000+0000,2008-10-01T20:53:08.000+0000,2017-02-06T20:49:36.000+0000,,Fixed,New Feature,Major,['3.6.0'],,,,,,,,"['c client', 'java client', 'server']","['The c client interface to ZooKeeper', 'The java client interface for ZooKeeper', 'General issues with the ZooKeeper server.']","Moved from SourceForge to Apache.
http://sourceforge.net/tracker/index.php?func=detail&aid=1937075&group_id=209147&atid=1008547

here is the text from sourceforge:

There should be a persistent unique identifier for an instance of ZooKeeper. Currently, if you bring a cluster down without stopping clients and reinitialize the servers, the servers will start logging client zxid errors because the clients have seen a later transaction than the server has. In reality the clients should detect that they are now talking to a new instance of the database and close the session.

A similar problem occurs when a server fails in a cluster of three machines, and the other two machines are reinitialized and restarted. If the failed machine starts up again, there is a chance that the old machine may get elected leader (since it will have the highest zxid) and overwrite new data.

A unique random id should probably get generated when a new cluster comes up. (It is easy to detect since the zxid will be zero.) Leader Election and the Leader should validate that the peers have the same database id. Clients should also validate that they are talking to servers with the same database id during a session.",Unique DB identifiers for servers and clients,4,,,phunt,True,,phunt
zookeeper,ZOOKEEPER-26,2008-06-10T21:34:32.000+0000,2008-07-24T20:14:05.000+0000,2008-08-11T18:30:43.000+0000,,Won't Fix,New Feature,Major,['3.0.0'],,,,,,,,['server'],['General issues with the ZooKeeper server.'],"Moved from SourceForge to Apache.
http://sourceforge.net/tracker/index.php?func=detail&aid=1894138&group_id=209147&atid=1008547",monitoring and management via JMX,,,,phunt,True,akornev,phunt
zookeeper,ZOOKEEPER-25,2008-06-10T21:32:30.000+0000,2008-08-13T17:59:36.000+0000,2008-10-26T01:10:37.000+0000,,Fixed,New Feature,Major,['3.0.0'],,,,,,,,,,"Moved from SourceForge to Apache.
http://sourceforge.net/tracker/index.php?func=detail&aid=1873981&group_id=209147&atid=1008547",FUSE for ZooKeeper,1,,,phunt,True,phunt,phunt
zookeeper,ZOOKEEPER-24,2008-06-10T21:30:20.000+0000,,2008-06-10T21:30:20.000+0000,,,New Feature,Major,,,,,,,,,['server'],['General issues with the ZooKeeper server.'],"Moved from SourceForge to Apache.
http://sourceforge.net/tracker/index.php?func=detail&aid=1848004&group_id=209147&atid=1008547",Do Application based outstanding request throttling,,,,phunt,True,breed,phunt
zookeeper,ZOOKEEPER-23,2008-06-10T21:29:10.000+0000,2008-10-22T01:22:32.000+0000,2013-05-02T02:29:16.000+0000,,Fixed,New Feature,Major,['3.0.0'],,,,,,,,"['c client', 'java client']","['The c client interface to ZooKeeper', 'The java client interface for ZooKeeper']","Moved from SourceForge to Apache.
http://sourceforge.net/tracker/index.php?func=detail&aid=1831413&group_id=209147&atid=1008547",Auto reset of watches on reconnect,,,,phunt,True,breed,phunt
zookeeper,ZOOKEEPER-22,2008-06-10T21:27:49.000+0000,,2016-03-08T19:59:25.000+0000,,,New Feature,Major,['3.6.0'],,,,,,,,"['c client', 'java client']","['The c client interface to ZooKeeper', 'The java client interface for ZooKeeper']","Moved from SourceForge to Apache.
http://sourceforge.net/tracker/index.php?func=detail&aid=1831412&group_id=209147&atid=1008547

When a connection to a ZooKeeper server fails, all of the pending requests
will return an error. In reality the requests should be resubmitted when
the client reestablishes a connection to ZooKeeper.

For read requests, it's no big deal to just reissue the request. For update
requests, the ZooKeeper must be able to detect if the request has been
processed and, if so, return the result of the previous execution;
otherwise, it should process the request.",Automatic request retries on connect failover,20,2,,phunt,True,mahadev,phunt
zookeeper,ZOOKEEPER-5,2008-06-09T23:43:48.000+0000,2008-10-17T00:24:34.000+0000,2011-03-30T19:28:06.000+0000,,Fixed,New Feature,Major,['3.0.0'],,,,,,,,['server'],['General issues with the ZooKeeper server.'],We need an upgrade feature in zookeeper where we can upgrade the old databases to a new one.,Upgrade Feature in Zookeeper server.,,,,mahadev,True,mahadev,mahadev
